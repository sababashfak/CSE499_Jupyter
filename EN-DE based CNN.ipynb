{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb078c6e-7496-4560-a667-a16c6c3b8b8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_6732\\2858651360.py:13: The name tf.logging.set_verbosity is deprecated. Please use tf.compat.v1.logging.set_verbosity instead.\n",
      "\n",
      "Warning: Missing input files for 339 IDs\n",
      "Found 1161 valid input-output pairs.\n",
      "\n",
      "Data verification:\n",
      "Sample 1:\n",
      "Even range: 0.00-1.00\n",
      "Odd range: 0.00-1.00\n",
      "Target unique values: [0. 1.]\n",
      "\n",
      "Sample 2:\n",
      "Even range: 0.00-1.00\n",
      "Odd range: 0.00-0.99\n",
      "Target unique values: [0. 1.]\n",
      "\n",
      "Sample 3:\n",
      "Even range: 0.00-1.00\n",
      "Odd range: 0.00-1.00\n",
      "Target unique values: [0. 1.]\n",
      "\n",
      "\n",
      "Data verification:\n",
      "Sample 1:\n",
      "Even range: 0.00-1.00\n",
      "Odd range: 0.00-1.00\n",
      "Target unique values: [0. 1.]\n",
      "\n",
      "Sample 2:\n",
      "Even range: 0.00-1.00\n",
      "Odd range: 0.00-1.00\n",
      "Target unique values: [0. 1.]\n",
      "\n",
      "Sample 3:\n",
      "Even range: 0.00-1.00\n",
      "Odd range: 0.00-1.00\n",
      "Target unique values: [0. 1.]\n",
      "\n",
      "Epoch 1/100\n",
      "\u001b[1m232/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 33ms/step - accuracy: 0.4962 - binary_io_u: 0.4320 - loss: 0.7446 - val_accuracy: 0.4999 - val_binary_io_u: 0.4999 - val_loss: 0.7109 - learning_rate: 1.0000e-04\n",
      "Epoch 2/100\n",
      "\u001b[1m232/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 28ms/step - accuracy: 0.4926 - binary_io_u: 0.4925 - loss: 0.6936 - val_accuracy: 0.4999 - val_binary_io_u: 0.4999 - val_loss: 0.6872 - learning_rate: 1.0000e-04\n",
      "Epoch 3/100\n",
      "\u001b[1m232/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 23ms/step - accuracy: 0.4961 - binary_io_u: 0.4961 - loss: 0.6877 - val_accuracy: 0.4999 - val_binary_io_u: 0.4999 - val_loss: 0.6842 - learning_rate: 1.0000e-04\n",
      "Epoch 4/100\n",
      "\u001b[1m232/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 30ms/step - accuracy: 0.4954 - binary_io_u: 0.4953 - loss: 0.6879 - val_accuracy: 0.5001 - val_binary_io_u: 0.4999 - val_loss: 0.6838 - learning_rate: 1.0000e-04\n",
      "Epoch 5/100\n",
      "\u001b[1m232/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 28ms/step - accuracy: 0.4947 - binary_io_u: 0.4947 - loss: 0.6869 - val_accuracy: 0.5001 - val_binary_io_u: 0.5000 - val_loss: 0.6837 - learning_rate: 1.0000e-04\n",
      "Epoch 6/100\n",
      "\u001b[1m232/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 21ms/step - accuracy: 0.4944 - binary_io_u: 0.4943 - loss: 0.6868 - val_accuracy: 0.4997 - val_binary_io_u: 0.4997 - val_loss: 0.6835 - learning_rate: 1.0000e-04\n",
      "Epoch 7/100\n",
      "\u001b[1m232/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 30ms/step - accuracy: 0.4925 - binary_io_u: 0.4924 - loss: 0.6863 - val_accuracy: 0.4996 - val_binary_io_u: 0.4992 - val_loss: 0.6841 - learning_rate: 5.0000e-05\n",
      "Epoch 8/100\n",
      "\u001b[1m232/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 27ms/step - accuracy: 0.4915 - binary_io_u: 0.4914 - loss: 0.6891 - val_accuracy: 0.5003 - val_binary_io_u: 0.4995 - val_loss: 0.6833 - learning_rate: 5.0000e-05\n",
      "Epoch 9/100\n",
      "\u001b[1m232/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 28ms/step - accuracy: 0.4880 - binary_io_u: 0.4879 - loss: 0.6890 - val_accuracy: 0.4997 - val_binary_io_u: 0.4992 - val_loss: 0.6828 - learning_rate: 5.0000e-05\n",
      "Epoch 10/100\n",
      "\u001b[1m232/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 28ms/step - accuracy: 0.4949 - binary_io_u: 0.4948 - loss: 0.6849 - val_accuracy: 0.4997 - val_binary_io_u: 0.4988 - val_loss: 0.6834 - learning_rate: 5.0000e-05\n",
      "Epoch 11/100\n",
      "\u001b[1m232/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 29ms/step - accuracy: 0.4893 - binary_io_u: 0.4890 - loss: 0.6892 - val_accuracy: 0.5006 - val_binary_io_u: 0.4997 - val_loss: 0.6840 - learning_rate: 5.0000e-05\n",
      "Epoch 12/100\n",
      "\u001b[1m232/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 27ms/step - accuracy: 0.4937 - binary_io_u: 0.4934 - loss: 0.6852 - val_accuracy: 0.5003 - val_binary_io_u: 0.4996 - val_loss: 0.6830 - learning_rate: 2.5000e-05\n",
      "Epoch 13/100\n",
      "\u001b[1m232/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 30ms/step - accuracy: 0.4955 - binary_io_u: 0.4950 - loss: 0.6823 - val_accuracy: 0.5015 - val_binary_io_u: 0.4997 - val_loss: 0.6834 - learning_rate: 2.5000e-05\n",
      "Epoch 14/100\n",
      "\u001b[1m232/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 28ms/step - accuracy: 0.4918 - binary_io_u: 0.4912 - loss: 0.6855 - val_accuracy: 0.5013 - val_binary_io_u: 0.4997 - val_loss: 0.6829 - learning_rate: 2.5000e-05\n",
      "Epoch 15/100\n",
      "\u001b[1m232/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 29ms/step - accuracy: 0.4960 - binary_io_u: 0.4954 - loss: 0.6839 - val_accuracy: 0.5023 - val_binary_io_u: 0.4997 - val_loss: 0.6836 - learning_rate: 2.5000e-05\n",
      "Epoch 16/100\n",
      "\u001b[1m232/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 27ms/step - accuracy: 0.5007 - binary_io_u: 0.5001 - loss: 0.6801 - val_accuracy: 0.5009 - val_binary_io_u: 0.4995 - val_loss: 0.6832 - learning_rate: 2.5000e-05\n",
      "Epoch 17/100\n",
      "\u001b[1m232/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 22ms/step - accuracy: 0.4945 - binary_io_u: 0.4937 - loss: 0.6833 - val_accuracy: 0.5015 - val_binary_io_u: 0.4993 - val_loss: 0.6833 - learning_rate: 1.2500e-05\n",
      "Epoch 18/100\n",
      "\u001b[1m232/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 28ms/step - accuracy: 0.4893 - binary_io_u: 0.4882 - loss: 0.6868 - val_accuracy: 0.5016 - val_binary_io_u: 0.4998 - val_loss: 0.6833 - learning_rate: 1.2500e-05\n",
      "Epoch 19/100\n",
      "\u001b[1m232/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 25ms/step - accuracy: 0.4976 - binary_io_u: 0.4969 - loss: 0.6824 - val_accuracy: 0.5009 - val_binary_io_u: 0.4991 - val_loss: 0.6832 - learning_rate: 1.2500e-05\n",
      "Epoch 20/100\n",
      "\u001b[1m232/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 30ms/step - accuracy: 0.4959 - binary_io_u: 0.4953 - loss: 0.6827 - val_accuracy: 0.5011 - val_binary_io_u: 0.4996 - val_loss: 0.6831 - learning_rate: 1.2500e-05\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model...\n",
      "\u001b[1m59/59\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.5010 - binary_io_u: 0.5008 - loss: 0.6825\n",
      "Validation Loss: 0.6837, Accuracy: 0.5001, IoU: 0.5000\n",
      "Visualizing predictions...\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 489ms/step\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1800x1500 with 12 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = ''  # Force CPU\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, optimizers\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator as KerasImageDataGenerator\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "\n",
    "# Configuration\n",
    "base_dir = \"F:/Education/NSU/CSE/CSE499/Implementation/Image Data\"\n",
    "input_even_dir = os.path.join(base_dir, \"gray_image\", \"even_images\")\n",
    "input_odd_dir = os.path.join(base_dir, \"gray_image\", \"odd_images\")\n",
    "output_dir = os.path.join(base_dir, \"matrix\")\n",
    "input_shape = (32, 32, 1)\n",
    "output_shape = (50, 50, 1)\n",
    "batch_size = 4\n",
    "learning_rate = 1e-4  # Reduced learning rate\n",
    "epochs = 100\n",
    "block_size = 10\n",
    "\n",
    "# Improved Custom Layer\n",
    "class BlockUniformityLayer(layers.Layer):\n",
    "    def __init__(self, block_size=10, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.block_size = block_size\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = tf.reshape(inputs, [-1, 5, self.block_size, 5, self.block_size, 1])\n",
    "        x = tf.reduce_mean(x, axis=[2, 4])  # -> (batch_size, 5, 5, 1)\n",
    "        x = tf.repeat(x, self.block_size, axis=1)  # -> (batch_size, 50, 5, 1)\n",
    "        x = tf.repeat(x, self.block_size, axis=2)  # -> (batch_size, 50, 50, 1)\n",
    "        return x\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], 50, 50, 1)\n",
    "\n",
    "# Enhanced Loss Functions\n",
    "def dice_loss(y_true, y_pred, smooth=1e-6):\n",
    "    intersection = tf.reduce_sum(y_true * y_pred)\n",
    "    union = tf.reduce_sum(y_true) + tf.reduce_sum(y_pred)\n",
    "    return 1 - (2. * intersection + smooth) / (union + smooth)\n",
    "\n",
    "def weighted_binary_crossentropy(y_true, y_pred):\n",
    "    bce = tf.keras.losses.binary_crossentropy(y_true, y_pred)\n",
    "    block_weight = tf.reduce_mean(tf.reshape(y_true, [-1, 5, block_size, 5, block_size, 1]), axis=[2, 4])\n",
    "    block_weight = tf.where(block_weight > 0.5, 2.0, 1.0)\n",
    "    block_weight = tf.repeat(tf.repeat(block_weight, block_size, axis=1), block_size, axis=2)\n",
    "    block_weight = tf.squeeze(block_weight)\n",
    "    weighted_bce = bce * block_weight\n",
    "    return tf.reduce_mean(weighted_bce)\n",
    "\n",
    "def combined_loss(y_true, y_pred):\n",
    "    return 0.5*weighted_binary_crossentropy(y_true, y_pred) + 0.5*dice_loss(y_true, y_pred)\n",
    "\n",
    "# Improved Model Architecture\n",
    "def build_improved_model(input_shape, output_shape):\n",
    "    # Input branches\n",
    "    input_even = layers.Input(shape=input_shape, name=\"even_input\")\n",
    "    input_odd = layers.Input(shape=input_shape, name=\"odd_input\")\n",
    "    \n",
    "    # Shared encoder\n",
    "    def create_encoder(input_layer):\n",
    "        x = layers.Conv2D(32, 3, padding=\"same\", activation=\"relu\")(input_layer)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.MaxPooling2D()(x)\n",
    "        x = layers.Conv2D(64, 3, padding=\"same\", activation=\"relu\")(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.MaxPooling2D()(x)\n",
    "        return x\n",
    "    \n",
    "    # Process both inputs\n",
    "    even_features = create_encoder(input_even)\n",
    "    odd_features = create_encoder(input_odd)\n",
    "    \n",
    "    # Combine features\n",
    "    x = layers.Concatenate()([even_features, odd_features])\n",
    "    \n",
    "    # Decoder\n",
    "    x = layers.Conv2DTranspose(64, 3, strides=2, padding=\"same\", activation=\"relu\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Conv2DTranspose(32, 3, strides=2, padding=\"same\", activation=\"relu\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Conv2D(1, 3, padding=\"same\", activation=\"sigmoid\")(x)\n",
    "    x = layers.Resizing(output_shape[0], output_shape[1])(x)\n",
    "    output = BlockUniformityLayer(block_size=10)(x)\n",
    "    \n",
    "    return models.Model(inputs=[input_even, input_odd], outputs=output)\n",
    "\n",
    "# Enhanced Data Generator\n",
    "class ImagePairGenerator(Sequence):\n",
    "    def __init__(self, even_dir, odd_dir, out_dir, batch_size, input_shape, output_shape, pairs, augment=False):\n",
    "        self.even_dir = even_dir\n",
    "        self.odd_dir = odd_dir\n",
    "        self.out_dir = out_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.input_shape = input_shape\n",
    "        self.output_shape = output_shape\n",
    "        self.pairs = pairs\n",
    "        self.augment = augment\n",
    "        self.augmentation = KerasImageDataGenerator(\n",
    "            rotation_range=15,\n",
    "            width_shift_range=0.1,\n",
    "            height_shift_range=0.1,\n",
    "            zoom_range=0.1,\n",
    "            horizontal_flip=True,\n",
    "            fill_mode='constant',\n",
    "            cval=0.0\n",
    "        ) if augment else None\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.pairs) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch = self.pairs[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        even_batch = np.zeros((len(batch), *self.input_shape))\n",
    "        odd_batch = np.zeros((len(batch), *self.input_shape))\n",
    "        out_batch = np.zeros((len(batch), *self.output_shape))\n",
    "\n",
    "        for i, (e_path, o_path, m_path) in enumerate(batch):\n",
    "            try:\n",
    "                # Load and preprocess images\n",
    "                even = np.expand_dims(np.array(Image.open(e_path).convert('L').resize(self.input_shape[:2])) / 255.0, -1)\n",
    "                odd = np.expand_dims(np.array(Image.open(o_path).convert('L').resize(self.input_shape[:2])) / 255.0, -1)\n",
    "                \n",
    "                # Process target matrix\n",
    "                matrix_img = np.array(Image.open(m_path).convert('L').resize(self.output_shape[:2]))\n",
    "                matrix_bin = (matrix_img > 128).astype(np.float32)\n",
    "                matrix_bin = matrix_bin.reshape(5, block_size, 5, block_size).mean(axis=(1, 3))\n",
    "                matrix_bin = np.repeat(np.repeat((matrix_bin > 0.5).astype(np.float32), block_size, axis=0), block_size, axis=1)\n",
    "                matrix_bin = np.expand_dims(matrix_bin, -1)\n",
    "\n",
    "                if self.augment:\n",
    "                    seed = np.random.randint(1e6)\n",
    "                    even = self.augmentation.random_transform(even, seed=seed)\n",
    "                    odd = self.augmentation.random_transform(odd, seed=seed)\n",
    "\n",
    "                even_batch[i] = even\n",
    "                odd_batch[i] = odd\n",
    "                out_batch[i] = matrix_bin\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing pair {e_path}, {o_path}, {m_path}: {str(e)}\")\n",
    "                continue\n",
    "\n",
    "        return (even_batch, odd_batch), out_batch\n",
    "\n",
    "# Data preparation and verification\n",
    "def get_valid_pairs():\n",
    "    pairs = []\n",
    "    missing_ids = []\n",
    "    for f in os.listdir(output_dir):\n",
    "        if not f.endswith('.png'):\n",
    "            continue\n",
    "        base = f[:-4]\n",
    "        even_path = os.path.join(input_even_dir, f\"{base}_even.png\")\n",
    "        odd_path = os.path.join(input_odd_dir, f\"{base}_odd.png\")\n",
    "        out_path = os.path.join(output_dir, f)\n",
    "        if os.path.exists(even_path) and os.path.exists(odd_path):\n",
    "            pairs.append((even_path, odd_path, out_path))\n",
    "        else:\n",
    "            missing_ids.append(base)\n",
    "    if missing_ids:\n",
    "        print(f\"Warning: Missing input files for {len(missing_ids)} IDs\")\n",
    "    return pairs\n",
    "\n",
    "# Enhanced visualization\n",
    "def visualize_predictions(model, generator, num_samples=3):\n",
    "    (even_inputs, odd_inputs), true_outputs = generator[0]\n",
    "    predictions = model.predict([even_inputs, odd_inputs])\n",
    "    \n",
    "    plt.figure(figsize=(18, 5 * num_samples))\n",
    "    for i in range(min(num_samples, len(even_inputs))):\n",
    "        # Show input pairs\n",
    "        plt.subplot(num_samples, 4, 1 + i*4)\n",
    "        plt.imshow(np.hstack([even_inputs[i].squeeze(), odd_inputs[i].squeeze()]), cmap='gray')\n",
    "        plt.title(f'Input Pair {i+1}')\n",
    "        plt.axis('off')\n",
    "        \n",
    "        # Show ground truth\n",
    "        plt.subplot(num_samples, 4, 2 + i*4)\n",
    "        plt.imshow(true_outputs[i].squeeze(), cmap='gray')\n",
    "        plt.title(f'Ground Truth {i+1}')\n",
    "        plt.axis('off')\n",
    "        \n",
    "        # Show raw prediction\n",
    "        plt.subplot(num_samples, 4, 3 + i*4)\n",
    "        plt.imshow(predictions[i].squeeze(), cmap='gray')\n",
    "        plt.title(f'Raw Prediction {i+1}')\n",
    "        plt.axis('off')\n",
    "        \n",
    "        # Show thresholded prediction\n",
    "        plt.subplot(num_samples, 4, 4 + i*4)\n",
    "        plt.imshow((predictions[i] > 0.5).astype(float).squeeze(), cmap='gray')\n",
    "        plt.title(f'Thresholded {i+1}')\n",
    "        plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Data verification function\n",
    "def verify_data(generator):\n",
    "    print(\"\\nData verification:\")\n",
    "    for i in range(3):\n",
    "        (even, odd), target = generator[i]\n",
    "        print(f\"Sample {i+1}:\")\n",
    "        print(f\"Even range: {even.min():.2f}-{even.max():.2f}\")\n",
    "        print(f\"Odd range: {odd.min():.2f}-{odd.max():.2f}\")\n",
    "        print(f\"Target unique values: {np.unique(target)}\")\n",
    "        print()\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Prepare data\n",
    "    pairs = get_valid_pairs()\n",
    "    print(f\"Found {len(pairs)} valid input-output pairs.\")\n",
    "    np.random.shuffle(pairs)\n",
    "    split = int(0.8 * len(pairs))\n",
    "    train_pairs = pairs[:split]\n",
    "    val_pairs = pairs[split:]\n",
    "\n",
    "    train_gen = ImagePairGenerator(input_even_dir, input_odd_dir, output_dir, batch_size, input_shape, output_shape, train_pairs, augment=True)\n",
    "    val_gen = ImagePairGenerator(input_even_dir, input_odd_dir, output_dir, batch_size, input_shape, output_shape, val_pairs)\n",
    "\n",
    "    # Verify data\n",
    "    verify_data(train_gen)\n",
    "    verify_data(val_gen)\n",
    "\n",
    "    # Build and compile model\n",
    "    model = build_improved_model(input_shape, output_shape)\n",
    "    model.compile(\n",
    "        optimizer=optimizers.Adam(learning_rate),\n",
    "        loss=combined_loss,\n",
    "        metrics=['accuracy', tf.keras.metrics.BinaryIoU(target_class_ids=[1])]\n",
    "    )\n",
    "\n",
    "    # Callbacks\n",
    "    callbacks = [\n",
    "        tf.keras.callbacks.ModelCheckpoint('best_model.keras', \n",
    "                                         monitor='val_binary_io_u',\n",
    "                                         mode='max',\n",
    "                                         save_best_only=True),\n",
    "        tf.keras.callbacks.EarlyStopping(monitor='val_binary_io_u',\n",
    "                                       patience=15,\n",
    "                                       mode='max',\n",
    "                                       restore_best_weights=True),\n",
    "        tf.keras.callbacks.ReduceLROnPlateau(monitor='val_binary_io_u',\n",
    "                                           factor=0.5,\n",
    "                                           patience=5,\n",
    "                                           mode='max')\n",
    "    ]\n",
    "\n",
    "    # Train model\n",
    "    history = model.fit(\n",
    "        train_gen,\n",
    "        validation_data=val_gen,\n",
    "        epochs=epochs,\n",
    "        callbacks=callbacks\n",
    "    )\n",
    "\n",
    "    # Save model\n",
    "    model.save(\"image_translation_model_final.keras\")\n",
    "\n",
    "    # Plot training history\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['loss'], label='Train Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Val Loss')\n",
    "    plt.title('Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['binary_io_u'], label='Train IoU')\n",
    "    plt.plot(history.history['val_binary_io_u'], label='Val IoU')\n",
    "    plt.title('IoU Metric')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Evaluate and visualize\n",
    "    print(\"Evaluating model...\")\n",
    "    loss, accuracy, iou = model.evaluate(val_gen)\n",
    "    print(f\"Validation Loss: {loss:.4f}, Accuracy: {accuracy:.4f}, IoU: {iou:.4f}\")\n",
    "\n",
    "    print(\"Visualizing predictions...\")\n",
    "    visualize_predictions(model, val_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848e3a28-f10c-4261-87e6-b4298ff531c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Missing input files for 339 IDs: 1000, 143, 155, 355, 396, 401, 402, 403, 404, 406, ...\n",
      "Found 1161 valid input-output pairs.\n",
      "Inspecting training data samples...\n",
      "Even input shape: (4, 32, 32, 1)\n",
      "Odd input shape: (4, 32, 32, 1)\n",
      "Output shape: (4, 50, 50, 1)\n",
      "Unique values in ground truth: [0. 1.]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x1500 with 9 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Epoch 1/200\n",
      "Batch 1, Loss Value: 0.9508\n",
      "Batch 1, Gradient Norm: 0.0424\n",
      "\u001b[1m  1/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:10:01\u001b[0m 18s/step - accuracy: 0.4100 - binary_io_u_5: 0.4200 - loss: 0.9413Batch 2, Loss Value: 0.9470\n",
      "Batch 2, Gradient Norm: 0.9881\n",
      "\u001b[1m  2/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:51\u001b[0m 486ms/step - accuracy: 0.4350 - binary_io_u_5: 0.4300 - loss: 0.9420 Batch 3, Loss Value: 0.9415\n",
      "Batch 3, Gradient Norm: 0.6809\n",
      "\u001b[1m  3/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:51\u001b[0m 488ms/step - accuracy: 0.4411 - binary_io_u_5: 0.4467 - loss: 0.9424Batch 4, Loss Value: 0.9112\n",
      "Batch 4, Gradient Norm: 0.5679\n",
      "\u001b[1m  4/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:50\u001b[0m 487ms/step - accuracy: 0.4515 - binary_io_u_5: 0.4531 - loss: 0.9421Batch 5, Loss Value: 0.9479\n",
      "Batch 5, Gradient Norm: 0.0294\n",
      "\u001b[1m  5/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:50\u001b[0m 486ms/step - accuracy: 0.4596 - binary_io_u_5: 0.4585 - loss: 0.9420Batch 6, Loss Value: 0.9269\n",
      "Batch 6, Gradient Norm: 0.5636\n",
      "\u001b[1m  6/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:49\u001b[0m 484ms/step - accuracy: 0.4646 - binary_io_u_5: 0.4612 - loss: 0.9422Batch 7, Loss Value: 0.9485\n",
      "Batch 7, Gradient Norm: 0.0140\n",
      "\u001b[1m  7/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:48\u001b[0m 483ms/step - accuracy: 0.4676 - binary_io_u_5: 0.4623 - loss: 0.9425Batch 8, Loss Value: 0.9484\n",
      "Batch 8, Gradient Norm: 0.0233\n",
      "\u001b[1m  8/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:48\u001b[0m 483ms/step - accuracy: 0.4708 - binary_io_u_5: 0.4647 - loss: 0.9425Batch 9, Loss Value: 0.9477\n",
      "Batch 9, Gradient Norm: 0.0430\n",
      "\u001b[1m  9/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:47\u001b[0m 482ms/step - accuracy: 0.4740 - binary_io_u_5: 0.4672 - loss: 0.9423Batch 10, Loss Value: 0.9442\n",
      "Batch 10, Gradient Norm: 0.1819\n",
      "\u001b[1m 10/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:46\u001b[0m 481ms/step - accuracy: 0.4766 - binary_io_u_5: 0.4698 - loss: 0.9422Batch 11, Loss Value: 0.9476\n",
      "Batch 11, Gradient Norm: 0.0104\n",
      "\u001b[1m 11/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:46\u001b[0m 481ms/step - accuracy: 0.4795 - binary_io_u_5: 0.4715 - loss: 0.9419Batch 12, Loss Value: 0.9276\n",
      "Batch 12, Gradient Norm: 0.2663\n",
      "\u001b[1m 12/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:45\u001b[0m 480ms/step - accuracy: 0.4816 - binary_io_u_5: 0.4726 - loss: 0.9416Batch 13, Loss Value: 0.9482\n",
      "Batch 13, Gradient Norm: 0.0202\n",
      "\u001b[1m 13/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:44\u001b[0m 479ms/step - accuracy: 0.4834 - binary_io_u_5: 0.4740 - loss: 0.9414Batch 14, Loss Value: 0.8994\n",
      "Batch 14, Gradient Norm: 0.2726\n",
      "\u001b[1m 14/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:44\u001b[0m 479ms/step - accuracy: 0.4851 - binary_io_u_5: 0.4754 - loss: 0.9413Batch 15, Loss Value: 0.9465\n",
      "Batch 15, Gradient Norm: 0.0033\n",
      "\u001b[1m 15/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:43\u001b[0m 478ms/step - accuracy: 0.4862 - binary_io_u_5: 0.4766 - loss: 0.9411Batch 16, Loss Value: 0.9262\n",
      "Batch 16, Gradient Norm: 0.4678\n",
      "\u001b[1m 16/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:43\u001b[0m 478ms/step - accuracy: 0.4870 - binary_io_u_5: 0.4778 - loss: 0.9410Batch 17, Loss Value: 0.9466\n",
      "Batch 17, Gradient Norm: 0.0127\n",
      "\u001b[1m 17/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:42\u001b[0m 478ms/step - accuracy: 0.4875 - binary_io_u_5: 0.4787 - loss: 0.9410Batch 18, Loss Value: 0.9426\n",
      "Batch 18, Gradient Norm: 0.3401\n",
      "\u001b[1m 18/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:42\u001b[0m 478ms/step - accuracy: 0.4880 - binary_io_u_5: 0.4794 - loss: 0.9409Batch 19, Loss Value: 0.9129\n",
      "Batch 19, Gradient Norm: 0.3376\n",
      "\u001b[1m 19/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:41\u001b[0m 479ms/step - accuracy: 0.4886 - binary_io_u_5: 0.4799 - loss: 0.9408Batch 20, Loss Value: 0.9479\n",
      "Batch 20, Gradient Norm: 0.0057\n",
      "\u001b[1m 20/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:41\u001b[0m 479ms/step - accuracy: 0.4891 - binary_io_u_5: 0.4804 - loss: 0.9406Batch 21, Loss Value: 0.9466\n",
      "Batch 21, Gradient Norm: 0.0057\n",
      "\u001b[1m 21/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:40\u001b[0m 478ms/step - accuracy: 0.4895 - binary_io_u_5: 0.4809 - loss: 0.9405Batch 22, Loss Value: 0.9468\n",
      "Batch 22, Gradient Norm: 0.0053\n",
      "\u001b[1m 22/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:40\u001b[0m 478ms/step - accuracy: 0.4899 - binary_io_u_5: 0.4815 - loss: 0.9404Batch 23, Loss Value: 0.9478\n",
      "Batch 23, Gradient Norm: 0.0151\n",
      "\u001b[1m 23/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:39\u001b[0m 478ms/step - accuracy: 0.4901 - binary_io_u_5: 0.4818 - loss: 0.9404Batch 24, Loss Value: 0.9464\n",
      "Batch 24, Gradient Norm: 0.2905\n",
      "\u001b[1m 24/232\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:39\u001b[0m 478ms/step - accuracy: 0.4904 - binary_io_u_5: 0.4823 - loss: 0.9403Batch 25, Loss Value: 0.9465\n",
      "Batch 25, Gradient Norm: 0.0113\n",
      "\u001b[1m 25/232\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:39\u001b[0m 478ms/step - accuracy: 0.4905 - binary_io_u_5: 0.4825 - loss: 0.9402Batch 26, Loss Value: 0.9455\n",
      "Batch 26, Gradient Norm: 0.5147\n",
      "\u001b[1m 26/232\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:38\u001b[0m 478ms/step - accuracy: 0.4906 - binary_io_u_5: 0.4828 - loss: 0.9402Batch 27, Loss Value: 0.9480\n",
      "Batch 27, Gradient Norm: 0.0257\n",
      "\u001b[1m 27/232\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:38\u001b[0m 478ms/step - accuracy: 0.4907 - binary_io_u_5: 0.4830 - loss: 0.9401Batch 28, Loss Value: 0.8910\n",
      "Batch 28, Gradient Norm: 0.6640\n",
      "\u001b[1m 28/232\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:37\u001b[0m 478ms/step - accuracy: 0.4908 - binary_io_u_5: 0.4831 - loss: 0.9400Batch 29, Loss Value: 0.9479\n",
      "Batch 29, Gradient Norm: 0.0289\n",
      "\u001b[1m 29/232\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:36\u001b[0m 477ms/step - accuracy: 0.4909 - binary_io_u_5: 0.4832 - loss: 0.9400Batch 30, Loss Value: 0.9395\n",
      "Batch 30, Gradient Norm: 0.2407\n",
      "\u001b[1m 30/232\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:36\u001b[0m 477ms/step - accuracy: 0.4908 - binary_io_u_5: 0.4832 - loss: 0.9399Batch 31, Loss Value: 0.9479\n",
      "Batch 31, Gradient Norm: 0.0224\n",
      "\u001b[1m 31/232\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:35\u001b[0m 477ms/step - accuracy: 0.4909 - binary_io_u_5: 0.4833 - loss: 0.9399Batch 32, Loss Value: 0.8859\n",
      "Batch 32, Gradient Norm: 0.3604\n",
      "\u001b[1m 32/232\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:35\u001b[0m 477ms/step - accuracy: 0.4909 - binary_io_u_5: 0.4833 - loss: 0.9398Batch 33, Loss Value: 0.9477\n",
      "Batch 33, Gradient Norm: 0.0307\n",
      "\u001b[1m 33/232\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:34\u001b[0m 476ms/step - accuracy: 0.4910 - binary_io_u_5: 0.4834 - loss: 0.9397Batch 34, Loss Value: 0.9389\n",
      "Batch 34, Gradient Norm: 0.4721\n",
      "\u001b[1m 34/232\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:34\u001b[0m 476ms/step - accuracy: 0.4910 - binary_io_u_5: 0.4834 - loss: 0.9397Batch 35, Loss Value: 0.9482\n",
      "Batch 35, Gradient Norm: 0.0077\n",
      "\u001b[1m 35/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:33\u001b[0m 476ms/step - accuracy: 0.4910 - binary_io_u_5: 0.4834 - loss: 0.9396Batch 36, Loss Value: 0.9099\n",
      "Batch 36, Gradient Norm: 0.4927\n",
      "\u001b[1m 36/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:33\u001b[0m 475ms/step - accuracy: 0.4910 - binary_io_u_5: 0.4834 - loss: 0.9396Batch 37, Loss Value: 0.9470\n",
      "Batch 37, Gradient Norm: 0.0150\n",
      "\u001b[1m 37/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:32\u001b[0m 475ms/step - accuracy: 0.4909 - binary_io_u_5: 0.4835 - loss: 0.9395Batch 38, Loss Value: 0.9490\n",
      "Batch 38, Gradient Norm: 0.0056\n",
      "\u001b[1m 38/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:32\u001b[0m 475ms/step - accuracy: 0.4908 - binary_io_u_5: 0.4835 - loss: 0.9395Batch 39, Loss Value: 0.9382\n",
      "Batch 39, Gradient Norm: 0.4609\n",
      "\u001b[1m 39/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:31\u001b[0m 475ms/step - accuracy: 0.4907 - binary_io_u_5: 0.4836 - loss: 0.9395Batch 40, Loss Value: 0.9487\n",
      "Batch 40, Gradient Norm: 0.0119\n",
      "\u001b[1m 40/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:31\u001b[0m 474ms/step - accuracy: 0.4906 - binary_io_u_5: 0.4838 - loss: 0.9394Batch 41, Loss Value: 0.9187\n",
      "Batch 41, Gradient Norm: 0.0422\n",
      "\u001b[1m 41/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:30\u001b[0m 474ms/step - accuracy: 0.4906 - binary_io_u_5: 0.4840 - loss: 0.9393Batch 42, Loss Value: 0.9477\n",
      "Batch 42, Gradient Norm: 0.0065\n",
      "\u001b[1m 42/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:30\u001b[0m 474ms/step - accuracy: 0.4905 - binary_io_u_5: 0.4842 - loss: 0.9392Batch 43, Loss Value: 0.9466\n",
      "Batch 43, Gradient Norm: 0.0131\n",
      "\u001b[1m 43/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:29\u001b[0m 474ms/step - accuracy: 0.4906 - binary_io_u_5: 0.4844 - loss: 0.9391Batch 44, Loss Value: 0.9163\n",
      "Batch 44, Gradient Norm: 0.1207\n",
      "\u001b[1m 44/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:29\u001b[0m 474ms/step - accuracy: 0.4906 - binary_io_u_5: 0.4846 - loss: 0.9390Batch 45, Loss Value: 0.9368\n",
      "Batch 45, Gradient Norm: 0.0260\n",
      "\u001b[1m 45/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:28\u001b[0m 474ms/step - accuracy: 0.4905 - binary_io_u_5: 0.4848 - loss: 0.9389Batch 46, Loss Value: 0.9468\n",
      "Batch 46, Gradient Norm: 0.0024\n",
      "\u001b[1m 46/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:28\u001b[0m 474ms/step - accuracy: 0.4905 - binary_io_u_5: 0.4849 - loss: 0.9387Batch 47, Loss Value: 0.9124\n",
      "Batch 47, Gradient Norm: 1.0539\n",
      "\u001b[1m 47/232\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:27\u001b[0m 474ms/step - accuracy: 0.4905 - binary_io_u_5: 0.4851 - loss: 0.9387Batch 48, Loss Value: 0.9344\n",
      "Batch 48, Gradient Norm: 0.2720\n",
      "\u001b[1m 48/232\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:27\u001b[0m 474ms/step - accuracy: 0.4904 - binary_io_u_5: 0.4852 - loss: 0.9386Batch 49, Loss Value: 0.9487\n",
      "Batch 49, Gradient Norm: 0.0048\n",
      "\u001b[1m 49/232\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:26\u001b[0m 473ms/step - accuracy: 0.4904 - binary_io_u_5: 0.4853 - loss: 0.9385Batch 50, Loss Value: 0.9470\n",
      "Batch 50, Gradient Norm: 0.0047\n",
      "\u001b[1m 50/232\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:26\u001b[0m 473ms/step - accuracy: 0.4904 - binary_io_u_5: 0.4854 - loss: 0.9384Batch 51, Loss Value: 0.9443\n",
      "Batch 51, Gradient Norm: 0.6692\n",
      "\u001b[1m 51/232\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:25\u001b[0m 474ms/step - accuracy: 0.4903 - binary_io_u_5: 0.4856 - loss: 0.9383Batch 52, Loss Value: 0.9164\n",
      "Batch 52, Gradient Norm: 0.2083\n",
      "\u001b[1m 52/232\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:25\u001b[0m 473ms/step - accuracy: 0.4903 - binary_io_u_5: 0.4857 - loss: 0.9382Batch 53, Loss Value: 0.9467\n",
      "Batch 53, Gradient Norm: 0.0111\n",
      "\u001b[1m 53/232\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:24\u001b[0m 473ms/step - accuracy: 0.4904 - binary_io_u_5: 0.4858 - loss: 0.9381Batch 54, Loss Value: 0.9448\n",
      "Batch 54, Gradient Norm: 0.4177\n",
      "\u001b[1m 54/232\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:24\u001b[0m 473ms/step - accuracy: 0.4904 - binary_io_u_5: 0.4860 - loss: 0.9381Batch 55, Loss Value: 0.9469\n",
      "Batch 55, Gradient Norm: 0.0005\n",
      "\u001b[1m 55/232\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:23\u001b[0m 473ms/step - accuracy: 0.4904 - binary_io_u_5: 0.4861 - loss: 0.9380Batch 56, Loss Value: 0.9239\n",
      "Batch 56, Gradient Norm: 0.3660\n",
      "\u001b[1m 56/232\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:23\u001b[0m 473ms/step - accuracy: 0.4905 - binary_io_u_5: 0.4863 - loss: 0.9379Batch 57, Loss Value: 0.9475\n",
      "Batch 57, Gradient Norm: 0.0130\n",
      "\u001b[1m 57/232\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:22\u001b[0m 473ms/step - accuracy: 0.4905 - binary_io_u_5: 0.4865 - loss: 0.9378Batch 58, Loss Value: 0.9313\n",
      "Batch 58, Gradient Norm: 0.3582\n",
      "\u001b[1m 58/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:22\u001b[0m 473ms/step - accuracy: 0.4906 - binary_io_u_5: 0.4867 - loss: 0.9377Batch 59, Loss Value: 0.9483\n",
      "Batch 59, Gradient Norm: 0.0011\n",
      "\u001b[1m 59/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:21\u001b[0m 473ms/step - accuracy: 0.4906 - binary_io_u_5: 0.4868 - loss: 0.9376Batch 60, Loss Value: 0.9306\n",
      "Batch 60, Gradient Norm: 0.2258\n",
      "\u001b[1m 60/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:21\u001b[0m 472ms/step - accuracy: 0.4907 - binary_io_u_5: 0.4870 - loss: 0.9376Batch 61, Loss Value: 0.9471\n",
      "Batch 61, Gradient Norm: 0.0046\n",
      "\u001b[1m 61/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:20\u001b[0m 472ms/step - accuracy: 0.4908 - binary_io_u_5: 0.4872 - loss: 0.9375Batch 62, Loss Value: 0.9477\n",
      "Batch 62, Gradient Norm: 0.0048\n",
      "\u001b[1m 62/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:20\u001b[0m 472ms/step - accuracy: 0.4908 - binary_io_u_5: 0.4874 - loss: 0.9374Batch 63, Loss Value: 0.9324\n",
      "Batch 63, Gradient Norm: 0.1289\n",
      "\u001b[1m 63/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:19\u001b[0m 472ms/step - accuracy: 0.4909 - binary_io_u_5: 0.4875 - loss: 0.9373Batch 64, Loss Value: 0.9079\n",
      "Batch 64, Gradient Norm: 0.2125\n",
      "\u001b[1m 64/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:19\u001b[0m 472ms/step - accuracy: 0.4910 - binary_io_u_5: 0.4877 - loss: 0.9372Batch 65, Loss Value: 0.9470\n",
      "Batch 65, Gradient Norm: 0.0195\n",
      "\u001b[1m 65/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:18\u001b[0m 472ms/step - accuracy: 0.4910 - binary_io_u_5: 0.4878 - loss: 0.9371Batch 66, Loss Value: 0.9461\n",
      "Batch 66, Gradient Norm: 0.2562\n",
      "\u001b[1m 66/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:18\u001b[0m 472ms/step - accuracy: 0.4911 - binary_io_u_5: 0.4879 - loss: 0.9371Batch 67, Loss Value: 0.9469\n",
      "Batch 67, Gradient Norm: 0.0078\n",
      "\u001b[1m 67/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:17\u001b[0m 472ms/step - accuracy: 0.4912 - binary_io_u_5: 0.4881 - loss: 0.9370Batch 68, Loss Value: 0.9483\n",
      "Batch 68, Gradient Norm: 0.0104\n",
      "\u001b[1m 68/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:17\u001b[0m 472ms/step - accuracy: 0.4912 - binary_io_u_5: 0.4882 - loss: 0.9369Batch 69, Loss Value: 0.9479\n",
      "Batch 69, Gradient Norm: 0.0053\n",
      "\u001b[1m 69/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:16\u001b[0m 472ms/step - accuracy: 0.4912 - binary_io_u_5: 0.4884 - loss: 0.9369Batch 70, Loss Value: 0.9466\n",
      "Batch 70, Gradient Norm: 0.0066\n",
      "\u001b[1m 70/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:16\u001b[0m 472ms/step - accuracy: 0.4913 - binary_io_u_5: 0.4885 - loss: 0.9368Batch 71, Loss Value: 0.9476\n",
      "Batch 71, Gradient Norm: 0.0116\n",
      "\u001b[1m 71/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:15\u001b[0m 471ms/step - accuracy: 0.4913 - binary_io_u_5: 0.4886 - loss: 0.9367Batch 72, Loss Value: 0.9465\n",
      "Batch 72, Gradient Norm: 0.0074\n",
      "\u001b[1m 72/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:15\u001b[0m 471ms/step - accuracy: 0.4914 - binary_io_u_5: 0.4887 - loss: 0.9367Batch 73, Loss Value: 0.9465\n",
      "Batch 73, Gradient Norm: 0.0102\n",
      "\u001b[1m 73/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:14\u001b[0m 471ms/step - accuracy: 0.4914 - binary_io_u_5: 0.4888 - loss: 0.9366Batch 74, Loss Value: 0.9476\n",
      "Batch 74, Gradient Norm: 0.0101\n",
      "\u001b[1m 74/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:14\u001b[0m 471ms/step - accuracy: 0.4914 - binary_io_u_5: 0.4888 - loss: 0.9365Batch 75, Loss Value: 0.9472\n",
      "Batch 75, Gradient Norm: 0.0030\n",
      "\u001b[1m 75/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:13\u001b[0m 471ms/step - accuracy: 0.4915 - binary_io_u_5: 0.4889 - loss: 0.9365Batch 76, Loss Value: 0.9477\n",
      "Batch 76, Gradient Norm: 0.0121\n",
      "\u001b[1m 76/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:13\u001b[0m 471ms/step - accuracy: 0.4915 - binary_io_u_5: 0.4890 - loss: 0.9364Batch 77, Loss Value: 0.9467\n",
      "Batch 77, Gradient Norm: 0.0045\n",
      "\u001b[1m 77/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:12\u001b[0m 471ms/step - accuracy: 0.4915 - binary_io_u_5: 0.4891 - loss: 0.9363Batch 78, Loss Value: 0.9473\n",
      "Batch 78, Gradient Norm: 0.0133\n",
      "\u001b[1m 78/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:12\u001b[0m 471ms/step - accuracy: 0.4915 - binary_io_u_5: 0.4892 - loss: 0.9363Batch 79, Loss Value: 0.9462\n",
      "Batch 79, Gradient Norm: 0.0060\n",
      "\u001b[1m 79/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:12\u001b[0m 471ms/step - accuracy: 0.4916 - binary_io_u_5: 0.4893 - loss: 0.9362Batch 80, Loss Value: 0.9313\n",
      "Batch 80, Gradient Norm: 0.3252\n",
      "\u001b[1m 80/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:11\u001b[0m 471ms/step - accuracy: 0.4916 - binary_io_u_5: 0.4894 - loss: 0.9362Batch 81, Loss Value: 0.8657\n",
      "Batch 81, Gradient Norm: 0.1141\n",
      "\u001b[1m 81/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:11\u001b[0m 471ms/step - accuracy: 0.4916 - binary_io_u_5: 0.4895 - loss: 0.9361Batch 82, Loss Value: 0.9173\n",
      "Batch 82, Gradient Norm: 0.3206\n",
      "\u001b[1m 82/232\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m1:10\u001b[0m 471ms/step - accuracy: 0.4916 - binary_io_u_5: 0.4896 - loss: 0.9361Batch 83, Loss Value: 0.9467\n",
      "Batch 83, Gradient Norm: 0.0019\n",
      "\u001b[1m 83/232\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m1:10\u001b[0m 471ms/step - accuracy: 0.4916 - binary_io_u_5: 0.4896 - loss: 0.9360Batch 84, Loss Value: 0.9467\n",
      "Batch 84, Gradient Norm: 0.0020\n",
      "\u001b[1m 84/232\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m1:09\u001b[0m 471ms/step - accuracy: 0.4916 - binary_io_u_5: 0.4897 - loss: 0.9360Batch 85, Loss Value: 0.9469\n",
      "Batch 85, Gradient Norm: 0.0057\n",
      "\u001b[1m 85/232\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m1:09\u001b[0m 471ms/step - accuracy: 0.4916 - binary_io_u_5: 0.4898 - loss: 0.9360Batch 86, Loss Value: 0.9474\n",
      "Batch 86, Gradient Norm: 0.0125\n",
      "\u001b[1m 86/232\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m1:08\u001b[0m 471ms/step - accuracy: 0.4917 - binary_io_u_5: 0.4899 - loss: 0.9359Batch 87, Loss Value: 0.9360\n",
      "Batch 87, Gradient Norm: 0.1053\n",
      "\u001b[1m 87/232\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m1:08\u001b[0m 471ms/step - accuracy: 0.4917 - binary_io_u_5: 0.4900 - loss: 0.9358Batch 88, Loss Value: 0.9455\n",
      "Batch 88, Gradient Norm: 0.3125\n",
      "\u001b[1m 88/232\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m1:07\u001b[0m 471ms/step - accuracy: 0.4917 - binary_io_u_5: 0.4901 - loss: 0.9358Batch 89, Loss Value: 0.9472\n",
      "Batch 89, Gradient Norm: 0.0137\n",
      "\u001b[1m 89/232\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m1:07\u001b[0m 470ms/step - accuracy: 0.4918 - binary_io_u_5: 0.4901 - loss: 0.9357Batch 90, Loss Value: 0.9455\n",
      "Batch 90, Gradient Norm: 0.0026\n",
      "\u001b[1m 90/232\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m1:06\u001b[0m 470ms/step - accuracy: 0.4918 - binary_io_u_5: 0.4902 - loss: 0.9357Batch 91, Loss Value: 0.9475\n",
      "Batch 91, Gradient Norm: 0.0079\n",
      "\u001b[1m 91/232\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m1:06\u001b[0m 470ms/step - accuracy: 0.4918 - binary_io_u_5: 0.4903 - loss: 0.9356Batch 92, Loss Value: 0.9461\n",
      "Batch 92, Gradient Norm: 0.0020\n",
      "\u001b[1m 92/232\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m1:05\u001b[0m 470ms/step - accuracy: 0.4918 - binary_io_u_5: 0.4904 - loss: 0.9356Batch 93, Loss Value: 0.9465\n",
      "Batch 93, Gradient Norm: 0.0010\n",
      "\u001b[1m 93/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m1:05\u001b[0m 470ms/step - accuracy: 0.4919 - binary_io_u_5: 0.4905 - loss: 0.9356Batch 94, Loss Value: 0.9473\n",
      "Batch 94, Gradient Norm: 0.0040\n",
      "\u001b[1m 94/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m1:04\u001b[0m 470ms/step - accuracy: 0.4919 - binary_io_u_5: 0.4906 - loss: 0.9355Batch 95, Loss Value: 0.9036\n",
      "Batch 95, Gradient Norm: 0.2076\n",
      "\u001b[1m 95/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m1:04\u001b[0m 470ms/step - accuracy: 0.4919 - binary_io_u_5: 0.4907 - loss: 0.9355Batch 96, Loss Value: 0.9468\n",
      "Batch 96, Gradient Norm: 0.0058\n",
      "\u001b[1m 96/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m1:03\u001b[0m 470ms/step - accuracy: 0.4919 - binary_io_u_5: 0.4907 - loss: 0.9355Batch 97, Loss Value: 0.9295\n",
      "Batch 97, Gradient Norm: 0.3389\n",
      "\u001b[1m 97/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m1:03\u001b[0m 470ms/step - accuracy: 0.4919 - binary_io_u_5: 0.4908 - loss: 0.9354Batch 98, Loss Value: 0.9469\n",
      "Batch 98, Gradient Norm: 0.0034\n",
      "\u001b[1m 98/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m1:02\u001b[0m 470ms/step - accuracy: 0.4919 - binary_io_u_5: 0.4909 - loss: 0.9354Batch 99, Loss Value: 0.9466\n",
      "Batch 99, Gradient Norm: 0.0038\n",
      "\u001b[1m 99/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m1:02\u001b[0m 470ms/step - accuracy: 0.4919 - binary_io_u_5: 0.4909 - loss: 0.9353Batch 100, Loss Value: 0.9468\n",
      "Batch 100, Gradient Norm: 0.0052\n",
      "\u001b[1m100/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m1:01\u001b[0m 470ms/step - accuracy: 0.4919 - binary_io_u_5: 0.4910 - loss: 0.9353Batch 101, Loss Value: 0.9476\n",
      "Batch 101, Gradient Norm: 0.0002\n",
      "\u001b[1m101/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m1:01\u001b[0m 470ms/step - accuracy: 0.4919 - binary_io_u_5: 0.4911 - loss: 0.9353Batch 102, Loss Value: 0.9397\n",
      "Batch 102, Gradient Norm: 0.2364\n",
      "\u001b[1m102/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m1:01\u001b[0m 470ms/step - accuracy: 0.4920 - binary_io_u_5: 0.4911 - loss: 0.9352Batch 103, Loss Value: 0.9469\n",
      "Batch 103, Gradient Norm: 0.0180\n",
      "\u001b[1m103/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m1:00\u001b[0m 470ms/step - accuracy: 0.4920 - binary_io_u_5: 0.4912 - loss: 0.9352Batch 104, Loss Value: 0.9475\n",
      "Batch 104, Gradient Norm: 0.0087\n",
      "\u001b[1m104/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m1:00\u001b[0m 470ms/step - accuracy: 0.4920 - binary_io_u_5: 0.4913 - loss: 0.9351Batch 105, Loss Value: 0.9485\n",
      "Batch 105, Gradient Norm: 0.0021\n",
      "\u001b[1m105/232\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m59s\u001b[0m 469ms/step - accuracy: 0.4920 - binary_io_u_5: 0.4913 - loss: 0.9351 Batch 106, Loss Value: 0.9473\n",
      "Batch 106, Gradient Norm: 0.0005\n",
      "\u001b[1m106/232\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m59s\u001b[0m 469ms/step - accuracy: 0.4921 - binary_io_u_5: 0.4914 - loss: 0.9351Batch 107, Loss Value: 0.9480\n",
      "Batch 107, Gradient Norm: 0.0009\n",
      "\u001b[1m107/232\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m58s\u001b[0m 469ms/step - accuracy: 0.4921 - binary_io_u_5: 0.4914 - loss: 0.9350Batch 108, Loss Value: 0.9466\n",
      "Batch 108, Gradient Norm: 0.0050\n",
      "\u001b[1m108/232\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m58s\u001b[0m 469ms/step - accuracy: 0.4921 - binary_io_u_5: 0.4915 - loss: 0.9350Batch 109, Loss Value: 0.9466\n",
      "Batch 109, Gradient Norm: 0.0060\n",
      "\u001b[1m109/232\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m57s\u001b[0m 469ms/step - accuracy: 0.4921 - binary_io_u_5: 0.4915 - loss: 0.9349Batch 110, Loss Value: 0.9469\n",
      "Batch 110, Gradient Norm: 0.0023\n",
      "\u001b[1m110/232\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m57s\u001b[0m 469ms/step - accuracy: 0.4922 - binary_io_u_5: 0.4916 - loss: 0.9349Batch 111, Loss Value: 0.9475\n",
      "Batch 111, Gradient Norm: 0.0172\n",
      "\u001b[1m111/232\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m56s\u001b[0m 469ms/step - accuracy: 0.4922 - binary_io_u_5: 0.4916 - loss: 0.9349Batch 112, Loss Value: 0.9473\n",
      "Batch 112, Gradient Norm: 0.0004\n",
      "\u001b[1m112/232\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m56s\u001b[0m 469ms/step - accuracy: 0.4922 - binary_io_u_5: 0.4916 - loss: 0.9348Batch 113, Loss Value: 0.9476\n",
      "Batch 113, Gradient Norm: 0.0018\n",
      "\u001b[1m113/232\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m55s\u001b[0m 469ms/step - accuracy: 0.4922 - binary_io_u_5: 0.4917 - loss: 0.9348Batch 114, Loss Value: 0.9475\n",
      "Batch 114, Gradient Norm: 0.0205\n",
      "\u001b[1m114/232\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m55s\u001b[0m 469ms/step - accuracy: 0.4922 - binary_io_u_5: 0.4917 - loss: 0.9348Batch 115, Loss Value: 0.9475\n",
      "Batch 115, Gradient Norm: 0.0176\n",
      "\u001b[1m115/232\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m54s\u001b[0m 469ms/step - accuracy: 0.4922 - binary_io_u_5: 0.4918 - loss: 0.9347Batch 116, Loss Value: 0.9466\n",
      "Batch 116, Gradient Norm: 0.0182\n",
      "\u001b[1m116/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m54s\u001b[0m 469ms/step - accuracy: 0.4922 - binary_io_u_5: 0.4918 - loss: 0.9347Batch 117, Loss Value: 0.9473\n",
      "Batch 117, Gradient Norm: 0.0094\n",
      "\u001b[1m117/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m53s\u001b[0m 469ms/step - accuracy: 0.4923 - binary_io_u_5: 0.4919 - loss: 0.9347Batch 118, Loss Value: 0.9470\n",
      "Batch 118, Gradient Norm: 0.0025\n",
      "\u001b[1m118/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m53s\u001b[0m 469ms/step - accuracy: 0.4923 - binary_io_u_5: 0.4919 - loss: 0.9346Batch 119, Loss Value: 0.9229\n",
      "Batch 119, Gradient Norm: 0.1373\n",
      "\u001b[1m119/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m53s\u001b[0m 469ms/step - accuracy: 0.4923 - binary_io_u_5: 0.4919 - loss: 0.9346Batch 120, Loss Value: 0.9468\n",
      "Batch 120, Gradient Norm: 0.0072\n",
      "\u001b[1m120/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m52s\u001b[0m 469ms/step - accuracy: 0.4923 - binary_io_u_5: 0.4920 - loss: 0.9346Batch 121, Loss Value: 0.9452\n",
      "Batch 121, Gradient Norm: 0.0518\n",
      "\u001b[1m121/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m52s\u001b[0m 469ms/step - accuracy: 0.4923 - binary_io_u_5: 0.4920 - loss: 0.9345Batch 122, Loss Value: 0.9467\n",
      "Batch 122, Gradient Norm: 0.0048\n",
      "\u001b[1m122/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m51s\u001b[0m 469ms/step - accuracy: 0.4923 - binary_io_u_5: 0.4920 - loss: 0.9345Batch 123, Loss Value: 0.9495\n",
      "Batch 123, Gradient Norm: 0.0077\n",
      "\u001b[1m123/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m51s\u001b[0m 469ms/step - accuracy: 0.4923 - binary_io_u_5: 0.4921 - loss: 0.9345Batch 124, Loss Value: 0.9477\n",
      "Batch 124, Gradient Norm: 0.0086\n",
      "\u001b[1m124/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m50s\u001b[0m 469ms/step - accuracy: 0.4923 - binary_io_u_5: 0.4921 - loss: 0.9345Batch 125, Loss Value: 0.9477\n",
      "Batch 125, Gradient Norm: 0.0064\n",
      "\u001b[1m125/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m50s\u001b[0m 469ms/step - accuracy: 0.4923 - binary_io_u_5: 0.4921 - loss: 0.9345Batch 126, Loss Value: 0.9361\n",
      "Batch 126, Gradient Norm: 0.4487\n",
      "\u001b[1m126/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m49s\u001b[0m 469ms/step - accuracy: 0.4923 - binary_io_u_5: 0.4921 - loss: 0.9344Batch 127, Loss Value: 0.9466\n",
      "Batch 127, Gradient Norm: 0.0066\n",
      "\u001b[1m127/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m49s\u001b[0m 469ms/step - accuracy: 0.4922 - binary_io_u_5: 0.4922 - loss: 0.9344Batch 128, Loss Value: 0.9476\n",
      "Batch 128, Gradient Norm: 0.0034\n",
      "\u001b[1m128/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m48s\u001b[0m 469ms/step - accuracy: 0.4922 - binary_io_u_5: 0.4922 - loss: 0.9344Batch 129, Loss Value: 0.9419\n",
      "Batch 129, Gradient Norm: 0.2132\n",
      "\u001b[1m129/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m48s\u001b[0m 469ms/step - accuracy: 0.4922 - binary_io_u_5: 0.4922 - loss: 0.9344Batch 130, Loss Value: 0.9466\n",
      "Batch 130, Gradient Norm: 0.0156\n",
      "\u001b[1m130/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m47s\u001b[0m 469ms/step - accuracy: 0.4922 - binary_io_u_5: 0.4922 - loss: 0.9344Batch 131, Loss Value: 0.9469\n",
      "Batch 131, Gradient Norm: 0.0124\n",
      "\u001b[1m131/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m47s\u001b[0m 469ms/step - accuracy: 0.4922 - binary_io_u_5: 0.4923 - loss: 0.9343Batch 132, Loss Value: 0.9471\n",
      "Batch 132, Gradient Norm: 0.0036\n",
      "\u001b[1m132/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m46s\u001b[0m 469ms/step - accuracy: 0.4922 - binary_io_u_5: 0.4923 - loss: 0.9343Batch 133, Loss Value: 0.9401\n",
      "Batch 133, Gradient Norm: 0.1284\n",
      "\u001b[1m133/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m46s\u001b[0m 469ms/step - accuracy: 0.4922 - binary_io_u_5: 0.4923 - loss: 0.9343Batch 134, Loss Value: 0.9485\n",
      "Batch 134, Gradient Norm: 0.0004\n",
      "\u001b[1m134/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m45s\u001b[0m 469ms/step - accuracy: 0.4922 - binary_io_u_5: 0.4923 - loss: 0.9343Batch 135, Loss Value: 0.9262\n",
      "Batch 135, Gradient Norm: 0.0600\n",
      "\u001b[1m135/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m45s\u001b[0m 469ms/step - accuracy: 0.4922 - binary_io_u_5: 0.4924 - loss: 0.9342Batch 136, Loss Value: 0.9155\n",
      "Batch 136, Gradient Norm: 0.0095\n",
      "\u001b[1m136/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m45s\u001b[0m 469ms/step - accuracy: 0.4922 - binary_io_u_5: 0.4924 - loss: 0.9342Batch 137, Loss Value: 0.9476\n",
      "Batch 137, Gradient Norm: 0.0092\n",
      "\u001b[1m137/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m44s\u001b[0m 469ms/step - accuracy: 0.4922 - binary_io_u_5: 0.4924 - loss: 0.9342Batch 138, Loss Value: 0.9104\n",
      "Batch 138, Gradient Norm: 0.1100\n",
      "\u001b[1m138/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m44s\u001b[0m 469ms/step - accuracy: 0.4922 - binary_io_u_5: 0.4924 - loss: 0.9342Batch 139, Loss Value: 0.9486\n",
      "Batch 139, Gradient Norm: 0.0085\n",
      "\u001b[1m139/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m43s\u001b[0m 469ms/step - accuracy: 0.4922 - binary_io_u_5: 0.4925 - loss: 0.9342Batch 140, Loss Value: 0.9434\n",
      "Batch 140, Gradient Norm: 0.2395\n",
      "\u001b[1m140/232\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m43s\u001b[0m 469ms/step - accuracy: 0.4922 - binary_io_u_5: 0.4925 - loss: 0.9341Batch 141, Loss Value: 0.9442\n",
      "Batch 141, Gradient Norm: 0.3953\n",
      "\u001b[1m141/232\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m42s\u001b[0m 469ms/step - accuracy: 0.4922 - binary_io_u_5: 0.4925 - loss: 0.9341Batch 142, Loss Value: 0.9144\n",
      "Batch 142, Gradient Norm: 0.2599\n",
      "\u001b[1m142/232\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m42s\u001b[0m 469ms/step - accuracy: 0.4922 - binary_io_u_5: 0.4926 - loss: 0.9341Batch 143, Loss Value: 0.9469\n",
      "Batch 143, Gradient Norm: 0.0001\n",
      "\u001b[1m143/232\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m41s\u001b[0m 469ms/step - accuracy: 0.4922 - binary_io_u_5: 0.4926 - loss: 0.9341Batch 144, Loss Value: 0.9467\n",
      "Batch 144, Gradient Norm: 0.0031\n",
      "\u001b[1m144/232\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m41s\u001b[0m 469ms/step - accuracy: 0.4922 - binary_io_u_5: 0.4926 - loss: 0.9340Batch 145, Loss Value: 0.9402\n",
      "Batch 145, Gradient Norm: 0.0537\n",
      "\u001b[1m145/232\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m40s\u001b[0m 469ms/step - accuracy: 0.4922 - binary_io_u_5: 0.4926 - loss: 0.9340Batch 146, Loss Value: 0.9428\n",
      "Batch 146, Gradient Norm: 0.4552\n",
      "\u001b[1m146/232\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m40s\u001b[0m 469ms/step - accuracy: 0.4922 - binary_io_u_5: 0.4927 - loss: 0.9340Batch 147, Loss Value: 0.9380\n",
      "Batch 147, Gradient Norm: 0.0702\n",
      "\u001b[1m147/232\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m39s\u001b[0m 469ms/step - accuracy: 0.4922 - binary_io_u_5: 0.4927 - loss: 0.9340Batch 148, Loss Value: 0.9424\n",
      "Batch 148, Gradient Norm: 0.0232\n",
      "\u001b[1m148/232\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m39s\u001b[0m 469ms/step - accuracy: 0.4922 - binary_io_u_5: 0.4927 - loss: 0.9340Batch 149, Loss Value: 0.9118\n",
      "Batch 149, Gradient Norm: 0.1488\n",
      "\u001b[1m149/232\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m38s\u001b[0m 469ms/step - accuracy: 0.4922 - binary_io_u_5: 0.4927 - loss: 0.9339Batch 150, Loss Value: 0.9481\n",
      "Batch 150, Gradient Norm: 0.0043\n",
      "\u001b[1m150/232\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m38s\u001b[0m 469ms/step - accuracy: 0.4922 - binary_io_u_5: 0.4928 - loss: 0.9339Batch 151, Loss Value: 0.9479\n",
      "Batch 151, Gradient Norm: 0.0102\n",
      "\u001b[1m151/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m37s\u001b[0m 469ms/step - accuracy: 0.4922 - binary_io_u_5: 0.4928 - loss: 0.9339Batch 152, Loss Value: 0.8957\n",
      "Batch 152, Gradient Norm: 0.2943\n",
      "\u001b[1m152/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m37s\u001b[0m 469ms/step - accuracy: 0.4922 - binary_io_u_5: 0.4928 - loss: 0.9339Batch 153, Loss Value: 0.9184\n",
      "Batch 153, Gradient Norm: 0.2363\n",
      "\u001b[1m153/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m37s\u001b[0m 469ms/step - accuracy: 0.4922 - binary_io_u_5: 0.4928 - loss: 0.9339Batch 154, Loss Value: 0.9470\n",
      "Batch 154, Gradient Norm: 0.0041\n",
      "\u001b[1m154/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m36s\u001b[0m 469ms/step - accuracy: 0.4922 - binary_io_u_5: 0.4929 - loss: 0.9338Batch 155, Loss Value: 0.9348\n",
      "Batch 155, Gradient Norm: 0.3261\n",
      "\u001b[1m155/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m36s\u001b[0m 469ms/step - accuracy: 0.4922 - binary_io_u_5: 0.4929 - loss: 0.9338Batch 156, Loss Value: 0.9413\n",
      "Batch 156, Gradient Norm: 0.0397\n",
      "\u001b[1m156/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m35s\u001b[0m 469ms/step - accuracy: 0.4922 - binary_io_u_5: 0.4929 - loss: 0.9338Batch 157, Loss Value: 0.9464\n",
      "Batch 157, Gradient Norm: 0.0915\n",
      "\u001b[1m157/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m35s\u001b[0m 469ms/step - accuracy: 0.4922 - binary_io_u_5: 0.4929 - loss: 0.9338Batch 158, Loss Value: 0.9399\n",
      "Batch 158, Gradient Norm: 0.2193\n",
      "\u001b[1m158/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m34s\u001b[0m 469ms/step - accuracy: 0.4922 - binary_io_u_5: 0.4930 - loss: 0.9338Batch 159, Loss Value: 0.9473\n",
      "Batch 159, Gradient Norm: 0.0274\n",
      "\u001b[1m159/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m34s\u001b[0m 469ms/step - accuracy: 0.4922 - binary_io_u_5: 0.4930 - loss: 0.9337Batch 160, Loss Value: 0.9460\n",
      "Batch 160, Gradient Norm: 0.0044\n",
      "\u001b[1m160/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m33s\u001b[0m 469ms/step - accuracy: 0.4922 - binary_io_u_5: 0.4930 - loss: 0.9337Batch 161, Loss Value: 0.9476\n",
      "Batch 161, Gradient Norm: 0.0025\n",
      "\u001b[1m161/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m33s\u001b[0m 468ms/step - accuracy: 0.4923 - binary_io_u_5: 0.4930 - loss: 0.9337Batch 162, Loss Value: 0.9479\n",
      "Batch 162, Gradient Norm: 0.0058\n",
      "\u001b[1m162/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m32s\u001b[0m 468ms/step - accuracy: 0.4922 - binary_io_u_5: 0.4930 - loss: 0.9337Batch 163, Loss Value: 0.9469\n",
      "Batch 163, Gradient Norm: 0.0142\n",
      "\u001b[1m163/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m32s\u001b[0m 468ms/step - accuracy: 0.4922 - binary_io_u_5: 0.4931 - loss: 0.9337Batch 164, Loss Value: 0.9474\n",
      "Batch 164, Gradient Norm: 0.0172\n",
      "\u001b[1m164/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m31s\u001b[0m 468ms/step - accuracy: 0.4922 - binary_io_u_5: 0.4931 - loss: 0.9337Batch 165, Loss Value: 0.8985\n",
      "Batch 165, Gradient Norm: 0.0102\n",
      "\u001b[1m165/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m31s\u001b[0m 468ms/step - accuracy: 0.4922 - binary_io_u_5: 0.4931 - loss: 0.9337Batch 166, Loss Value: 0.9330\n",
      "Batch 166, Gradient Norm: 1.2023\n",
      "\u001b[1m166/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m30s\u001b[0m 468ms/step - accuracy: 0.4923 - binary_io_u_5: 0.4931 - loss: 0.9336Batch 167, Loss Value: 0.9367\n",
      "Batch 167, Gradient Norm: 0.7273\n",
      "\u001b[1m167/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m30s\u001b[0m 468ms/step - accuracy: 0.4923 - binary_io_u_5: 0.4931 - loss: 0.9336Batch 168, Loss Value: 0.9476\n",
      "Batch 168, Gradient Norm: 0.0108\n",
      "\u001b[1m168/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m29s\u001b[0m 468ms/step - accuracy: 0.4923 - binary_io_u_5: 0.4932 - loss: 0.9336Batch 169, Loss Value: 0.9464\n",
      "Batch 169, Gradient Norm: 0.0060\n",
      "\u001b[1m169/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m29s\u001b[0m 468ms/step - accuracy: 0.4923 - binary_io_u_5: 0.4932 - loss: 0.9336Batch 170, Loss Value: 0.9476\n",
      "Batch 170, Gradient Norm: 0.0179\n",
      "\u001b[1m170/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m29s\u001b[0m 468ms/step - accuracy: 0.4923 - binary_io_u_5: 0.4932 - loss: 0.9336Batch 171, Loss Value: 0.9173\n",
      "Batch 171, Gradient Norm: 0.2677\n",
      "\u001b[1m171/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m28s\u001b[0m 468ms/step - accuracy: 0.4923 - binary_io_u_5: 0.4932 - loss: 0.9335Batch 172, Loss Value: 0.9470\n",
      "Batch 172, Gradient Norm: 0.0406\n",
      "\u001b[1m172/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m28s\u001b[0m 468ms/step - accuracy: 0.4923 - binary_io_u_5: 0.4932 - loss: 0.9335Batch 173, Loss Value: 0.9475\n",
      "Batch 173, Gradient Norm: 0.0819\n",
      "\u001b[1m173/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m27s\u001b[0m 468ms/step - accuracy: 0.4923 - binary_io_u_5: 0.4932 - loss: 0.9335Batch 174, Loss Value: 0.9343\n",
      "Batch 174, Gradient Norm: 0.6298\n",
      "\u001b[1m174/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m27s\u001b[0m 468ms/step - accuracy: 0.4923 - binary_io_u_5: 0.4932 - loss: 0.9335Batch 175, Loss Value: 0.9482\n",
      "Batch 175, Gradient Norm: 0.0682\n",
      "\u001b[1m175/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m26s\u001b[0m 468ms/step - accuracy: 0.4923 - binary_io_u_5: 0.4933 - loss: 0.9335Batch 176, Loss Value: 0.9472\n",
      "Batch 176, Gradient Norm: 0.0145\n",
      "\u001b[1m176/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m26s\u001b[0m 468ms/step - accuracy: 0.4923 - binary_io_u_5: 0.4933 - loss: 0.9335Batch 177, Loss Value: 0.9469\n",
      "Batch 177, Gradient Norm: 0.0247\n",
      "\u001b[1m177/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m25s\u001b[0m 468ms/step - accuracy: 0.4923 - binary_io_u_5: 0.4933 - loss: 0.9334Batch 178, Loss Value: 0.9464\n",
      "Batch 178, Gradient Norm: 0.0225\n",
      "\u001b[1m178/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m25s\u001b[0m 468ms/step - accuracy: 0.4923 - binary_io_u_5: 0.4933 - loss: 0.9334Batch 179, Loss Value: 0.9462\n",
      "Batch 179, Gradient Norm: 0.0130\n",
      "\u001b[1m179/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m24s\u001b[0m 468ms/step - accuracy: 0.4923 - binary_io_u_5: 0.4933 - loss: 0.9334Batch 180, Loss Value: 0.9463\n",
      "Batch 180, Gradient Norm: 0.0310\n",
      "\u001b[1m180/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m24s\u001b[0m 468ms/step - accuracy: 0.4923 - binary_io_u_5: 0.4934 - loss: 0.9334Batch 181, Loss Value: 0.9463\n",
      "Batch 181, Gradient Norm: 0.0232\n",
      "\u001b[1m181/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m23s\u001b[0m 468ms/step - accuracy: 0.4923 - binary_io_u_5: 0.4934 - loss: 0.9334Batch 182, Loss Value: 0.9478\n",
      "Batch 182, Gradient Norm: 0.0263\n",
      "\u001b[1m182/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m23s\u001b[0m 468ms/step - accuracy: 0.4924 - binary_io_u_5: 0.4934 - loss: 0.9334Batch 183, Loss Value: 0.9480\n",
      "Batch 183, Gradient Norm: 0.0099\n",
      "\u001b[1m183/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m22s\u001b[0m 468ms/step - accuracy: 0.4924 - binary_io_u_5: 0.4934 - loss: 0.9333Batch 184, Loss Value: 0.9465\n",
      "Batch 184, Gradient Norm: 0.0128\n",
      "\u001b[1m184/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m22s\u001b[0m 468ms/step - accuracy: 0.4924 - binary_io_u_5: 0.4934 - loss: 0.9333Batch 185, Loss Value: 0.9470\n",
      "Batch 185, Gradient Norm: 0.0097\n",
      "\u001b[1m185/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m22s\u001b[0m 468ms/step - accuracy: 0.4924 - binary_io_u_5: 0.4934 - loss: 0.9333Batch 186, Loss Value: 0.9445\n",
      "Batch 186, Gradient Norm: 0.2259\n",
      "\u001b[1m186/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m21s\u001b[0m 468ms/step - accuracy: 0.4924 - binary_io_u_5: 0.4935 - loss: 0.9333Batch 187, Loss Value: 0.9478\n",
      "Batch 187, Gradient Norm: 0.0312\n",
      "\u001b[1m187/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m21s\u001b[0m 468ms/step - accuracy: 0.4924 - binary_io_u_5: 0.4935 - loss: 0.9333Batch 188, Loss Value: 0.9474\n",
      "Batch 188, Gradient Norm: 0.0424\n",
      "\u001b[1m188/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m20s\u001b[0m 468ms/step - accuracy: 0.4924 - binary_io_u_5: 0.4935 - loss: 0.9333Batch 189, Loss Value: 0.9472\n",
      "Batch 189, Gradient Norm: 0.0093\n",
      "\u001b[1m189/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m20s\u001b[0m 468ms/step - accuracy: 0.4924 - binary_io_u_5: 0.4935 - loss: 0.9332Batch 190, Loss Value: 0.9465\n",
      "Batch 190, Gradient Norm: 0.0070\n",
      "\u001b[1m190/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m19s\u001b[0m 468ms/step - accuracy: 0.4924 - binary_io_u_5: 0.4935 - loss: 0.9332Batch 191, Loss Value: 0.9467\n",
      "Batch 191, Gradient Norm: 0.0241\n",
      "\u001b[1m191/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m19s\u001b[0m 468ms/step - accuracy: 0.4925 - binary_io_u_5: 0.4935 - loss: 0.9332Batch 192, Loss Value: 0.9465\n",
      "Batch 192, Gradient Norm: 0.0060\n",
      "\u001b[1m192/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m18s\u001b[0m 468ms/step - accuracy: 0.4925 - binary_io_u_5: 0.4935 - loss: 0.9332Batch 193, Loss Value: 0.9468\n",
      "Batch 193, Gradient Norm: 0.0191\n",
      "\u001b[1m193/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m18s\u001b[0m 468ms/step - accuracy: 0.4925 - binary_io_u_5: 0.4935 - loss: 0.9332Batch 194, Loss Value: 0.9278\n",
      "Batch 194, Gradient Norm: 0.1588\n",
      "\u001b[1m194/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m17s\u001b[0m 468ms/step - accuracy: 0.4925 - binary_io_u_5: 0.4936 - loss: 0.9332Batch 195, Loss Value: 0.9248\n",
      "Batch 195, Gradient Norm: 0.2104\n",
      "\u001b[1m195/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m17s\u001b[0m 468ms/step - accuracy: 0.4925 - binary_io_u_5: 0.4936 - loss: 0.9331Batch 196, Loss Value: 0.9157\n",
      "Batch 196, Gradient Norm: 0.6013\n",
      "\u001b[1m196/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m16s\u001b[0m 468ms/step - accuracy: 0.4925 - binary_io_u_5: 0.4936 - loss: 0.9331Batch 197, Loss Value: 0.9466\n",
      "Batch 197, Gradient Norm: 0.0029\n",
      "\u001b[1m197/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m16s\u001b[0m 468ms/step - accuracy: 0.4925 - binary_io_u_5: 0.4936 - loss: 0.9331Batch 198, Loss Value: 0.9474\n",
      "Batch 198, Gradient Norm: 0.0300\n",
      "\u001b[1m198/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m15s\u001b[0m 468ms/step - accuracy: 0.4926 - binary_io_u_5: 0.4936 - loss: 0.9331Batch 199, Loss Value: 0.9426\n",
      "Batch 199, Gradient Norm: 1.1447\n",
      "\u001b[1m199/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m15s\u001b[0m 468ms/step - accuracy: 0.4926 - binary_io_u_5: 0.4936 - loss: 0.9331Batch 200, Loss Value: 0.9470\n",
      "Batch 200, Gradient Norm: 0.0206\n",
      "\u001b[1m200/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m14s\u001b[0m 468ms/step - accuracy: 0.4926 - binary_io_u_5: 0.4936 - loss: 0.9331Batch 201, Loss Value: 0.9474\n",
      "Batch 201, Gradient Norm: 0.0197\n",
      "\u001b[1m201/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m14s\u001b[0m 468ms/step - accuracy: 0.4926 - binary_io_u_5: 0.4937 - loss: 0.9330Batch 202, Loss Value: 0.9478\n",
      "Batch 202, Gradient Norm: 0.0219\n",
      "\u001b[1m202/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m14s\u001b[0m 468ms/step - accuracy: 0.4926 - binary_io_u_5: 0.4937 - loss: 0.9330Batch 203, Loss Value: 0.9468\n",
      "Batch 203, Gradient Norm: 0.0117\n",
      "\u001b[1m203/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m13s\u001b[0m 468ms/step - accuracy: 0.4926 - binary_io_u_5: 0.4937 - loss: 0.9330Batch 204, Loss Value: 0.9470\n",
      "Batch 204, Gradient Norm: 0.0362\n",
      "\u001b[1m204/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m13s\u001b[0m 468ms/step - accuracy: 0.4926 - binary_io_u_5: 0.4937 - loss: 0.9330Batch 205, Loss Value: 0.9483\n",
      "Batch 205, Gradient Norm: 0.0569\n",
      "\u001b[1m205/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m12s\u001b[0m 468ms/step - accuracy: 0.4926 - binary_io_u_5: 0.4937 - loss: 0.9330Batch 206, Loss Value: 0.9463\n",
      "Batch 206, Gradient Norm: 0.0009\n",
      "\u001b[1m206/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m12s\u001b[0m 468ms/step - accuracy: 0.4926 - binary_io_u_5: 0.4937 - loss: 0.9330Batch 207, Loss Value: 0.9312\n",
      "Batch 207, Gradient Norm: 0.0589\n",
      "\u001b[1m207/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m11s\u001b[0m 468ms/step - accuracy: 0.4927 - binary_io_u_5: 0.4937 - loss: 0.9330Batch 208, Loss Value: 0.9473\n",
      "Batch 208, Gradient Norm: 0.0549\n",
      "\u001b[1m208/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m11s\u001b[0m 468ms/step - accuracy: 0.4927 - binary_io_u_5: 0.4937 - loss: 0.9329Batch 209, Loss Value: 0.9391\n",
      "Batch 209, Gradient Norm: 0.4353\n",
      "\u001b[1m209/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m10s\u001b[0m 468ms/step - accuracy: 0.4927 - binary_io_u_5: 0.4938 - loss: 0.9329Batch 210, Loss Value: 0.9473\n",
      "Batch 210, Gradient Norm: 0.0112\n",
      "\u001b[1m210/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m10s\u001b[0m 467ms/step - accuracy: 0.4927 - binary_io_u_5: 0.4938 - loss: 0.9329Batch 211, Loss Value: 0.9297\n",
      "Batch 211, Gradient Norm: 0.5068\n",
      "\u001b[1m211/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m9s\u001b[0m 467ms/step - accuracy: 0.4927 - binary_io_u_5: 0.4938 - loss: 0.9329 Batch 212, Loss Value: 0.9460\n",
      "Batch 212, Gradient Norm: 0.0038\n",
      "\u001b[1m212/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m9s\u001b[0m 467ms/step - accuracy: 0.4927 - binary_io_u_5: 0.4938 - loss: 0.9329Batch 213, Loss Value: 0.9467\n",
      "Batch 213, Gradient Norm: 0.0177\n",
      "\u001b[1m213/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m8s\u001b[0m 467ms/step - accuracy: 0.4927 - binary_io_u_5: 0.4938 - loss: 0.9329Batch 214, Loss Value: 0.9470\n",
      "Batch 214, Gradient Norm: 0.0026\n",
      "\u001b[1m214/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m8s\u001b[0m 467ms/step - accuracy: 0.4927 - binary_io_u_5: 0.4938 - loss: 0.9329Batch 215, Loss Value: 0.9477\n",
      "Batch 215, Gradient Norm: 0.0118\n",
      "\u001b[1m215/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m7s\u001b[0m 467ms/step - accuracy: 0.4927 - binary_io_u_5: 0.4938 - loss: 0.9329Batch 216, Loss Value: 0.9487\n",
      "Batch 216, Gradient Norm: 0.0049\n",
      "\u001b[1m216/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m7s\u001b[0m 467ms/step - accuracy: 0.4927 - binary_io_u_5: 0.4938 - loss: 0.9328Batch 217, Loss Value: 0.8644\n",
      "Batch 217, Gradient Norm: 0.3768\n",
      "\u001b[1m217/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m7s\u001b[0m 467ms/step - accuracy: 0.4927 - binary_io_u_5: 0.4938 - loss: 0.9328Batch 218, Loss Value: 0.9467\n",
      "Batch 218, Gradient Norm: 0.0040\n",
      "\u001b[1m218/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m6s\u001b[0m 467ms/step - accuracy: 0.4927 - binary_io_u_5: 0.4938 - loss: 0.9328Batch 219, Loss Value: 0.9245\n",
      "Batch 219, Gradient Norm: 0.6344\n",
      "\u001b[1m219/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m6s\u001b[0m 467ms/step - accuracy: 0.4927 - binary_io_u_5: 0.4938 - loss: 0.9328Batch 220, Loss Value: 0.9483\n",
      "Batch 220, Gradient Norm: 0.0362\n",
      "\u001b[1m220/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m5s\u001b[0m 467ms/step - accuracy: 0.4927 - binary_io_u_5: 0.4938 - loss: 0.9328Batch 221, Loss Value: 0.9494\n",
      "Batch 221, Gradient Norm: 0.0137\n",
      "\u001b[1m221/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m5s\u001b[0m 467ms/step - accuracy: 0.4927 - binary_io_u_5: 0.4938 - loss: 0.9328Batch 222, Loss Value: 0.9407\n",
      "Batch 222, Gradient Norm: 0.4395\n",
      "\u001b[1m222/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m4s\u001b[0m 467ms/step - accuracy: 0.4927 - binary_io_u_5: 0.4938 - loss: 0.9328Batch 223, Loss Value: 0.9339\n",
      "Batch 223, Gradient Norm: 0.3641\n",
      "\u001b[1m223/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m4s\u001b[0m 467ms/step - accuracy: 0.4927 - binary_io_u_5: 0.4938 - loss: 0.9328Batch 224, Loss Value: 0.8950\n",
      "Batch 224, Gradient Norm: 0.4550\n",
      "\u001b[1m224/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m3s\u001b[0m 467ms/step - accuracy: 0.4927 - binary_io_u_5: 0.4938 - loss: 0.9328Batch 225, Loss Value: 0.9235\n",
      "Batch 225, Gradient Norm: 0.5648\n",
      "\u001b[1m225/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m3s\u001b[0m 467ms/step - accuracy: 0.4927 - binary_io_u_5: 0.4938 - loss: 0.9327Batch 226, Loss Value: 0.9155\n",
      "Batch 226, Gradient Norm: 0.3362\n",
      "\u001b[1m226/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m2s\u001b[0m 467ms/step - accuracy: 0.4927 - binary_io_u_5: 0.4938 - loss: 0.9327Batch 227, Loss Value: 0.9365\n",
      "Batch 227, Gradient Norm: 0.2657\n",
      "\u001b[1m227/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m2s\u001b[0m 467ms/step - accuracy: 0.4927 - binary_io_u_5: 0.4938 - loss: 0.9327Batch 228, Loss Value: 0.9323\n",
      "Batch 228, Gradient Norm: 0.4696\n",
      "\u001b[1m228/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m1s\u001b[0m 467ms/step - accuracy: 0.4927 - binary_io_u_5: 0.4938 - loss: 0.9327Batch 229, Loss Value: 0.9420\n",
      "Batch 229, Gradient Norm: 0.4648\n",
      "\u001b[1m229/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m1s\u001b[0m 467ms/step - accuracy: 0.4927 - binary_io_u_5: 0.4939 - loss: 0.9327Batch 230, Loss Value: 0.9426\n",
      "Batch 230, Gradient Norm: 0.5613\n",
      "\u001b[1m230/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 467ms/step - accuracy: 0.4927 - binary_io_u_5: 0.4939 - loss: 0.9327Batch 231, Loss Value: 0.9468\n",
      "Batch 231, Gradient Norm: 0.0096\n",
      "\u001b[1m231/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 467ms/step - accuracy: 0.4927 - binary_io_u_5: 0.4939 - loss: 0.9327Batch 232, Loss Value: 0.9043\n",
      "Batch 232, Gradient Norm: 0.4901\n",
      "\u001b[1m232/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m129s\u001b[0m 481ms/step - accuracy: 0.4928 - binary_io_u_5: 0.4939 - loss: 0.9327 - val_accuracy: 0.4973 - val_binary_io_u_5: 0.4949 - val_loss: 0.9248 - learning_rate: 0.0010\n",
      "Epoch 2/200\n",
      "Batch 1, Loss Value: 0.8553\n",
      "Batch 1, Gradient Norm: 0.6143\n",
      "\u001b[1m  1/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:58:35\u001b[0m 31s/step - accuracy: 0.4800 - binary_io_u_5: 0.4600 - loss: 0.9047Batch 2, Loss Value: 0.9283\n",
      "Batch 2, Gradient Norm: 0.1257\n",
      "\u001b[1m  2/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:49\u001b[0m 475ms/step - accuracy: 0.4625 - binary_io_u_5: 0.4575 - loss: 0.9155 Batch 3, Loss Value: 0.9434\n",
      "Batch 3, Gradient Norm: 0.0184\n",
      "\u001b[1m  3/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:49\u001b[0m 477ms/step - accuracy: 0.4628 - binary_io_u_5: 0.4694 - loss: 0.9205Batch 4, Loss Value: 0.8909\n",
      "Batch 4, Gradient Norm: 0.4706\n",
      "\u001b[1m  4/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:49\u001b[0m 480ms/step - accuracy: 0.4583 - binary_io_u_5: 0.4696 - loss: 0.9243Batch 5, Loss Value: 0.9001\n",
      "Batch 5, Gradient Norm: 0.8016\n",
      "\u001b[1m  5/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:49\u001b[0m 480ms/step - accuracy: 0.4567 - binary_io_u_5: 0.4701 - loss: 0.9270Batch 6, Loss Value: 0.9432\n",
      "Batch 6, Gradient Norm: 0.0688\n",
      "\u001b[1m  6/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:48\u001b[0m 480ms/step - accuracy: 0.4572 - binary_io_u_5: 0.4717 - loss: 0.9286Batch 7, Loss Value: 0.9445\n",
      "Batch 7, Gradient Norm: 0.0266\n",
      "\u001b[1m  7/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:47\u001b[0m 478ms/step - accuracy: 0.4588 - binary_io_u_5: 0.4725 - loss: 0.9293Batch 8, Loss Value: 0.9446\n",
      "Batch 8, Gradient Norm: 0.0216\n",
      "\u001b[1m  8/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:46\u001b[0m 477ms/step - accuracy: 0.4593 - binary_io_u_5: 0.4719 - loss: 0.9302Batch 9, Loss Value: 0.8803\n",
      "Batch 9, Gradient Norm: 0.2912\n",
      "\u001b[1m  9/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:46\u001b[0m 477ms/step - accuracy: 0.4601 - binary_io_u_5: 0.4724 - loss: 0.9306Batch 10, Loss Value: 0.9157\n",
      "Batch 10, Gradient Norm: 0.7649\n",
      "\u001b[1m 10/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:45\u001b[0m 477ms/step - accuracy: 0.4611 - binary_io_u_5: 0.4731 - loss: 0.9305Batch 11, Loss Value: 0.9437\n",
      "Batch 11, Gradient Norm: 0.0286\n",
      "\u001b[1m 11/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:45\u001b[0m 477ms/step - accuracy: 0.4625 - binary_io_u_5: 0.4744 - loss: 0.9302Batch 12, Loss Value: 0.9368\n",
      "Batch 12, Gradient Norm: 0.4838\n",
      "\u001b[1m 12/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:45\u001b[0m 478ms/step - accuracy: 0.4636 - binary_io_u_5: 0.4753 - loss: 0.9299Batch 13, Loss Value: 0.9161\n",
      "Batch 13, Gradient Norm: 0.0050\n",
      "\u001b[1m 13/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:44\u001b[0m 479ms/step - accuracy: 0.4647 - binary_io_u_5: 0.4760 - loss: 0.9295Batch 14, Loss Value: 0.9372\n",
      "Batch 14, Gradient Norm: 0.7174\n",
      "\u001b[1m 14/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:44\u001b[0m 479ms/step - accuracy: 0.4659 - binary_io_u_5: 0.4768 - loss: 0.9291Batch 15, Loss Value: 0.9259\n",
      "Batch 15, Gradient Norm: 0.0208\n",
      "\u001b[1m 15/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:43\u001b[0m 479ms/step - accuracy: 0.4668 - binary_io_u_5: 0.4773 - loss: 0.9289Batch 16, Loss Value: 0.9433\n",
      "Batch 16, Gradient Norm: 0.0053\n",
      "\u001b[1m 16/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:43\u001b[0m 479ms/step - accuracy: 0.4674 - binary_io_u_5: 0.4775 - loss: 0.9287Batch 17, Loss Value: 0.8880\n",
      "Batch 17, Gradient Norm: 0.2815\n",
      "\u001b[1m 17/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:43\u001b[0m 480ms/step - accuracy: 0.4677 - binary_io_u_5: 0.4778 - loss: 0.9287Batch 18, Loss Value: 0.9263\n",
      "Batch 18, Gradient Norm: 0.2772\n",
      "\u001b[1m 18/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:42\u001b[0m 480ms/step - accuracy: 0.4680 - binary_io_u_5: 0.4782 - loss: 0.9287Batch 19, Loss Value: 0.9043\n",
      "Batch 19, Gradient Norm: 0.6111\n",
      "\u001b[1m 19/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:42\u001b[0m 481ms/step - accuracy: 0.4683 - binary_io_u_5: 0.4785 - loss: 0.9287Batch 20, Loss Value: 0.8724\n",
      "Batch 20, Gradient Norm: 0.0695\n",
      "\u001b[1m 20/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:42\u001b[0m 481ms/step - accuracy: 0.4684 - binary_io_u_5: 0.4788 - loss: 0.9287Batch 21, Loss Value: 0.9449\n",
      "Batch 21, Gradient Norm: 0.0094\n",
      "\u001b[1m 21/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:41\u001b[0m 481ms/step - accuracy: 0.4684 - binary_io_u_5: 0.4791 - loss: 0.9288Batch 22, Loss Value: 0.9165\n",
      "Batch 22, Gradient Norm: 0.2531\n",
      "\u001b[1m 22/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:41\u001b[0m 482ms/step - accuracy: 0.4685 - binary_io_u_5: 0.4794 - loss: 0.9289Batch 23, Loss Value: 0.9093\n",
      "Batch 23, Gradient Norm: 0.5195\n",
      "\u001b[1m 23/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:40\u001b[0m 482ms/step - accuracy: 0.4684 - binary_io_u_5: 0.4795 - loss: 0.9290Batch 24, Loss Value: 0.8833\n",
      "Batch 24, Gradient Norm: 0.0293\n",
      "\u001b[1m 24/232\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:40\u001b[0m 482ms/step - accuracy: 0.4683 - binary_io_u_5: 0.4798 - loss: 0.9291Batch 25, Loss Value: 0.9134\n",
      "Batch 25, Gradient Norm: 0.0435\n",
      "\u001b[1m 25/232\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:39\u001b[0m 482ms/step - accuracy: 0.4683 - binary_io_u_5: 0.4799 - loss: 0.9292Batch 26, Loss Value: 0.9097\n",
      "Batch 26, Gradient Norm: 0.7594\n",
      "\u001b[1m 26/232\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:39\u001b[0m 482ms/step - accuracy: 0.4681 - binary_io_u_5: 0.4801 - loss: 0.9293Batch 27, Loss Value: 0.9441\n",
      "Batch 27, Gradient Norm: 0.0539\n",
      "\u001b[1m 27/232\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:38\u001b[0m 483ms/step - accuracy: 0.4682 - binary_io_u_5: 0.4804 - loss: 0.9293Batch 28, Loss Value: 0.8985\n",
      "Batch 28, Gradient Norm: 0.5103\n",
      "\u001b[1m 28/232\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:38\u001b[0m 483ms/step - accuracy: 0.4683 - binary_io_u_5: 0.4806 - loss: 0.9293Batch 29, Loss Value: 0.8917\n",
      "Batch 29, Gradient Norm: 0.9931\n",
      "\u001b[1m 29/232\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:38\u001b[0m 483ms/step - accuracy: 0.4685 - binary_io_u_5: 0.4809 - loss: 0.9293Batch 30, Loss Value: 0.9438\n",
      "Batch 30, Gradient Norm: 0.0132\n",
      "\u001b[1m 30/232\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:37\u001b[0m 483ms/step - accuracy: 0.4687 - binary_io_u_5: 0.4811 - loss: 0.9293Batch 31, Loss Value: 0.9219\n",
      "Batch 31, Gradient Norm: 0.8915\n",
      "\u001b[1m 31/232\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:37\u001b[0m 483ms/step - accuracy: 0.4689 - binary_io_u_5: 0.4814 - loss: 0.9293Batch 32, Loss Value: 0.9332\n",
      "Batch 32, Gradient Norm: 0.0616\n",
      "\u001b[1m 32/232\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:36\u001b[0m 483ms/step - accuracy: 0.4692 - binary_io_u_5: 0.4817 - loss: 0.9292Batch 33, Loss Value: 0.9325\n",
      "Batch 33, Gradient Norm: 0.2001\n",
      "\u001b[1m 33/232\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:36\u001b[0m 483ms/step - accuracy: 0.4694 - binary_io_u_5: 0.4819 - loss: 0.9291Batch 34, Loss Value: 0.9444\n",
      "Batch 34, Gradient Norm: 0.0051\n",
      "\u001b[1m 34/232\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:35\u001b[0m 483ms/step - accuracy: 0.4697 - binary_io_u_5: 0.4822 - loss: 0.9291Batch 35, Loss Value: 0.9421\n",
      "Batch 35, Gradient Norm: 1.9468\n",
      "\u001b[1m 35/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:35\u001b[0m 483ms/step - accuracy: 0.4701 - binary_io_u_5: 0.4824 - loss: 0.9290Batch 36, Loss Value: 0.9421\n",
      "Batch 36, Gradient Norm: 0.6747\n",
      "\u001b[1m 36/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:34\u001b[0m 484ms/step - accuracy: 0.4704 - binary_io_u_5: 0.4827 - loss: 0.9288Batch 37, Loss Value: 0.8979\n",
      "Batch 37, Gradient Norm: 0.1277\n",
      "\u001b[1m 37/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:34\u001b[0m 484ms/step - accuracy: 0.4708 - binary_io_u_5: 0.4829 - loss: 0.9287Batch 38, Loss Value: 0.9444\n",
      "Batch 38, Gradient Norm: 0.0234\n",
      "\u001b[1m 38/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:33\u001b[0m 484ms/step - accuracy: 0.4711 - binary_io_u_5: 0.4831 - loss: 0.9286Batch 39, Loss Value: 0.9175\n",
      "Batch 39, Gradient Norm: 0.3356\n",
      "\u001b[1m 39/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:33\u001b[0m 484ms/step - accuracy: 0.4715 - binary_io_u_5: 0.4833 - loss: 0.9285Batch 40, Loss Value: 0.9180\n",
      "Batch 40, Gradient Norm: 0.0914\n",
      "\u001b[1m 40/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:32\u001b[0m 484ms/step - accuracy: 0.4718 - binary_io_u_5: 0.4834 - loss: 0.9285Batch 41, Loss Value: 0.9331\n",
      "Batch 41, Gradient Norm: 0.7596\n",
      "\u001b[1m 41/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:32\u001b[0m 484ms/step - accuracy: 0.4721 - binary_io_u_5: 0.4836 - loss: 0.9284Batch 42, Loss Value: 0.9181\n",
      "Batch 42, Gradient Norm: 0.5499\n",
      "\u001b[1m 42/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:31\u001b[0m 484ms/step - accuracy: 0.4723 - binary_io_u_5: 0.4838 - loss: 0.9283Batch 43, Loss Value: 0.9217\n",
      "Batch 43, Gradient Norm: 1.0385\n",
      "\u001b[1m 43/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:31\u001b[0m 484ms/step - accuracy: 0.4726 - binary_io_u_5: 0.4839 - loss: 0.9283Batch 44, Loss Value: 0.9033\n",
      "Batch 44, Gradient Norm: 0.0005\n",
      "\u001b[1m 44/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:31\u001b[0m 484ms/step - accuracy: 0.4728 - binary_io_u_5: 0.4840 - loss: 0.9282Batch 45, Loss Value: 0.9429\n",
      "Batch 45, Gradient Norm: 0.9091\n",
      "\u001b[1m 45/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:30\u001b[0m 484ms/step - accuracy: 0.4730 - binary_io_u_5: 0.4841 - loss: 0.9282Batch 46, Loss Value: 0.9307\n",
      "Batch 46, Gradient Norm: 0.1610\n",
      "\u001b[1m 46/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:30\u001b[0m 484ms/step - accuracy: 0.4732 - binary_io_u_5: 0.4842 - loss: 0.9282Batch 47, Loss Value: 0.9349\n",
      "Batch 47, Gradient Norm: 0.9777\n",
      "\u001b[1m 47/232\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:29\u001b[0m 484ms/step - accuracy: 0.4734 - binary_io_u_5: 0.4843 - loss: 0.9282Batch 48, Loss Value: 0.8888\n",
      "Batch 48, Gradient Norm: 0.3182\n",
      "\u001b[1m 48/232\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:29\u001b[0m 484ms/step - accuracy: 0.4736 - binary_io_u_5: 0.4844 - loss: 0.9281Batch 49, Loss Value: 0.9435\n",
      "Batch 49, Gradient Norm: 0.0013\n",
      "\u001b[1m 49/232\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:28\u001b[0m 484ms/step - accuracy: 0.4738 - binary_io_u_5: 0.4844 - loss: 0.9281Batch 50, Loss Value: 0.8967\n",
      "Batch 50, Gradient Norm: 0.1195\n",
      "\u001b[1m 50/232\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:28\u001b[0m 484ms/step - accuracy: 0.4740 - binary_io_u_5: 0.4845 - loss: 0.9280Batch 51, Loss Value: 0.9441\n",
      "Batch 51, Gradient Norm: 0.0150\n",
      "\u001b[1m 51/232\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:27\u001b[0m 484ms/step - accuracy: 0.4742 - binary_io_u_5: 0.4846 - loss: 0.9280Batch 52, Loss Value: 0.8845\n",
      "Batch 52, Gradient Norm: 0.3986\n",
      "\u001b[1m 52/232\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:27\u001b[0m 485ms/step - accuracy: 0.4743 - binary_io_u_5: 0.4846 - loss: 0.9279Batch 53, Loss Value: 0.9043\n",
      "Batch 53, Gradient Norm: 0.4078\n",
      "\u001b[1m 53/232\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:26\u001b[0m 485ms/step - accuracy: 0.4746 - binary_io_u_5: 0.4847 - loss: 0.9279Batch 54, Loss Value: 0.9436\n",
      "Batch 54, Gradient Norm: 0.0255\n",
      "\u001b[1m 54/232\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:26\u001b[0m 485ms/step - accuracy: 0.4748 - binary_io_u_5: 0.4847 - loss: 0.9279Batch 55, Loss Value: 0.9439\n",
      "Batch 55, Gradient Norm: 0.0094\n",
      "\u001b[1m 55/232\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:25\u001b[0m 485ms/step - accuracy: 0.4749 - binary_io_u_5: 0.4848 - loss: 0.9278Batch 56, Loss Value: 0.8788\n",
      "Batch 56, Gradient Norm: 0.6665\n",
      "\u001b[1m 56/232\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:25\u001b[0m 485ms/step - accuracy: 0.4751 - binary_io_u_5: 0.4848 - loss: 0.9278Batch 57, Loss Value: 0.9438\n",
      "Batch 57, Gradient Norm: 0.0137\n",
      "\u001b[1m 57/232\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:24\u001b[0m 485ms/step - accuracy: 0.4753 - binary_io_u_5: 0.4848 - loss: 0.9278Batch 58, Loss Value: 0.9403\n",
      "Batch 58, Gradient Norm: 0.3397\n",
      "\u001b[1m 58/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:24\u001b[0m 485ms/step - accuracy: 0.4754 - binary_io_u_5: 0.4848 - loss: 0.9278Batch 59, Loss Value: 0.9339\n",
      "Batch 59, Gradient Norm: 0.6153\n",
      "\u001b[1m 59/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:23\u001b[0m 485ms/step - accuracy: 0.4756 - binary_io_u_5: 0.4848 - loss: 0.9278Batch 60, Loss Value: 0.9431\n",
      "Batch 60, Gradient Norm: 0.0546\n",
      "\u001b[1m 60/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:23\u001b[0m 485ms/step - accuracy: 0.4757 - binary_io_u_5: 0.4849 - loss: 0.9277Batch 61, Loss Value: 0.9047\n",
      "Batch 61, Gradient Norm: 0.0103\n",
      "\u001b[1m 61/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:22\u001b[0m 485ms/step - accuracy: 0.4759 - binary_io_u_5: 0.4849 - loss: 0.9277Batch 62, Loss Value: 0.9435\n",
      "Batch 62, Gradient Norm: 0.0009\n",
      "\u001b[1m 62/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:22\u001b[0m 485ms/step - accuracy: 0.4760 - binary_io_u_5: 0.4849 - loss: 0.9277Batch 63, Loss Value: 0.9309\n",
      "Batch 63, Gradient Norm: 0.6371\n",
      "\u001b[1m 63/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:21\u001b[0m 485ms/step - accuracy: 0.4761 - binary_io_u_5: 0.4849 - loss: 0.9277Batch 64, Loss Value: 0.8855\n",
      "Batch 64, Gradient Norm: 0.6957\n",
      "\u001b[1m 64/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:21\u001b[0m 485ms/step - accuracy: 0.4763 - binary_io_u_5: 0.4850 - loss: 0.9277Batch 65, Loss Value: 0.9447\n",
      "Batch 65, Gradient Norm: 0.0548\n",
      "\u001b[1m 65/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:20\u001b[0m 485ms/step - accuracy: 0.4764 - binary_io_u_5: 0.4851 - loss: 0.9276Batch 66, Loss Value: 0.9441\n",
      "Batch 66, Gradient Norm: 0.0312\n",
      "\u001b[1m 66/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:20\u001b[0m 485ms/step - accuracy: 0.4765 - binary_io_u_5: 0.4851 - loss: 0.9276Batch 67, Loss Value: 0.9451\n",
      "Batch 67, Gradient Norm: 0.0506\n",
      "\u001b[1m 67/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:20\u001b[0m 485ms/step - accuracy: 0.4766 - binary_io_u_5: 0.4852 - loss: 0.9276Batch 68, Loss Value: 0.9239\n",
      "Batch 68, Gradient Norm: 0.7750\n",
      "\u001b[1m 68/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:19\u001b[0m 485ms/step - accuracy: 0.4768 - binary_io_u_5: 0.4852 - loss: 0.9276Batch 69, Loss Value: 0.9314\n",
      "Batch 69, Gradient Norm: 0.1422\n",
      "\u001b[1m 69/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:19\u001b[0m 485ms/step - accuracy: 0.4769 - binary_io_u_5: 0.4853 - loss: 0.9276Batch 70, Loss Value: 0.9121\n",
      "Batch 70, Gradient Norm: 0.2129\n",
      "\u001b[1m 70/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:18\u001b[0m 485ms/step - accuracy: 0.4771 - binary_io_u_5: 0.4854 - loss: 0.9275Batch 71, Loss Value: 0.9036\n",
      "Batch 71, Gradient Norm: 0.8355\n",
      "\u001b[1m 71/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:18\u001b[0m 485ms/step - accuracy: 0.4772 - binary_io_u_5: 0.4855 - loss: 0.9275Batch 72, Loss Value: 0.9438\n",
      "Batch 72, Gradient Norm: 0.0217\n",
      "\u001b[1m 72/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:17\u001b[0m 485ms/step - accuracy: 0.4774 - binary_io_u_5: 0.4856 - loss: 0.9275Batch 73, Loss Value: 0.9442\n",
      "Batch 73, Gradient Norm: 0.0079\n",
      "\u001b[1m 73/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:17\u001b[0m 485ms/step - accuracy: 0.4775 - binary_io_u_5: 0.4856 - loss: 0.9275Batch 74, Loss Value: 0.9406\n",
      "Batch 74, Gradient Norm: 0.8499\n",
      "\u001b[1m 74/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:16\u001b[0m 485ms/step - accuracy: 0.4777 - binary_io_u_5: 0.4857 - loss: 0.9274Batch 75, Loss Value: 0.9229\n",
      "Batch 75, Gradient Norm: 0.3832\n",
      "\u001b[1m 75/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:16\u001b[0m 485ms/step - accuracy: 0.4778 - binary_io_u_5: 0.4858 - loss: 0.9274Batch 76, Loss Value: 0.9441\n",
      "Batch 76, Gradient Norm: 0.0002\n",
      "\u001b[1m 76/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:15\u001b[0m 485ms/step - accuracy: 0.4779 - binary_io_u_5: 0.4858 - loss: 0.9274Batch 77, Loss Value: 0.9441\n",
      "Batch 77, Gradient Norm: 0.0112\n",
      "\u001b[1m 77/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:15\u001b[0m 485ms/step - accuracy: 0.4781 - binary_io_u_5: 0.4859 - loss: 0.9274Batch 78, Loss Value: 0.9445\n",
      "Batch 78, Gradient Norm: 0.0030\n",
      "\u001b[1m 78/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:14\u001b[0m 485ms/step - accuracy: 0.4782 - binary_io_u_5: 0.4860 - loss: 0.9273Batch 79, Loss Value: 0.9124\n",
      "Batch 79, Gradient Norm: 0.2474\n",
      "\u001b[1m 79/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:14\u001b[0m 485ms/step - accuracy: 0.4783 - binary_io_u_5: 0.4860 - loss: 0.9273Batch 80, Loss Value: 0.9423\n",
      "Batch 80, Gradient Norm: 1.4946\n",
      "\u001b[1m 80/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:13\u001b[0m 485ms/step - accuracy: 0.4785 - binary_io_u_5: 0.4861 - loss: 0.9273Batch 81, Loss Value: 0.9175\n",
      "Batch 81, Gradient Norm: 0.0003\n",
      "\u001b[1m 81/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:13\u001b[0m 485ms/step - accuracy: 0.4786 - binary_io_u_5: 0.4861 - loss: 0.9272Batch 82, Loss Value: 0.9408\n",
      "Batch 82, Gradient Norm: 0.3638\n",
      "\u001b[1m 82/232\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m1:12\u001b[0m 485ms/step - accuracy: 0.4787 - binary_io_u_5: 0.4862 - loss: 0.9272Batch 83, Loss Value: 0.9449\n",
      "Batch 83, Gradient Norm: 0.0105\n",
      "\u001b[1m 83/232\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m1:12\u001b[0m 485ms/step - accuracy: 0.4789 - binary_io_u_5: 0.4862 - loss: 0.9272Batch 84, Loss Value: 0.9440\n",
      "Batch 84, Gradient Norm: 0.0173\n",
      "\u001b[1m 84/232\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m1:11\u001b[0m 485ms/step - accuracy: 0.4790 - binary_io_u_5: 0.4863 - loss: 0.9271Batch 85, Loss Value: 0.9089\n",
      "Batch 85, Gradient Norm: 0.4241\n",
      "\u001b[1m 85/232\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m1:11\u001b[0m 485ms/step - accuracy: 0.4792 - binary_io_u_5: 0.4864 - loss: 0.9271Batch 86, Loss Value: 0.9232\n",
      "Batch 86, Gradient Norm: 0.1494\n",
      "\u001b[1m 86/232\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m1:10\u001b[0m 485ms/step - accuracy: 0.4793 - binary_io_u_5: 0.4864 - loss: 0.9271Batch 87, Loss Value: 0.9445\n",
      "Batch 87, Gradient Norm: 0.0063\n",
      "\u001b[1m 87/232\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m1:10\u001b[0m 485ms/step - accuracy: 0.4794 - binary_io_u_5: 0.4865 - loss: 0.9270Batch 88, Loss Value: 0.9440\n",
      "Batch 88, Gradient Norm: 0.0030\n",
      "\u001b[1m 88/232\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m1:09\u001b[0m 485ms/step - accuracy: 0.4796 - binary_io_u_5: 0.4866 - loss: 0.9270Batch 89, Loss Value: 0.9306\n",
      "Batch 89, Gradient Norm: 0.2112\n",
      "\u001b[1m 89/232\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m1:09\u001b[0m 485ms/step - accuracy: 0.4797 - binary_io_u_5: 0.4866 - loss: 0.9270Batch 90, Loss Value: 0.9100\n",
      "Batch 90, Gradient Norm: 0.4168\n",
      "\u001b[1m 90/232\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m1:08\u001b[0m 485ms/step - accuracy: 0.4799 - binary_io_u_5: 0.4867 - loss: 0.9270Batch 91, Loss Value: 0.9441\n",
      "Batch 91, Gradient Norm: 0.0044\n",
      "\u001b[1m 91/232\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m1:08\u001b[0m 485ms/step - accuracy: 0.4800 - binary_io_u_5: 0.4868 - loss: 0.9269Batch 92, Loss Value: 0.8883\n",
      "Batch 92, Gradient Norm: 0.3541\n",
      "\u001b[1m 92/232\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m1:07\u001b[0m 485ms/step - accuracy: 0.4801 - binary_io_u_5: 0.4868 - loss: 0.9269Batch 93, Loss Value: 0.9445\n",
      "Batch 93, Gradient Norm: 0.0042\n",
      "\u001b[1m 93/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m1:07\u001b[0m 485ms/step - accuracy: 0.4803 - binary_io_u_5: 0.4869 - loss: 0.9269Batch 94, Loss Value: 0.9448\n",
      "Batch 94, Gradient Norm: 0.0053\n",
      "\u001b[1m 94/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m1:06\u001b[0m 485ms/step - accuracy: 0.4804 - binary_io_u_5: 0.4870 - loss: 0.9269Batch 95, Loss Value: 0.9443\n",
      "Batch 95, Gradient Norm: 0.0121\n",
      "\u001b[1m 95/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m1:06\u001b[0m 485ms/step - accuracy: 0.4805 - binary_io_u_5: 0.4870 - loss: 0.9268Batch 96, Loss Value: 0.9277\n",
      "Batch 96, Gradient Norm: 0.9166\n",
      "\u001b[1m 96/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m1:05\u001b[0m 485ms/step - accuracy: 0.4807 - binary_io_u_5: 0.4871 - loss: 0.9268Batch 97, Loss Value: 0.8890\n",
      "Batch 97, Gradient Norm: 0.3370\n",
      "\u001b[1m 97/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m1:05\u001b[0m 485ms/step - accuracy: 0.4808 - binary_io_u_5: 0.4872 - loss: 0.9268Batch 98, Loss Value: 0.9443\n",
      "Batch 98, Gradient Norm: 0.0278\n",
      "\u001b[1m 98/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m1:04\u001b[0m 485ms/step - accuracy: 0.4809 - binary_io_u_5: 0.4872 - loss: 0.9268Batch 99, Loss Value: 0.9146\n",
      "Batch 99, Gradient Norm: 0.0092\n",
      "\u001b[1m 99/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m1:04\u001b[0m 485ms/step - accuracy: 0.4811 - binary_io_u_5: 0.4873 - loss: 0.9267Batch 100, Loss Value: 0.9080\n",
      "Batch 100, Gradient Norm: 0.5218\n",
      "\u001b[1m100/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m1:03\u001b[0m 485ms/step - accuracy: 0.4812 - binary_io_u_5: 0.4873 - loss: 0.9267Batch 101, Loss Value: 0.9292\n",
      "Batch 101, Gradient Norm: 0.3462\n",
      "\u001b[1m101/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m1:03\u001b[0m 485ms/step - accuracy: 0.4813 - binary_io_u_5: 0.4874 - loss: 0.9267Batch 102, Loss Value: 0.9444\n",
      "Batch 102, Gradient Norm: 0.0001\n",
      "\u001b[1m102/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m1:03\u001b[0m 485ms/step - accuracy: 0.4814 - binary_io_u_5: 0.4874 - loss: 0.9267Batch 103, Loss Value: 0.9437\n",
      "Batch 103, Gradient Norm: 0.0655\n",
      "\u001b[1m103/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m1:02\u001b[0m 485ms/step - accuracy: 0.4815 - binary_io_u_5: 0.4875 - loss: 0.9267Batch 104, Loss Value: 0.9434\n",
      "Batch 104, Gradient Norm: 0.6855\n",
      "\u001b[1m104/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m1:02\u001b[0m 485ms/step - accuracy: 0.4816 - binary_io_u_5: 0.4875 - loss: 0.9266Batch 105, Loss Value: 0.9038\n",
      "Batch 105, Gradient Norm: 0.2472\n",
      "\u001b[1m105/232\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m1:01\u001b[0m 485ms/step - accuracy: 0.4817 - binary_io_u_5: 0.4876 - loss: 0.9266Batch 106, Loss Value: 0.9329\n",
      "Batch 106, Gradient Norm: 0.4447\n",
      "\u001b[1m106/232\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m1:01\u001b[0m 485ms/step - accuracy: 0.4818 - binary_io_u_5: 0.4876 - loss: 0.9266Batch 107, Loss Value: 0.9451\n",
      "Batch 107, Gradient Norm: 0.0008\n",
      "\u001b[1m107/232\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m1:00\u001b[0m 485ms/step - accuracy: 0.4820 - binary_io_u_5: 0.4877 - loss: 0.9266Batch 108, Loss Value: 0.9440\n",
      "Batch 108, Gradient Norm: 0.0135\n",
      "\u001b[1m108/232\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m1:00\u001b[0m 485ms/step - accuracy: 0.4821 - binary_io_u_5: 0.4877 - loss: 0.9265Batch 109, Loss Value: 0.9446\n",
      "Batch 109, Gradient Norm: 0.0304\n",
      "\u001b[1m109/232\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m59s\u001b[0m 485ms/step - accuracy: 0.4822 - binary_io_u_5: 0.4878 - loss: 0.9265 Batch 110, Loss Value: 0.9441\n",
      "Batch 110, Gradient Norm: 0.0042\n",
      "\u001b[1m110/232\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m59s\u001b[0m 485ms/step - accuracy: 0.4824 - binary_io_u_5: 0.4878 - loss: 0.9265Batch 111, Loss Value: 0.9000\n",
      "Batch 111, Gradient Norm: 0.1375\n",
      "\u001b[1m111/232\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m58s\u001b[0m 485ms/step - accuracy: 0.4825 - binary_io_u_5: 0.4879 - loss: 0.9264Batch 112, Loss Value: 0.9456\n",
      "Batch 112, Gradient Norm: 0.0033\n",
      "\u001b[1m112/232\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m58s\u001b[0m 485ms/step - accuracy: 0.4826 - binary_io_u_5: 0.4879 - loss: 0.9264Batch 113, Loss Value: 0.9276\n",
      "Batch 113, Gradient Norm: 0.1451\n",
      "\u001b[1m113/232\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m57s\u001b[0m 484ms/step - accuracy: 0.4828 - binary_io_u_5: 0.4880 - loss: 0.9264Batch 114, Loss Value: 0.9443\n",
      "Batch 114, Gradient Norm: 0.0053\n",
      "\u001b[1m114/232\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m57s\u001b[0m 485ms/step - accuracy: 0.4829 - binary_io_u_5: 0.4880 - loss: 0.9263Batch 115, Loss Value: 0.9206\n",
      "Batch 115, Gradient Norm: 0.2556\n",
      "\u001b[1m115/232\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m56s\u001b[0m 485ms/step - accuracy: 0.4830 - binary_io_u_5: 0.4881 - loss: 0.9263Batch 116, Loss Value: 0.9442\n",
      "Batch 116, Gradient Norm: 0.0013\n",
      "\u001b[1m116/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m56s\u001b[0m 485ms/step - accuracy: 0.4831 - binary_io_u_5: 0.4882 - loss: 0.9263Batch 117, Loss Value: 0.9444\n",
      "Batch 117, Gradient Norm: 0.0183\n",
      "\u001b[1m117/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m55s\u001b[0m 485ms/step - accuracy: 0.4833 - binary_io_u_5: 0.4882 - loss: 0.9263Batch 118, Loss Value: 0.9061\n",
      "Batch 118, Gradient Norm: 0.0208\n",
      "\u001b[1m118/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m55s\u001b[0m 485ms/step - accuracy: 0.4834 - binary_io_u_5: 0.4883 - loss: 0.9262Batch 119, Loss Value: 0.9450\n",
      "Batch 119, Gradient Norm: 0.0021\n",
      "\u001b[1m119/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m54s\u001b[0m 485ms/step - accuracy: 0.4835 - binary_io_u_5: 0.4883 - loss: 0.9262Batch 120, Loss Value: 0.9448\n",
      "Batch 120, Gradient Norm: 0.0400\n",
      "\u001b[1m120/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m54s\u001b[0m 485ms/step - accuracy: 0.4836 - binary_io_u_5: 0.4883 - loss: 0.9262Batch 121, Loss Value: 0.9047\n",
      "Batch 121, Gradient Norm: 0.0001\n",
      "\u001b[1m121/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m53s\u001b[0m 485ms/step - accuracy: 0.4837 - binary_io_u_5: 0.4884 - loss: 0.9261Batch 122, Loss Value: 0.9440\n",
      "Batch 122, Gradient Norm: 0.0139\n",
      "\u001b[1m122/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m53s\u001b[0m 485ms/step - accuracy: 0.4838 - binary_io_u_5: 0.4884 - loss: 0.9261Batch 123, Loss Value: 0.9399\n",
      "Batch 123, Gradient Norm: 0.1867\n",
      "\u001b[1m123/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m52s\u001b[0m 484ms/step - accuracy: 0.4839 - binary_io_u_5: 0.4885 - loss: 0.9261Batch 124, Loss Value: 0.9403\n",
      "Batch 124, Gradient Norm: 0.1548\n",
      "\u001b[1m124/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m52s\u001b[0m 485ms/step - accuracy: 0.4840 - binary_io_u_5: 0.4885 - loss: 0.9261Batch 125, Loss Value: 0.9074\n",
      "Batch 125, Gradient Norm: 0.4368\n",
      "\u001b[1m125/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m51s\u001b[0m 485ms/step - accuracy: 0.4841 - binary_io_u_5: 0.4886 - loss: 0.9261Batch 126, Loss Value: 0.9132\n",
      "Batch 126, Gradient Norm: 0.0026\n",
      "\u001b[1m126/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m51s\u001b[0m 485ms/step - accuracy: 0.4842 - binary_io_u_5: 0.4886 - loss: 0.9260Batch 127, Loss Value: 0.9090\n",
      "Batch 127, Gradient Norm: 0.0003\n",
      "\u001b[1m127/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m50s\u001b[0m 485ms/step - accuracy: 0.4843 - binary_io_u_5: 0.4887 - loss: 0.9260Batch 128, Loss Value: 0.9224\n",
      "Batch 128, Gradient Norm: 0.1940\n",
      "\u001b[1m128/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m50s\u001b[0m 485ms/step - accuracy: 0.4844 - binary_io_u_5: 0.4887 - loss: 0.9260Batch 129, Loss Value: 0.9076\n",
      "Batch 129, Gradient Norm: 0.0200\n",
      "\u001b[1m129/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m49s\u001b[0m 485ms/step - accuracy: 0.4844 - binary_io_u_5: 0.4887 - loss: 0.9260Batch 130, Loss Value: 0.9218\n",
      "Batch 130, Gradient Norm: 0.0040\n",
      "\u001b[1m130/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m49s\u001b[0m 485ms/step - accuracy: 0.4845 - binary_io_u_5: 0.4888 - loss: 0.9260Batch 131, Loss Value: 0.9092\n",
      "Batch 131, Gradient Norm: 0.1974\n",
      "\u001b[1m131/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m48s\u001b[0m 485ms/step - accuracy: 0.4846 - binary_io_u_5: 0.4888 - loss: 0.9260Batch 132, Loss Value: 0.9431\n",
      "Batch 132, Gradient Norm: 0.0033\n",
      "\u001b[1m132/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m48s\u001b[0m 485ms/step - accuracy: 0.4846 - binary_io_u_5: 0.4888 - loss: 0.9259Batch 133, Loss Value: 0.9362\n",
      "Batch 133, Gradient Norm: 0.1459\n",
      "\u001b[1m133/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m47s\u001b[0m 485ms/step - accuracy: 0.4847 - binary_io_u_5: 0.4889 - loss: 0.9259Batch 134, Loss Value: 0.9232\n",
      "Batch 134, Gradient Norm: 0.0002\n",
      "\u001b[1m134/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m47s\u001b[0m 485ms/step - accuracy: 0.4848 - binary_io_u_5: 0.4889 - loss: 0.9259Batch 135, Loss Value: 0.9443\n",
      "Batch 135, Gradient Norm: 0.0036\n",
      "\u001b[1m135/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m47s\u001b[0m 485ms/step - accuracy: 0.4848 - binary_io_u_5: 0.4889 - loss: 0.9259Batch 136, Loss Value: 0.9445\n",
      "Batch 136, Gradient Norm: 0.0184\n",
      "\u001b[1m136/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m46s\u001b[0m 485ms/step - accuracy: 0.4849 - binary_io_u_5: 0.4890 - loss: 0.9259Batch 137, Loss Value: 0.9255\n",
      "Batch 137, Gradient Norm: 0.0805\n",
      "\u001b[1m137/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m46s\u001b[0m 485ms/step - accuracy: 0.4850 - binary_io_u_5: 0.4890 - loss: 0.9259Batch 138, Loss Value: 0.8982\n",
      "Batch 138, Gradient Norm: 0.2532\n",
      "\u001b[1m138/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m45s\u001b[0m 484ms/step - accuracy: 0.4850 - binary_io_u_5: 0.4890 - loss: 0.9259Batch 139, Loss Value: 0.9018\n",
      "Batch 139, Gradient Norm: 0.5813\n",
      "\u001b[1m139/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m45s\u001b[0m 484ms/step - accuracy: 0.4851 - binary_io_u_5: 0.4891 - loss: 0.9259Batch 140, Loss Value: 0.9362\n",
      "Batch 140, Gradient Norm: 0.0429\n",
      "\u001b[1m140/232\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m44s\u001b[0m 484ms/step - accuracy: 0.4851 - binary_io_u_5: 0.4891 - loss: 0.9258Batch 141, Loss Value: 0.9032\n",
      "Batch 141, Gradient Norm: 0.0160\n",
      "\u001b[1m141/232\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m44s\u001b[0m 484ms/step - accuracy: 0.4852 - binary_io_u_5: 0.4891 - loss: 0.9258Batch 142, Loss Value: 0.9124\n",
      "Batch 142, Gradient Norm: 0.3098\n",
      "\u001b[1m142/232\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m43s\u001b[0m 484ms/step - accuracy: 0.4852 - binary_io_u_5: 0.4892 - loss: 0.9258Batch 143, Loss Value: 0.9440\n",
      "Batch 143, Gradient Norm: 0.0006\n",
      "\u001b[1m143/232\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m43s\u001b[0m 484ms/step - accuracy: 0.4853 - binary_io_u_5: 0.4892 - loss: 0.9258Batch 144, Loss Value: 0.9217\n",
      "Batch 144, Gradient Norm: 0.0404\n",
      "\u001b[1m144/232\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m42s\u001b[0m 484ms/step - accuracy: 0.4854 - binary_io_u_5: 0.4892 - loss: 0.9258Batch 145, Loss Value: 0.9446\n",
      "Batch 145, Gradient Norm: 0.0038\n",
      "\u001b[1m145/232\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m42s\u001b[0m 484ms/step - accuracy: 0.4854 - binary_io_u_5: 0.4893 - loss: 0.9258Batch 146, Loss Value: 0.9440\n",
      "Batch 146, Gradient Norm: 0.0001\n",
      "\u001b[1m146/232\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m41s\u001b[0m 484ms/step - accuracy: 0.4855 - binary_io_u_5: 0.4893 - loss: 0.9258Batch 147, Loss Value: 0.9439\n",
      "Batch 147, Gradient Norm: 0.0000\n",
      "\u001b[1m147/232\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m41s\u001b[0m 484ms/step - accuracy: 0.4855 - binary_io_u_5: 0.4893 - loss: 0.9258Batch 148, Loss Value: 0.9204\n",
      "Batch 148, Gradient Norm: 0.2295\n",
      "\u001b[1m148/232\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m40s\u001b[0m 484ms/step - accuracy: 0.4856 - binary_io_u_5: 0.4893 - loss: 0.9258Batch 149, Loss Value: 0.9440\n",
      "Batch 149, Gradient Norm: 0.0005\n",
      "\u001b[1m149/232\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m40s\u001b[0m 484ms/step - accuracy: 0.4856 - binary_io_u_5: 0.4894 - loss: 0.9258Batch 150, Loss Value: 0.9449\n",
      "Batch 150, Gradient Norm: 0.0013\n",
      "\u001b[1m150/232\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m39s\u001b[0m 484ms/step - accuracy: 0.4857 - binary_io_u_5: 0.4894 - loss: 0.9257Batch 151, Loss Value: 0.9244\n",
      "Batch 151, Gradient Norm: 0.3017\n",
      "\u001b[1m151/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m39s\u001b[0m 484ms/step - accuracy: 0.4857 - binary_io_u_5: 0.4894 - loss: 0.9257Batch 152, Loss Value: 0.9454\n",
      "Batch 152, Gradient Norm: 0.0043\n",
      "\u001b[1m152/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m38s\u001b[0m 484ms/step - accuracy: 0.4858 - binary_io_u_5: 0.4894 - loss: 0.9257Batch 153, Loss Value: 0.9442\n",
      "Batch 153, Gradient Norm: 0.0029\n",
      "\u001b[1m153/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m38s\u001b[0m 484ms/step - accuracy: 0.4858 - binary_io_u_5: 0.4895 - loss: 0.9257Batch 154, Loss Value: 0.9442\n",
      "Batch 154, Gradient Norm: 0.0136\n",
      "\u001b[1m154/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m37s\u001b[0m 484ms/step - accuracy: 0.4859 - binary_io_u_5: 0.4895 - loss: 0.9257Batch 155, Loss Value: 0.9243\n",
      "Batch 155, Gradient Norm: 0.1959\n",
      "\u001b[1m155/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m37s\u001b[0m 484ms/step - accuracy: 0.4859 - binary_io_u_5: 0.4895 - loss: 0.9257Batch 156, Loss Value: 0.9447\n",
      "Batch 156, Gradient Norm: 0.0008\n",
      "\u001b[1m156/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m36s\u001b[0m 484ms/step - accuracy: 0.4860 - binary_io_u_5: 0.4895 - loss: 0.9257Batch 157, Loss Value: 0.9444\n",
      "Batch 157, Gradient Norm: 0.0009\n",
      "\u001b[1m157/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m36s\u001b[0m 484ms/step - accuracy: 0.4860 - binary_io_u_5: 0.4896 - loss: 0.9257Batch 158, Loss Value: 0.9148\n",
      "Batch 158, Gradient Norm: 0.2845\n",
      "\u001b[1m158/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m35s\u001b[0m 484ms/step - accuracy: 0.4861 - binary_io_u_5: 0.4896 - loss: 0.9257Batch 159, Loss Value: 0.9315\n",
      "Batch 159, Gradient Norm: 0.7155\n",
      "\u001b[1m159/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m35s\u001b[0m 484ms/step - accuracy: 0.4861 - binary_io_u_5: 0.4896 - loss: 0.9257Batch 160, Loss Value: 0.9441\n",
      "Batch 160, Gradient Norm: 0.0048\n",
      "\u001b[1m160/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m34s\u001b[0m 484ms/step - accuracy: 0.4862 - binary_io_u_5: 0.4897 - loss: 0.9256Batch 161, Loss Value: 0.9348\n",
      "Batch 161, Gradient Norm: 0.2175\n",
      "\u001b[1m161/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m34s\u001b[0m 484ms/step - accuracy: 0.4862 - binary_io_u_5: 0.4897 - loss: 0.9256Batch 162, Loss Value: 0.9318\n",
      "Batch 162, Gradient Norm: 0.0091\n",
      "\u001b[1m162/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m33s\u001b[0m 484ms/step - accuracy: 0.4863 - binary_io_u_5: 0.4897 - loss: 0.9256Batch 163, Loss Value: 0.9270\n",
      "Batch 163, Gradient Norm: 0.3911\n",
      "\u001b[1m163/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m33s\u001b[0m 484ms/step - accuracy: 0.4863 - binary_io_u_5: 0.4897 - loss: 0.9256Batch 164, Loss Value: 0.9163\n",
      "Batch 164, Gradient Norm: 0.0475\n",
      "\u001b[1m164/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m32s\u001b[0m 484ms/step - accuracy: 0.4863 - binary_io_u_5: 0.4897 - loss: 0.9256Batch 165, Loss Value: 0.9442\n",
      "Batch 165, Gradient Norm: 0.0121\n",
      "\u001b[1m165/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m32s\u001b[0m 484ms/step - accuracy: 0.4864 - binary_io_u_5: 0.4898 - loss: 0.9256Batch 166, Loss Value: 0.9162\n",
      "Batch 166, Gradient Norm: 0.0132\n",
      "\u001b[1m166/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m31s\u001b[0m 484ms/step - accuracy: 0.4864 - binary_io_u_5: 0.4898 - loss: 0.9256Batch 167, Loss Value: 0.9015\n",
      "Batch 167, Gradient Norm: 0.1861\n",
      "\u001b[1m167/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m31s\u001b[0m 484ms/step - accuracy: 0.4865 - binary_io_u_5: 0.4898 - loss: 0.9256Batch 168, Loss Value: 0.9449\n",
      "Batch 168, Gradient Norm: 0.0003\n",
      "\u001b[1m168/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m30s\u001b[0m 484ms/step - accuracy: 0.4865 - binary_io_u_5: 0.4898 - loss: 0.9256Batch 169, Loss Value: 0.9439\n",
      "Batch 169, Gradient Norm: 0.0005\n",
      "\u001b[1m169/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m30s\u001b[0m 484ms/step - accuracy: 0.4866 - binary_io_u_5: 0.4899 - loss: 0.9256Batch 170, Loss Value: 0.9450\n",
      "Batch 170, Gradient Norm: 0.0181\n",
      "\u001b[1m170/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m29s\u001b[0m 484ms/step - accuracy: 0.4866 - binary_io_u_5: 0.4899 - loss: 0.9256Batch 171, Loss Value: 0.9442\n",
      "Batch 171, Gradient Norm: 0.0123\n",
      "\u001b[1m171/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m29s\u001b[0m 484ms/step - accuracy: 0.4866 - binary_io_u_5: 0.4899 - loss: 0.9256Batch 172, Loss Value: 0.9448\n",
      "Batch 172, Gradient Norm: 0.0032\n",
      "\u001b[1m172/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m29s\u001b[0m 484ms/step - accuracy: 0.4867 - binary_io_u_5: 0.4899 - loss: 0.9256Batch 173, Loss Value: 0.9032\n",
      "Batch 173, Gradient Norm: 0.0105\n",
      "\u001b[1m173/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m28s\u001b[0m 484ms/step - accuracy: 0.4867 - binary_io_u_5: 0.4899 - loss: 0.9255Batch 174, Loss Value: 0.9444\n",
      "Batch 174, Gradient Norm: 0.0032\n",
      "\u001b[1m174/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m28s\u001b[0m 484ms/step - accuracy: 0.4868 - binary_io_u_5: 0.4900 - loss: 0.9255Batch 175, Loss Value: 0.9061\n",
      "Batch 175, Gradient Norm: 0.2437\n",
      "\u001b[1m175/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m27s\u001b[0m 484ms/step - accuracy: 0.4868 - binary_io_u_5: 0.4900 - loss: 0.9255Batch 176, Loss Value: 0.9344\n",
      "Batch 176, Gradient Norm: 0.4102\n",
      "\u001b[1m176/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m27s\u001b[0m 484ms/step - accuracy: 0.4868 - binary_io_u_5: 0.4900 - loss: 0.9255Batch 177, Loss Value: 0.9310\n",
      "Batch 177, Gradient Norm: 0.2933\n",
      "\u001b[1m177/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m26s\u001b[0m 484ms/step - accuracy: 0.4869 - binary_io_u_5: 0.4900 - loss: 0.9255Batch 178, Loss Value: 0.9449\n",
      "Batch 178, Gradient Norm: 0.0075\n",
      "\u001b[1m178/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m26s\u001b[0m 484ms/step - accuracy: 0.4869 - binary_io_u_5: 0.4901 - loss: 0.9255Batch 179, Loss Value: 0.9445\n",
      "Batch 179, Gradient Norm: 0.0203\n",
      "\u001b[1m179/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m25s\u001b[0m 484ms/step - accuracy: 0.4870 - binary_io_u_5: 0.4901 - loss: 0.9255Batch 180, Loss Value: 0.9354\n",
      "Batch 180, Gradient Norm: 0.2357\n",
      "\u001b[1m180/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m25s\u001b[0m 484ms/step - accuracy: 0.4870 - binary_io_u_5: 0.4901 - loss: 0.9255Batch 181, Loss Value: 0.9443\n",
      "Batch 181, Gradient Norm: 0.0031\n",
      "\u001b[1m181/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m24s\u001b[0m 484ms/step - accuracy: 0.4871 - binary_io_u_5: 0.4901 - loss: 0.9255Batch 182, Loss Value: 0.9437\n",
      "Batch 182, Gradient Norm: 0.0000\n",
      "\u001b[1m182/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m24s\u001b[0m 483ms/step - accuracy: 0.4871 - binary_io_u_5: 0.4902 - loss: 0.9255Batch 183, Loss Value: 0.9136\n",
      "Batch 183, Gradient Norm: 0.2989\n",
      "\u001b[1m183/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m23s\u001b[0m 483ms/step - accuracy: 0.4872 - binary_io_u_5: 0.4902 - loss: 0.9255Batch 184, Loss Value: 0.9437\n",
      "Batch 184, Gradient Norm: 0.0144\n",
      "\u001b[1m184/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m23s\u001b[0m 483ms/step - accuracy: 0.4872 - binary_io_u_5: 0.4902 - loss: 0.9255Batch 185, Loss Value: 0.9440\n",
      "Batch 185, Gradient Norm: 0.0051\n",
      "\u001b[1m185/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m22s\u001b[0m 483ms/step - accuracy: 0.4872 - binary_io_u_5: 0.4902 - loss: 0.9254Batch 186, Loss Value: 0.9345\n",
      "Batch 186, Gradient Norm: 0.0295\n",
      "\u001b[1m186/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m22s\u001b[0m 483ms/step - accuracy: 0.4873 - binary_io_u_5: 0.4903 - loss: 0.9254Batch 187, Loss Value: 0.9436\n",
      "Batch 187, Gradient Norm: 0.0140\n",
      "\u001b[1m187/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m21s\u001b[0m 483ms/step - accuracy: 0.4873 - binary_io_u_5: 0.4903 - loss: 0.9254Batch 188, Loss Value: 0.9172\n",
      "Batch 188, Gradient Norm: 0.1785\n",
      "\u001b[1m188/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m21s\u001b[0m 483ms/step - accuracy: 0.4874 - binary_io_u_5: 0.4903 - loss: 0.9254Batch 189, Loss Value: 0.9443\n",
      "Batch 189, Gradient Norm: 0.0357\n",
      "\u001b[1m189/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m20s\u001b[0m 483ms/step - accuracy: 0.4874 - binary_io_u_5: 0.4903 - loss: 0.9254Batch 190, Loss Value: 0.9017\n",
      "Batch 190, Gradient Norm: 0.5634\n",
      "\u001b[1m190/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m20s\u001b[0m 483ms/step - accuracy: 0.4875 - binary_io_u_5: 0.4903 - loss: 0.9254Batch 191, Loss Value: 0.9429\n",
      "Batch 191, Gradient Norm: 0.2743\n",
      "\u001b[1m191/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m19s\u001b[0m 483ms/step - accuracy: 0.4875 - binary_io_u_5: 0.4904 - loss: 0.9254Batch 192, Loss Value: 0.9449\n",
      "Batch 192, Gradient Norm: 0.0014\n",
      "\u001b[1m192/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m19s\u001b[0m 483ms/step - accuracy: 0.4875 - binary_io_u_5: 0.4904 - loss: 0.9254Batch 193, Loss Value: 0.9445\n",
      "Batch 193, Gradient Norm: 0.0178\n",
      "\u001b[1m193/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m18s\u001b[0m 483ms/step - accuracy: 0.4876 - binary_io_u_5: 0.4904 - loss: 0.9254Batch 194, Loss Value: 0.9303\n",
      "Batch 194, Gradient Norm: 0.0045\n",
      "\u001b[1m194/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m18s\u001b[0m 483ms/step - accuracy: 0.4876 - binary_io_u_5: 0.4904 - loss: 0.9254Batch 195, Loss Value: 0.9450\n",
      "Batch 195, Gradient Norm: 0.0082\n",
      "\u001b[1m195/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m17s\u001b[0m 483ms/step - accuracy: 0.4877 - binary_io_u_5: 0.4905 - loss: 0.9254Batch 196, Loss Value: 0.9442\n",
      "Batch 196, Gradient Norm: 0.0144\n",
      "\u001b[1m196/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m17s\u001b[0m 483ms/step - accuracy: 0.4877 - binary_io_u_5: 0.4905 - loss: 0.9254Batch 197, Loss Value: 0.9304\n",
      "Batch 197, Gradient Norm: 0.0019\n",
      "\u001b[1m197/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m16s\u001b[0m 483ms/step - accuracy: 0.4877 - binary_io_u_5: 0.4905 - loss: 0.9254Batch 198, Loss Value: 0.9352\n",
      "Batch 198, Gradient Norm: 0.5445\n",
      "\u001b[1m198/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m16s\u001b[0m 483ms/step - accuracy: 0.4878 - binary_io_u_5: 0.4905 - loss: 0.9254Batch 199, Loss Value: 0.9448\n",
      "Batch 199, Gradient Norm: 0.0155\n",
      "\u001b[1m199/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m15s\u001b[0m 483ms/step - accuracy: 0.4878 - binary_io_u_5: 0.4905 - loss: 0.9254Batch 200, Loss Value: 0.9442\n",
      "Batch 200, Gradient Norm: 0.0042\n",
      "\u001b[1m200/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m15s\u001b[0m 483ms/step - accuracy: 0.4879 - binary_io_u_5: 0.4906 - loss: 0.9254Batch 201, Loss Value: 0.8914\n",
      "Batch 201, Gradient Norm: 0.4566\n",
      "\u001b[1m201/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m14s\u001b[0m 483ms/step - accuracy: 0.4879 - binary_io_u_5: 0.4906 - loss: 0.9254Batch 202, Loss Value: 0.9440\n",
      "Batch 202, Gradient Norm: 0.0012\n",
      "\u001b[1m202/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m14s\u001b[0m 483ms/step - accuracy: 0.4879 - binary_io_u_5: 0.4906 - loss: 0.9253Batch 203, Loss Value: 0.9079\n",
      "Batch 203, Gradient Norm: 0.0921\n",
      "\u001b[1m203/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m14s\u001b[0m 483ms/step - accuracy: 0.4880 - binary_io_u_5: 0.4906 - loss: 0.9253Batch 204, Loss Value: 0.8970\n",
      "Batch 204, Gradient Norm: 0.2473\n",
      "\u001b[1m204/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m13s\u001b[0m 483ms/step - accuracy: 0.4880 - binary_io_u_5: 0.4907 - loss: 0.9253Batch 205, Loss Value: 0.8933\n",
      "Batch 205, Gradient Norm: 0.1402\n",
      "\u001b[1m205/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m13s\u001b[0m 483ms/step - accuracy: 0.4881 - binary_io_u_5: 0.4907 - loss: 0.9253Batch 206, Loss Value: 0.9448\n",
      "Batch 206, Gradient Norm: 0.0135\n",
      "\u001b[1m206/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m12s\u001b[0m 483ms/step - accuracy: 0.4881 - binary_io_u_5: 0.4907 - loss: 0.9253Batch 207, Loss Value: 0.8879\n",
      "Batch 207, Gradient Norm: 0.0339\n",
      "\u001b[1m207/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m12s\u001b[0m 483ms/step - accuracy: 0.4882 - binary_io_u_5: 0.4908 - loss: 0.9253Batch 208, Loss Value: 0.9390\n",
      "Batch 208, Gradient Norm: 0.5272\n",
      "\u001b[1m208/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m11s\u001b[0m 483ms/step - accuracy: 0.4882 - binary_io_u_5: 0.4908 - loss: 0.9253Batch 209, Loss Value: 0.8951\n",
      "Batch 209, Gradient Norm: 0.4672\n",
      "\u001b[1m209/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m11s\u001b[0m 483ms/step - accuracy: 0.4882 - binary_io_u_5: 0.4908 - loss: 0.9253Batch 210, Loss Value: 0.9361\n",
      "Batch 210, Gradient Norm: 0.0329\n",
      "\u001b[1m210/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m10s\u001b[0m 483ms/step - accuracy: 0.4883 - binary_io_u_5: 0.4908 - loss: 0.9253Batch 211, Loss Value: 0.9212\n",
      "Batch 211, Gradient Norm: 0.3941\n",
      "\u001b[1m211/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m10s\u001b[0m 483ms/step - accuracy: 0.4883 - binary_io_u_5: 0.4909 - loss: 0.9253Batch 212, Loss Value: 0.9255\n",
      "Batch 212, Gradient Norm: 0.1423\n",
      "\u001b[1m212/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m9s\u001b[0m 483ms/step - accuracy: 0.4883 - binary_io_u_5: 0.4909 - loss: 0.9253 Batch 213, Loss Value: 0.9284\n",
      "Batch 213, Gradient Norm: 0.2972\n",
      "\u001b[1m213/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m9s\u001b[0m 483ms/step - accuracy: 0.4884 - binary_io_u_5: 0.4909 - loss: 0.9253Batch 214, Loss Value: 0.9237\n",
      "Batch 214, Gradient Norm: 0.5370\n",
      "\u001b[1m214/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m8s\u001b[0m 483ms/step - accuracy: 0.4884 - binary_io_u_5: 0.4909 - loss: 0.9253Batch 215, Loss Value: 0.9426\n",
      "Batch 215, Gradient Norm: 0.2646\n",
      "\u001b[1m215/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m8s\u001b[0m 483ms/step - accuracy: 0.4884 - binary_io_u_5: 0.4909 - loss: 0.9253Batch 216, Loss Value: 0.9437\n",
      "Batch 216, Gradient Norm: 0.0002\n",
      "\u001b[1m216/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m7s\u001b[0m 483ms/step - accuracy: 0.4885 - binary_io_u_5: 0.4910 - loss: 0.9253Batch 217, Loss Value: 0.9451\n",
      "Batch 217, Gradient Norm: 0.0000\n",
      "\u001b[1m217/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m7s\u001b[0m 483ms/step - accuracy: 0.4885 - binary_io_u_5: 0.4910 - loss: 0.9253Batch 218, Loss Value: 0.9272\n",
      "Batch 218, Gradient Norm: 0.5198\n",
      "\u001b[1m218/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m6s\u001b[0m 483ms/step - accuracy: 0.4885 - binary_io_u_5: 0.4910 - loss: 0.9253Batch 219, Loss Value: 0.9441\n",
      "Batch 219, Gradient Norm: 0.0002\n",
      "\u001b[1m219/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m6s\u001b[0m 483ms/step - accuracy: 0.4886 - binary_io_u_5: 0.4910 - loss: 0.9253Batch 220, Loss Value: 0.9027\n",
      "Batch 220, Gradient Norm: 0.5445\n",
      "\u001b[1m220/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m5s\u001b[0m 483ms/step - accuracy: 0.4886 - binary_io_u_5: 0.4910 - loss: 0.9253Batch 221, Loss Value: 0.9440\n",
      "Batch 221, Gradient Norm: 0.0008\n",
      "\u001b[1m221/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m5s\u001b[0m 483ms/step - accuracy: 0.4886 - binary_io_u_5: 0.4910 - loss: 0.9253Batch 222, Loss Value: 0.9433\n",
      "Batch 222, Gradient Norm: 0.1461\n",
      "\u001b[1m222/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m4s\u001b[0m 483ms/step - accuracy: 0.4887 - binary_io_u_5: 0.4911 - loss: 0.9253Batch 223, Loss Value: 0.9443\n",
      "Batch 223, Gradient Norm: 0.0078\n",
      "\u001b[1m223/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m4s\u001b[0m 483ms/step - accuracy: 0.4887 - binary_io_u_5: 0.4911 - loss: 0.9253Batch 224, Loss Value: 0.8979\n",
      "Batch 224, Gradient Norm: 0.3849\n",
      "\u001b[1m224/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m3s\u001b[0m 483ms/step - accuracy: 0.4887 - binary_io_u_5: 0.4911 - loss: 0.9253Batch 225, Loss Value: 0.9442\n",
      "Batch 225, Gradient Norm: 0.0102\n",
      "\u001b[1m225/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m3s\u001b[0m 483ms/step - accuracy: 0.4887 - binary_io_u_5: 0.4911 - loss: 0.9253Batch 226, Loss Value: 0.9445\n",
      "Batch 226, Gradient Norm: 0.0043\n",
      "\u001b[1m226/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m2s\u001b[0m 483ms/step - accuracy: 0.4888 - binary_io_u_5: 0.4911 - loss: 0.9253Batch 227, Loss Value: 0.9219\n",
      "Batch 227, Gradient Norm: 0.0130\n",
      "\u001b[1m227/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m2s\u001b[0m 483ms/step - accuracy: 0.4888 - binary_io_u_5: 0.4911 - loss: 0.9253Batch 228, Loss Value: 0.9441\n",
      "Batch 228, Gradient Norm: 0.0105\n",
      "\u001b[1m228/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m1s\u001b[0m 483ms/step - accuracy: 0.4888 - binary_io_u_5: 0.4911 - loss: 0.9253Batch 229, Loss Value: 0.9239\n",
      "Batch 229, Gradient Norm: 0.1172\n",
      "\u001b[1m229/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m1s\u001b[0m 483ms/step - accuracy: 0.4888 - binary_io_u_5: 0.4912 - loss: 0.9253Batch 230, Loss Value: 0.9441\n",
      "Batch 230, Gradient Norm: 0.0167\n",
      "\u001b[1m230/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 483ms/step - accuracy: 0.4889 - binary_io_u_5: 0.4912 - loss: 0.9253Batch 231, Loss Value: 0.8947\n",
      "Batch 231, Gradient Norm: 0.0110\n",
      "\u001b[1m231/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 483ms/step - accuracy: 0.4889 - binary_io_u_5: 0.4912 - loss: 0.9253Batch 232, Loss Value: 0.9230\n",
      "Batch 232, Gradient Norm: 0.0232\n",
      "\u001b[1m232/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m145s\u001b[0m 492ms/step - accuracy: 0.4890 - binary_io_u_5: 0.4912 - loss: 0.9253 - val_accuracy: 0.4941 - val_binary_io_u_5: 0.4949 - val_loss: 0.9119 - learning_rate: 0.0010\n",
      "Epoch 3/200\n",
      "Batch 1, Loss Value: 0.9439\n",
      "Batch 1, Gradient Norm: 0.0013\n",
      "\u001b[1m  1/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:48:46\u001b[0m 28s/step - accuracy: 0.5200 - binary_io_u_5: 0.4600 - loss: 0.9203Batch 2, Loss Value: 0.9406\n",
      "Batch 2, Gradient Norm: 0.3242\n",
      "\u001b[1m  2/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:52\u001b[0m 490ms/step - accuracy: 0.5150 - binary_io_u_5: 0.4800 - loss: 0.9248 Batch 3, Loss Value: 0.9102\n",
      "Batch 3, Gradient Norm: 0.3473\n",
      "\u001b[1m  3/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:49\u001b[0m 477ms/step - accuracy: 0.5144 - binary_io_u_5: 0.4856 - loss: 0.9252Batch 4, Loss Value: 0.9224\n",
      "Batch 4, Gradient Norm: 0.1107\n",
      "\u001b[1m  4/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:47\u001b[0m 472ms/step - accuracy: 0.5052 - binary_io_u_5: 0.4860 - loss: 0.9268Batch 5, Loss Value: 0.9265\n",
      "Batch 5, Gradient Norm: 0.2738\n",
      "\u001b[1m  5/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:46\u001b[0m 469ms/step - accuracy: 0.5002 - binary_io_u_5: 0.4904 - loss: 0.9280Batch 6, Loss Value: 0.9441\n",
      "Batch 6, Gradient Norm: 0.0004\n",
      "\u001b[1m  6/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:45\u001b[0m 467ms/step - accuracy: 0.4976 - binary_io_u_5: 0.4906 - loss: 0.9286Batch 7, Loss Value: 0.9439\n",
      "Batch 7, Gradient Norm: 0.0012\n",
      "\u001b[1m  7/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:44\u001b[0m 465ms/step - accuracy: 0.4968 - binary_io_u_5: 0.4912 - loss: 0.9288Batch 8, Loss Value: 0.9461\n",
      "Batch 8, Gradient Norm: 0.0000\n",
      "\u001b[1m  8/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:44\u001b[0m 465ms/step - accuracy: 0.4967 - binary_io_u_5: 0.4932 - loss: 0.9289Batch 9, Loss Value: 0.9442\n",
      "Batch 9, Gradient Norm: 0.0116\n",
      "\u001b[1m  9/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:43\u001b[0m 464ms/step - accuracy: 0.4971 - binary_io_u_5: 0.4954 - loss: 0.9288Batch 10, Loss Value: 0.9076\n",
      "Batch 10, Gradient Norm: 0.0019\n",
      "\u001b[1m 10/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:43\u001b[0m 465ms/step - accuracy: 0.4973 - binary_io_u_5: 0.4964 - loss: 0.9289Batch 11, Loss Value: 0.9445\n",
      "Batch 11, Gradient Norm: 0.0081\n",
      "\u001b[1m 11/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:42\u001b[0m 464ms/step - accuracy: 0.4969 - binary_io_u_5: 0.4973 - loss: 0.9290Batch 12, Loss Value: 0.9159\n",
      "Batch 12, Gradient Norm: 0.0729\n",
      "\u001b[1m 12/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:42\u001b[0m 464ms/step - accuracy: 0.4967 - binary_io_u_5: 0.4977 - loss: 0.9292Batch 13, Loss Value: 0.9443\n",
      "Batch 13, Gradient Norm: 0.0006\n",
      "\u001b[1m 13/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:41\u001b[0m 464ms/step - accuracy: 0.4970 - binary_io_u_5: 0.4984 - loss: 0.9292Batch 14, Loss Value: 0.9258\n",
      "Batch 14, Gradient Norm: 0.0598\n",
      "\u001b[1m 14/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:41\u001b[0m 464ms/step - accuracy: 0.4973 - binary_io_u_5: 0.4988 - loss: 0.9292Batch 15, Loss Value: 0.9419\n",
      "Batch 15, Gradient Norm: 0.1804\n",
      "\u001b[1m 15/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:40\u001b[0m 464ms/step - accuracy: 0.4979 - binary_io_u_5: 0.4992 - loss: 0.9290Batch 16, Loss Value: 0.9448\n",
      "Batch 16, Gradient Norm: 0.0005\n",
      "\u001b[1m 16/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:40\u001b[0m 464ms/step - accuracy: 0.4984 - binary_io_u_5: 0.4994 - loss: 0.9289Batch 17, Loss Value: 0.9104\n",
      "Batch 17, Gradient Norm: 0.0000\n",
      "\u001b[1m 17/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:39\u001b[0m 464ms/step - accuracy: 0.4989 - binary_io_u_5: 0.4995 - loss: 0.9286Batch 18, Loss Value: 0.9438\n",
      "Batch 18, Gradient Norm: 0.0094\n",
      "\u001b[1m 18/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:39\u001b[0m 463ms/step - accuracy: 0.4994 - binary_io_u_5: 0.4997 - loss: 0.9284Batch 19, Loss Value: 0.9440\n",
      "Batch 19, Gradient Norm: 0.0002\n",
      "\u001b[1m 19/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:38\u001b[0m 463ms/step - accuracy: 0.4998 - binary_io_u_5: 0.4998 - loss: 0.9283Batch 20, Loss Value: 0.9449\n",
      "Batch 20, Gradient Norm: 0.0167\n",
      "\u001b[1m 20/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:38\u001b[0m 463ms/step - accuracy: 0.5000 - binary_io_u_5: 0.5000 - loss: 0.9282Batch 21, Loss Value: 0.9450\n",
      "Batch 21, Gradient Norm: 0.0049\n",
      "\u001b[1m 21/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:37\u001b[0m 463ms/step - accuracy: 0.5001 - binary_io_u_5: 0.5001 - loss: 0.9282Batch 22, Loss Value: 0.9446\n",
      "Batch 22, Gradient Norm: 0.0001\n",
      "\u001b[1m 22/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:37\u001b[0m 463ms/step - accuracy: 0.5003 - binary_io_u_5: 0.5004 - loss: 0.9282Batch 23, Loss Value: 0.9450\n",
      "Batch 23, Gradient Norm: 0.0074\n",
      "\u001b[1m 23/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:36\u001b[0m 463ms/step - accuracy: 0.5004 - binary_io_u_5: 0.5007 - loss: 0.9281Batch 24, Loss Value: 0.8548\n",
      "Batch 24, Gradient Norm: 0.0064\n",
      "\u001b[1m 24/232\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:36\u001b[0m 463ms/step - accuracy: 0.5006 - binary_io_u_5: 0.5011 - loss: 0.9281Batch 25, Loss Value: 0.9042\n",
      "Batch 25, Gradient Norm: 0.1300\n",
      "\u001b[1m 25/232\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:35\u001b[0m 463ms/step - accuracy: 0.5007 - binary_io_u_5: 0.5015 - loss: 0.9281Batch 26, Loss Value: 0.9455\n",
      "Batch 26, Gradient Norm: 0.0006\n",
      "\u001b[1m 26/232\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:35\u001b[0m 463ms/step - accuracy: 0.5008 - binary_io_u_5: 0.5018 - loss: 0.9280Batch 27, Loss Value: 0.9452\n",
      "Batch 27, Gradient Norm: 0.0043\n",
      "\u001b[1m 27/232\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:34\u001b[0m 463ms/step - accuracy: 0.5008 - binary_io_u_5: 0.5022 - loss: 0.9281Batch 28, Loss Value: 0.9447\n",
      "Batch 28, Gradient Norm: 0.0014\n",
      "\u001b[1m 28/232\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:34\u001b[0m 463ms/step - accuracy: 0.5008 - binary_io_u_5: 0.5024 - loss: 0.9280Batch 29, Loss Value: 0.9451\n",
      "Batch 29, Gradient Norm: 0.0001\n",
      "\u001b[1m 29/232\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:34\u001b[0m 464ms/step - accuracy: 0.5007 - binary_io_u_5: 0.5026 - loss: 0.9281Batch 30, Loss Value: 0.9449\n",
      "Batch 30, Gradient Norm: 0.0147\n",
      "\u001b[1m 30/232\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:33\u001b[0m 464ms/step - accuracy: 0.5005 - binary_io_u_5: 0.5027 - loss: 0.9281Batch 31, Loss Value: 0.9401\n",
      "Batch 31, Gradient Norm: 0.3481\n",
      "\u001b[1m 31/232\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:33\u001b[0m 464ms/step - accuracy: 0.5004 - binary_io_u_5: 0.5028 - loss: 0.9281Batch 32, Loss Value: 0.9450\n",
      "Batch 32, Gradient Norm: 0.0230\n",
      "\u001b[1m 32/232\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:32\u001b[0m 464ms/step - accuracy: 0.5003 - binary_io_u_5: 0.5028 - loss: 0.9282Batch 33, Loss Value: 0.9457\n",
      "Batch 33, Gradient Norm: 0.0005\n",
      "\u001b[1m 33/232\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:32\u001b[0m 464ms/step - accuracy: 0.5002 - binary_io_u_5: 0.5029 - loss: 0.9282Batch 34, Loss Value: 0.9451\n",
      "Batch 34, Gradient Norm: 0.0004\n",
      "\u001b[1m 34/232\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:31\u001b[0m 464ms/step - accuracy: 0.5000 - binary_io_u_5: 0.5028 - loss: 0.9283Batch 35, Loss Value: 0.9439\n",
      "Batch 35, Gradient Norm: 0.0130\n",
      "\u001b[1m 35/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:31\u001b[0m 464ms/step - accuracy: 0.4998 - binary_io_u_5: 0.5027 - loss: 0.9283Batch 36, Loss Value: 0.9454\n",
      "Batch 36, Gradient Norm: 0.0103\n",
      "\u001b[1m 36/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:31\u001b[0m 464ms/step - accuracy: 0.4995 - binary_io_u_5: 0.5027 - loss: 0.9284Batch 37, Loss Value: 0.9448\n",
      "Batch 37, Gradient Norm: 0.0003\n",
      "\u001b[1m 37/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:30\u001b[0m 464ms/step - accuracy: 0.4993 - binary_io_u_5: 0.5026 - loss: 0.9285Batch 38, Loss Value: 0.9449\n",
      "Batch 38, Gradient Norm: 0.0299\n",
      "\u001b[1m 38/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:30\u001b[0m 464ms/step - accuracy: 0.4991 - binary_io_u_5: 0.5025 - loss: 0.9286Batch 39, Loss Value: 0.9444\n",
      "Batch 39, Gradient Norm: 0.0012\n",
      "\u001b[1m 39/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:29\u001b[0m 464ms/step - accuracy: 0.4989 - binary_io_u_5: 0.5025 - loss: 0.9286Batch 40, Loss Value: 0.9444\n",
      "Batch 40, Gradient Norm: 0.0055\n",
      "\u001b[1m 40/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:29\u001b[0m 464ms/step - accuracy: 0.4987 - binary_io_u_5: 0.5025 - loss: 0.9286Batch 41, Loss Value: 0.9447\n",
      "Batch 41, Gradient Norm: 0.0016\n",
      "\u001b[1m 41/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:28\u001b[0m 464ms/step - accuracy: 0.4986 - binary_io_u_5: 0.5025 - loss: 0.9286Batch 42, Loss Value: 0.9433\n",
      "Batch 42, Gradient Norm: 0.1532\n",
      "\u001b[1m 42/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:28\u001b[0m 464ms/step - accuracy: 0.4984 - binary_io_u_5: 0.5025 - loss: 0.9287Batch 43, Loss Value: 0.9436\n",
      "Batch 43, Gradient Norm: 0.0003\n",
      "\u001b[1m 43/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:27\u001b[0m 464ms/step - accuracy: 0.4983 - binary_io_u_5: 0.5025 - loss: 0.9287Batch 44, Loss Value: 0.9453\n",
      "Batch 44, Gradient Norm: 0.0249\n",
      "\u001b[1m 44/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:27\u001b[0m 464ms/step - accuracy: 0.4981 - binary_io_u_5: 0.5025 - loss: 0.9287Batch 45, Loss Value: 0.9324\n",
      "Batch 45, Gradient Norm: 0.1088\n",
      "\u001b[1m 45/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:26\u001b[0m 464ms/step - accuracy: 0.4980 - binary_io_u_5: 0.5024 - loss: 0.9287Batch 46, Loss Value: 0.9374\n",
      "Batch 46, Gradient Norm: 0.0030\n",
      "\u001b[1m 46/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:26\u001b[0m 464ms/step - accuracy: 0.4978 - binary_io_u_5: 0.5024 - loss: 0.9288Batch 47, Loss Value: 0.9441\n",
      "Batch 47, Gradient Norm: 0.0124\n",
      "\u001b[1m 47/232\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:25\u001b[0m 464ms/step - accuracy: 0.4977 - binary_io_u_5: 0.5024 - loss: 0.9288Batch 48, Loss Value: 0.9449\n",
      "Batch 48, Gradient Norm: 0.0076\n",
      "\u001b[1m 48/232\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:25\u001b[0m 465ms/step - accuracy: 0.4976 - binary_io_u_5: 0.5024 - loss: 0.9288Batch 49, Loss Value: 0.9305\n",
      "Batch 49, Gradient Norm: 0.3374\n",
      "\u001b[1m 49/232\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:24\u001b[0m 464ms/step - accuracy: 0.4975 - binary_io_u_5: 0.5024 - loss: 0.9288Batch 50, Loss Value: 0.9457\n",
      "Batch 50, Gradient Norm: 0.0049\n",
      "\u001b[1m 50/232\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:24\u001b[0m 464ms/step - accuracy: 0.4974 - binary_io_u_5: 0.5024 - loss: 0.9288Batch 51, Loss Value: 0.9442\n",
      "Batch 51, Gradient Norm: 0.0015\n",
      "\u001b[1m 51/232\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:24\u001b[0m 464ms/step - accuracy: 0.4973 - binary_io_u_5: 0.5024 - loss: 0.9288Batch 52, Loss Value: 0.9440\n",
      "Batch 52, Gradient Norm: 0.0000\n",
      "\u001b[1m 52/232\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:23\u001b[0m 464ms/step - accuracy: 0.4973 - binary_io_u_5: 0.5023 - loss: 0.9288Batch 53, Loss Value: 0.9455\n",
      "Batch 53, Gradient Norm: 0.0096\n",
      "\u001b[1m 53/232\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:23\u001b[0m 465ms/step - accuracy: 0.4972 - binary_io_u_5: 0.5023 - loss: 0.9288Batch 54, Loss Value: 0.9445\n",
      "Batch 54, Gradient Norm: 0.0187\n",
      "\u001b[1m 54/232\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:22\u001b[0m 465ms/step - accuracy: 0.4971 - binary_io_u_5: 0.5023 - loss: 0.9288Batch 55, Loss Value: 0.9334\n",
      "Batch 55, Gradient Norm: 0.7521\n",
      "\u001b[1m 55/232\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:22\u001b[0m 465ms/step - accuracy: 0.4971 - binary_io_u_5: 0.5022 - loss: 0.9288Batch 56, Loss Value: 0.9447\n",
      "Batch 56, Gradient Norm: 0.0066\n",
      "\u001b[1m 56/232\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:21\u001b[0m 465ms/step - accuracy: 0.4971 - binary_io_u_5: 0.5022 - loss: 0.9288Batch 57, Loss Value: 0.9151\n",
      "Batch 57, Gradient Norm: 0.1515\n",
      "\u001b[1m 57/232\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:21\u001b[0m 465ms/step - accuracy: 0.4970 - binary_io_u_5: 0.5021 - loss: 0.9288Batch 58, Loss Value: 0.9442\n",
      "Batch 58, Gradient Norm: 0.0038\n",
      "\u001b[1m 58/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:20\u001b[0m 465ms/step - accuracy: 0.4970 - binary_io_u_5: 0.5021 - loss: 0.9288Batch 59, Loss Value: 0.9246\n",
      "Batch 59, Gradient Norm: 0.0056\n",
      "\u001b[1m 59/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:20\u001b[0m 465ms/step - accuracy: 0.4970 - binary_io_u_5: 0.5020 - loss: 0.9287Batch 60, Loss Value: 0.9197\n",
      "Batch 60, Gradient Norm: 0.1450\n",
      "\u001b[1m 60/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:19\u001b[0m 465ms/step - accuracy: 0.4969 - binary_io_u_5: 0.5020 - loss: 0.9287Batch 61, Loss Value: 0.9445\n",
      "Batch 61, Gradient Norm: 0.0080\n",
      "\u001b[1m 61/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:19\u001b[0m 465ms/step - accuracy: 0.4969 - binary_io_u_5: 0.5019 - loss: 0.9287Batch 62, Loss Value: 0.9398\n",
      "Batch 62, Gradient Norm: 0.0870\n",
      "\u001b[1m 62/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:19\u001b[0m 465ms/step - accuracy: 0.4969 - binary_io_u_5: 0.5019 - loss: 0.9287Batch 63, Loss Value: 0.9437\n",
      "Batch 63, Gradient Norm: 0.0124\n",
      "\u001b[1m 63/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:18\u001b[0m 465ms/step - accuracy: 0.4969 - binary_io_u_5: 0.5019 - loss: 0.9287Batch 64, Loss Value: 0.9438\n",
      "Batch 64, Gradient Norm: 0.0035\n",
      "\u001b[1m 64/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:18\u001b[0m 465ms/step - accuracy: 0.4970 - binary_io_u_5: 0.5019 - loss: 0.9287Batch 65, Loss Value: 0.9386\n",
      "Batch 65, Gradient Norm: 0.1441\n",
      "\u001b[1m 65/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:17\u001b[0m 465ms/step - accuracy: 0.4970 - binary_io_u_5: 0.5019 - loss: 0.9287Batch 66, Loss Value: 0.9278\n",
      "Batch 66, Gradient Norm: 0.4359\n",
      "\u001b[1m 66/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:17\u001b[0m 465ms/step - accuracy: 0.4969 - binary_io_u_5: 0.5019 - loss: 0.9287Batch 67, Loss Value: 0.9437\n",
      "Batch 67, Gradient Norm: 0.0244\n",
      "\u001b[1m 67/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:16\u001b[0m 465ms/step - accuracy: 0.4969 - binary_io_u_5: 0.5018 - loss: 0.9287Batch 68, Loss Value: 0.9449\n",
      "Batch 68, Gradient Norm: 0.0007\n",
      "\u001b[1m 68/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:16\u001b[0m 465ms/step - accuracy: 0.4969 - binary_io_u_5: 0.5018 - loss: 0.9287Batch 69, Loss Value: 0.9444\n",
      "Batch 69, Gradient Norm: 0.0064\n",
      "\u001b[1m 69/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:15\u001b[0m 465ms/step - accuracy: 0.4968 - binary_io_u_5: 0.5018 - loss: 0.9287Batch 70, Loss Value: 0.8672\n",
      "Batch 70, Gradient Norm: 0.1945\n",
      "\u001b[1m 70/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:15\u001b[0m 465ms/step - accuracy: 0.4968 - binary_io_u_5: 0.5017 - loss: 0.9287Batch 71, Loss Value: 0.9458\n",
      "Batch 71, Gradient Norm: 0.0039\n",
      "\u001b[1m 71/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:14\u001b[0m 465ms/step - accuracy: 0.4967 - binary_io_u_5: 0.5016 - loss: 0.9287Batch 72, Loss Value: 0.9436\n",
      "Batch 72, Gradient Norm: 0.0004\n",
      "\u001b[1m 72/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:14\u001b[0m 465ms/step - accuracy: 0.4966 - binary_io_u_5: 0.5016 - loss: 0.9288Batch 73, Loss Value: 0.9450\n",
      "Batch 73, Gradient Norm: 0.0001\n",
      "\u001b[1m 73/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:13\u001b[0m 465ms/step - accuracy: 0.4966 - binary_io_u_5: 0.5015 - loss: 0.9288Batch 74, Loss Value: 0.9459\n",
      "Batch 74, Gradient Norm: 0.0002\n",
      "\u001b[1m 74/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:13\u001b[0m 465ms/step - accuracy: 0.4965 - binary_io_u_5: 0.5014 - loss: 0.9288Batch 75, Loss Value: 0.9452\n",
      "Batch 75, Gradient Norm: 0.0007\n",
      "\u001b[1m 75/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:13\u001b[0m 465ms/step - accuracy: 0.4965 - binary_io_u_5: 0.5013 - loss: 0.9288Batch 76, Loss Value: 0.9226\n",
      "Batch 76, Gradient Norm: 0.1840\n",
      "\u001b[1m 76/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:12\u001b[0m 465ms/step - accuracy: 0.4964 - binary_io_u_5: 0.5013 - loss: 0.9288Batch 77, Loss Value: 0.9309\n",
      "Batch 77, Gradient Norm: 0.1266\n",
      "\u001b[1m 77/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:12\u001b[0m 465ms/step - accuracy: 0.4964 - binary_io_u_5: 0.5012 - loss: 0.9288Batch 78, Loss Value: 0.9432\n",
      "Batch 78, Gradient Norm: 0.0000\n",
      "\u001b[1m 78/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:11\u001b[0m 465ms/step - accuracy: 0.4963 - binary_io_u_5: 0.5011 - loss: 0.9289Batch 79, Loss Value: 0.9444\n",
      "Batch 79, Gradient Norm: 0.0261\n",
      "\u001b[1m 79/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:11\u001b[0m 465ms/step - accuracy: 0.4963 - binary_io_u_5: 0.5010 - loss: 0.9289Batch 80, Loss Value: 0.9294\n",
      "Batch 80, Gradient Norm: 0.2642\n",
      "\u001b[1m 80/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:10\u001b[0m 465ms/step - accuracy: 0.4962 - binary_io_u_5: 0.5009 - loss: 0.9289Batch 81, Loss Value: 0.9209\n",
      "Batch 81, Gradient Norm: 0.2023\n",
      "\u001b[1m 81/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:10\u001b[0m 465ms/step - accuracy: 0.4961 - binary_io_u_5: 0.5008 - loss: 0.9289Batch 82, Loss Value: 0.9332\n",
      "Batch 82, Gradient Norm: 0.0161\n",
      "\u001b[1m 82/232\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m1:09\u001b[0m 466ms/step - accuracy: 0.4961 - binary_io_u_5: 0.5007 - loss: 0.9289Batch 83, Loss Value: 0.9448\n",
      "Batch 83, Gradient Norm: 0.0085\n",
      "\u001b[1m 83/232\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m1:09\u001b[0m 466ms/step - accuracy: 0.4960 - binary_io_u_5: 0.5006 - loss: 0.9289Batch 84, Loss Value: 0.9444\n",
      "Batch 84, Gradient Norm: 0.0000\n",
      "\u001b[1m 84/232\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m1:08\u001b[0m 466ms/step - accuracy: 0.4960 - binary_io_u_5: 0.5005 - loss: 0.9290Batch 85, Loss Value: 0.9147\n",
      "Batch 85, Gradient Norm: 0.0000\n",
      "\u001b[1m 85/232\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m1:08\u001b[0m 466ms/step - accuracy: 0.4959 - binary_io_u_5: 0.5004 - loss: 0.9290Batch 86, Loss Value: 0.9342\n",
      "Batch 86, Gradient Norm: 0.2165\n",
      "\u001b[1m 86/232\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m1:08\u001b[0m 466ms/step - accuracy: 0.4958 - binary_io_u_5: 0.5003 - loss: 0.9290Batch 87, Loss Value: 0.9450\n",
      "Batch 87, Gradient Norm: 0.0083\n",
      "\u001b[1m 87/232\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m1:07\u001b[0m 466ms/step - accuracy: 0.4958 - binary_io_u_5: 0.5003 - loss: 0.9290Batch 88, Loss Value: 0.9441\n",
      "Batch 88, Gradient Norm: 0.0088\n",
      "\u001b[1m 88/232\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m1:07\u001b[0m 466ms/step - accuracy: 0.4957 - binary_io_u_5: 0.5002 - loss: 0.9290Batch 89, Loss Value: 0.9435\n",
      "Batch 89, Gradient Norm: 0.0096\n",
      "\u001b[1m 89/232\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m1:06\u001b[0m 466ms/step - accuracy: 0.4957 - binary_io_u_5: 0.5001 - loss: 0.9290Batch 90, Loss Value: 0.9432\n",
      "Batch 90, Gradient Norm: 0.0005\n",
      "\u001b[1m 90/232\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m1:06\u001b[0m 466ms/step - accuracy: 0.4956 - binary_io_u_5: 0.5000 - loss: 0.9290Batch 91, Loss Value: 0.9443\n",
      "Batch 91, Gradient Norm: 0.0070\n",
      "\u001b[1m 91/232\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m1:05\u001b[0m 466ms/step - accuracy: 0.4956 - binary_io_u_5: 0.4999 - loss: 0.9291Batch 92, Loss Value: 0.9410\n",
      "Batch 92, Gradient Norm: 0.6097\n",
      "\u001b[1m 92/232\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m1:05\u001b[0m 466ms/step - accuracy: 0.4956 - binary_io_u_5: 0.4998 - loss: 0.9291Batch 93, Loss Value: 0.8934\n",
      "Batch 93, Gradient Norm: 0.0183\n",
      "\u001b[1m 93/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m1:04\u001b[0m 466ms/step - accuracy: 0.4955 - binary_io_u_5: 0.4997 - loss: 0.9291Batch 94, Loss Value: 0.9201\n",
      "Batch 94, Gradient Norm: 0.1149\n",
      "\u001b[1m 94/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m1:04\u001b[0m 466ms/step - accuracy: 0.4955 - binary_io_u_5: 0.4996 - loss: 0.9291Batch 95, Loss Value: 0.9437\n",
      "Batch 95, Gradient Norm: 0.0024\n",
      "\u001b[1m 95/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m1:03\u001b[0m 466ms/step - accuracy: 0.4955 - binary_io_u_5: 0.4996 - loss: 0.9291Batch 96, Loss Value: 0.9459\n",
      "Batch 96, Gradient Norm: 0.0024\n",
      "\u001b[1m 96/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m1:03\u001b[0m 466ms/step - accuracy: 0.4955 - binary_io_u_5: 0.4995 - loss: 0.9290Batch 97, Loss Value: 0.9443\n",
      "Batch 97, Gradient Norm: 0.0308\n",
      "\u001b[1m 97/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m1:02\u001b[0m 466ms/step - accuracy: 0.4955 - binary_io_u_5: 0.4994 - loss: 0.9290Batch 98, Loss Value: 0.9427\n",
      "Batch 98, Gradient Norm: 0.1549\n",
      "\u001b[1m 98/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m1:02\u001b[0m 466ms/step - accuracy: 0.4954 - binary_io_u_5: 0.4993 - loss: 0.9290Batch 99, Loss Value: 0.9452\n",
      "Batch 99, Gradient Norm: 0.0106\n",
      "\u001b[1m 99/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m1:01\u001b[0m 466ms/step - accuracy: 0.4954 - binary_io_u_5: 0.4993 - loss: 0.9290Batch 100, Loss Value: 0.9442\n",
      "Batch 100, Gradient Norm: 0.0314\n",
      "\u001b[1m100/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m1:01\u001b[0m 466ms/step - accuracy: 0.4954 - binary_io_u_5: 0.4992 - loss: 0.9290Batch 101, Loss Value: 0.9440\n",
      "Batch 101, Gradient Norm: 0.0314\n",
      "\u001b[1m101/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m1:01\u001b[0m 466ms/step - accuracy: 0.4954 - binary_io_u_5: 0.4991 - loss: 0.9290Batch 102, Loss Value: 0.9449\n",
      "Batch 102, Gradient Norm: 0.0041\n",
      "\u001b[1m102/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m1:00\u001b[0m 466ms/step - accuracy: 0.4953 - binary_io_u_5: 0.4991 - loss: 0.9290Batch 103, Loss Value: 0.9239\n",
      "Batch 103, Gradient Norm: 0.4295\n",
      "\u001b[1m103/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m1:00\u001b[0m 466ms/step - accuracy: 0.4953 - binary_io_u_5: 0.4990 - loss: 0.9290Batch 104, Loss Value: 0.9443\n",
      "Batch 104, Gradient Norm: 0.0143\n",
      "\u001b[1m104/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m59s\u001b[0m 466ms/step - accuracy: 0.4953 - binary_io_u_5: 0.4989 - loss: 0.9290 Batch 105, Loss Value: 0.9441\n",
      "Batch 105, Gradient Norm: 0.0079\n",
      "\u001b[1m105/232\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m59s\u001b[0m 466ms/step - accuracy: 0.4953 - binary_io_u_5: 0.4989 - loss: 0.9291Batch 106, Loss Value: 0.9150\n",
      "Batch 106, Gradient Norm: 1.5341\n",
      "\u001b[1m106/232\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m58s\u001b[0m 466ms/step - accuracy: 0.4952 - binary_io_u_5: 0.4988 - loss: 0.9291Batch 107, Loss Value: 0.9440\n",
      "Batch 107, Gradient Norm: 0.0000\n",
      "\u001b[1m107/232\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m58s\u001b[0m 466ms/step - accuracy: 0.4952 - binary_io_u_5: 0.4988 - loss: 0.9291Batch 108, Loss Value: 0.9086\n",
      "Batch 108, Gradient Norm: 0.1365\n",
      "\u001b[1m108/232\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m57s\u001b[0m 466ms/step - accuracy: 0.4952 - binary_io_u_5: 0.4987 - loss: 0.9291Batch 109, Loss Value: 0.9246\n",
      "Batch 109, Gradient Norm: 0.0001\n",
      "\u001b[1m109/232\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m57s\u001b[0m 466ms/step - accuracy: 0.4951 - binary_io_u_5: 0.4987 - loss: 0.9291Batch 110, Loss Value: 0.9441\n",
      "Batch 110, Gradient Norm: 0.0157\n",
      "\u001b[1m110/232\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m56s\u001b[0m 466ms/step - accuracy: 0.4951 - binary_io_u_5: 0.4986 - loss: 0.9291Batch 111, Loss Value: 0.9437\n",
      "Batch 111, Gradient Norm: 0.0025\n",
      "\u001b[1m111/232\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m56s\u001b[0m 466ms/step - accuracy: 0.4951 - binary_io_u_5: 0.4986 - loss: 0.9291Batch 112, Loss Value: 0.9431\n",
      "Batch 112, Gradient Norm: 0.0055\n",
      "\u001b[1m112/232\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m55s\u001b[0m 466ms/step - accuracy: 0.4951 - binary_io_u_5: 0.4985 - loss: 0.9291Batch 113, Loss Value: 0.9432\n",
      "Batch 113, Gradient Norm: 0.0080\n",
      "\u001b[1m113/232\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m55s\u001b[0m 466ms/step - accuracy: 0.4950 - binary_io_u_5: 0.4985 - loss: 0.9291Batch 114, Loss Value: 0.9387\n",
      "Batch 114, Gradient Norm: 0.1797\n",
      "\u001b[1m114/232\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m54s\u001b[0m 466ms/step - accuracy: 0.4950 - binary_io_u_5: 0.4984 - loss: 0.9291Batch 115, Loss Value: 0.9433\n",
      "Batch 115, Gradient Norm: 0.0185\n",
      "\u001b[1m115/232\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m54s\u001b[0m 466ms/step - accuracy: 0.4950 - binary_io_u_5: 0.4984 - loss: 0.9291Batch 116, Loss Value: 0.9030\n",
      "Batch 116, Gradient Norm: 0.1782\n",
      "\u001b[1m116/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m54s\u001b[0m 466ms/step - accuracy: 0.4950 - binary_io_u_5: 0.4983 - loss: 0.9291Batch 117, Loss Value: 0.9037\n",
      "Batch 117, Gradient Norm: 1.1329\n",
      "\u001b[1m117/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m53s\u001b[0m 466ms/step - accuracy: 0.4950 - binary_io_u_5: 0.4983 - loss: 0.9290Batch 118, Loss Value: 0.9430\n",
      "Batch 118, Gradient Norm: 0.0504\n",
      "\u001b[1m118/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m53s\u001b[0m 466ms/step - accuracy: 0.4950 - binary_io_u_5: 0.4983 - loss: 0.9290Batch 119, Loss Value: 0.9251\n",
      "Batch 119, Gradient Norm: 0.2709\n",
      "\u001b[1m119/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m52s\u001b[0m 465ms/step - accuracy: 0.4950 - binary_io_u_5: 0.4982 - loss: 0.9290Batch 120, Loss Value: 0.9296\n",
      "Batch 120, Gradient Norm: 0.1344\n",
      "\u001b[1m120/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m52s\u001b[0m 466ms/step - accuracy: 0.4950 - binary_io_u_5: 0.4982 - loss: 0.9290Batch 121, Loss Value: 0.9227\n",
      "Batch 121, Gradient Norm: 0.1072\n",
      "\u001b[1m121/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m51s\u001b[0m 466ms/step - accuracy: 0.4950 - binary_io_u_5: 0.4982 - loss: 0.9290Batch 122, Loss Value: 0.9261\n",
      "Batch 122, Gradient Norm: 1.3860\n",
      "\u001b[1m122/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m51s\u001b[0m 466ms/step - accuracy: 0.4949 - binary_io_u_5: 0.4981 - loss: 0.9290Batch 123, Loss Value: 0.9437\n",
      "Batch 123, Gradient Norm: 0.0049\n",
      "\u001b[1m123/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m50s\u001b[0m 466ms/step - accuracy: 0.4949 - binary_io_u_5: 0.4981 - loss: 0.9290Batch 124, Loss Value: 0.9432\n",
      "Batch 124, Gradient Norm: 0.0336\n",
      "\u001b[1m124/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m50s\u001b[0m 466ms/step - accuracy: 0.4949 - binary_io_u_5: 0.4980 - loss: 0.9290Batch 125, Loss Value: 0.9357\n",
      "Batch 125, Gradient Norm: 0.3492\n",
      "\u001b[1m125/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m49s\u001b[0m 466ms/step - accuracy: 0.4949 - binary_io_u_5: 0.4980 - loss: 0.9290Batch 126, Loss Value: 0.9295\n",
      "Batch 126, Gradient Norm: 0.1738\n",
      "\u001b[1m126/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m49s\u001b[0m 466ms/step - accuracy: 0.4949 - binary_io_u_5: 0.4980 - loss: 0.9290Batch 127, Loss Value: 0.9415\n",
      "Batch 127, Gradient Norm: 0.0878\n",
      "\u001b[1m127/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m48s\u001b[0m 466ms/step - accuracy: 0.4949 - binary_io_u_5: 0.4979 - loss: 0.9289Batch 128, Loss Value: 0.9202\n",
      "Batch 128, Gradient Norm: 0.4839\n",
      "\u001b[1m128/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m48s\u001b[0m 466ms/step - accuracy: 0.4949 - binary_io_u_5: 0.4979 - loss: 0.9289Batch 129, Loss Value: 0.9432\n",
      "Batch 129, Gradient Norm: 0.0573\n",
      "\u001b[1m129/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m47s\u001b[0m 466ms/step - accuracy: 0.4949 - binary_io_u_5: 0.4978 - loss: 0.9289Batch 130, Loss Value: 0.9360\n",
      "Batch 130, Gradient Norm: 0.4102\n",
      "\u001b[1m130/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m47s\u001b[0m 466ms/step - accuracy: 0.4949 - binary_io_u_5: 0.4978 - loss: 0.9289Batch 131, Loss Value: 0.9284\n",
      "Batch 131, Gradient Norm: 0.0272\n",
      "\u001b[1m131/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m47s\u001b[0m 466ms/step - accuracy: 0.4949 - binary_io_u_5: 0.4978 - loss: 0.9289Batch 132, Loss Value: 0.9430\n",
      "Batch 132, Gradient Norm: 0.0114\n",
      "\u001b[1m132/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m46s\u001b[0m 466ms/step - accuracy: 0.4949 - binary_io_u_5: 0.4977 - loss: 0.9289Batch 133, Loss Value: 0.9406\n",
      "Batch 133, Gradient Norm: 0.2054\n",
      "\u001b[1m133/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m46s\u001b[0m 466ms/step - accuracy: 0.4949 - binary_io_u_5: 0.4977 - loss: 0.9289Batch 134, Loss Value: 0.9209\n",
      "Batch 134, Gradient Norm: 0.0551\n",
      "\u001b[1m134/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m45s\u001b[0m 466ms/step - accuracy: 0.4949 - binary_io_u_5: 0.4976 - loss: 0.9289Batch 135, Loss Value: 0.9281\n",
      "Batch 135, Gradient Norm: 0.5211\n",
      "\u001b[1m135/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m45s\u001b[0m 466ms/step - accuracy: 0.4949 - binary_io_u_5: 0.4976 - loss: 0.9289Batch 136, Loss Value: 0.9431\n",
      "Batch 136, Gradient Norm: 0.0175\n",
      "\u001b[1m136/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m44s\u001b[0m 466ms/step - accuracy: 0.4948 - binary_io_u_5: 0.4975 - loss: 0.9289Batch 137, Loss Value: 0.8921\n",
      "Batch 137, Gradient Norm: 0.1093\n",
      "\u001b[1m137/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m44s\u001b[0m 466ms/step - accuracy: 0.4948 - binary_io_u_5: 0.4975 - loss: 0.9289Batch 138, Loss Value: 0.9104\n",
      "Batch 138, Gradient Norm: 0.4099\n",
      "\u001b[1m138/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m43s\u001b[0m 466ms/step - accuracy: 0.4948 - binary_io_u_5: 0.4975 - loss: 0.9288Batch 139, Loss Value: 0.9431\n",
      "Batch 139, Gradient Norm: 0.0115\n",
      "\u001b[1m139/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m43s\u001b[0m 466ms/step - accuracy: 0.4948 - binary_io_u_5: 0.4974 - loss: 0.9288Batch 140, Loss Value: 0.9436\n",
      "Batch 140, Gradient Norm: 0.0353\n",
      "\u001b[1m140/232\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m42s\u001b[0m 466ms/step - accuracy: 0.4948 - binary_io_u_5: 0.4974 - loss: 0.9288Batch 141, Loss Value: 0.9282\n",
      "Batch 141, Gradient Norm: 0.3734\n",
      "\u001b[1m141/232\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m42s\u001b[0m 466ms/step - accuracy: 0.4947 - binary_io_u_5: 0.4973 - loss: 0.9288Batch 142, Loss Value: 0.9160\n",
      "Batch 142, Gradient Norm: 0.5859\n",
      "\u001b[1m142/232\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m41s\u001b[0m 466ms/step - accuracy: 0.4947 - binary_io_u_5: 0.4973 - loss: 0.9288Batch 143, Loss Value: 0.9441\n",
      "Batch 143, Gradient Norm: 0.0156\n",
      "\u001b[1m143/232\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m41s\u001b[0m 466ms/step - accuracy: 0.4947 - binary_io_u_5: 0.4973 - loss: 0.9288Batch 144, Loss Value: 0.9243\n",
      "Batch 144, Gradient Norm: 0.3854\n",
      "\u001b[1m144/232\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m40s\u001b[0m 466ms/step - accuracy: 0.4947 - binary_io_u_5: 0.4972 - loss: 0.9288Batch 145, Loss Value: 0.9414\n",
      "Batch 145, Gradient Norm: 0.2540\n",
      "\u001b[1m145/232\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m40s\u001b[0m 466ms/step - accuracy: 0.4947 - binary_io_u_5: 0.4972 - loss: 0.9288Batch 146, Loss Value: 0.9410\n",
      "Batch 146, Gradient Norm: 0.1301\n",
      "\u001b[1m146/232\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m40s\u001b[0m 466ms/step - accuracy: 0.4946 - binary_io_u_5: 0.4971 - loss: 0.9288Batch 147, Loss Value: 0.9436\n",
      "Batch 147, Gradient Norm: 0.0106\n",
      "\u001b[1m147/232\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m39s\u001b[0m 466ms/step - accuracy: 0.4946 - binary_io_u_5: 0.4971 - loss: 0.9288Batch 148, Loss Value: 0.9436\n",
      "Batch 148, Gradient Norm: 0.0076\n",
      "\u001b[1m148/232\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m39s\u001b[0m 466ms/step - accuracy: 0.4946 - binary_io_u_5: 0.4970 - loss: 0.9288Batch 149, Loss Value: 0.9432\n",
      "Batch 149, Gradient Norm: 0.0064\n",
      "\u001b[1m149/232\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m38s\u001b[0m 466ms/step - accuracy: 0.4946 - binary_io_u_5: 0.4970 - loss: 0.9288Batch 150, Loss Value: 0.9448\n",
      "Batch 150, Gradient Norm: 0.0041\n",
      "\u001b[1m150/232\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m38s\u001b[0m 466ms/step - accuracy: 0.4945 - binary_io_u_5: 0.4970 - loss: 0.9288Batch 151, Loss Value: 0.9384\n",
      "Batch 151, Gradient Norm: 0.6563\n",
      "\u001b[1m151/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m37s\u001b[0m 466ms/step - accuracy: 0.4945 - binary_io_u_5: 0.4969 - loss: 0.9288Batch 152, Loss Value: 0.9169\n",
      "Batch 152, Gradient Norm: 0.0975\n",
      "\u001b[1m152/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m37s\u001b[0m 466ms/step - accuracy: 0.4945 - binary_io_u_5: 0.4969 - loss: 0.9288Batch 153, Loss Value: 0.9444\n",
      "Batch 153, Gradient Norm: 0.0225\n",
      "\u001b[1m153/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m36s\u001b[0m 466ms/step - accuracy: 0.4945 - binary_io_u_5: 0.4968 - loss: 0.9288Batch 154, Loss Value: 0.9451\n",
      "Batch 154, Gradient Norm: 0.0081\n",
      "\u001b[1m154/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m36s\u001b[0m 466ms/step - accuracy: 0.4945 - binary_io_u_5: 0.4968 - loss: 0.9288Batch 155, Loss Value: 0.9149\n",
      "Batch 155, Gradient Norm: 0.3381\n",
      "\u001b[1m155/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m35s\u001b[0m 466ms/step - accuracy: 0.4944 - binary_io_u_5: 0.4968 - loss: 0.9287Batch 156, Loss Value: 0.9433\n",
      "Batch 156, Gradient Norm: 0.0057\n",
      "\u001b[1m156/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m35s\u001b[0m 466ms/step - accuracy: 0.4944 - binary_io_u_5: 0.4967 - loss: 0.9287Batch 157, Loss Value: 0.9163\n",
      "Batch 157, Gradient Norm: 0.9446\n",
      "\u001b[1m157/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m34s\u001b[0m 466ms/step - accuracy: 0.4944 - binary_io_u_5: 0.4967 - loss: 0.9287Batch 158, Loss Value: 0.9435\n",
      "Batch 158, Gradient Norm: 0.0074\n",
      "\u001b[1m158/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m34s\u001b[0m 466ms/step - accuracy: 0.4944 - binary_io_u_5: 0.4967 - loss: 0.9287Batch 159, Loss Value: 0.9439\n",
      "Batch 159, Gradient Norm: 0.0062\n",
      "\u001b[1m159/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m34s\u001b[0m 466ms/step - accuracy: 0.4944 - binary_io_u_5: 0.4966 - loss: 0.9287Batch 160, Loss Value: 0.9287\n",
      "Batch 160, Gradient Norm: 0.4846\n",
      "\u001b[1m160/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m33s\u001b[0m 466ms/step - accuracy: 0.4944 - binary_io_u_5: 0.4966 - loss: 0.9287Batch 161, Loss Value: 0.9434\n",
      "Batch 161, Gradient Norm: 0.0031\n",
      "\u001b[1m161/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m33s\u001b[0m 466ms/step - accuracy: 0.4943 - binary_io_u_5: 0.4966 - loss: 0.9287Batch 162, Loss Value: 0.9321\n",
      "Batch 162, Gradient Norm: 0.2204\n",
      "\u001b[1m162/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m32s\u001b[0m 466ms/step - accuracy: 0.4943 - binary_io_u_5: 0.4965 - loss: 0.9287Batch 163, Loss Value: 0.9336\n",
      "Batch 163, Gradient Norm: 0.2139\n",
      "\u001b[1m163/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m32s\u001b[0m 466ms/step - accuracy: 0.4943 - binary_io_u_5: 0.4965 - loss: 0.9287Batch 164, Loss Value: 0.9276\n",
      "Batch 164, Gradient Norm: 0.2792\n",
      "\u001b[1m164/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m31s\u001b[0m 466ms/step - accuracy: 0.4943 - binary_io_u_5: 0.4965 - loss: 0.9287Batch 165, Loss Value: 0.9053\n",
      "Batch 165, Gradient Norm: 0.1455\n",
      "\u001b[1m165/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m31s\u001b[0m 466ms/step - accuracy: 0.4943 - binary_io_u_5: 0.4964 - loss: 0.9287Batch 166, Loss Value: 0.9420\n",
      "Batch 166, Gradient Norm: 0.2227\n",
      "\u001b[1m166/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m30s\u001b[0m 466ms/step - accuracy: 0.4943 - binary_io_u_5: 0.4964 - loss: 0.9287Batch 167, Loss Value: 0.9436\n",
      "Batch 167, Gradient Norm: 0.0024\n",
      "\u001b[1m167/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m30s\u001b[0m 466ms/step - accuracy: 0.4943 - binary_io_u_5: 0.4964 - loss: 0.9287Batch 168, Loss Value: 0.9431\n",
      "Batch 168, Gradient Norm: 0.0130\n",
      "\u001b[1m168/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m29s\u001b[0m 466ms/step - accuracy: 0.4943 - binary_io_u_5: 0.4963 - loss: 0.9287Batch 169, Loss Value: 0.9441\n",
      "Batch 169, Gradient Norm: 0.0019\n",
      "\u001b[1m169/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m29s\u001b[0m 466ms/step - accuracy: 0.4942 - binary_io_u_5: 0.4963 - loss: 0.9287Batch 170, Loss Value: 0.9317\n",
      "Batch 170, Gradient Norm: 0.3558\n",
      "\u001b[1m170/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m28s\u001b[0m 466ms/step - accuracy: 0.4942 - binary_io_u_5: 0.4963 - loss: 0.9286Batch 171, Loss Value: 0.9106\n",
      "Batch 171, Gradient Norm: 0.2524\n",
      "\u001b[1m171/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m28s\u001b[0m 466ms/step - accuracy: 0.4942 - binary_io_u_5: 0.4963 - loss: 0.9286Batch 172, Loss Value: 0.9412\n",
      "Batch 172, Gradient Norm: 0.2019\n",
      "\u001b[1m172/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m27s\u001b[0m 466ms/step - accuracy: 0.4942 - binary_io_u_5: 0.4962 - loss: 0.9286Batch 173, Loss Value: 0.9268\n",
      "Batch 173, Gradient Norm: 0.0807\n",
      "\u001b[1m173/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m27s\u001b[0m 466ms/step - accuracy: 0.4942 - binary_io_u_5: 0.4962 - loss: 0.9286Batch 174, Loss Value: 0.9434\n",
      "Batch 174, Gradient Norm: 0.0008\n",
      "\u001b[1m174/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m27s\u001b[0m 466ms/step - accuracy: 0.4942 - binary_io_u_5: 0.4962 - loss: 0.9286Batch 175, Loss Value: 0.9180\n",
      "Batch 175, Gradient Norm: 0.1876\n",
      "\u001b[1m175/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m26s\u001b[0m 466ms/step - accuracy: 0.4942 - binary_io_u_5: 0.4962 - loss: 0.9286Batch 176, Loss Value: 0.9433\n",
      "Batch 176, Gradient Norm: 0.0021\n",
      "\u001b[1m176/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m26s\u001b[0m 466ms/step - accuracy: 0.4942 - binary_io_u_5: 0.4962 - loss: 0.9286Batch 177, Loss Value: 0.9401\n",
      "Batch 177, Gradient Norm: 0.2527\n",
      "\u001b[1m177/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m25s\u001b[0m 466ms/step - accuracy: 0.4942 - binary_io_u_5: 0.4962 - loss: 0.9285Batch 178, Loss Value: 0.9437\n",
      "Batch 178, Gradient Norm: 0.0080\n",
      "\u001b[1m178/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m25s\u001b[0m 466ms/step - accuracy: 0.4942 - binary_io_u_5: 0.4961 - loss: 0.9285Batch 179, Loss Value: 0.9439\n",
      "Batch 179, Gradient Norm: 0.0682\n",
      "\u001b[1m179/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m24s\u001b[0m 466ms/step - accuracy: 0.4942 - binary_io_u_5: 0.4961 - loss: 0.9285Batch 180, Loss Value: 0.9434\n",
      "Batch 180, Gradient Norm: 0.0638\n",
      "\u001b[1m180/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m24s\u001b[0m 466ms/step - accuracy: 0.4942 - binary_io_u_5: 0.4961 - loss: 0.9285Batch 181, Loss Value: 0.9435\n",
      "Batch 181, Gradient Norm: 0.0026\n",
      "\u001b[1m181/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m23s\u001b[0m 466ms/step - accuracy: 0.4942 - binary_io_u_5: 0.4961 - loss: 0.9285Batch 182, Loss Value: 0.9436\n",
      "Batch 182, Gradient Norm: 0.0037\n",
      "\u001b[1m182/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m23s\u001b[0m 466ms/step - accuracy: 0.4942 - binary_io_u_5: 0.4961 - loss: 0.9285Batch 183, Loss Value: 0.9435\n",
      "Batch 183, Gradient Norm: 0.0561\n",
      "\u001b[1m183/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m22s\u001b[0m 466ms/step - accuracy: 0.4942 - binary_io_u_5: 0.4961 - loss: 0.9284Batch 184, Loss Value: 0.9319\n",
      "Batch 184, Gradient Norm: 0.8452\n",
      "\u001b[1m184/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m22s\u001b[0m 466ms/step - accuracy: 0.4942 - binary_io_u_5: 0.4960 - loss: 0.9284Batch 185, Loss Value: 0.9434\n",
      "Batch 185, Gradient Norm: 0.0006\n",
      "\u001b[1m185/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m21s\u001b[0m 466ms/step - accuracy: 0.4942 - binary_io_u_5: 0.4960 - loss: 0.9284Batch 186, Loss Value: 0.9233\n",
      "Batch 186, Gradient Norm: 1.6209\n",
      "\u001b[1m186/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m21s\u001b[0m 466ms/step - accuracy: 0.4943 - binary_io_u_5: 0.4960 - loss: 0.9284Batch 187, Loss Value: 0.9432\n",
      "Batch 187, Gradient Norm: 0.0100\n",
      "\u001b[1m187/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m20s\u001b[0m 466ms/step - accuracy: 0.4943 - binary_io_u_5: 0.4960 - loss: 0.9284Batch 188, Loss Value: 0.9028\n",
      "Batch 188, Gradient Norm: 0.2889\n",
      "\u001b[1m188/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m20s\u001b[0m 466ms/step - accuracy: 0.4943 - binary_io_u_5: 0.4960 - loss: 0.9284Batch 189, Loss Value: 0.9434\n",
      "Batch 189, Gradient Norm: 0.0095\n",
      "\u001b[1m189/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m20s\u001b[0m 466ms/step - accuracy: 0.4943 - binary_io_u_5: 0.4960 - loss: 0.9284Batch 190, Loss Value: 0.9273\n",
      "Batch 190, Gradient Norm: 0.0866\n",
      "\u001b[1m190/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m19s\u001b[0m 466ms/step - accuracy: 0.4943 - binary_io_u_5: 0.4960 - loss: 0.9283Batch 191, Loss Value: 0.9307\n",
      "Batch 191, Gradient Norm: 0.1371\n",
      "\u001b[1m191/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m19s\u001b[0m 466ms/step - accuracy: 0.4943 - binary_io_u_5: 0.4960 - loss: 0.9283Batch 192, Loss Value: 0.9299\n",
      "Batch 192, Gradient Norm: 0.1733\n",
      "\u001b[1m192/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m18s\u001b[0m 466ms/step - accuracy: 0.4943 - binary_io_u_5: 0.4959 - loss: 0.9283Batch 193, Loss Value: 0.9433\n",
      "Batch 193, Gradient Norm: 0.0194\n",
      "\u001b[1m193/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m18s\u001b[0m 466ms/step - accuracy: 0.4942 - binary_io_u_5: 0.4959 - loss: 0.9283Batch 194, Loss Value: 0.9433\n",
      "Batch 194, Gradient Norm: 0.0045\n",
      "\u001b[1m194/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m17s\u001b[0m 466ms/step - accuracy: 0.4942 - binary_io_u_5: 0.4959 - loss: 0.9283Batch 195, Loss Value: 0.9433\n",
      "Batch 195, Gradient Norm: 0.0253\n",
      "\u001b[1m195/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m17s\u001b[0m 466ms/step - accuracy: 0.4942 - binary_io_u_5: 0.4959 - loss: 0.9283Batch 196, Loss Value: 0.9360\n",
      "Batch 196, Gradient Norm: 0.0071\n",
      "\u001b[1m196/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m16s\u001b[0m 466ms/step - accuracy: 0.4942 - binary_io_u_5: 0.4959 - loss: 0.9283Batch 197, Loss Value: 0.9441\n",
      "Batch 197, Gradient Norm: 0.0048\n",
      "\u001b[1m197/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m16s\u001b[0m 466ms/step - accuracy: 0.4942 - binary_io_u_5: 0.4959 - loss: 0.9283Batch 198, Loss Value: 0.9432\n",
      "Batch 198, Gradient Norm: 0.0057\n",
      "\u001b[1m198/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m15s\u001b[0m 466ms/step - accuracy: 0.4942 - binary_io_u_5: 0.4958 - loss: 0.9283Batch 199, Loss Value: 0.9169\n",
      "Batch 199, Gradient Norm: 0.1223\n",
      "\u001b[1m199/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m15s\u001b[0m 466ms/step - accuracy: 0.4942 - binary_io_u_5: 0.4958 - loss: 0.9282Batch 200, Loss Value: 0.9390\n",
      "Batch 200, Gradient Norm: 1.0766\n",
      "\u001b[1m200/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m14s\u001b[0m 466ms/step - accuracy: 0.4942 - binary_io_u_5: 0.4958 - loss: 0.9282Batch 201, Loss Value: 0.9440\n",
      "Batch 201, Gradient Norm: 0.0273\n",
      "\u001b[1m201/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m14s\u001b[0m 466ms/step - accuracy: 0.4942 - binary_io_u_5: 0.4958 - loss: 0.9282Batch 202, Loss Value: 0.9434\n",
      "Batch 202, Gradient Norm: 0.0023\n",
      "\u001b[1m202/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m13s\u001b[0m 466ms/step - accuracy: 0.4942 - binary_io_u_5: 0.4958 - loss: 0.9282Batch 203, Loss Value: 0.9438\n",
      "Batch 203, Gradient Norm: 0.0080\n",
      "\u001b[1m203/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m13s\u001b[0m 466ms/step - accuracy: 0.4942 - binary_io_u_5: 0.4958 - loss: 0.9282Batch 204, Loss Value: 0.9431\n",
      "Batch 204, Gradient Norm: 0.0202\n",
      "\u001b[1m204/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m13s\u001b[0m 466ms/step - accuracy: 0.4942 - binary_io_u_5: 0.4958 - loss: 0.9282Batch 205, Loss Value: 0.9430\n",
      "Batch 205, Gradient Norm: 0.0189\n",
      "\u001b[1m205/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m12s\u001b[0m 466ms/step - accuracy: 0.4942 - binary_io_u_5: 0.4957 - loss: 0.9282Batch 206, Loss Value: 0.9434\n",
      "Batch 206, Gradient Norm: 0.0017\n",
      "\u001b[1m206/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m12s\u001b[0m 466ms/step - accuracy: 0.4942 - binary_io_u_5: 0.4957 - loss: 0.9282Batch 207, Loss Value: 0.9296\n",
      "Batch 207, Gradient Norm: 0.0644\n",
      "\u001b[1m207/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m11s\u001b[0m 466ms/step - accuracy: 0.4942 - binary_io_u_5: 0.4957 - loss: 0.9282Batch 208, Loss Value: 0.9327\n",
      "Batch 208, Gradient Norm: 0.6504\n",
      "\u001b[1m208/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m11s\u001b[0m 466ms/step - accuracy: 0.4942 - binary_io_u_5: 0.4957 - loss: 0.9281Batch 209, Loss Value: 0.9399\n",
      "Batch 209, Gradient Norm: 1.3726\n",
      "\u001b[1m209/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m10s\u001b[0m 466ms/step - accuracy: 0.4942 - binary_io_u_5: 0.4957 - loss: 0.9281Batch 210, Loss Value: 0.8943\n",
      "Batch 210, Gradient Norm: 0.8381\n",
      "\u001b[1m210/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m10s\u001b[0m 466ms/step - accuracy: 0.4942 - binary_io_u_5: 0.4957 - loss: 0.9281Batch 211, Loss Value: 0.9430\n",
      "Batch 211, Gradient Norm: 0.0237\n",
      "\u001b[1m211/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m9s\u001b[0m 466ms/step - accuracy: 0.4942 - binary_io_u_5: 0.4957 - loss: 0.9281 Batch 212, Loss Value: 0.9321\n",
      "Batch 212, Gradient Norm: 0.5670\n",
      "\u001b[1m212/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m9s\u001b[0m 466ms/step - accuracy: 0.4942 - binary_io_u_5: 0.4957 - loss: 0.9281Batch 213, Loss Value: 0.9104\n",
      "Batch 213, Gradient Norm: 0.0062\n",
      "\u001b[1m213/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m8s\u001b[0m 466ms/step - accuracy: 0.4942 - binary_io_u_5: 0.4957 - loss: 0.9281Batch 214, Loss Value: 0.9183\n",
      "Batch 214, Gradient Norm: 0.6328\n",
      "\u001b[1m214/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m8s\u001b[0m 466ms/step - accuracy: 0.4942 - binary_io_u_5: 0.4957 - loss: 0.9280Batch 215, Loss Value: 0.9339\n",
      "Batch 215, Gradient Norm: 0.0788\n",
      "\u001b[1m215/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m7s\u001b[0m 466ms/step - accuracy: 0.4942 - binary_io_u_5: 0.4956 - loss: 0.9280Batch 216, Loss Value: 0.8913\n",
      "Batch 216, Gradient Norm: 0.4807\n",
      "\u001b[1m216/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m7s\u001b[0m 466ms/step - accuracy: 0.4942 - binary_io_u_5: 0.4956 - loss: 0.9280Batch 217, Loss Value: 0.9218\n",
      "Batch 217, Gradient Norm: 0.0143\n",
      "\u001b[1m217/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m6s\u001b[0m 466ms/step - accuracy: 0.4942 - binary_io_u_5: 0.4956 - loss: 0.9280Batch 218, Loss Value: 0.9431\n",
      "Batch 218, Gradient Norm: 0.0577\n",
      "\u001b[1m218/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m6s\u001b[0m 466ms/step - accuracy: 0.4943 - binary_io_u_5: 0.4956 - loss: 0.9280Batch 219, Loss Value: 0.9432\n",
      "Batch 219, Gradient Norm: 0.0020\n",
      "\u001b[1m219/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m6s\u001b[0m 466ms/step - accuracy: 0.4943 - binary_io_u_5: 0.4956 - loss: 0.9280Batch 220, Loss Value: 0.9389\n",
      "Batch 220, Gradient Norm: 0.4245\n",
      "\u001b[1m220/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m5s\u001b[0m 466ms/step - accuracy: 0.4943 - binary_io_u_5: 0.4956 - loss: 0.9280Batch 221, Loss Value: 0.9274\n",
      "Batch 221, Gradient Norm: 0.0164\n",
      "\u001b[1m221/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m5s\u001b[0m 466ms/step - accuracy: 0.4943 - binary_io_u_5: 0.4956 - loss: 0.9279Batch 222, Loss Value: 0.9093\n",
      "Batch 222, Gradient Norm: 0.1208\n",
      "\u001b[1m222/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m4s\u001b[0m 466ms/step - accuracy: 0.4943 - binary_io_u_5: 0.4956 - loss: 0.9279Batch 223, Loss Value: 0.9019\n",
      "Batch 223, Gradient Norm: 0.0058\n",
      "\u001b[1m223/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m4s\u001b[0m 466ms/step - accuracy: 0.4943 - binary_io_u_5: 0.4956 - loss: 0.9279Batch 224, Loss Value: 0.9430\n",
      "Batch 224, Gradient Norm: 0.0012\n",
      "\u001b[1m224/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m3s\u001b[0m 466ms/step - accuracy: 0.4943 - binary_io_u_5: 0.4956 - loss: 0.9279Batch 225, Loss Value: 0.9432\n",
      "Batch 225, Gradient Norm: 0.0092\n",
      "\u001b[1m225/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m3s\u001b[0m 466ms/step - accuracy: 0.4943 - binary_io_u_5: 0.4956 - loss: 0.9279Batch 226, Loss Value: 0.9259\n",
      "Batch 226, Gradient Norm: 0.5256\n",
      "\u001b[1m226/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m2s\u001b[0m 466ms/step - accuracy: 0.4943 - binary_io_u_5: 0.4956 - loss: 0.9279Batch 227, Loss Value: 0.9367\n",
      "Batch 227, Gradient Norm: 0.0793\n",
      "\u001b[1m227/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m2s\u001b[0m 466ms/step - accuracy: 0.4943 - binary_io_u_5: 0.4956 - loss: 0.9278Batch 228, Loss Value: 0.9436\n",
      "Batch 228, Gradient Norm: 0.0087\n",
      "\u001b[1m228/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m1s\u001b[0m 466ms/step - accuracy: 0.4943 - binary_io_u_5: 0.4956 - loss: 0.9278Batch 229, Loss Value: 0.9430\n",
      "Batch 229, Gradient Norm: 0.0322\n",
      "\u001b[1m229/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m1s\u001b[0m 466ms/step - accuracy: 0.4944 - binary_io_u_5: 0.4956 - loss: 0.9278Batch 230, Loss Value: 0.9438\n",
      "Batch 230, Gradient Norm: 0.0245\n",
      "\u001b[1m230/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 466ms/step - accuracy: 0.4944 - binary_io_u_5: 0.4956 - loss: 0.9278Batch 231, Loss Value: 0.9428\n",
      "Batch 231, Gradient Norm: 0.0005\n",
      "\u001b[1m231/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 466ms/step - accuracy: 0.4944 - binary_io_u_5: 0.4956 - loss: 0.9278Batch 232, Loss Value: 0.9428\n",
      "Batch 232, Gradient Norm: 0.0065\n",
      "\u001b[1m232/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m137s\u001b[0m 472ms/step - accuracy: 0.4944 - binary_io_u_5: 0.4956 - loss: 0.9277 - val_accuracy: 0.4949 - val_binary_io_u_5: 0.4949 - val_loss: 0.9126 - learning_rate: 0.0010\n",
      "Epoch 4/200\n",
      "Batch 1, Loss Value: 0.8816\n",
      "Batch 1, Gradient Norm: 0.0917\n",
      "\u001b[1m  1/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:56\u001b[0m 506ms/step - accuracy: 0.4700 - binary_io_u_5: 0.4800 - loss: 0.9354Batch 2, Loss Value: 0.9000\n",
      "Batch 2, Gradient Norm: 0.0877\n",
      "\u001b[1m  2/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:47\u001b[0m 467ms/step - accuracy: 0.4825 - binary_io_u_5: 0.5025 - loss: 0.9289Batch 3, Loss Value: 0.9227\n",
      "Batch 3, Gradient Norm: 0.5017\n",
      "\u001b[1m  3/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:48\u001b[0m 472ms/step - accuracy: 0.4850 - binary_io_u_5: 0.5039 - loss: 0.9292Batch 4, Loss Value: 0.9425\n",
      "Batch 4, Gradient Norm: 0.0352\n",
      "\u001b[1m  4/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:47\u001b[0m 471ms/step - accuracy: 0.4881 - binary_io_u_5: 0.5042 - loss: 0.9280Batch 5, Loss Value: 0.9289\n",
      "Batch 5, Gradient Norm: 0.0901\n",
      "\u001b[1m  5/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:46\u001b[0m 468ms/step - accuracy: 0.4905 - binary_io_u_5: 0.5021 - loss: 0.9272Batch 6, Loss Value: 0.9411\n",
      "Batch 6, Gradient Norm: 0.3352\n",
      "\u001b[1m  6/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:46\u001b[0m 469ms/step - accuracy: 0.4921 - binary_io_u_5: 0.4998 - loss: 0.9272Batch 7, Loss Value: 0.8980\n",
      "Batch 7, Gradient Norm: 0.0000\n",
      "\u001b[1m  7/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:45\u001b[0m 468ms/step - accuracy: 0.4942 - binary_io_u_5: 0.4990 - loss: 0.9265Batch 8, Loss Value: 0.9119\n",
      "Batch 8, Gradient Norm: 0.3564\n",
      "\u001b[1m  8/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:44\u001b[0m 468ms/step - accuracy: 0.4957 - binary_io_u_5: 0.4988 - loss: 0.9259Batch 9, Loss Value: 0.9058\n",
      "Batch 9, Gradient Norm: 0.6110\n",
      "\u001b[1m  9/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:44\u001b[0m 469ms/step - accuracy: 0.4960 - binary_io_u_5: 0.4981 - loss: 0.9258Batch 10, Loss Value: 0.9323\n",
      "Batch 10, Gradient Norm: 0.0666\n",
      "\u001b[1m 10/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:44\u001b[0m 469ms/step - accuracy: 0.4955 - binary_io_u_5: 0.4972 - loss: 0.9259Batch 11, Loss Value: 0.9315\n",
      "Batch 11, Gradient Norm: 0.0645\n",
      "\u001b[1m 11/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:43\u001b[0m 469ms/step - accuracy: 0.4950 - binary_io_u_5: 0.4965 - loss: 0.9260Batch 12, Loss Value: 0.9367\n",
      "Batch 12, Gradient Norm: 0.2148\n",
      "\u001b[1m 12/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:43\u001b[0m 469ms/step - accuracy: 0.4948 - binary_io_u_5: 0.4965 - loss: 0.9259Batch 13, Loss Value: 0.8812\n",
      "Batch 13, Gradient Norm: 0.0293\n",
      "\u001b[1m 13/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:42\u001b[0m 470ms/step - accuracy: 0.4946 - binary_io_u_5: 0.4961 - loss: 0.9259Batch 14, Loss Value: 0.9282\n",
      "Batch 14, Gradient Norm: 0.4773\n",
      "\u001b[1m 14/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:42\u001b[0m 469ms/step - accuracy: 0.4946 - binary_io_u_5: 0.4961 - loss: 0.9257Batch 15, Loss Value: 0.9421\n",
      "Batch 15, Gradient Norm: 0.0187\n",
      "\u001b[1m 15/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:41\u001b[0m 470ms/step - accuracy: 0.4944 - binary_io_u_5: 0.4961 - loss: 0.9257Batch 16, Loss Value: 0.9423\n",
      "Batch 16, Gradient Norm: 0.0227\n",
      "\u001b[1m 16/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:41\u001b[0m 470ms/step - accuracy: 0.4944 - binary_io_u_5: 0.4962 - loss: 0.9255Batch 17, Loss Value: 0.8977\n",
      "Batch 17, Gradient Norm: 0.0678\n",
      "\u001b[1m 17/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:40\u001b[0m 470ms/step - accuracy: 0.4944 - binary_io_u_5: 0.4963 - loss: 0.9254Batch 18, Loss Value: 0.9239\n",
      "Batch 18, Gradient Norm: 0.4199\n",
      "\u001b[1m 18/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:40\u001b[0m 470ms/step - accuracy: 0.4946 - binary_io_u_5: 0.4965 - loss: 0.9251Batch 19, Loss Value: 0.9378\n",
      "Batch 19, Gradient Norm: 0.3591\n",
      "\u001b[1m 19/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:40\u001b[0m 470ms/step - accuracy: 0.4947 - binary_io_u_5: 0.4965 - loss: 0.9250Batch 20, Loss Value: 0.9165\n",
      "Batch 20, Gradient Norm: 0.0026\n",
      "\u001b[1m 20/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:39\u001b[0m 470ms/step - accuracy: 0.4948 - binary_io_u_5: 0.4965 - loss: 0.9248Batch 21, Loss Value: 0.9417\n",
      "Batch 21, Gradient Norm: 0.0000\n",
      "\u001b[1m 21/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:39\u001b[0m 470ms/step - accuracy: 0.4950 - binary_io_u_5: 0.4966 - loss: 0.9246Batch 22, Loss Value: 0.9397\n",
      "Batch 22, Gradient Norm: 0.7261\n",
      "\u001b[1m 22/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:38\u001b[0m 470ms/step - accuracy: 0.4950 - binary_io_u_5: 0.4966 - loss: 0.9245Batch 23, Loss Value: 0.8654\n",
      "Batch 23, Gradient Norm: 0.0080\n",
      "\u001b[1m 23/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:38\u001b[0m 470ms/step - accuracy: 0.4951 - binary_io_u_5: 0.4968 - loss: 0.9243Batch 24, Loss Value: 0.9258\n",
      "Batch 24, Gradient Norm: 0.4878\n",
      "\u001b[1m 24/232\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:37\u001b[0m 470ms/step - accuracy: 0.4952 - binary_io_u_5: 0.4970 - loss: 0.9241Batch 25, Loss Value: 0.9425\n",
      "Batch 25, Gradient Norm: 0.0209\n",
      "\u001b[1m 25/232\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:37\u001b[0m 470ms/step - accuracy: 0.4954 - binary_io_u_5: 0.4974 - loss: 0.9240Batch 26, Loss Value: 0.9261\n",
      "Batch 26, Gradient Norm: 0.4249\n",
      "\u001b[1m 26/232\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:36\u001b[0m 470ms/step - accuracy: 0.4956 - binary_io_u_5: 0.4978 - loss: 0.9238Batch 27, Loss Value: 0.9112\n",
      "Batch 27, Gradient Norm: 0.1046\n",
      "\u001b[1m 27/232\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:36\u001b[0m 470ms/step - accuracy: 0.4957 - binary_io_u_5: 0.4980 - loss: 0.9237Batch 28, Loss Value: 0.9419\n",
      "Batch 28, Gradient Norm: 0.0167\n",
      "\u001b[1m 28/232\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:35\u001b[0m 470ms/step - accuracy: 0.4959 - binary_io_u_5: 0.4983 - loss: 0.9236Batch 29, Loss Value: 0.9419\n",
      "Batch 29, Gradient Norm: 0.0245\n",
      "\u001b[1m 29/232\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:35\u001b[0m 470ms/step - accuracy: 0.4960 - binary_io_u_5: 0.4985 - loss: 0.9234Batch 30, Loss Value: 0.8826\n",
      "Batch 30, Gradient Norm: 0.2807\n",
      "\u001b[1m 30/232\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:35\u001b[0m 471ms/step - accuracy: 0.4962 - binary_io_u_5: 0.4988 - loss: 0.9233Batch 31, Loss Value: 0.9071\n",
      "Batch 31, Gradient Norm: 0.3006\n",
      "\u001b[1m 31/232\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:34\u001b[0m 471ms/step - accuracy: 0.4964 - binary_io_u_5: 0.4991 - loss: 0.9232Batch 32, Loss Value: 0.9093\n",
      "Batch 32, Gradient Norm: 0.1152\n",
      "\u001b[1m 32/232\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:34\u001b[0m 471ms/step - accuracy: 0.4966 - binary_io_u_5: 0.4993 - loss: 0.9231Batch 33, Loss Value: 0.9412\n",
      "Batch 33, Gradient Norm: 0.1182\n",
      "\u001b[1m 33/232\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:33\u001b[0m 471ms/step - accuracy: 0.4968 - binary_io_u_5: 0.4996 - loss: 0.9229Batch 34, Loss Value: 0.9213\n",
      "Batch 34, Gradient Norm: 0.2085\n",
      "\u001b[1m 34/232\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:33\u001b[0m 471ms/step - accuracy: 0.4969 - binary_io_u_5: 0.4998 - loss: 0.9229Batch 35, Loss Value: 0.9157\n",
      "Batch 35, Gradient Norm: 0.3540\n",
      "\u001b[1m 35/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:32\u001b[0m 471ms/step - accuracy: 0.4970 - binary_io_u_5: 0.4999 - loss: 0.9228Batch 36, Loss Value: 0.9225\n",
      "Batch 36, Gradient Norm: 0.2269\n",
      "\u001b[1m 36/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:32\u001b[0m 471ms/step - accuracy: 0.4971 - binary_io_u_5: 0.5000 - loss: 0.9227Batch 37, Loss Value: 0.9043\n",
      "Batch 37, Gradient Norm: 0.2426\n",
      "\u001b[1m 37/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:31\u001b[0m 471ms/step - accuracy: 0.4971 - binary_io_u_5: 0.5002 - loss: 0.9227Batch 38, Loss Value: 0.9299\n",
      "Batch 38, Gradient Norm: 1.0648\n",
      "\u001b[1m 38/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:31\u001b[0m 472ms/step - accuracy: 0.4971 - binary_io_u_5: 0.5003 - loss: 0.9226Batch 39, Loss Value: 0.9247\n",
      "Batch 39, Gradient Norm: 0.1259\n",
      "\u001b[1m 39/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:31\u001b[0m 472ms/step - accuracy: 0.4972 - binary_io_u_5: 0.5005 - loss: 0.9226Batch 40, Loss Value: 0.9419\n",
      "Batch 40, Gradient Norm: 0.1725\n",
      "\u001b[1m 40/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:30\u001b[0m 472ms/step - accuracy: 0.4973 - binary_io_u_5: 0.5006 - loss: 0.9225Batch 41, Loss Value: 0.9228\n",
      "Batch 41, Gradient Norm: 0.2817\n",
      "\u001b[1m 41/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:30\u001b[0m 472ms/step - accuracy: 0.4973 - binary_io_u_5: 0.5008 - loss: 0.9225Batch 42, Loss Value: 0.9049\n",
      "Batch 42, Gradient Norm: 0.0096\n",
      "\u001b[1m 42/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:29\u001b[0m 471ms/step - accuracy: 0.4974 - binary_io_u_5: 0.5009 - loss: 0.9224Batch 43, Loss Value: 0.9274\n",
      "Batch 43, Gradient Norm: 0.0540\n",
      "\u001b[1m 43/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:29\u001b[0m 471ms/step - accuracy: 0.4975 - binary_io_u_5: 0.5010 - loss: 0.9224Batch 44, Loss Value: 0.9183\n",
      "Batch 44, Gradient Norm: 0.5633\n",
      "\u001b[1m 44/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:28\u001b[0m 471ms/step - accuracy: 0.4975 - binary_io_u_5: 0.5011 - loss: 0.9223Batch 45, Loss Value: 0.9302\n",
      "Batch 45, Gradient Norm: 0.5461\n",
      "\u001b[1m 45/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:28\u001b[0m 471ms/step - accuracy: 0.4975 - binary_io_u_5: 0.5012 - loss: 0.9223Batch 46, Loss Value: 0.9201\n",
      "Batch 46, Gradient Norm: 0.2275\n",
      "\u001b[1m 46/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:27\u001b[0m 471ms/step - accuracy: 0.4975 - binary_io_u_5: 0.5013 - loss: 0.9223Batch 47, Loss Value: 0.9438\n",
      "Batch 47, Gradient Norm: 0.0566\n",
      "\u001b[1m 47/232\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:27\u001b[0m 471ms/step - accuracy: 0.4975 - binary_io_u_5: 0.5014 - loss: 0.9222Batch 48, Loss Value: 0.9050\n",
      "Batch 48, Gradient Norm: 0.5515\n",
      "\u001b[1m 48/232\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:26\u001b[0m 471ms/step - accuracy: 0.4975 - binary_io_u_5: 0.5015 - loss: 0.9222Batch 49, Loss Value: 0.9432\n",
      "Batch 49, Gradient Norm: 0.0253\n",
      "\u001b[1m 49/232\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:26\u001b[0m 471ms/step - accuracy: 0.4975 - binary_io_u_5: 0.5016 - loss: 0.9221Batch 50, Loss Value: 0.9389\n",
      "Batch 50, Gradient Norm: 0.5503\n",
      "\u001b[1m 50/232\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:25\u001b[0m 470ms/step - accuracy: 0.4975 - binary_io_u_5: 0.5016 - loss: 0.9221Batch 51, Loss Value: 0.9402\n",
      "Batch 51, Gradient Norm: 1.8187\n",
      "\u001b[1m 51/232\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:25\u001b[0m 470ms/step - accuracy: 0.4975 - binary_io_u_5: 0.5017 - loss: 0.9220Batch 52, Loss Value: 0.9038\n",
      "Batch 52, Gradient Norm: 0.4083\n",
      "\u001b[1m 52/232\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:24\u001b[0m 470ms/step - accuracy: 0.4975 - binary_io_u_5: 0.5018 - loss: 0.9220Batch 53, Loss Value: 0.9287\n",
      "Batch 53, Gradient Norm: 0.5046\n",
      "\u001b[1m 53/232\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:24\u001b[0m 470ms/step - accuracy: 0.4975 - binary_io_u_5: 0.5018 - loss: 0.9219Batch 54, Loss Value: 0.8900\n",
      "Batch 54, Gradient Norm: 1.1714\n",
      "\u001b[1m 54/232\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:23\u001b[0m 470ms/step - accuracy: 0.4975 - binary_io_u_5: 0.5018 - loss: 0.9219Batch 55, Loss Value: 0.9298\n",
      "Batch 55, Gradient Norm: 0.2485\n",
      "\u001b[1m 55/232\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:23\u001b[0m 470ms/step - accuracy: 0.4974 - binary_io_u_5: 0.5018 - loss: 0.9218Batch 56, Loss Value: 0.9446\n",
      "Batch 56, Gradient Norm: 0.0324\n",
      "\u001b[1m 56/232\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:22\u001b[0m 470ms/step - accuracy: 0.4974 - binary_io_u_5: 0.5018 - loss: 0.9218Batch 57, Loss Value: 0.9193\n",
      "Batch 57, Gradient Norm: 0.4601\n",
      "\u001b[1m 57/232\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:22\u001b[0m 470ms/step - accuracy: 0.4974 - binary_io_u_5: 0.5018 - loss: 0.9218Batch 58, Loss Value: 0.9342\n",
      "Batch 58, Gradient Norm: 0.0891\n",
      "\u001b[1m 58/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:21\u001b[0m 469ms/step - accuracy: 0.4974 - binary_io_u_5: 0.5018 - loss: 0.9217Batch 59, Loss Value: 0.9093\n",
      "Batch 59, Gradient Norm: 0.5578\n",
      "\u001b[1m 59/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:21\u001b[0m 469ms/step - accuracy: 0.4974 - binary_io_u_5: 0.5018 - loss: 0.9217Batch 60, Loss Value: 0.9258\n",
      "Batch 60, Gradient Norm: 0.7005\n",
      "\u001b[1m 60/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:20\u001b[0m 469ms/step - accuracy: 0.4973 - binary_io_u_5: 0.5018 - loss: 0.9216Batch 61, Loss Value: 0.9152\n",
      "Batch 61, Gradient Norm: 0.3600\n",
      "\u001b[1m 61/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:20\u001b[0m 469ms/step - accuracy: 0.4973 - binary_io_u_5: 0.5018 - loss: 0.9216Batch 62, Loss Value: 0.9338\n",
      "Batch 62, Gradient Norm: 0.1336\n",
      "\u001b[1m 62/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:19\u001b[0m 469ms/step - accuracy: 0.4972 - binary_io_u_5: 0.5018 - loss: 0.9216Batch 63, Loss Value: 0.9170\n",
      "Batch 63, Gradient Norm: 0.2427\n",
      "\u001b[1m 63/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:19\u001b[0m 469ms/step - accuracy: 0.4972 - binary_io_u_5: 0.5018 - loss: 0.9216Batch 64, Loss Value: 0.9186\n",
      "Batch 64, Gradient Norm: 0.1155\n",
      "\u001b[1m 64/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:18\u001b[0m 469ms/step - accuracy: 0.4971 - binary_io_u_5: 0.5017 - loss: 0.9215Batch 65, Loss Value: 0.9055\n",
      "Batch 65, Gradient Norm: 0.0978\n",
      "\u001b[1m 65/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:18\u001b[0m 469ms/step - accuracy: 0.4970 - binary_io_u_5: 0.5017 - loss: 0.9215Batch 66, Loss Value: 0.9343\n",
      "Batch 66, Gradient Norm: 0.4273\n",
      "\u001b[1m 66/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:17\u001b[0m 469ms/step - accuracy: 0.4970 - binary_io_u_5: 0.5016 - loss: 0.9215Batch 67, Loss Value: 0.9381\n",
      "Batch 67, Gradient Norm: 0.0629\n",
      "\u001b[1m 67/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:17\u001b[0m 469ms/step - accuracy: 0.4969 - binary_io_u_5: 0.5016 - loss: 0.9215Batch 68, Loss Value: 0.9409\n",
      "Batch 68, Gradient Norm: 0.2199\n",
      "\u001b[1m 68/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:16\u001b[0m 469ms/step - accuracy: 0.4968 - binary_io_u_5: 0.5015 - loss: 0.9215Batch 69, Loss Value: 0.8953\n",
      "Batch 69, Gradient Norm: 2.9876\n",
      "\u001b[1m 69/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:16\u001b[0m 469ms/step - accuracy: 0.4967 - binary_io_u_5: 0.5015 - loss: 0.9215Batch 70, Loss Value: 0.9333\n",
      "Batch 70, Gradient Norm: 0.1198\n",
      "\u001b[1m 70/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:16\u001b[0m 469ms/step - accuracy: 0.4966 - binary_io_u_5: 0.5014 - loss: 0.9215Batch 71, Loss Value: 0.9068\n",
      "Batch 71, Gradient Norm: 0.0884\n",
      "\u001b[1m 71/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:15\u001b[0m 469ms/step - accuracy: 0.4966 - binary_io_u_5: 0.5013 - loss: 0.9215Batch 72, Loss Value: 0.9330\n",
      "Batch 72, Gradient Norm: 0.2405\n",
      "\u001b[1m 72/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:15\u001b[0m 469ms/step - accuracy: 0.4965 - binary_io_u_5: 0.5013 - loss: 0.9215Batch 73, Loss Value: 0.9180\n",
      "Batch 73, Gradient Norm: 0.5119\n",
      "\u001b[1m 73/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:14\u001b[0m 469ms/step - accuracy: 0.4964 - binary_io_u_5: 0.5012 - loss: 0.9215Batch 74, Loss Value: 0.9230\n",
      "Batch 74, Gradient Norm: 0.4361\n",
      "\u001b[1m 74/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:14\u001b[0m 469ms/step - accuracy: 0.4964 - binary_io_u_5: 0.5012 - loss: 0.9215Batch 75, Loss Value: 0.9012\n",
      "Batch 75, Gradient Norm: 0.0471\n",
      "\u001b[1m 75/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:13\u001b[0m 469ms/step - accuracy: 0.4963 - binary_io_u_5: 0.5011 - loss: 0.9215Batch 76, Loss Value: 0.9212\n",
      "Batch 76, Gradient Norm: 0.6438\n",
      "\u001b[1m 76/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:13\u001b[0m 469ms/step - accuracy: 0.4962 - binary_io_u_5: 0.5011 - loss: 0.9215Batch 77, Loss Value: 0.8915\n",
      "Batch 77, Gradient Norm: 0.2598\n",
      "\u001b[1m 77/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:12\u001b[0m 469ms/step - accuracy: 0.4962 - binary_io_u_5: 0.5010 - loss: 0.9215Batch 78, Loss Value: 0.9025\n",
      "Batch 78, Gradient Norm: 0.1131\n",
      "\u001b[1m 78/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:12\u001b[0m 469ms/step - accuracy: 0.4962 - binary_io_u_5: 0.5010 - loss: 0.9215Batch 79, Loss Value: 0.9306\n",
      "Batch 79, Gradient Norm: 0.3750\n",
      "\u001b[1m 79/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:11\u001b[0m 469ms/step - accuracy: 0.4962 - binary_io_u_5: 0.5010 - loss: 0.9214Batch 80, Loss Value: 0.9311\n",
      "Batch 80, Gradient Norm: 0.2114\n",
      "\u001b[1m 80/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:11\u001b[0m 469ms/step - accuracy: 0.4961 - binary_io_u_5: 0.5009 - loss: 0.9214Batch 81, Loss Value: 0.9226\n",
      "Batch 81, Gradient Norm: 0.8960\n",
      "\u001b[1m 81/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:10\u001b[0m 469ms/step - accuracy: 0.4961 - binary_io_u_5: 0.5009 - loss: 0.9214Batch 82, Loss Value: 0.9029\n",
      "Batch 82, Gradient Norm: 0.2150\n",
      "\u001b[1m 82/232\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m1:10\u001b[0m 469ms/step - accuracy: 0.4961 - binary_io_u_5: 0.5009 - loss: 0.9214Batch 83, Loss Value: 0.9037\n",
      "Batch 83, Gradient Norm: 0.0009\n",
      "\u001b[1m 83/232\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m1:09\u001b[0m 469ms/step - accuracy: 0.4961 - binary_io_u_5: 0.5009 - loss: 0.9214Batch 84, Loss Value: 0.9080\n",
      "Batch 84, Gradient Norm: 2.7084\n",
      "\u001b[1m 84/232\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m1:09\u001b[0m 469ms/step - accuracy: 0.4960 - binary_io_u_5: 0.5009 - loss: 0.9214Batch 85, Loss Value: 0.9419\n",
      "Batch 85, Gradient Norm: 0.0016\n",
      "\u001b[1m 85/232\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m1:09\u001b[0m 469ms/step - accuracy: 0.4960 - binary_io_u_5: 0.5008 - loss: 0.9213Batch 86, Loss Value: 0.9347\n",
      "Batch 86, Gradient Norm: 0.0238\n",
      "\u001b[1m 86/232\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m1:08\u001b[0m 469ms/step - accuracy: 0.4960 - binary_io_u_5: 0.5008 - loss: 0.9213Batch 87, Loss Value: 0.8831\n",
      "Batch 87, Gradient Norm: 0.5638\n",
      "\u001b[1m 87/232\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m1:08\u001b[0m 470ms/step - accuracy: 0.4960 - binary_io_u_5: 0.5008 - loss: 0.9213Batch 88, Loss Value: 0.9145\n",
      "Batch 88, Gradient Norm: 0.4028\n",
      "\u001b[1m 88/232\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m1:07\u001b[0m 470ms/step - accuracy: 0.4959 - binary_io_u_5: 0.5008 - loss: 0.9213Batch 89, Loss Value: 0.9050\n",
      "Batch 89, Gradient Norm: 0.1593\n",
      "\u001b[1m 89/232\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m1:07\u001b[0m 470ms/step - accuracy: 0.4959 - binary_io_u_5: 0.5008 - loss: 0.9213Batch 90, Loss Value: 0.9201\n",
      "Batch 90, Gradient Norm: 0.5760\n",
      "\u001b[1m 90/232\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m1:06\u001b[0m 470ms/step - accuracy: 0.4959 - binary_io_u_5: 0.5007 - loss: 0.9213Batch 91, Loss Value: 0.9317\n",
      "Batch 91, Gradient Norm: 0.6666\n",
      "\u001b[1m 91/232\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m1:06\u001b[0m 470ms/step - accuracy: 0.4958 - binary_io_u_5: 0.5007 - loss: 0.9213Batch 92, Loss Value: 0.8888\n",
      "Batch 92, Gradient Norm: 0.3479\n",
      "\u001b[1m 92/232\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m1:05\u001b[0m 470ms/step - accuracy: 0.4958 - binary_io_u_5: 0.5006 - loss: 0.9212Batch 93, Loss Value: 0.9159\n",
      "Batch 93, Gradient Norm: 0.6688\n",
      "\u001b[1m 93/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m1:05\u001b[0m 470ms/step - accuracy: 0.4957 - binary_io_u_5: 0.5006 - loss: 0.9212Batch 94, Loss Value: 0.8942\n",
      "Batch 94, Gradient Norm: 0.5273\n",
      "\u001b[1m 94/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m1:04\u001b[0m 470ms/step - accuracy: 0.4957 - binary_io_u_5: 0.5006 - loss: 0.9212Batch 95, Loss Value: 0.9041\n",
      "Batch 95, Gradient Norm: 0.6313\n",
      "\u001b[1m 95/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m1:04\u001b[0m 470ms/step - accuracy: 0.4957 - binary_io_u_5: 0.5005 - loss: 0.9212Batch 96, Loss Value: 0.8979\n",
      "Batch 96, Gradient Norm: 0.3595\n",
      "\u001b[1m 96/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m1:03\u001b[0m 470ms/step - accuracy: 0.4957 - binary_io_u_5: 0.5005 - loss: 0.9212Batch 97, Loss Value: 0.9046\n",
      "Batch 97, Gradient Norm: 0.2265\n",
      "\u001b[1m 97/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m1:03\u001b[0m 470ms/step - accuracy: 0.4957 - binary_io_u_5: 0.5004 - loss: 0.9212Batch 98, Loss Value: 0.9109\n",
      "Batch 98, Gradient Norm: 0.1450\n",
      "\u001b[1m 98/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m1:02\u001b[0m 470ms/step - accuracy: 0.4957 - binary_io_u_5: 0.5004 - loss: 0.9212Batch 99, Loss Value: 0.9055\n",
      "Batch 99, Gradient Norm: 0.6637\n",
      "\u001b[1m 99/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m1:02\u001b[0m 470ms/step - accuracy: 0.4956 - binary_io_u_5: 0.5004 - loss: 0.9211Batch 100, Loss Value: 0.8886\n",
      "Batch 100, Gradient Norm: 0.1902\n",
      "\u001b[1m100/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m1:02\u001b[0m 470ms/step - accuracy: 0.4956 - binary_io_u_5: 0.5004 - loss: 0.9211Batch 101, Loss Value: 0.9127\n",
      "Batch 101, Gradient Norm: 0.5553\n",
      "\u001b[1m101/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m1:01\u001b[0m 470ms/step - accuracy: 0.4956 - binary_io_u_5: 0.5003 - loss: 0.9211Batch 102, Loss Value: 0.9397\n",
      "Batch 102, Gradient Norm: 0.4915\n",
      "\u001b[1m102/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m1:01\u001b[0m 470ms/step - accuracy: 0.4956 - binary_io_u_5: 0.5003 - loss: 0.9211Batch 103, Loss Value: 0.8989\n",
      "Batch 103, Gradient Norm: 0.1159\n",
      "\u001b[1m103/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m1:00\u001b[0m 470ms/step - accuracy: 0.4956 - binary_io_u_5: 0.5003 - loss: 0.9211Batch 104, Loss Value: 0.9018\n",
      "Batch 104, Gradient Norm: 0.2250\n",
      "\u001b[1m104/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m1:00\u001b[0m 470ms/step - accuracy: 0.4956 - binary_io_u_5: 0.5002 - loss: 0.9211Batch 105, Loss Value: 0.9393\n",
      "Batch 105, Gradient Norm: 0.0511\n",
      "\u001b[1m105/232\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m59s\u001b[0m 470ms/step - accuracy: 0.4956 - binary_io_u_5: 0.5002 - loss: 0.9211 Batch 106, Loss Value: 0.9376\n",
      "Batch 106, Gradient Norm: 0.0191\n",
      "\u001b[1m106/232\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m59s\u001b[0m 470ms/step - accuracy: 0.4956 - binary_io_u_5: 0.5002 - loss: 0.9211Batch 107, Loss Value: 0.9004\n",
      "Batch 107, Gradient Norm: 0.2051\n",
      "\u001b[1m107/232\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m58s\u001b[0m 470ms/step - accuracy: 0.4955 - binary_io_u_5: 0.5001 - loss: 0.9211Batch 108, Loss Value: 0.8996\n",
      "Batch 108, Gradient Norm: 0.0503\n",
      "\u001b[1m108/232\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m58s\u001b[0m 470ms/step - accuracy: 0.4955 - binary_io_u_5: 0.5001 - loss: 0.9211Batch 109, Loss Value: 0.9290\n",
      "Batch 109, Gradient Norm: 0.3568\n",
      "\u001b[1m109/232\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m57s\u001b[0m 470ms/step - accuracy: 0.4955 - binary_io_u_5: 0.5001 - loss: 0.9211Batch 110, Loss Value: 0.9430\n",
      "Batch 110, Gradient Norm: 0.0123\n",
      "\u001b[1m110/232\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m57s\u001b[0m 470ms/step - accuracy: 0.4955 - binary_io_u_5: 0.5000 - loss: 0.9211Batch 111, Loss Value: 0.9432\n",
      "Batch 111, Gradient Norm: 0.0223\n",
      "\u001b[1m111/232\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m56s\u001b[0m 470ms/step - accuracy: 0.4955 - binary_io_u_5: 0.5000 - loss: 0.9210Batch 112, Loss Value: 0.9087\n",
      "Batch 112, Gradient Norm: 0.3185\n",
      "\u001b[1m112/232\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m56s\u001b[0m 470ms/step - accuracy: 0.4955 - binary_io_u_5: 0.5000 - loss: 0.9210Batch 113, Loss Value: 0.8807\n",
      "Batch 113, Gradient Norm: 2.0671\n",
      "\u001b[1m113/232\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m55s\u001b[0m 470ms/step - accuracy: 0.4955 - binary_io_u_5: 0.4999 - loss: 0.9210Batch 114, Loss Value: 0.8906\n",
      "Batch 114, Gradient Norm: 0.2937\n",
      "\u001b[1m114/232\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m55s\u001b[0m 470ms/step - accuracy: 0.4955 - binary_io_u_5: 0.4999 - loss: 0.9210Batch 115, Loss Value: 0.8787\n",
      "Batch 115, Gradient Norm: 0.7041\n",
      "\u001b[1m115/232\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m54s\u001b[0m 470ms/step - accuracy: 0.4955 - binary_io_u_5: 0.4999 - loss: 0.9210Batch 116, Loss Value: 0.9371\n",
      "Batch 116, Gradient Norm: 0.4961\n",
      "\u001b[1m116/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m54s\u001b[0m 470ms/step - accuracy: 0.4955 - binary_io_u_5: 0.4998 - loss: 0.9210Batch 117, Loss Value: 0.9424\n",
      "Batch 117, Gradient Norm: 0.0159\n",
      "\u001b[1m117/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m54s\u001b[0m 470ms/step - accuracy: 0.4955 - binary_io_u_5: 0.4998 - loss: 0.9210Batch 118, Loss Value: 0.9423\n",
      "Batch 118, Gradient Norm: 0.0196\n",
      "\u001b[1m118/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m53s\u001b[0m 470ms/step - accuracy: 0.4955 - binary_io_u_5: 0.4998 - loss: 0.9210Batch 119, Loss Value: 0.8936\n",
      "Batch 119, Gradient Norm: 0.1691\n",
      "\u001b[1m119/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m53s\u001b[0m 470ms/step - accuracy: 0.4955 - binary_io_u_5: 0.4997 - loss: 0.9210Batch 120, Loss Value: 0.9414\n",
      "Batch 120, Gradient Norm: 1.2450\n",
      "\u001b[1m120/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m52s\u001b[0m 470ms/step - accuracy: 0.4955 - binary_io_u_5: 0.4997 - loss: 0.9209Batch 121, Loss Value: 0.9341\n",
      "Batch 121, Gradient Norm: 0.3081\n",
      "\u001b[1m121/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m52s\u001b[0m 470ms/step - accuracy: 0.4956 - binary_io_u_5: 0.4997 - loss: 0.9209Batch 122, Loss Value: 0.9240\n",
      "Batch 122, Gradient Norm: 0.1830\n",
      "\u001b[1m122/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m51s\u001b[0m 470ms/step - accuracy: 0.4956 - binary_io_u_5: 0.4997 - loss: 0.9209Batch 123, Loss Value: 0.9171\n",
      "Batch 123, Gradient Norm: 0.3564\n",
      "\u001b[1m123/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m51s\u001b[0m 470ms/step - accuracy: 0.4956 - binary_io_u_5: 0.4996 - loss: 0.9209Batch 124, Loss Value: 0.9300\n",
      "Batch 124, Gradient Norm: 0.0994\n",
      "\u001b[1m124/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m50s\u001b[0m 470ms/step - accuracy: 0.4956 - binary_io_u_5: 0.4996 - loss: 0.9209Batch 125, Loss Value: 0.9243\n",
      "Batch 125, Gradient Norm: 0.3636\n",
      "\u001b[1m125/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m50s\u001b[0m 470ms/step - accuracy: 0.4956 - binary_io_u_5: 0.4996 - loss: 0.9209Batch 126, Loss Value: 0.8849\n",
      "Batch 126, Gradient Norm: 0.2604\n",
      "\u001b[1m126/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m49s\u001b[0m 470ms/step - accuracy: 0.4956 - binary_io_u_5: 0.4995 - loss: 0.9209Batch 127, Loss Value: 0.9357\n",
      "Batch 127, Gradient Norm: 0.6898\n",
      "\u001b[1m127/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m49s\u001b[0m 470ms/step - accuracy: 0.4956 - binary_io_u_5: 0.4995 - loss: 0.9209Batch 128, Loss Value: 0.8980\n",
      "Batch 128, Gradient Norm: 0.0879\n",
      "\u001b[1m128/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m48s\u001b[0m 470ms/step - accuracy: 0.4956 - binary_io_u_5: 0.4995 - loss: 0.9209Batch 129, Loss Value: 0.9095\n",
      "Batch 129, Gradient Norm: 0.0153\n",
      "\u001b[1m129/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m48s\u001b[0m 470ms/step - accuracy: 0.4956 - binary_io_u_5: 0.4995 - loss: 0.9209Batch 130, Loss Value: 0.8797\n",
      "Batch 130, Gradient Norm: 0.0454\n",
      "\u001b[1m130/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m47s\u001b[0m 469ms/step - accuracy: 0.4956 - binary_io_u_5: 0.4995 - loss: 0.9209Batch 131, Loss Value: 0.9142\n",
      "Batch 131, Gradient Norm: 0.1738\n",
      "\u001b[1m131/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m47s\u001b[0m 469ms/step - accuracy: 0.4956 - binary_io_u_5: 0.4994 - loss: 0.9209Batch 132, Loss Value: 0.9422\n",
      "Batch 132, Gradient Norm: 0.0135\n",
      "\u001b[1m132/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m46s\u001b[0m 469ms/step - accuracy: 0.4956 - binary_io_u_5: 0.4994 - loss: 0.9209Batch 133, Loss Value: 0.9269\n",
      "Batch 133, Gradient Norm: 1.1667\n",
      "\u001b[1m133/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m46s\u001b[0m 469ms/step - accuracy: 0.4956 - binary_io_u_5: 0.4994 - loss: 0.9209Batch 134, Loss Value: 0.9052\n",
      "Batch 134, Gradient Norm: 0.5376\n",
      "\u001b[1m134/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m46s\u001b[0m 469ms/step - accuracy: 0.4956 - binary_io_u_5: 0.4994 - loss: 0.9209Batch 135, Loss Value: 0.9194\n",
      "Batch 135, Gradient Norm: 0.1016\n",
      "\u001b[1m135/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m45s\u001b[0m 469ms/step - accuracy: 0.4957 - binary_io_u_5: 0.4994 - loss: 0.9209Batch 136, Loss Value: 0.9038\n",
      "Batch 136, Gradient Norm: 0.6322\n",
      "\u001b[1m136/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m45s\u001b[0m 469ms/step - accuracy: 0.4957 - binary_io_u_5: 0.4993 - loss: 0.9209Batch 137, Loss Value: 0.9123\n",
      "Batch 137, Gradient Norm: 0.0883\n",
      "\u001b[1m137/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m44s\u001b[0m 469ms/step - accuracy: 0.4957 - binary_io_u_5: 0.4993 - loss: 0.9208Batch 138, Loss Value: 0.8931\n",
      "Batch 138, Gradient Norm: 0.0509\n",
      "\u001b[1m138/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m44s\u001b[0m 469ms/step - accuracy: 0.4957 - binary_io_u_5: 0.4993 - loss: 0.9208Batch 139, Loss Value: 0.9150\n",
      "Batch 139, Gradient Norm: 0.0067\n",
      "\u001b[1m139/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m43s\u001b[0m 469ms/step - accuracy: 0.4957 - binary_io_u_5: 0.4993 - loss: 0.9208Batch 140, Loss Value: 0.9292\n",
      "Batch 140, Gradient Norm: 0.7932\n",
      "\u001b[1m140/232\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m43s\u001b[0m 469ms/step - accuracy: 0.4957 - binary_io_u_5: 0.4993 - loss: 0.9208Batch 141, Loss Value: 0.8985\n",
      "Batch 141, Gradient Norm: 0.0585\n",
      "\u001b[1m141/232\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m42s\u001b[0m 469ms/step - accuracy: 0.4957 - binary_io_u_5: 0.4993 - loss: 0.9208Batch 142, Loss Value: 0.9173\n",
      "Batch 142, Gradient Norm: 0.1699\n",
      "\u001b[1m142/232\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m42s\u001b[0m 469ms/step - accuracy: 0.4958 - binary_io_u_5: 0.4993 - loss: 0.9208Batch 143, Loss Value: 0.9364\n",
      "Batch 143, Gradient Norm: 0.1236\n",
      "\u001b[1m143/232\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m41s\u001b[0m 469ms/step - accuracy: 0.4958 - binary_io_u_5: 0.4992 - loss: 0.9208Batch 144, Loss Value: 0.9174\n",
      "Batch 144, Gradient Norm: 0.0760\n",
      "\u001b[1m144/232\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m41s\u001b[0m 469ms/step - accuracy: 0.4958 - binary_io_u_5: 0.4992 - loss: 0.9208Batch 145, Loss Value: 0.8961\n",
      "Batch 145, Gradient Norm: 0.0864\n",
      "\u001b[1m145/232\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m40s\u001b[0m 469ms/step - accuracy: 0.4958 - binary_io_u_5: 0.4992 - loss: 0.9208Batch 146, Loss Value: 0.8896\n",
      "Batch 146, Gradient Norm: 0.2996\n",
      "\u001b[1m146/232\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m40s\u001b[0m 469ms/step - accuracy: 0.4958 - binary_io_u_5: 0.4992 - loss: 0.9208Batch 147, Loss Value: 0.9087\n",
      "Batch 147, Gradient Norm: 1.6829\n",
      "\u001b[1m147/232\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m39s\u001b[0m 469ms/step - accuracy: 0.4958 - binary_io_u_5: 0.4992 - loss: 0.9207Batch 148, Loss Value: 0.9075\n",
      "Batch 148, Gradient Norm: 0.1940\n",
      "\u001b[1m148/232\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m39s\u001b[0m 469ms/step - accuracy: 0.4959 - binary_io_u_5: 0.4992 - loss: 0.9207Batch 149, Loss Value: 0.9114\n",
      "Batch 149, Gradient Norm: 0.0475\n",
      "\u001b[1m149/232\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m38s\u001b[0m 469ms/step - accuracy: 0.4959 - binary_io_u_5: 0.4992 - loss: 0.9207Batch 150, Loss Value: 0.9031\n",
      "Batch 150, Gradient Norm: 0.6094\n",
      "\u001b[1m150/232\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m38s\u001b[0m 469ms/step - accuracy: 0.4959 - binary_io_u_5: 0.4991 - loss: 0.9207Batch 151, Loss Value: 0.8986\n",
      "Batch 151, Gradient Norm: 0.5297\n",
      "\u001b[1m151/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m37s\u001b[0m 469ms/step - accuracy: 0.4959 - binary_io_u_5: 0.4991 - loss: 0.9207Batch 152, Loss Value: 0.9257\n",
      "Batch 152, Gradient Norm: 1.0993\n",
      "\u001b[1m152/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m37s\u001b[0m 469ms/step - accuracy: 0.4959 - binary_io_u_5: 0.4991 - loss: 0.9207Batch 153, Loss Value: 0.9002\n",
      "Batch 153, Gradient Norm: 0.7046\n",
      "\u001b[1m153/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m37s\u001b[0m 469ms/step - accuracy: 0.4959 - binary_io_u_5: 0.4991 - loss: 0.9207Batch 154, Loss Value: 0.9236\n",
      "Batch 154, Gradient Norm: 0.4097\n",
      "\u001b[1m154/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m36s\u001b[0m 469ms/step - accuracy: 0.4959 - binary_io_u_5: 0.4991 - loss: 0.9207Batch 155, Loss Value: 0.9033\n",
      "Batch 155, Gradient Norm: 0.4416\n",
      "\u001b[1m155/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m36s\u001b[0m 469ms/step - accuracy: 0.4960 - binary_io_u_5: 0.4991 - loss: 0.9207Batch 156, Loss Value: 0.9419\n",
      "Batch 156, Gradient Norm: 0.0289\n",
      "\u001b[1m156/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m35s\u001b[0m 469ms/step - accuracy: 0.4960 - binary_io_u_5: 0.4991 - loss: 0.9207Batch 157, Loss Value: 0.9150\n",
      "Batch 157, Gradient Norm: 0.2347\n",
      "\u001b[1m157/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m35s\u001b[0m 469ms/step - accuracy: 0.4960 - binary_io_u_5: 0.4990 - loss: 0.9207Batch 158, Loss Value: 0.8937\n",
      "Batch 158, Gradient Norm: 0.1156\n",
      "\u001b[1m158/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m34s\u001b[0m 469ms/step - accuracy: 0.4960 - binary_io_u_5: 0.4990 - loss: 0.9207Batch 159, Loss Value: 0.9350\n",
      "Batch 159, Gradient Norm: 0.0007\n",
      "\u001b[1m159/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m34s\u001b[0m 468ms/step - accuracy: 0.4960 - binary_io_u_5: 0.4990 - loss: 0.9206Batch 160, Loss Value: 0.9256\n",
      "Batch 160, Gradient Norm: 0.5210\n",
      "\u001b[1m160/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m33s\u001b[0m 468ms/step - accuracy: 0.4960 - binary_io_u_5: 0.4990 - loss: 0.9206Batch 161, Loss Value: 0.9417\n",
      "Batch 161, Gradient Norm: 0.0001\n",
      "\u001b[1m161/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m33s\u001b[0m 468ms/step - accuracy: 0.4960 - binary_io_u_5: 0.4990 - loss: 0.9206Batch 162, Loss Value: 0.9242\n",
      "Batch 162, Gradient Norm: 0.5379\n",
      "\u001b[1m162/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m32s\u001b[0m 468ms/step - accuracy: 0.4960 - binary_io_u_5: 0.4990 - loss: 0.9206Batch 163, Loss Value: 0.8926\n",
      "Batch 163, Gradient Norm: 0.1584\n",
      "\u001b[1m163/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m32s\u001b[0m 468ms/step - accuracy: 0.4960 - binary_io_u_5: 0.4990 - loss: 0.9206Batch 164, Loss Value: 0.9130\n",
      "Batch 164, Gradient Norm: 0.3574\n",
      "\u001b[1m164/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m31s\u001b[0m 468ms/step - accuracy: 0.4960 - binary_io_u_5: 0.4990 - loss: 0.9206Batch 165, Loss Value: 0.8962\n",
      "Batch 165, Gradient Norm: 0.1043\n",
      "\u001b[1m165/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m31s\u001b[0m 468ms/step - accuracy: 0.4960 - binary_io_u_5: 0.4989 - loss: 0.9206Batch 166, Loss Value: 0.9039\n",
      "Batch 166, Gradient Norm: 0.3690\n",
      "\u001b[1m166/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m30s\u001b[0m 468ms/step - accuracy: 0.4961 - binary_io_u_5: 0.4989 - loss: 0.9206Batch 167, Loss Value: 0.9059\n",
      "Batch 167, Gradient Norm: 0.3522\n",
      "\u001b[1m167/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m30s\u001b[0m 468ms/step - accuracy: 0.4961 - binary_io_u_5: 0.4989 - loss: 0.9206Batch 168, Loss Value: 0.9143\n",
      "Batch 168, Gradient Norm: 0.1670\n",
      "\u001b[1m168/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m29s\u001b[0m 468ms/step - accuracy: 0.4961 - binary_io_u_5: 0.4989 - loss: 0.9206Batch 169, Loss Value: 0.9396\n",
      "Batch 169, Gradient Norm: 0.2578\n",
      "\u001b[1m169/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m29s\u001b[0m 468ms/step - accuracy: 0.4961 - binary_io_u_5: 0.4989 - loss: 0.9206Batch 170, Loss Value: 0.9140\n",
      "Batch 170, Gradient Norm: 0.0927\n",
      "\u001b[1m170/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m29s\u001b[0m 468ms/step - accuracy: 0.4961 - binary_io_u_5: 0.4988 - loss: 0.9206Batch 171, Loss Value: 0.9119\n",
      "Batch 171, Gradient Norm: 0.1563\n",
      "\u001b[1m171/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m28s\u001b[0m 468ms/step - accuracy: 0.4961 - binary_io_u_5: 0.4988 - loss: 0.9206Batch 172, Loss Value: 0.9092\n",
      "Batch 172, Gradient Norm: 0.0187\n",
      "\u001b[1m172/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m28s\u001b[0m 468ms/step - accuracy: 0.4961 - binary_io_u_5: 0.4988 - loss: 0.9206Batch 173, Loss Value: 0.8907\n",
      "Batch 173, Gradient Norm: 0.3032\n",
      "\u001b[1m173/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m27s\u001b[0m 468ms/step - accuracy: 0.4961 - binary_io_u_5: 0.4988 - loss: 0.9206Batch 174, Loss Value: 0.9359\n",
      "Batch 174, Gradient Norm: 0.1081\n",
      "\u001b[1m174/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m27s\u001b[0m 468ms/step - accuracy: 0.4961 - binary_io_u_5: 0.4988 - loss: 0.9206Batch 175, Loss Value: 0.9151\n",
      "Batch 175, Gradient Norm: 0.5651\n",
      "\u001b[1m175/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m26s\u001b[0m 468ms/step - accuracy: 0.4961 - binary_io_u_5: 0.4988 - loss: 0.9206Batch 176, Loss Value: 0.9353\n",
      "Batch 176, Gradient Norm: 0.3947\n",
      "\u001b[1m176/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m26s\u001b[0m 468ms/step - accuracy: 0.4961 - binary_io_u_5: 0.4987 - loss: 0.9206Batch 177, Loss Value: 0.9424\n",
      "Batch 177, Gradient Norm: 0.0097\n",
      "\u001b[1m177/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m25s\u001b[0m 468ms/step - accuracy: 0.4961 - binary_io_u_5: 0.4987 - loss: 0.9206Batch 178, Loss Value: 0.8829\n",
      "Batch 178, Gradient Norm: 0.3076\n",
      "\u001b[1m178/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m25s\u001b[0m 468ms/step - accuracy: 0.4961 - binary_io_u_5: 0.4987 - loss: 0.9206Batch 179, Loss Value: 0.8911\n",
      "Batch 179, Gradient Norm: 0.3807\n",
      "\u001b[1m179/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m24s\u001b[0m 468ms/step - accuracy: 0.4961 - binary_io_u_5: 0.4987 - loss: 0.9206Batch 180, Loss Value: 0.8925\n",
      "Batch 180, Gradient Norm: 0.4268\n",
      "\u001b[1m180/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m24s\u001b[0m 468ms/step - accuracy: 0.4961 - binary_io_u_5: 0.4987 - loss: 0.9206Batch 181, Loss Value: 0.8839\n",
      "Batch 181, Gradient Norm: 0.0094\n",
      "\u001b[1m181/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m23s\u001b[0m 468ms/step - accuracy: 0.4961 - binary_io_u_5: 0.4986 - loss: 0.9206Batch 182, Loss Value: 0.9116\n",
      "Batch 182, Gradient Norm: 0.2963\n",
      "\u001b[1m182/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m23s\u001b[0m 468ms/step - accuracy: 0.4961 - binary_io_u_5: 0.4986 - loss: 0.9206Batch 183, Loss Value: 0.9042\n",
      "Batch 183, Gradient Norm: 0.6308\n",
      "\u001b[1m183/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m22s\u001b[0m 468ms/step - accuracy: 0.4961 - binary_io_u_5: 0.4986 - loss: 0.9206Batch 184, Loss Value: 0.9368\n",
      "Batch 184, Gradient Norm: 0.3370\n",
      "\u001b[1m184/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m22s\u001b[0m 468ms/step - accuracy: 0.4961 - binary_io_u_5: 0.4986 - loss: 0.9206Batch 185, Loss Value: 0.9075\n",
      "Batch 185, Gradient Norm: 0.4759\n",
      "\u001b[1m185/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m21s\u001b[0m 468ms/step - accuracy: 0.4961 - binary_io_u_5: 0.4986 - loss: 0.9206Batch 186, Loss Value: 0.9423\n",
      "Batch 186, Gradient Norm: 0.0048\n",
      "\u001b[1m186/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m21s\u001b[0m 467ms/step - accuracy: 0.4961 - binary_io_u_5: 0.4986 - loss: 0.9206Batch 187, Loss Value: 0.9310\n",
      "Batch 187, Gradient Norm: 0.2604\n",
      "\u001b[1m187/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m21s\u001b[0m 467ms/step - accuracy: 0.4961 - binary_io_u_5: 0.4985 - loss: 0.9206Batch 188, Loss Value: 0.8895\n",
      "Batch 188, Gradient Norm: 0.2347\n",
      "\u001b[1m188/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m20s\u001b[0m 467ms/step - accuracy: 0.4961 - binary_io_u_5: 0.4985 - loss: 0.9206Batch 189, Loss Value: 0.9274\n",
      "Batch 189, Gradient Norm: 0.1935\n",
      "\u001b[1m189/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m20s\u001b[0m 467ms/step - accuracy: 0.4961 - binary_io_u_5: 0.4985 - loss: 0.9206Batch 190, Loss Value: 0.9184\n",
      "Batch 190, Gradient Norm: 0.2289\n",
      "\u001b[1m190/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m19s\u001b[0m 467ms/step - accuracy: 0.4961 - binary_io_u_5: 0.4985 - loss: 0.9206Batch 191, Loss Value: 0.9423\n",
      "Batch 191, Gradient Norm: 0.0105\n",
      "\u001b[1m191/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m19s\u001b[0m 467ms/step - accuracy: 0.4961 - binary_io_u_5: 0.4985 - loss: 0.9206Batch 192, Loss Value: 0.9148\n",
      "Batch 192, Gradient Norm: 0.7735\n",
      "\u001b[1m192/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m18s\u001b[0m 467ms/step - accuracy: 0.4961 - binary_io_u_5: 0.4985 - loss: 0.9206Batch 193, Loss Value: 0.9362\n",
      "Batch 193, Gradient Norm: 0.6114\n",
      "\u001b[1m193/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m18s\u001b[0m 467ms/step - accuracy: 0.4961 - binary_io_u_5: 0.4984 - loss: 0.9206Batch 194, Loss Value: 0.9178\n",
      "Batch 194, Gradient Norm: 0.5570\n",
      "\u001b[1m194/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m17s\u001b[0m 467ms/step - accuracy: 0.4961 - binary_io_u_5: 0.4984 - loss: 0.9206Batch 195, Loss Value: 0.9153\n",
      "Batch 195, Gradient Norm: 0.3971\n",
      "\u001b[1m195/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m17s\u001b[0m 467ms/step - accuracy: 0.4961 - binary_io_u_5: 0.4984 - loss: 0.9206Batch 196, Loss Value: 0.9287\n",
      "Batch 196, Gradient Norm: 0.7041\n",
      "\u001b[1m196/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m16s\u001b[0m 467ms/step - accuracy: 0.4961 - binary_io_u_5: 0.4984 - loss: 0.9205Batch 197, Loss Value: 0.9228\n",
      "Batch 197, Gradient Norm: 0.3828\n",
      "\u001b[1m197/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m16s\u001b[0m 468ms/step - accuracy: 0.4961 - binary_io_u_5: 0.4984 - loss: 0.9205Batch 198, Loss Value: 0.9430\n",
      "Batch 198, Gradient Norm: 0.0309\n",
      "\u001b[1m198/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m15s\u001b[0m 468ms/step - accuracy: 0.4961 - binary_io_u_5: 0.4984 - loss: 0.9205Batch 199, Loss Value: 0.9341\n",
      "Batch 199, Gradient Norm: 0.6729\n",
      "\u001b[1m199/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m15s\u001b[0m 468ms/step - accuracy: 0.4961 - binary_io_u_5: 0.4983 - loss: 0.9205Batch 200, Loss Value: 0.9020\n",
      "Batch 200, Gradient Norm: 0.4359\n",
      "\u001b[1m200/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m14s\u001b[0m 468ms/step - accuracy: 0.4961 - binary_io_u_5: 0.4983 - loss: 0.9205Batch 201, Loss Value: 0.9094\n",
      "Batch 201, Gradient Norm: 0.9476\n",
      "\u001b[1m201/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m14s\u001b[0m 468ms/step - accuracy: 0.4961 - binary_io_u_5: 0.4983 - loss: 0.9205Batch 202, Loss Value: 0.9032\n",
      "Batch 202, Gradient Norm: 0.3163\n",
      "\u001b[1m202/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m14s\u001b[0m 468ms/step - accuracy: 0.4961 - binary_io_u_5: 0.4983 - loss: 0.9205Batch 203, Loss Value: 0.9232\n",
      "Batch 203, Gradient Norm: 0.1677\n",
      "\u001b[1m203/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m13s\u001b[0m 468ms/step - accuracy: 0.4961 - binary_io_u_5: 0.4983 - loss: 0.9205Batch 204, Loss Value: 0.8970\n",
      "Batch 204, Gradient Norm: 0.0798\n",
      "\u001b[1m204/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m13s\u001b[0m 468ms/step - accuracy: 0.4961 - binary_io_u_5: 0.4983 - loss: 0.9205Batch 205, Loss Value: 0.8926\n",
      "Batch 205, Gradient Norm: 0.3921\n",
      "\u001b[1m205/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m12s\u001b[0m 468ms/step - accuracy: 0.4961 - binary_io_u_5: 0.4983 - loss: 0.9205Batch 206, Loss Value: 0.9097\n",
      "Batch 206, Gradient Norm: 0.2754\n",
      "\u001b[1m206/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m12s\u001b[0m 468ms/step - accuracy: 0.4961 - binary_io_u_5: 0.4983 - loss: 0.9205Batch 207, Loss Value: 0.9374\n",
      "Batch 207, Gradient Norm: 0.3507\n",
      "\u001b[1m207/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m11s\u001b[0m 468ms/step - accuracy: 0.4962 - binary_io_u_5: 0.4983 - loss: 0.9205Batch 208, Loss Value: 0.9351\n",
      "Batch 208, Gradient Norm: 0.0064\n",
      "\u001b[1m208/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m11s\u001b[0m 468ms/step - accuracy: 0.4962 - binary_io_u_5: 0.4982 - loss: 0.9205Batch 209, Loss Value: 0.9303\n",
      "Batch 209, Gradient Norm: 0.0972\n",
      "\u001b[1m209/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m10s\u001b[0m 468ms/step - accuracy: 0.4962 - binary_io_u_5: 0.4982 - loss: 0.9205Batch 210, Loss Value: 0.8850\n",
      "Batch 210, Gradient Norm: 0.1238\n",
      "\u001b[1m210/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m10s\u001b[0m 468ms/step - accuracy: 0.4962 - binary_io_u_5: 0.4982 - loss: 0.9205Batch 211, Loss Value: 0.9417\n",
      "Batch 211, Gradient Norm: 0.0021\n",
      "\u001b[1m211/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m9s\u001b[0m 468ms/step - accuracy: 0.4962 - binary_io_u_5: 0.4982 - loss: 0.9205 Batch 212, Loss Value: 0.9286\n",
      "Batch 212, Gradient Norm: 0.0612\n",
      "\u001b[1m212/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m9s\u001b[0m 468ms/step - accuracy: 0.4962 - binary_io_u_5: 0.4982 - loss: 0.9205Batch 213, Loss Value: 0.8928\n",
      "Batch 213, Gradient Norm: 0.2821\n",
      "\u001b[1m213/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m8s\u001b[0m 468ms/step - accuracy: 0.4962 - binary_io_u_5: 0.4982 - loss: 0.9205Batch 214, Loss Value: 0.8848\n",
      "Batch 214, Gradient Norm: 0.4173\n",
      "\u001b[1m214/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m8s\u001b[0m 468ms/step - accuracy: 0.4962 - binary_io_u_5: 0.4982 - loss: 0.9205Batch 215, Loss Value: 0.9103\n",
      "Batch 215, Gradient Norm: 0.4575\n",
      "\u001b[1m215/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m7s\u001b[0m 468ms/step - accuracy: 0.4962 - binary_io_u_5: 0.4982 - loss: 0.9205Batch 216, Loss Value: 0.8878\n",
      "Batch 216, Gradient Norm: 0.2604\n",
      "\u001b[1m216/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m7s\u001b[0m 468ms/step - accuracy: 0.4962 - binary_io_u_5: 0.4982 - loss: 0.9204Batch 217, Loss Value: 0.8910\n",
      "Batch 217, Gradient Norm: 0.5370\n",
      "\u001b[1m217/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m7s\u001b[0m 468ms/step - accuracy: 0.4962 - binary_io_u_5: 0.4982 - loss: 0.9204Batch 218, Loss Value: 0.9278\n",
      "Batch 218, Gradient Norm: 0.0439\n",
      "\u001b[1m218/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m6s\u001b[0m 468ms/step - accuracy: 0.4962 - binary_io_u_5: 0.4982 - loss: 0.9204Batch 219, Loss Value: 0.9035\n",
      "Batch 219, Gradient Norm: 0.1963\n",
      "\u001b[1m219/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m6s\u001b[0m 468ms/step - accuracy: 0.4962 - binary_io_u_5: 0.4982 - loss: 0.9204Batch 220, Loss Value: 0.9119\n",
      "Batch 220, Gradient Norm: 0.0824\n",
      "\u001b[1m220/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m5s\u001b[0m 468ms/step - accuracy: 0.4962 - binary_io_u_5: 0.4982 - loss: 0.9204Batch 221, Loss Value: 0.8862\n",
      "Batch 221, Gradient Norm: 0.4776\n",
      "\u001b[1m221/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m5s\u001b[0m 468ms/step - accuracy: 0.4962 - binary_io_u_5: 0.4982 - loss: 0.9204Batch 222, Loss Value: 0.9178\n",
      "Batch 222, Gradient Norm: 0.0096\n",
      "\u001b[1m222/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m4s\u001b[0m 468ms/step - accuracy: 0.4963 - binary_io_u_5: 0.4982 - loss: 0.9204Batch 223, Loss Value: 0.8757\n",
      "Batch 223, Gradient Norm: 0.2888\n",
      "\u001b[1m223/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m4s\u001b[0m 468ms/step - accuracy: 0.4963 - binary_io_u_5: 0.4981 - loss: 0.9204Batch 224, Loss Value: 0.8919\n",
      "Batch 224, Gradient Norm: 0.0878\n",
      "\u001b[1m224/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m3s\u001b[0m 468ms/step - accuracy: 0.4963 - binary_io_u_5: 0.4981 - loss: 0.9204Batch 225, Loss Value: 0.9000\n",
      "Batch 225, Gradient Norm: 0.7762\n",
      "\u001b[1m225/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m3s\u001b[0m 468ms/step - accuracy: 0.4963 - binary_io_u_5: 0.4981 - loss: 0.9204Batch 226, Loss Value: 0.9065\n",
      "Batch 226, Gradient Norm: 0.0075\n",
      "\u001b[1m226/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m2s\u001b[0m 468ms/step - accuracy: 0.4963 - binary_io_u_5: 0.4981 - loss: 0.9204Batch 227, Loss Value: 0.8956\n",
      "Batch 227, Gradient Norm: 0.5266\n",
      "\u001b[1m227/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m2s\u001b[0m 468ms/step - accuracy: 0.4963 - binary_io_u_5: 0.4981 - loss: 0.9204Batch 228, Loss Value: 0.9288\n",
      "Batch 228, Gradient Norm: 0.4329\n",
      "\u001b[1m228/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m1s\u001b[0m 468ms/step - accuracy: 0.4963 - binary_io_u_5: 0.4981 - loss: 0.9204Batch 229, Loss Value: 0.9248\n",
      "Batch 229, Gradient Norm: 0.1957\n",
      "\u001b[1m229/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m1s\u001b[0m 468ms/step - accuracy: 0.4963 - binary_io_u_5: 0.4981 - loss: 0.9204Batch 230, Loss Value: 0.8911\n",
      "Batch 230, Gradient Norm: 0.3111\n",
      "\u001b[1m230/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 468ms/step - accuracy: 0.4963 - binary_io_u_5: 0.4981 - loss: 0.9204Batch 231, Loss Value: 0.9131\n",
      "Batch 231, Gradient Norm: 0.3184\n",
      "\u001b[1m231/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 468ms/step - accuracy: 0.4963 - binary_io_u_5: 0.4981 - loss: 0.9204Batch 232, Loss Value: 0.9355\n",
      "Batch 232, Gradient Norm: 2.3008\n",
      "\u001b[1m232/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m111s\u001b[0m 476ms/step - accuracy: 0.4963 - binary_io_u_5: 0.4980 - loss: 0.9204 - val_accuracy: 0.5075 - val_binary_io_u_5: 0.4949 - val_loss: 0.9110 - learning_rate: 0.0010\n",
      "Epoch 5/200\n",
      "Batch 1, Loss Value: 0.9418\n",
      "Batch 1, Gradient Norm: 0.1085\n",
      "\u001b[1m  1/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:02:56\u001b[0m 32s/step - accuracy: 0.5100 - binary_io_u_5: 0.5300 - loss: 0.8985Batch 2, Loss Value: 0.9376\n",
      "Batch 2, Gradient Norm: 0.1595\n",
      "\u001b[1m  2/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:49\u001b[0m 478ms/step - accuracy: 0.5175 - binary_io_u_5: 0.5275 - loss: 0.8938 Batch 3, Loss Value: 0.9176\n",
      "Batch 3, Gradient Norm: 0.2854\n",
      "\u001b[1m  3/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:48\u001b[0m 475ms/step - accuracy: 0.5061 - binary_io_u_5: 0.5128 - loss: 0.8989Batch 4, Loss Value: 0.9294\n",
      "Batch 4, Gradient Norm: 0.7995\n",
      "\u001b[1m  4/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:48\u001b[0m 477ms/step - accuracy: 0.4996 - binary_io_u_5: 0.5040 - loss: 0.9033Batch 5, Loss Value: 0.9325\n",
      "Batch 5, Gradient Norm: 0.1537\n",
      "\u001b[1m  5/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:47\u001b[0m 475ms/step - accuracy: 0.4953 - binary_io_u_5: 0.4984 - loss: 0.9070Batch 6, Loss Value: 0.9308\n",
      "Batch 6, Gradient Norm: 0.8392\n",
      "\u001b[1m  6/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:47\u001b[0m 474ms/step - accuracy: 0.4941 - binary_io_u_5: 0.4970 - loss: 0.9086Batch 7, Loss Value: 0.9029\n",
      "Batch 7, Gradient Norm: 0.0042\n",
      "\u001b[1m  7/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:46\u001b[0m 475ms/step - accuracy: 0.4939 - binary_io_u_5: 0.4962 - loss: 0.9095Batch 8, Loss Value: 0.9397\n",
      "Batch 8, Gradient Norm: 0.0279\n",
      "\u001b[1m  8/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:46\u001b[0m 475ms/step - accuracy: 0.4955 - binary_io_u_5: 0.4974 - loss: 0.9093Batch 9, Loss Value: 0.9203\n",
      "Batch 9, Gradient Norm: 0.4004\n",
      "\u001b[1m  9/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:46\u001b[0m 476ms/step - accuracy: 0.4968 - binary_io_u_5: 0.4982 - loss: 0.9089Batch 10, Loss Value: 0.9242\n",
      "Batch 10, Gradient Norm: 0.0063\n",
      "\u001b[1m 10/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:45\u001b[0m 474ms/step - accuracy: 0.4967 - binary_io_u_5: 0.4974 - loss: 0.9090Batch 11, Loss Value: 0.9430\n",
      "Batch 11, Gradient Norm: 1.0941\n",
      "\u001b[1m 11/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:44\u001b[0m 475ms/step - accuracy: 0.4965 - binary_io_u_5: 0.4969 - loss: 0.9092Batch 12, Loss Value: 0.9419\n",
      "Batch 12, Gradient Norm: 0.3299\n",
      "\u001b[1m 12/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:44\u001b[0m 475ms/step - accuracy: 0.4965 - binary_io_u_5: 0.4967 - loss: 0.9094Batch 13, Loss Value: 0.9178\n",
      "Batch 13, Gradient Norm: 0.2041\n",
      "\u001b[1m 13/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:44\u001b[0m 476ms/step - accuracy: 0.4965 - binary_io_u_5: 0.4969 - loss: 0.9096Batch 14, Loss Value: 0.9280\n",
      "Batch 14, Gradient Norm: 0.1623\n",
      "\u001b[1m 14/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:43\u001b[0m 476ms/step - accuracy: 0.4962 - binary_io_u_5: 0.4967 - loss: 0.9099Batch 15, Loss Value: 0.9437\n",
      "Batch 15, Gradient Norm: 0.0224\n",
      "\u001b[1m 15/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:43\u001b[0m 476ms/step - accuracy: 0.4953 - binary_io_u_5: 0.4961 - loss: 0.9103Batch 16, Loss Value: 0.9159\n",
      "Batch 16, Gradient Norm: 0.1452\n",
      "\u001b[1m 16/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:42\u001b[0m 475ms/step - accuracy: 0.4948 - binary_io_u_5: 0.4959 - loss: 0.9106Batch 17, Loss Value: 0.9203\n",
      "Batch 17, Gradient Norm: 0.3429\n",
      "\u001b[1m 17/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:42\u001b[0m 476ms/step - accuracy: 0.4944 - binary_io_u_5: 0.4955 - loss: 0.9108Batch 18, Loss Value: 0.9126\n",
      "Batch 18, Gradient Norm: 0.0237\n",
      "\u001b[1m 18/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:41\u001b[0m 476ms/step - accuracy: 0.4939 - binary_io_u_5: 0.4950 - loss: 0.9112Batch 19, Loss Value: 0.9117\n",
      "Batch 19, Gradient Norm: 0.7296\n",
      "\u001b[1m 19/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:41\u001b[0m 476ms/step - accuracy: 0.4934 - binary_io_u_5: 0.4944 - loss: 0.9115Batch 20, Loss Value: 0.9152\n",
      "Batch 20, Gradient Norm: 0.0790\n",
      "\u001b[1m 20/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:40\u001b[0m 476ms/step - accuracy: 0.4930 - binary_io_u_5: 0.4939 - loss: 0.9119Batch 21, Loss Value: 0.9153\n",
      "Batch 21, Gradient Norm: 0.0393\n",
      "\u001b[1m 21/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:40\u001b[0m 476ms/step - accuracy: 0.4927 - binary_io_u_5: 0.4936 - loss: 0.9121Batch 22, Loss Value: 0.9435\n",
      "Batch 22, Gradient Norm: 0.0022\n",
      "\u001b[1m 22/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:39\u001b[0m 476ms/step - accuracy: 0.4924 - binary_io_u_5: 0.4932 - loss: 0.9124Batch 23, Loss Value: 0.9356\n",
      "Batch 23, Gradient Norm: 0.5938\n",
      "\u001b[1m 23/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:39\u001b[0m 477ms/step - accuracy: 0.4921 - binary_io_u_5: 0.4930 - loss: 0.9127Batch 24, Loss Value: 0.9083\n",
      "Batch 24, Gradient Norm: 0.2505\n",
      "\u001b[1m 24/232\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:39\u001b[0m 477ms/step - accuracy: 0.4920 - binary_io_u_5: 0.4928 - loss: 0.9129Batch 25, Loss Value: 0.9217\n",
      "Batch 25, Gradient Norm: 0.0900\n",
      "\u001b[1m 25/232\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:38\u001b[0m 477ms/step - accuracy: 0.4918 - binary_io_u_5: 0.4926 - loss: 0.9132Batch 26, Loss Value: 0.9272\n",
      "Batch 26, Gradient Norm: 0.0823\n",
      "\u001b[1m 26/232\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:38\u001b[0m 477ms/step - accuracy: 0.4918 - binary_io_u_5: 0.4924 - loss: 0.9133Batch 27, Loss Value: 0.9404\n",
      "Batch 27, Gradient Norm: 0.6603\n",
      "\u001b[1m 27/232\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:37\u001b[0m 477ms/step - accuracy: 0.4917 - binary_io_u_5: 0.4921 - loss: 0.9134Batch 28, Loss Value: 0.9370\n",
      "Batch 28, Gradient Norm: 0.2781\n",
      "\u001b[1m 28/232\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:37\u001b[0m 478ms/step - accuracy: 0.4917 - binary_io_u_5: 0.4919 - loss: 0.9136Batch 29, Loss Value: 0.9186\n",
      "Batch 29, Gradient Norm: 0.0018\n",
      "\u001b[1m 29/232\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:36\u001b[0m 478ms/step - accuracy: 0.4917 - binary_io_u_5: 0.4917 - loss: 0.9137Batch 30, Loss Value: 0.9441\n",
      "Batch 30, Gradient Norm: 0.0085\n",
      "\u001b[1m 30/232\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:36\u001b[0m 478ms/step - accuracy: 0.4916 - binary_io_u_5: 0.4915 - loss: 0.9139Batch 31, Loss Value: 0.9150\n",
      "Batch 31, Gradient Norm: 0.6862\n",
      "\u001b[1m 31/232\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:36\u001b[0m 478ms/step - accuracy: 0.4915 - binary_io_u_5: 0.4913 - loss: 0.9141Batch 32, Loss Value: 0.9250\n",
      "Batch 32, Gradient Norm: 0.2681\n",
      "\u001b[1m 32/232\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:35\u001b[0m 478ms/step - accuracy: 0.4915 - binary_io_u_5: 0.4912 - loss: 0.9142Batch 33, Loss Value: 0.9278\n",
      "Batch 33, Gradient Norm: 0.4260\n",
      "\u001b[1m 33/232\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:35\u001b[0m 478ms/step - accuracy: 0.4915 - binary_io_u_5: 0.4910 - loss: 0.9144Batch 34, Loss Value: 0.9076\n",
      "Batch 34, Gradient Norm: 0.1052\n",
      "\u001b[1m 34/232\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:34\u001b[0m 478ms/step - accuracy: 0.4915 - binary_io_u_5: 0.4909 - loss: 0.9145Batch 35, Loss Value: 0.9149\n",
      "Batch 35, Gradient Norm: 0.1919\n",
      "\u001b[1m 35/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:34\u001b[0m 478ms/step - accuracy: 0.4915 - binary_io_u_5: 0.4908 - loss: 0.9146Batch 36, Loss Value: 0.9042\n",
      "Batch 36, Gradient Norm: 0.2180\n",
      "\u001b[1m 36/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:33\u001b[0m 478ms/step - accuracy: 0.4916 - binary_io_u_5: 0.4907 - loss: 0.9147Batch 37, Loss Value: 0.8974\n",
      "Batch 37, Gradient Norm: 0.0740\n",
      "\u001b[1m 37/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:33\u001b[0m 478ms/step - accuracy: 0.4916 - binary_io_u_5: 0.4906 - loss: 0.9148Batch 38, Loss Value: 0.9181\n",
      "Batch 38, Gradient Norm: 0.3112\n",
      "\u001b[1m 38/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:32\u001b[0m 478ms/step - accuracy: 0.4918 - binary_io_u_5: 0.4906 - loss: 0.9148Batch 39, Loss Value: 0.9198\n",
      "Batch 39, Gradient Norm: 0.0192\n",
      "\u001b[1m 39/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:32\u001b[0m 478ms/step - accuracy: 0.4919 - binary_io_u_5: 0.4905 - loss: 0.9148Batch 40, Loss Value: 0.9133\n",
      "Batch 40, Gradient Norm: 0.0694\n",
      "\u001b[1m 40/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:31\u001b[0m 479ms/step - accuracy: 0.4920 - binary_io_u_5: 0.4905 - loss: 0.9148Batch 41, Loss Value: 0.9216\n",
      "Batch 41, Gradient Norm: 0.0705\n",
      "\u001b[1m 41/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:31\u001b[0m 480ms/step - accuracy: 0.4921 - binary_io_u_5: 0.4905 - loss: 0.9149Batch 42, Loss Value: 0.9000\n",
      "Batch 42, Gradient Norm: 0.2665\n",
      "\u001b[1m 42/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:31\u001b[0m 480ms/step - accuracy: 0.4922 - binary_io_u_5: 0.4905 - loss: 0.9149Batch 43, Loss Value: 0.9157\n",
      "Batch 43, Gradient Norm: 0.0075\n",
      "\u001b[1m 43/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:30\u001b[0m 481ms/step - accuracy: 0.4922 - binary_io_u_5: 0.4905 - loss: 0.9149Batch 44, Loss Value: 0.9106\n",
      "Batch 44, Gradient Norm: 0.0806\n",
      "\u001b[1m 44/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:30\u001b[0m 481ms/step - accuracy: 0.4923 - binary_io_u_5: 0.4905 - loss: 0.9149Batch 45, Loss Value: 0.9095\n",
      "Batch 45, Gradient Norm: 0.8274\n",
      "\u001b[1m 45/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:29\u001b[0m 481ms/step - accuracy: 0.4924 - binary_io_u_5: 0.4905 - loss: 0.9150Batch 46, Loss Value: 0.9438\n",
      "Batch 46, Gradient Norm: 0.0113\n",
      "\u001b[1m 46/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:29\u001b[0m 482ms/step - accuracy: 0.4924 - binary_io_u_5: 0.4904 - loss: 0.9150Batch 47, Loss Value: 0.9068\n",
      "Batch 47, Gradient Norm: 0.0278\n",
      "\u001b[1m 47/232\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:29\u001b[0m 482ms/step - accuracy: 0.4924 - binary_io_u_5: 0.4904 - loss: 0.9151Batch 48, Loss Value: 0.9367\n",
      "Batch 48, Gradient Norm: 0.1216\n",
      "\u001b[1m 48/232\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:28\u001b[0m 482ms/step - accuracy: 0.4925 - binary_io_u_5: 0.4904 - loss: 0.9151Batch 49, Loss Value: 0.9265\n",
      "Batch 49, Gradient Norm: 0.3441\n",
      "\u001b[1m 49/232\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:28\u001b[0m 482ms/step - accuracy: 0.4925 - binary_io_u_5: 0.4903 - loss: 0.9152Batch 50, Loss Value: 0.9135\n",
      "Batch 50, Gradient Norm: 0.6212\n",
      "\u001b[1m 50/232\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:27\u001b[0m 482ms/step - accuracy: 0.4925 - binary_io_u_5: 0.4902 - loss: 0.9153Batch 51, Loss Value: 0.9436\n",
      "Batch 51, Gradient Norm: 0.0005\n",
      "\u001b[1m 51/232\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:27\u001b[0m 482ms/step - accuracy: 0.4924 - binary_io_u_5: 0.4901 - loss: 0.9153Batch 52, Loss Value: 0.9366\n",
      "Batch 52, Gradient Norm: 0.2279\n",
      "\u001b[1m 52/232\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:26\u001b[0m 483ms/step - accuracy: 0.4924 - binary_io_u_5: 0.4900 - loss: 0.9154Batch 53, Loss Value: 0.9177\n",
      "Batch 53, Gradient Norm: 0.0614\n",
      "\u001b[1m 53/232\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:26\u001b[0m 483ms/step - accuracy: 0.4924 - binary_io_u_5: 0.4900 - loss: 0.9154Batch 54, Loss Value: 0.9255\n",
      "Batch 54, Gradient Norm: 0.0158\n",
      "\u001b[1m 54/232\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:25\u001b[0m 483ms/step - accuracy: 0.4924 - binary_io_u_5: 0.4899 - loss: 0.9155Batch 55, Loss Value: 0.9291\n",
      "Batch 55, Gradient Norm: 0.2145\n",
      "\u001b[1m 55/232\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:25\u001b[0m 483ms/step - accuracy: 0.4924 - binary_io_u_5: 0.4898 - loss: 0.9155Batch 56, Loss Value: 0.9400\n",
      "Batch 56, Gradient Norm: 0.2101\n",
      "\u001b[1m 56/232\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:25\u001b[0m 483ms/step - accuracy: 0.4923 - binary_io_u_5: 0.4897 - loss: 0.9156Batch 57, Loss Value: 0.9435\n",
      "Batch 57, Gradient Norm: 0.0120\n",
      "\u001b[1m 57/232\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:24\u001b[0m 483ms/step - accuracy: 0.4923 - binary_io_u_5: 0.4897 - loss: 0.9156Batch 58, Loss Value: 0.9250\n",
      "Batch 58, Gradient Norm: 0.0450\n",
      "\u001b[1m 58/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:24\u001b[0m 483ms/step - accuracy: 0.4923 - binary_io_u_5: 0.4896 - loss: 0.9157Batch 59, Loss Value: 0.9230\n",
      "Batch 59, Gradient Norm: 0.1636\n",
      "\u001b[1m 59/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:23\u001b[0m 483ms/step - accuracy: 0.4922 - binary_io_u_5: 0.4895 - loss: 0.9157Batch 60, Loss Value: 0.9347\n",
      "Batch 60, Gradient Norm: 0.5142\n",
      "\u001b[1m 60/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:23\u001b[0m 483ms/step - accuracy: 0.4922 - binary_io_u_5: 0.4895 - loss: 0.9158Batch 61, Loss Value: 0.9292\n",
      "Batch 61, Gradient Norm: 0.1071\n",
      "\u001b[1m 61/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:22\u001b[0m 483ms/step - accuracy: 0.4922 - binary_io_u_5: 0.4894 - loss: 0.9158Batch 62, Loss Value: 0.9184\n",
      "Batch 62, Gradient Norm: 0.0774\n",
      "\u001b[1m 62/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:22\u001b[0m 483ms/step - accuracy: 0.4921 - binary_io_u_5: 0.4893 - loss: 0.9159Batch 63, Loss Value: 0.9196\n",
      "Batch 63, Gradient Norm: 0.0238\n",
      "\u001b[1m 63/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:21\u001b[0m 483ms/step - accuracy: 0.4921 - binary_io_u_5: 0.4892 - loss: 0.9159Batch 64, Loss Value: 0.9237\n",
      "Batch 64, Gradient Norm: 0.4034\n",
      "\u001b[1m 64/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:21\u001b[0m 483ms/step - accuracy: 0.4920 - binary_io_u_5: 0.4891 - loss: 0.9160Batch 65, Loss Value: 0.9184\n",
      "Batch 65, Gradient Norm: 0.0398\n",
      "\u001b[1m 65/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:20\u001b[0m 483ms/step - accuracy: 0.4919 - binary_io_u_5: 0.4890 - loss: 0.9160Batch 66, Loss Value: 0.9365\n",
      "Batch 66, Gradient Norm: 0.2756\n",
      "\u001b[1m 66/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:20\u001b[0m 483ms/step - accuracy: 0.4919 - binary_io_u_5: 0.4889 - loss: 0.9161Batch 67, Loss Value: 0.9229\n",
      "Batch 67, Gradient Norm: 0.3593\n",
      "\u001b[1m 67/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:19\u001b[0m 483ms/step - accuracy: 0.4918 - binary_io_u_5: 0.4888 - loss: 0.9162Batch 68, Loss Value: 0.9294\n",
      "Batch 68, Gradient Norm: 0.4601\n",
      "\u001b[1m 68/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:19\u001b[0m 483ms/step - accuracy: 0.4918 - binary_io_u_5: 0.4887 - loss: 0.9162Batch 69, Loss Value: 0.9229\n",
      "Batch 69, Gradient Norm: 0.1061\n",
      "\u001b[1m 69/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:18\u001b[0m 483ms/step - accuracy: 0.4917 - binary_io_u_5: 0.4887 - loss: 0.9163Batch 70, Loss Value: 0.9326\n",
      "Batch 70, Gradient Norm: 0.1080\n",
      "\u001b[1m 70/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:18\u001b[0m 483ms/step - accuracy: 0.4917 - binary_io_u_5: 0.4886 - loss: 0.9163Batch 71, Loss Value: 0.9094\n",
      "Batch 71, Gradient Norm: 0.0883\n",
      "\u001b[1m 71/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:17\u001b[0m 483ms/step - accuracy: 0.4916 - binary_io_u_5: 0.4886 - loss: 0.9163Batch 72, Loss Value: 0.9435\n",
      "Batch 72, Gradient Norm: 0.0124\n",
      "\u001b[1m 72/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:17\u001b[0m 483ms/step - accuracy: 0.4916 - binary_io_u_5: 0.4885 - loss: 0.9164Batch 73, Loss Value: 0.9454\n",
      "Batch 73, Gradient Norm: 0.0124\n",
      "\u001b[1m 73/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:16\u001b[0m 483ms/step - accuracy: 0.4916 - binary_io_u_5: 0.4885 - loss: 0.9164Batch 74, Loss Value: 0.9401\n",
      "Batch 74, Gradient Norm: 0.2185\n",
      "\u001b[1m 74/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:16\u001b[0m 483ms/step - accuracy: 0.4916 - binary_io_u_5: 0.4884 - loss: 0.9165Batch 75, Loss Value: 0.9039\n",
      "Batch 75, Gradient Norm: 0.2560\n",
      "\u001b[1m 75/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:15\u001b[0m 483ms/step - accuracy: 0.4915 - binary_io_u_5: 0.4884 - loss: 0.9165Batch 76, Loss Value: 0.9234\n",
      "Batch 76, Gradient Norm: 0.0562\n",
      "\u001b[1m 76/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:15\u001b[0m 483ms/step - accuracy: 0.4915 - binary_io_u_5: 0.4883 - loss: 0.9166Batch 77, Loss Value: 0.9379\n",
      "Batch 77, Gradient Norm: 0.1989\n",
      "\u001b[1m 77/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:14\u001b[0m 483ms/step - accuracy: 0.4915 - binary_io_u_5: 0.4883 - loss: 0.9166Batch 78, Loss Value: 0.9099\n",
      "Batch 78, Gradient Norm: 0.3013\n",
      "\u001b[1m 78/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:14\u001b[0m 482ms/step - accuracy: 0.4915 - binary_io_u_5: 0.4882 - loss: 0.9166Batch 79, Loss Value: 0.9119\n",
      "Batch 79, Gradient Norm: 0.1115\n",
      "\u001b[1m 79/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:13\u001b[0m 482ms/step - accuracy: 0.4915 - binary_io_u_5: 0.4882 - loss: 0.9167Batch 80, Loss Value: 0.9191\n",
      "Batch 80, Gradient Norm: 0.3985\n",
      "\u001b[1m 80/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:13\u001b[0m 482ms/step - accuracy: 0.4915 - binary_io_u_5: 0.4882 - loss: 0.9167Batch 81, Loss Value: 0.9229\n",
      "Batch 81, Gradient Norm: 0.1527\n",
      "\u001b[1m 81/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:12\u001b[0m 482ms/step - accuracy: 0.4915 - binary_io_u_5: 0.4881 - loss: 0.9168Batch 82, Loss Value: 0.9188\n",
      "Batch 82, Gradient Norm: 0.3127\n",
      "\u001b[1m 82/232\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m1:12\u001b[0m 482ms/step - accuracy: 0.4914 - binary_io_u_5: 0.4881 - loss: 0.9168Batch 83, Loss Value: 0.9285\n",
      "Batch 83, Gradient Norm: 0.3020\n",
      "\u001b[1m 83/232\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m1:11\u001b[0m 481ms/step - accuracy: 0.4914 - binary_io_u_5: 0.4880 - loss: 0.9169Batch 84, Loss Value: 0.9120\n",
      "Batch 84, Gradient Norm: 0.3427\n",
      "\u001b[1m 84/232\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m1:11\u001b[0m 481ms/step - accuracy: 0.4914 - binary_io_u_5: 0.4880 - loss: 0.9169Batch 85, Loss Value: 0.9146\n",
      "Batch 85, Gradient Norm: 0.1578\n",
      "\u001b[1m 85/232\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m1:10\u001b[0m 481ms/step - accuracy: 0.4914 - binary_io_u_5: 0.4880 - loss: 0.9169Batch 86, Loss Value: 0.9405\n",
      "Batch 86, Gradient Norm: 0.1593\n",
      "\u001b[1m 86/232\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m1:10\u001b[0m 481ms/step - accuracy: 0.4914 - binary_io_u_5: 0.4880 - loss: 0.9170Batch 87, Loss Value: 0.9176\n",
      "Batch 87, Gradient Norm: 0.1858\n",
      "\u001b[1m 87/232\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m1:09\u001b[0m 481ms/step - accuracy: 0.4913 - binary_io_u_5: 0.4879 - loss: 0.9170Batch 88, Loss Value: 0.9436\n",
      "Batch 88, Gradient Norm: 0.0102\n",
      "\u001b[1m 88/232\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m1:09\u001b[0m 481ms/step - accuracy: 0.4913 - binary_io_u_5: 0.4879 - loss: 0.9170Batch 89, Loss Value: 0.9416\n",
      "Batch 89, Gradient Norm: 0.0599\n",
      "\u001b[1m 89/232\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m1:08\u001b[0m 480ms/step - accuracy: 0.4913 - binary_io_u_5: 0.4879 - loss: 0.9171Batch 90, Loss Value: 0.9208\n",
      "Batch 90, Gradient Norm: 0.2254\n",
      "\u001b[1m 90/232\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m1:08\u001b[0m 480ms/step - accuracy: 0.4913 - binary_io_u_5: 0.4879 - loss: 0.9171Batch 91, Loss Value: 0.9254\n",
      "Batch 91, Gradient Norm: 0.7685\n",
      "\u001b[1m 91/232\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m1:07\u001b[0m 480ms/step - accuracy: 0.4913 - binary_io_u_5: 0.4879 - loss: 0.9171Batch 92, Loss Value: 0.9327\n",
      "Batch 92, Gradient Norm: 0.4346\n",
      "\u001b[1m 92/232\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m1:07\u001b[0m 480ms/step - accuracy: 0.4913 - binary_io_u_5: 0.4878 - loss: 0.9171Batch 93, Loss Value: 0.9372\n",
      "Batch 93, Gradient Norm: 0.0973\n",
      "\u001b[1m 93/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m1:06\u001b[0m 480ms/step - accuracy: 0.4913 - binary_io_u_5: 0.4878 - loss: 0.9172Batch 94, Loss Value: 0.9436\n",
      "Batch 94, Gradient Norm: 0.0038\n",
      "\u001b[1m 94/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m1:06\u001b[0m 480ms/step - accuracy: 0.4913 - binary_io_u_5: 0.4878 - loss: 0.9172Batch 95, Loss Value: 0.9423\n",
      "Batch 95, Gradient Norm: 0.0448\n",
      "\u001b[1m 95/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m1:05\u001b[0m 480ms/step - accuracy: 0.4913 - binary_io_u_5: 0.4878 - loss: 0.9172Batch 96, Loss Value: 0.9296\n",
      "Batch 96, Gradient Norm: 0.0365\n",
      "\u001b[1m 96/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m1:05\u001b[0m 480ms/step - accuracy: 0.4913 - binary_io_u_5: 0.4878 - loss: 0.9172Batch 97, Loss Value: 0.9256\n",
      "Batch 97, Gradient Norm: 0.0044\n",
      "\u001b[1m 97/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m1:04\u001b[0m 480ms/step - accuracy: 0.4913 - binary_io_u_5: 0.4878 - loss: 0.9172Batch 98, Loss Value: 0.9377\n",
      "Batch 98, Gradient Norm: 0.2778\n",
      "\u001b[1m 98/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m1:04\u001b[0m 479ms/step - accuracy: 0.4913 - binary_io_u_5: 0.4878 - loss: 0.9173Batch 99, Loss Value: 0.9438\n",
      "Batch 99, Gradient Norm: 0.0004\n",
      "\u001b[1m 99/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m1:03\u001b[0m 479ms/step - accuracy: 0.4913 - binary_io_u_5: 0.4878 - loss: 0.9173Batch 100, Loss Value: 0.9278\n",
      "Batch 100, Gradient Norm: 0.6667\n",
      "\u001b[1m100/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m1:03\u001b[0m 479ms/step - accuracy: 0.4913 - binary_io_u_5: 0.4878 - loss: 0.9173Batch 101, Loss Value: 0.9331\n",
      "Batch 101, Gradient Norm: 0.0927\n",
      "\u001b[1m101/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m1:02\u001b[0m 479ms/step - accuracy: 0.4913 - binary_io_u_5: 0.4878 - loss: 0.9173Batch 102, Loss Value: 0.9436\n",
      "Batch 102, Gradient Norm: 0.0001\n",
      "\u001b[1m102/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m1:02\u001b[0m 479ms/step - accuracy: 0.4913 - binary_io_u_5: 0.4878 - loss: 0.9173Batch 103, Loss Value: 0.9385\n",
      "Batch 103, Gradient Norm: 0.0112\n",
      "\u001b[1m103/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m1:01\u001b[0m 479ms/step - accuracy: 0.4913 - binary_io_u_5: 0.4878 - loss: 0.9173Batch 104, Loss Value: 0.9371\n",
      "Batch 104, Gradient Norm: 0.0008\n",
      "\u001b[1m104/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m1:01\u001b[0m 478ms/step - accuracy: 0.4913 - binary_io_u_5: 0.4878 - loss: 0.9174Batch 105, Loss Value: 0.9427\n",
      "Batch 105, Gradient Norm: 0.0031\n",
      "\u001b[1m105/232\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m1:00\u001b[0m 479ms/step - accuracy: 0.4913 - binary_io_u_5: 0.4878 - loss: 0.9174Batch 106, Loss Value: 0.9360\n",
      "Batch 106, Gradient Norm: 0.7244\n",
      "\u001b[1m106/232\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m1:00\u001b[0m 478ms/step - accuracy: 0.4913 - binary_io_u_5: 0.4878 - loss: 0.9174Batch 107, Loss Value: 0.9348\n",
      "Batch 107, Gradient Norm: 0.4184\n",
      "\u001b[1m107/232\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m59s\u001b[0m 478ms/step - accuracy: 0.4913 - binary_io_u_5: 0.4878 - loss: 0.9174 Batch 108, Loss Value: 0.9435\n",
      "Batch 108, Gradient Norm: 0.0036\n",
      "\u001b[1m108/232\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m59s\u001b[0m 478ms/step - accuracy: 0.4913 - binary_io_u_5: 0.4878 - loss: 0.9174Batch 109, Loss Value: 0.9369\n",
      "Batch 109, Gradient Norm: 0.0187\n",
      "\u001b[1m109/232\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m58s\u001b[0m 478ms/step - accuracy: 0.4913 - binary_io_u_5: 0.4878 - loss: 0.9175Batch 110, Loss Value: 0.9065\n",
      "Batch 110, Gradient Norm: 1.0348\n",
      "\u001b[1m110/232\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m58s\u001b[0m 478ms/step - accuracy: 0.4913 - binary_io_u_5: 0.4878 - loss: 0.9175Batch 111, Loss Value: 0.9365\n",
      "Batch 111, Gradient Norm: 0.1264\n",
      "\u001b[1m111/232\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m57s\u001b[0m 478ms/step - accuracy: 0.4913 - binary_io_u_5: 0.4878 - loss: 0.9175Batch 112, Loss Value: 0.9186\n",
      "Batch 112, Gradient Norm: 0.0985\n",
      "\u001b[1m112/232\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m57s\u001b[0m 478ms/step - accuracy: 0.4913 - binary_io_u_5: 0.4878 - loss: 0.9175Batch 113, Loss Value: 0.9287\n",
      "Batch 113, Gradient Norm: 0.7468\n",
      "\u001b[1m113/232\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m56s\u001b[0m 477ms/step - accuracy: 0.4913 - binary_io_u_5: 0.4877 - loss: 0.9175Batch 114, Loss Value: 0.9314\n",
      "Batch 114, Gradient Norm: 0.0010\n",
      "\u001b[1m114/232\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m56s\u001b[0m 477ms/step - accuracy: 0.4913 - binary_io_u_5: 0.4877 - loss: 0.9176Batch 115, Loss Value: 0.9200\n",
      "Batch 115, Gradient Norm: 0.3359\n",
      "\u001b[1m115/232\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m55s\u001b[0m 477ms/step - accuracy: 0.4913 - binary_io_u_5: 0.4877 - loss: 0.9176Batch 116, Loss Value: 0.9127\n",
      "Batch 116, Gradient Norm: 0.1388\n",
      "\u001b[1m116/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m55s\u001b[0m 477ms/step - accuracy: 0.4913 - binary_io_u_5: 0.4877 - loss: 0.9176Batch 117, Loss Value: 0.9291\n",
      "Batch 117, Gradient Norm: 0.4305\n",
      "\u001b[1m117/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m54s\u001b[0m 477ms/step - accuracy: 0.4913 - binary_io_u_5: 0.4877 - loss: 0.9176Batch 118, Loss Value: 0.9299\n",
      "Batch 118, Gradient Norm: 0.4662\n",
      "\u001b[1m118/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m54s\u001b[0m 477ms/step - accuracy: 0.4913 - binary_io_u_5: 0.4877 - loss: 0.9176Batch 119, Loss Value: 0.9444\n",
      "Batch 119, Gradient Norm: 0.0108\n",
      "\u001b[1m119/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m53s\u001b[0m 477ms/step - accuracy: 0.4913 - binary_io_u_5: 0.4877 - loss: 0.9176Batch 120, Loss Value: 0.9440\n",
      "Batch 120, Gradient Norm: 0.0126\n",
      "\u001b[1m120/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m53s\u001b[0m 477ms/step - accuracy: 0.4913 - binary_io_u_5: 0.4877 - loss: 0.9176Batch 121, Loss Value: 0.9183\n",
      "Batch 121, Gradient Norm: 0.2451\n",
      "\u001b[1m121/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m52s\u001b[0m 477ms/step - accuracy: 0.4913 - binary_io_u_5: 0.4877 - loss: 0.9177Batch 122, Loss Value: 0.9331\n",
      "Batch 122, Gradient Norm: 0.1812\n",
      "\u001b[1m122/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m52s\u001b[0m 477ms/step - accuracy: 0.4913 - binary_io_u_5: 0.4877 - loss: 0.9177Batch 123, Loss Value: 0.9195\n",
      "Batch 123, Gradient Norm: 0.4301\n",
      "\u001b[1m123/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m51s\u001b[0m 477ms/step - accuracy: 0.4913 - binary_io_u_5: 0.4877 - loss: 0.9177Batch 124, Loss Value: 0.9341\n",
      "Batch 124, Gradient Norm: 0.0386\n",
      "\u001b[1m124/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m51s\u001b[0m 477ms/step - accuracy: 0.4913 - binary_io_u_5: 0.4877 - loss: 0.9177Batch 125, Loss Value: 0.9115\n",
      "Batch 125, Gradient Norm: 0.1043\n",
      "\u001b[1m125/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m51s\u001b[0m 477ms/step - accuracy: 0.4913 - binary_io_u_5: 0.4878 - loss: 0.9177Batch 126, Loss Value: 0.9116\n",
      "Batch 126, Gradient Norm: 0.1850\n",
      "\u001b[1m126/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m50s\u001b[0m 477ms/step - accuracy: 0.4914 - binary_io_u_5: 0.4878 - loss: 0.9177Batch 127, Loss Value: 0.9444\n",
      "Batch 127, Gradient Norm: 0.0114\n",
      "\u001b[1m127/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m50s\u001b[0m 477ms/step - accuracy: 0.4914 - binary_io_u_5: 0.4878 - loss: 0.9177Batch 128, Loss Value: 0.9160\n",
      "Batch 128, Gradient Norm: 0.3545\n",
      "\u001b[1m128/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m49s\u001b[0m 477ms/step - accuracy: 0.4914 - binary_io_u_5: 0.4878 - loss: 0.9177Batch 129, Loss Value: 0.8985\n",
      "Batch 129, Gradient Norm: 0.0736\n",
      "\u001b[1m129/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m49s\u001b[0m 477ms/step - accuracy: 0.4914 - binary_io_u_5: 0.4878 - loss: 0.9177Batch 130, Loss Value: 0.9153\n",
      "Batch 130, Gradient Norm: 1.3004\n",
      "\u001b[1m130/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m48s\u001b[0m 477ms/step - accuracy: 0.4914 - binary_io_u_5: 0.4878 - loss: 0.9177Batch 131, Loss Value: 0.9446\n",
      "Batch 131, Gradient Norm: 0.0270\n",
      "\u001b[1m131/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m48s\u001b[0m 477ms/step - accuracy: 0.4914 - binary_io_u_5: 0.4878 - loss: 0.9177Batch 132, Loss Value: 0.9126\n",
      "Batch 132, Gradient Norm: 0.4235\n",
      "\u001b[1m132/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m47s\u001b[0m 477ms/step - accuracy: 0.4914 - binary_io_u_5: 0.4878 - loss: 0.9177Batch 133, Loss Value: 0.9444\n",
      "Batch 133, Gradient Norm: 0.0091\n",
      "\u001b[1m133/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m47s\u001b[0m 477ms/step - accuracy: 0.4915 - binary_io_u_5: 0.4879 - loss: 0.9178Batch 134, Loss Value: 0.9300\n",
      "Batch 134, Gradient Norm: 0.2462\n",
      "\u001b[1m134/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m46s\u001b[0m 477ms/step - accuracy: 0.4915 - binary_io_u_5: 0.4879 - loss: 0.9178Batch 135, Loss Value: 0.9443\n",
      "Batch 135, Gradient Norm: 0.0191\n",
      "\u001b[1m135/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m46s\u001b[0m 477ms/step - accuracy: 0.4915 - binary_io_u_5: 0.4879 - loss: 0.9178Batch 136, Loss Value: 0.9371\n",
      "Batch 136, Gradient Norm: 0.0176\n",
      "\u001b[1m136/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m45s\u001b[0m 477ms/step - accuracy: 0.4915 - binary_io_u_5: 0.4879 - loss: 0.9178Batch 137, Loss Value: 0.9246\n",
      "Batch 137, Gradient Norm: 0.7139\n",
      "\u001b[1m137/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m45s\u001b[0m 477ms/step - accuracy: 0.4915 - binary_io_u_5: 0.4879 - loss: 0.9178Batch 138, Loss Value: 0.9319\n",
      "Batch 138, Gradient Norm: 0.2720\n",
      "\u001b[1m138/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m44s\u001b[0m 477ms/step - accuracy: 0.4915 - binary_io_u_5: 0.4879 - loss: 0.9178Batch 139, Loss Value: 0.9318\n",
      "Batch 139, Gradient Norm: 0.0944\n",
      "\u001b[1m139/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m44s\u001b[0m 477ms/step - accuracy: 0.4915 - binary_io_u_5: 0.4879 - loss: 0.9178Batch 140, Loss Value: 0.9366\n",
      "Batch 140, Gradient Norm: 0.1032\n",
      "\u001b[1m140/232\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m43s\u001b[0m 477ms/step - accuracy: 0.4915 - binary_io_u_5: 0.4880 - loss: 0.9178Batch 141, Loss Value: 0.9451\n",
      "Batch 141, Gradient Norm: 0.0037\n",
      "\u001b[1m141/232\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m43s\u001b[0m 477ms/step - accuracy: 0.4915 - binary_io_u_5: 0.4880 - loss: 0.9178Batch 142, Loss Value: 0.9444\n",
      "Batch 142, Gradient Norm: 0.0070\n",
      "\u001b[1m142/232\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m42s\u001b[0m 477ms/step - accuracy: 0.4915 - binary_io_u_5: 0.4880 - loss: 0.9178Batch 143, Loss Value: 0.9423\n",
      "Batch 143, Gradient Norm: 0.4362\n",
      "\u001b[1m143/232\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m42s\u001b[0m 477ms/step - accuracy: 0.4915 - binary_io_u_5: 0.4880 - loss: 0.9179Batch 144, Loss Value: 0.9121\n",
      "Batch 144, Gradient Norm: 0.1496\n",
      "\u001b[1m144/232\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m41s\u001b[0m 477ms/step - accuracy: 0.4915 - binary_io_u_5: 0.4880 - loss: 0.9179Batch 145, Loss Value: 0.9405\n",
      "Batch 145, Gradient Norm: 0.3299\n",
      "\u001b[1m145/232\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m41s\u001b[0m 477ms/step - accuracy: 0.4915 - binary_io_u_5: 0.4880 - loss: 0.9179Batch 146, Loss Value: 0.9183\n",
      "Batch 146, Gradient Norm: 0.1804\n",
      "\u001b[1m146/232\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m41s\u001b[0m 477ms/step - accuracy: 0.4915 - binary_io_u_5: 0.4880 - loss: 0.9179Batch 147, Loss Value: 0.9445\n",
      "Batch 147, Gradient Norm: 0.0065\n",
      "\u001b[1m147/232\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m40s\u001b[0m 477ms/step - accuracy: 0.4916 - binary_io_u_5: 0.4880 - loss: 0.9179Batch 148, Loss Value: 0.9417\n",
      "Batch 148, Gradient Norm: 0.0505\n",
      "\u001b[1m148/232\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m40s\u001b[0m 477ms/step - accuracy: 0.4916 - binary_io_u_5: 0.4881 - loss: 0.9179Batch 149, Loss Value: 0.9307\n",
      "Batch 149, Gradient Norm: 0.3765\n",
      "\u001b[1m149/232\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m39s\u001b[0m 477ms/step - accuracy: 0.4916 - binary_io_u_5: 0.4881 - loss: 0.9179Batch 150, Loss Value: 0.9346\n",
      "Batch 150, Gradient Norm: 0.0632\n",
      "\u001b[1m150/232\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m39s\u001b[0m 477ms/step - accuracy: 0.4916 - binary_io_u_5: 0.4881 - loss: 0.9179Batch 151, Loss Value: 0.9200\n",
      "Batch 151, Gradient Norm: 0.0027\n",
      "\u001b[1m151/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m38s\u001b[0m 477ms/step - accuracy: 0.4916 - binary_io_u_5: 0.4881 - loss: 0.9179Batch 152, Loss Value: 0.8858\n",
      "Batch 152, Gradient Norm: 0.3121\n",
      "\u001b[1m152/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m38s\u001b[0m 477ms/step - accuracy: 0.4916 - binary_io_u_5: 0.4881 - loss: 0.9179Batch 153, Loss Value: 0.9298\n",
      "Batch 153, Gradient Norm: 0.0324\n",
      "\u001b[1m153/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m37s\u001b[0m 477ms/step - accuracy: 0.4916 - binary_io_u_5: 0.4881 - loss: 0.9180Batch 154, Loss Value: 0.9273\n",
      "Batch 154, Gradient Norm: 0.1045\n",
      "\u001b[1m154/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m37s\u001b[0m 476ms/step - accuracy: 0.4916 - binary_io_u_5: 0.4882 - loss: 0.9180Batch 155, Loss Value: 0.9459\n",
      "Batch 155, Gradient Norm: 0.0006\n",
      "\u001b[1m155/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m36s\u001b[0m 476ms/step - accuracy: 0.4916 - binary_io_u_5: 0.4882 - loss: 0.9180Batch 156, Loss Value: 0.9423\n",
      "Batch 156, Gradient Norm: 0.1583\n",
      "\u001b[1m156/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m36s\u001b[0m 476ms/step - accuracy: 0.4917 - binary_io_u_5: 0.4882 - loss: 0.9180Batch 157, Loss Value: 0.9233\n",
      "Batch 157, Gradient Norm: 0.1372\n",
      "\u001b[1m157/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m35s\u001b[0m 476ms/step - accuracy: 0.4917 - binary_io_u_5: 0.4882 - loss: 0.9180Batch 158, Loss Value: 0.9069\n",
      "Batch 158, Gradient Norm: 0.0580\n",
      "\u001b[1m158/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m35s\u001b[0m 476ms/step - accuracy: 0.4917 - binary_io_u_5: 0.4882 - loss: 0.9180Batch 159, Loss Value: 0.9437\n",
      "Batch 159, Gradient Norm: 0.3789\n",
      "\u001b[1m159/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m34s\u001b[0m 476ms/step - accuracy: 0.4917 - binary_io_u_5: 0.4882 - loss: 0.9180Batch 160, Loss Value: 0.9449\n",
      "Batch 160, Gradient Norm: 0.0124\n",
      "\u001b[1m160/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m34s\u001b[0m 476ms/step - accuracy: 0.4917 - binary_io_u_5: 0.4883 - loss: 0.9180Batch 161, Loss Value: 0.9365\n",
      "Batch 161, Gradient Norm: 0.1927\n",
      "\u001b[1m161/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m33s\u001b[0m 476ms/step - accuracy: 0.4917 - binary_io_u_5: 0.4883 - loss: 0.9180Batch 162, Loss Value: 0.8804\n",
      "Batch 162, Gradient Norm: 0.7483\n",
      "\u001b[1m162/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m33s\u001b[0m 476ms/step - accuracy: 0.4917 - binary_io_u_5: 0.4883 - loss: 0.9180Batch 163, Loss Value: 0.9428\n",
      "Batch 163, Gradient Norm: 0.0017\n",
      "\u001b[1m163/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m32s\u001b[0m 476ms/step - accuracy: 0.4917 - binary_io_u_5: 0.4883 - loss: 0.9181Batch 164, Loss Value: 0.9226\n",
      "Batch 164, Gradient Norm: 0.0604\n",
      "\u001b[1m164/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m32s\u001b[0m 476ms/step - accuracy: 0.4917 - binary_io_u_5: 0.4884 - loss: 0.9181Batch 165, Loss Value: 0.9447\n",
      "Batch 165, Gradient Norm: 0.0083\n",
      "\u001b[1m165/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m31s\u001b[0m 476ms/step - accuracy: 0.4917 - binary_io_u_5: 0.4884 - loss: 0.9181Batch 166, Loss Value: 0.9403\n",
      "Batch 166, Gradient Norm: 0.2278\n",
      "\u001b[1m166/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m31s\u001b[0m 476ms/step - accuracy: 0.4917 - binary_io_u_5: 0.4884 - loss: 0.9181Batch 167, Loss Value: 0.9449\n",
      "Batch 167, Gradient Norm: 0.0075\n",
      "\u001b[1m167/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m30s\u001b[0m 476ms/step - accuracy: 0.4918 - binary_io_u_5: 0.4884 - loss: 0.9181Batch 168, Loss Value: 0.9452\n",
      "Batch 168, Gradient Norm: 0.0291\n",
      "\u001b[1m168/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m30s\u001b[0m 476ms/step - accuracy: 0.4918 - binary_io_u_5: 0.4884 - loss: 0.9181Batch 169, Loss Value: 0.9445\n",
      "Batch 169, Gradient Norm: 0.0030\n",
      "\u001b[1m169/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m29s\u001b[0m 476ms/step - accuracy: 0.4918 - binary_io_u_5: 0.4884 - loss: 0.9181Batch 170, Loss Value: 0.9210\n",
      "Batch 170, Gradient Norm: 1.1766\n",
      "\u001b[1m170/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m29s\u001b[0m 476ms/step - accuracy: 0.4918 - binary_io_u_5: 0.4885 - loss: 0.9181Batch 171, Loss Value: 0.9379\n",
      "Batch 171, Gradient Norm: 0.1109\n",
      "\u001b[1m171/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m29s\u001b[0m 476ms/step - accuracy: 0.4918 - binary_io_u_5: 0.4885 - loss: 0.9181Batch 172, Loss Value: 0.9461\n",
      "Batch 172, Gradient Norm: 0.0016\n",
      "\u001b[1m172/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m28s\u001b[0m 476ms/step - accuracy: 0.4918 - binary_io_u_5: 0.4885 - loss: 0.9182Batch 173, Loss Value: 0.9443\n",
      "Batch 173, Gradient Norm: 0.0011\n",
      "\u001b[1m173/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m28s\u001b[0m 476ms/step - accuracy: 0.4918 - binary_io_u_5: 0.4885 - loss: 0.9182Batch 174, Loss Value: 0.9383\n",
      "Batch 174, Gradient Norm: 0.0785\n",
      "\u001b[1m174/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m27s\u001b[0m 476ms/step - accuracy: 0.4918 - binary_io_u_5: 0.4885 - loss: 0.9182Batch 175, Loss Value: 0.9448\n",
      "Batch 175, Gradient Norm: 0.2615\n",
      "\u001b[1m175/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m27s\u001b[0m 476ms/step - accuracy: 0.4918 - binary_io_u_5: 0.4886 - loss: 0.9182Batch 176, Loss Value: 0.9446\n",
      "Batch 176, Gradient Norm: 0.0057\n",
      "\u001b[1m176/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m26s\u001b[0m 476ms/step - accuracy: 0.4918 - binary_io_u_5: 0.4886 - loss: 0.9182Batch 177, Loss Value: 0.9129\n",
      "Batch 177, Gradient Norm: 0.3162\n",
      "\u001b[1m177/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m26s\u001b[0m 476ms/step - accuracy: 0.4918 - binary_io_u_5: 0.4886 - loss: 0.9182Batch 178, Loss Value: 0.8965\n",
      "Batch 178, Gradient Norm: 0.0719\n",
      "\u001b[1m178/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m25s\u001b[0m 476ms/step - accuracy: 0.4919 - binary_io_u_5: 0.4886 - loss: 0.9182Batch 179, Loss Value: 0.9394\n",
      "Batch 179, Gradient Norm: 0.3052\n",
      "\u001b[1m179/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m25s\u001b[0m 476ms/step - accuracy: 0.4919 - binary_io_u_5: 0.4887 - loss: 0.9182Batch 180, Loss Value: 0.8819\n",
      "Batch 180, Gradient Norm: 0.4084\n",
      "\u001b[1m180/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m24s\u001b[0m 476ms/step - accuracy: 0.4919 - binary_io_u_5: 0.4887 - loss: 0.9182Batch 181, Loss Value: 0.9339\n",
      "Batch 181, Gradient Norm: 0.0648\n",
      "\u001b[1m181/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m24s\u001b[0m 476ms/step - accuracy: 0.4919 - binary_io_u_5: 0.4887 - loss: 0.9182Batch 182, Loss Value: 0.9294\n",
      "Batch 182, Gradient Norm: 0.2840\n",
      "\u001b[1m182/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m23s\u001b[0m 476ms/step - accuracy: 0.4919 - binary_io_u_5: 0.4887 - loss: 0.9182Batch 183, Loss Value: 0.9456\n",
      "Batch 183, Gradient Norm: 0.0148\n",
      "\u001b[1m183/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m23s\u001b[0m 475ms/step - accuracy: 0.4919 - binary_io_u_5: 0.4887 - loss: 0.9182Batch 184, Loss Value: 0.9268\n",
      "Batch 184, Gradient Norm: 0.2260\n",
      "\u001b[1m184/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m22s\u001b[0m 475ms/step - accuracy: 0.4920 - binary_io_u_5: 0.4888 - loss: 0.9182Batch 185, Loss Value: 0.9282\n",
      "Batch 185, Gradient Norm: 0.7984\n",
      "\u001b[1m185/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m22s\u001b[0m 475ms/step - accuracy: 0.4920 - binary_io_u_5: 0.4888 - loss: 0.9183Batch 186, Loss Value: 0.9443\n",
      "Batch 186, Gradient Norm: 0.0122\n",
      "\u001b[1m186/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m21s\u001b[0m 475ms/step - accuracy: 0.4920 - binary_io_u_5: 0.4888 - loss: 0.9183Batch 187, Loss Value: 0.9443\n",
      "Batch 187, Gradient Norm: 0.0018\n",
      "\u001b[1m187/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m21s\u001b[0m 475ms/step - accuracy: 0.4920 - binary_io_u_5: 0.4888 - loss: 0.9183Batch 188, Loss Value: 0.9180\n",
      "Batch 188, Gradient Norm: 0.0776\n",
      "\u001b[1m188/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m20s\u001b[0m 475ms/step - accuracy: 0.4920 - binary_io_u_5: 0.4889 - loss: 0.9183Batch 189, Loss Value: 0.9442\n",
      "Batch 189, Gradient Norm: 0.0034\n",
      "\u001b[1m189/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m20s\u001b[0m 475ms/step - accuracy: 0.4920 - binary_io_u_5: 0.4889 - loss: 0.9183Batch 190, Loss Value: 0.9231\n",
      "Batch 190, Gradient Norm: 0.1769\n",
      "\u001b[1m190/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m19s\u001b[0m 475ms/step - accuracy: 0.4921 - binary_io_u_5: 0.4889 - loss: 0.9183Batch 191, Loss Value: 0.9423\n",
      "Batch 191, Gradient Norm: 0.2137\n",
      "\u001b[1m191/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m19s\u001b[0m 475ms/step - accuracy: 0.4921 - binary_io_u_5: 0.4889 - loss: 0.9183Batch 192, Loss Value: 0.9170\n",
      "Batch 192, Gradient Norm: 0.1567\n",
      "\u001b[1m192/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m19s\u001b[0m 475ms/step - accuracy: 0.4921 - binary_io_u_5: 0.4890 - loss: 0.9183Batch 193, Loss Value: 0.9441\n",
      "Batch 193, Gradient Norm: 0.0027\n",
      "\u001b[1m193/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m18s\u001b[0m 475ms/step - accuracy: 0.4921 - binary_io_u_5: 0.4890 - loss: 0.9183Batch 194, Loss Value: 0.9029\n",
      "Batch 194, Gradient Norm: 0.0717\n",
      "\u001b[1m194/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m18s\u001b[0m 475ms/step - accuracy: 0.4921 - binary_io_u_5: 0.4890 - loss: 0.9183Batch 195, Loss Value: 0.9320\n",
      "Batch 195, Gradient Norm: 0.0904\n",
      "\u001b[1m195/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m17s\u001b[0m 475ms/step - accuracy: 0.4922 - binary_io_u_5: 0.4891 - loss: 0.9183Batch 196, Loss Value: 0.9435\n",
      "Batch 196, Gradient Norm: 0.0049\n",
      "\u001b[1m196/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m17s\u001b[0m 475ms/step - accuracy: 0.4922 - binary_io_u_5: 0.4891 - loss: 0.9183Batch 197, Loss Value: 0.9440\n",
      "Batch 197, Gradient Norm: 0.0100\n",
      "\u001b[1m197/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m16s\u001b[0m 475ms/step - accuracy: 0.4922 - binary_io_u_5: 0.4891 - loss: 0.9183Batch 198, Loss Value: 0.9231\n",
      "Batch 198, Gradient Norm: 0.3120\n",
      "\u001b[1m198/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m16s\u001b[0m 475ms/step - accuracy: 0.4922 - binary_io_u_5: 0.4891 - loss: 0.9183Batch 199, Loss Value: 0.9302\n",
      "Batch 199, Gradient Norm: 0.3447\n",
      "\u001b[1m199/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m15s\u001b[0m 475ms/step - accuracy: 0.4922 - binary_io_u_5: 0.4892 - loss: 0.9183Batch 200, Loss Value: 0.9381\n",
      "Batch 200, Gradient Norm: 0.2685\n",
      "\u001b[1m200/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m15s\u001b[0m 475ms/step - accuracy: 0.4922 - binary_io_u_5: 0.4892 - loss: 0.9183Batch 201, Loss Value: 0.9356\n",
      "Batch 201, Gradient Norm: 0.2500\n",
      "\u001b[1m201/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m14s\u001b[0m 475ms/step - accuracy: 0.4923 - binary_io_u_5: 0.4892 - loss: 0.9183Batch 202, Loss Value: 0.8985\n",
      "Batch 202, Gradient Norm: 0.6968\n",
      "\u001b[1m202/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m14s\u001b[0m 475ms/step - accuracy: 0.4923 - binary_io_u_5: 0.4892 - loss: 0.9183Batch 203, Loss Value: 0.9439\n",
      "Batch 203, Gradient Norm: 0.0043\n",
      "\u001b[1m203/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m13s\u001b[0m 475ms/step - accuracy: 0.4923 - binary_io_u_5: 0.4893 - loss: 0.9184Batch 204, Loss Value: 0.9372\n",
      "Batch 204, Gradient Norm: 0.0251\n",
      "\u001b[1m204/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m13s\u001b[0m 475ms/step - accuracy: 0.4923 - binary_io_u_5: 0.4893 - loss: 0.9184Batch 205, Loss Value: 0.9314\n",
      "Batch 205, Gradient Norm: 0.0047\n",
      "\u001b[1m205/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m12s\u001b[0m 475ms/step - accuracy: 0.4923 - binary_io_u_5: 0.4893 - loss: 0.9184Batch 206, Loss Value: 0.9012\n",
      "Batch 206, Gradient Norm: 0.1718\n",
      "\u001b[1m206/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m12s\u001b[0m 475ms/step - accuracy: 0.4923 - binary_io_u_5: 0.4893 - loss: 0.9184Batch 207, Loss Value: 0.9182\n",
      "Batch 207, Gradient Norm: 0.6010\n",
      "\u001b[1m207/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m11s\u001b[0m 475ms/step - accuracy: 0.4923 - binary_io_u_5: 0.4893 - loss: 0.9184Batch 208, Loss Value: 0.9309\n",
      "Batch 208, Gradient Norm: 0.5012\n",
      "\u001b[1m208/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m11s\u001b[0m 475ms/step - accuracy: 0.4923 - binary_io_u_5: 0.4894 - loss: 0.9184Batch 209, Loss Value: 0.9355\n",
      "Batch 209, Gradient Norm: 0.3264\n",
      "\u001b[1m209/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m10s\u001b[0m 475ms/step - accuracy: 0.4923 - binary_io_u_5: 0.4894 - loss: 0.9184Batch 210, Loss Value: 0.9125\n",
      "Batch 210, Gradient Norm: 0.0646\n",
      "\u001b[1m210/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m10s\u001b[0m 475ms/step - accuracy: 0.4923 - binary_io_u_5: 0.4894 - loss: 0.9184Batch 211, Loss Value: 0.9057\n",
      "Batch 211, Gradient Norm: 0.0125\n",
      "\u001b[1m211/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m9s\u001b[0m 475ms/step - accuracy: 0.4924 - binary_io_u_5: 0.4894 - loss: 0.9184 Batch 212, Loss Value: 0.9437\n",
      "Batch 212, Gradient Norm: 0.0000\n",
      "\u001b[1m212/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m9s\u001b[0m 475ms/step - accuracy: 0.4924 - binary_io_u_5: 0.4894 - loss: 0.9184Batch 213, Loss Value: 0.9434\n",
      "Batch 213, Gradient Norm: 0.0005\n",
      "\u001b[1m213/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m9s\u001b[0m 475ms/step - accuracy: 0.4924 - binary_io_u_5: 0.4895 - loss: 0.9184Batch 214, Loss Value: 0.9222\n",
      "Batch 214, Gradient Norm: 0.1196\n",
      "\u001b[1m214/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m8s\u001b[0m 475ms/step - accuracy: 0.4924 - binary_io_u_5: 0.4895 - loss: 0.9184Batch 215, Loss Value: 0.9219\n",
      "Batch 215, Gradient Norm: 0.1441\n",
      "\u001b[1m215/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m8s\u001b[0m 475ms/step - accuracy: 0.4924 - binary_io_u_5: 0.4895 - loss: 0.9184Batch 216, Loss Value: 0.9398\n",
      "Batch 216, Gradient Norm: 0.0505\n",
      "\u001b[1m216/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m7s\u001b[0m 475ms/step - accuracy: 0.4924 - binary_io_u_5: 0.4895 - loss: 0.9184Batch 217, Loss Value: 0.9248\n",
      "Batch 217, Gradient Norm: 0.3215\n",
      "\u001b[1m217/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m7s\u001b[0m 475ms/step - accuracy: 0.4924 - binary_io_u_5: 0.4896 - loss: 0.9184Batch 218, Loss Value: 0.9440\n",
      "Batch 218, Gradient Norm: 0.0066\n",
      "\u001b[1m218/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m6s\u001b[0m 475ms/step - accuracy: 0.4924 - binary_io_u_5: 0.4896 - loss: 0.9184Batch 219, Loss Value: 0.9435\n",
      "Batch 219, Gradient Norm: 0.0107\n",
      "\u001b[1m219/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m6s\u001b[0m 475ms/step - accuracy: 0.4925 - binary_io_u_5: 0.4896 - loss: 0.9184Batch 220, Loss Value: 0.9215\n",
      "Batch 220, Gradient Norm: 0.0175\n",
      "\u001b[1m220/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m5s\u001b[0m 475ms/step - accuracy: 0.4925 - binary_io_u_5: 0.4896 - loss: 0.9184Batch 221, Loss Value: 0.9271\n",
      "Batch 221, Gradient Norm: 0.0039\n",
      "\u001b[1m221/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m5s\u001b[0m 475ms/step - accuracy: 0.4925 - binary_io_u_5: 0.4897 - loss: 0.9184Batch 222, Loss Value: 0.9171\n",
      "Batch 222, Gradient Norm: 0.5216\n",
      "\u001b[1m222/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m4s\u001b[0m 475ms/step - accuracy: 0.4925 - binary_io_u_5: 0.4897 - loss: 0.9184Batch 223, Loss Value: 0.9342\n",
      "Batch 223, Gradient Norm: 0.0004\n",
      "\u001b[1m223/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m4s\u001b[0m 475ms/step - accuracy: 0.4925 - binary_io_u_5: 0.4897 - loss: 0.9185Batch 224, Loss Value: 0.9257\n",
      "Batch 224, Gradient Norm: 0.0013\n",
      "\u001b[1m224/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m3s\u001b[0m 475ms/step - accuracy: 0.4925 - binary_io_u_5: 0.4897 - loss: 0.9185Batch 225, Loss Value: 0.9328\n",
      "Batch 225, Gradient Norm: 0.0044\n",
      "\u001b[1m225/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m3s\u001b[0m 475ms/step - accuracy: 0.4925 - binary_io_u_5: 0.4897 - loss: 0.9185Batch 226, Loss Value: 0.9351\n",
      "Batch 226, Gradient Norm: 0.4081\n",
      "\u001b[1m226/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m2s\u001b[0m 475ms/step - accuracy: 0.4925 - binary_io_u_5: 0.4898 - loss: 0.9185Batch 227, Loss Value: 0.9083\n",
      "Batch 227, Gradient Norm: 0.0387\n",
      "\u001b[1m227/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m2s\u001b[0m 475ms/step - accuracy: 0.4925 - binary_io_u_5: 0.4898 - loss: 0.9185Batch 228, Loss Value: 0.9029\n",
      "Batch 228, Gradient Norm: 0.0701\n",
      "\u001b[1m228/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m1s\u001b[0m 475ms/step - accuracy: 0.4926 - binary_io_u_5: 0.4898 - loss: 0.9185Batch 229, Loss Value: 0.9129\n",
      "Batch 229, Gradient Norm: 0.0110\n",
      "\u001b[1m229/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m1s\u001b[0m 475ms/step - accuracy: 0.4926 - binary_io_u_5: 0.4898 - loss: 0.9185Batch 230, Loss Value: 0.9400\n",
      "Batch 230, Gradient Norm: 0.0290\n",
      "\u001b[1m230/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 475ms/step - accuracy: 0.4926 - binary_io_u_5: 0.4898 - loss: 0.9185Batch 231, Loss Value: 0.9440\n",
      "Batch 231, Gradient Norm: 0.0010\n",
      "\u001b[1m231/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 475ms/step - accuracy: 0.4926 - binary_io_u_5: 0.4899 - loss: 0.9185Batch 232, Loss Value: 0.9438\n",
      "Batch 232, Gradient Norm: 0.0114\n",
      "\u001b[1m232/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 481ms/step - accuracy: 0.4926 - binary_io_u_5: 0.4899 - loss: 0.9185 - val_accuracy: 0.5016 - val_binary_io_u_5: 0.4949 - val_loss: 0.9263 - learning_rate: 0.0010\n",
      "Epoch 6/200\n",
      "Batch 1, Loss Value: 0.9306\n",
      "Batch 1, Gradient Norm: 0.0025\n",
      "\u001b[1m  1/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:55\u001b[0m 498ms/step - accuracy: 0.6100 - binary_io_u_5: 0.5900 - loss: 0.8463Batch 2, Loss Value: 0.9363\n",
      "Batch 2, Gradient Norm: 0.1365\n",
      "\u001b[1m  2/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:49\u001b[0m 475ms/step - accuracy: 0.5850 - binary_io_u_5: 0.5625 - loss: 0.8608Batch 3, Loss Value: 0.9309\n",
      "Batch 3, Gradient Norm: 0.2323\n",
      "\u001b[1m  3/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:47\u001b[0m 469ms/step - accuracy: 0.5733 - binary_io_u_5: 0.5539 - loss: 0.8679Batch 4, Loss Value: 0.9389\n",
      "Batch 4, Gradient Norm: 0.0211\n",
      "\u001b[1m  4/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:46\u001b[0m 467ms/step - accuracy: 0.5669 - binary_io_u_5: 0.5510 - loss: 0.8724Batch 5, Loss Value: 0.9196\n",
      "Batch 5, Gradient Norm: 0.2022\n",
      "\u001b[1m  5/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:45\u001b[0m 464ms/step - accuracy: 0.5591 - binary_io_u_5: 0.5460 - loss: 0.8774Batch 6, Loss Value: 0.9304\n",
      "Batch 6, Gradient Norm: 0.0544\n",
      "\u001b[1m  6/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:45\u001b[0m 469ms/step - accuracy: 0.5531 - binary_io_u_5: 0.5417 - loss: 0.8816Batch 7, Loss Value: 0.9220\n",
      "Batch 7, Gradient Norm: 0.6808\n",
      "\u001b[1m  7/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:45\u001b[0m 468ms/step - accuracy: 0.5482 - binary_io_u_5: 0.5380 - loss: 0.8848Batch 8, Loss Value: 0.9162\n",
      "Batch 8, Gradient Norm: 0.1718\n",
      "\u001b[1m  8/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:44\u001b[0m 467ms/step - accuracy: 0.5448 - binary_io_u_5: 0.5357 - loss: 0.8870Batch 9, Loss Value: 0.9224\n",
      "Batch 9, Gradient Norm: 0.0819\n",
      "\u001b[1m  9/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:44\u001b[0m 467ms/step - accuracy: 0.5419 - binary_io_u_5: 0.5341 - loss: 0.8888Batch 10, Loss Value: 0.9233\n",
      "Batch 10, Gradient Norm: 0.0070\n",
      "\u001b[1m 10/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:43\u001b[0m 467ms/step - accuracy: 0.5399 - binary_io_u_5: 0.5327 - loss: 0.8902Batch 11, Loss Value: 0.9357\n",
      "Batch 11, Gradient Norm: 0.0742\n",
      "\u001b[1m 11/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:43\u001b[0m 467ms/step - accuracy: 0.5377 - binary_io_u_5: 0.5310 - loss: 0.8916Batch 12, Loss Value: 0.9234\n",
      "Batch 12, Gradient Norm: 0.0064\n",
      "\u001b[1m 12/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:42\u001b[0m 466ms/step - accuracy: 0.5351 - binary_io_u_5: 0.5290 - loss: 0.8930Batch 13, Loss Value: 0.9150\n",
      "Batch 13, Gradient Norm: 0.1189\n",
      "\u001b[1m 13/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:42\u001b[0m 466ms/step - accuracy: 0.5329 - binary_io_u_5: 0.5274 - loss: 0.8943Batch 14, Loss Value: 0.9055\n",
      "Batch 14, Gradient Norm: 0.5097\n",
      "\u001b[1m 14/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:41\u001b[0m 466ms/step - accuracy: 0.5308 - binary_io_u_5: 0.5257 - loss: 0.8955Batch 15, Loss Value: 0.9160\n",
      "Batch 15, Gradient Norm: 0.1413\n",
      "\u001b[1m 15/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:41\u001b[0m 466ms/step - accuracy: 0.5288 - binary_io_u_5: 0.5243 - loss: 0.8967Batch 16, Loss Value: 0.9367\n",
      "Batch 16, Gradient Norm: 0.1174\n",
      "\u001b[1m 16/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:40\u001b[0m 466ms/step - accuracy: 0.5271 - binary_io_u_5: 0.5231 - loss: 0.8976Batch 17, Loss Value: 0.9233\n",
      "Batch 17, Gradient Norm: 0.0108\n",
      "\u001b[1m 17/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:40\u001b[0m 466ms/step - accuracy: 0.5259 - binary_io_u_5: 0.5221 - loss: 0.8984Batch 18, Loss Value: 0.9233\n",
      "Batch 18, Gradient Norm: 0.1326\n",
      "\u001b[1m 18/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:39\u001b[0m 466ms/step - accuracy: 0.5248 - binary_io_u_5: 0.5214 - loss: 0.8991Batch 19, Loss Value: 0.9119\n",
      "Batch 19, Gradient Norm: 0.3636\n",
      "\u001b[1m 19/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:39\u001b[0m 466ms/step - accuracy: 0.5240 - binary_io_u_5: 0.5208 - loss: 0.8996Batch 20, Loss Value: 0.9126\n",
      "Batch 20, Gradient Norm: 0.2104\n",
      "\u001b[1m 20/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:38\u001b[0m 466ms/step - accuracy: 0.5232 - binary_io_u_5: 0.5203 - loss: 0.9000Batch 21, Loss Value: 0.9083\n",
      "Batch 21, Gradient Norm: 0.8303\n",
      "\u001b[1m 21/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:38\u001b[0m 465ms/step - accuracy: 0.5225 - binary_io_u_5: 0.5199 - loss: 0.9002Batch 22, Loss Value: 0.9222\n",
      "Batch 22, Gradient Norm: 0.8661\n",
      "\u001b[1m 22/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:37\u001b[0m 466ms/step - accuracy: 0.5218 - binary_io_u_5: 0.5194 - loss: 0.9006Batch 23, Loss Value: 0.9306\n",
      "Batch 23, Gradient Norm: 0.2138\n",
      "\u001b[1m 23/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:37\u001b[0m 466ms/step - accuracy: 0.5211 - binary_io_u_5: 0.5188 - loss: 0.9010Batch 24, Loss Value: 0.8994\n",
      "Batch 24, Gradient Norm: 0.1996\n",
      "\u001b[1m 24/232\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:36\u001b[0m 466ms/step - accuracy: 0.5204 - binary_io_u_5: 0.5184 - loss: 0.9013Batch 25, Loss Value: 0.9441\n",
      "Batch 25, Gradient Norm: 0.0543\n",
      "\u001b[1m 25/232\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:36\u001b[0m 466ms/step - accuracy: 0.5198 - binary_io_u_5: 0.5180 - loss: 0.9016Batch 26, Loss Value: 0.9236\n",
      "Batch 26, Gradient Norm: 0.3249\n",
      "\u001b[1m 26/232\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:35\u001b[0m 466ms/step - accuracy: 0.5192 - binary_io_u_5: 0.5177 - loss: 0.9018Batch 27, Loss Value: 0.9227\n",
      "Batch 27, Gradient Norm: 0.2686\n",
      "\u001b[1m 27/232\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:35\u001b[0m 465ms/step - accuracy: 0.5187 - binary_io_u_5: 0.5173 - loss: 0.9021Batch 28, Loss Value: 0.9094\n",
      "Batch 28, Gradient Norm: 0.4110\n",
      "\u001b[1m 28/232\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:34\u001b[0m 465ms/step - accuracy: 0.5181 - binary_io_u_5: 0.5169 - loss: 0.9024Batch 29, Loss Value: 0.9354\n",
      "Batch 29, Gradient Norm: 0.3147\n",
      "\u001b[1m 29/232\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:34\u001b[0m 465ms/step - accuracy: 0.5175 - binary_io_u_5: 0.5164 - loss: 0.9027Batch 30, Loss Value: 0.9237\n",
      "Batch 30, Gradient Norm: 0.3034\n",
      "\u001b[1m 30/232\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:33\u001b[0m 465ms/step - accuracy: 0.5169 - binary_io_u_5: 0.5158 - loss: 0.9029Batch 31, Loss Value: 0.9443\n",
      "Batch 31, Gradient Norm: 0.0502\n",
      "\u001b[1m 31/232\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:33\u001b[0m 465ms/step - accuracy: 0.5164 - binary_io_u_5: 0.5154 - loss: 0.9032Batch 32, Loss Value: 0.8872\n",
      "Batch 32, Gradient Norm: 0.1498\n",
      "\u001b[1m 32/232\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:33\u001b[0m 465ms/step - accuracy: 0.5158 - binary_io_u_5: 0.5148 - loss: 0.9035Batch 33, Loss Value: 0.9261\n",
      "Batch 33, Gradient Norm: 0.3478\n",
      "\u001b[1m 33/232\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:32\u001b[0m 465ms/step - accuracy: 0.5152 - binary_io_u_5: 0.5143 - loss: 0.9037Batch 34, Loss Value: 0.9266\n",
      "Batch 34, Gradient Norm: 0.1468\n",
      "\u001b[1m 34/232\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:32\u001b[0m 465ms/step - accuracy: 0.5147 - binary_io_u_5: 0.5137 - loss: 0.9040Batch 35, Loss Value: 0.9246\n",
      "Batch 35, Gradient Norm: 0.4608\n",
      "\u001b[1m 35/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:31\u001b[0m 465ms/step - accuracy: 0.5142 - binary_io_u_5: 0.5133 - loss: 0.9042Batch 36, Loss Value: 0.9035\n",
      "Batch 36, Gradient Norm: 0.4060\n",
      "\u001b[1m 36/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:31\u001b[0m 465ms/step - accuracy: 0.5138 - binary_io_u_5: 0.5130 - loss: 0.9043Batch 37, Loss Value: 0.8869\n",
      "Batch 37, Gradient Norm: 0.1215\n",
      "\u001b[1m 37/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:30\u001b[0m 465ms/step - accuracy: 0.5134 - binary_io_u_5: 0.5126 - loss: 0.9045Batch 38, Loss Value: 0.9328\n",
      "Batch 38, Gradient Norm: 0.4443\n",
      "\u001b[1m 38/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:30\u001b[0m 465ms/step - accuracy: 0.5130 - binary_io_u_5: 0.5122 - loss: 0.9047Batch 39, Loss Value: 0.9044\n",
      "Batch 39, Gradient Norm: 0.6073\n",
      "\u001b[1m 39/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:29\u001b[0m 465ms/step - accuracy: 0.5127 - binary_io_u_5: 0.5119 - loss: 0.9048Batch 40, Loss Value: 0.9093\n",
      "Batch 40, Gradient Norm: 0.4477\n",
      "\u001b[1m 40/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:29\u001b[0m 466ms/step - accuracy: 0.5124 - binary_io_u_5: 0.5116 - loss: 0.9050Batch 41, Loss Value: 0.8946\n",
      "Batch 41, Gradient Norm: 0.2987\n",
      "\u001b[1m 41/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:28\u001b[0m 466ms/step - accuracy: 0.5121 - binary_io_u_5: 0.5114 - loss: 0.9051Batch 42, Loss Value: 0.9076\n",
      "Batch 42, Gradient Norm: 0.1742\n",
      "\u001b[1m 42/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:28\u001b[0m 466ms/step - accuracy: 0.5119 - binary_io_u_5: 0.5112 - loss: 0.9052Batch 43, Loss Value: 0.9050\n",
      "Batch 43, Gradient Norm: 0.4073\n",
      "\u001b[1m 43/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:27\u001b[0m 465ms/step - accuracy: 0.5117 - binary_io_u_5: 0.5110 - loss: 0.9053Batch 44, Loss Value: 0.9366\n",
      "Batch 44, Gradient Norm: 0.0552\n",
      "\u001b[1m 44/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:27\u001b[0m 466ms/step - accuracy: 0.5115 - binary_io_u_5: 0.5108 - loss: 0.9054Batch 45, Loss Value: 0.9118\n",
      "Batch 45, Gradient Norm: 0.0144\n",
      "\u001b[1m 45/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:27\u001b[0m 466ms/step - accuracy: 0.5113 - binary_io_u_5: 0.5105 - loss: 0.9055Batch 46, Loss Value: 0.9251\n",
      "Batch 46, Gradient Norm: 0.1788\n",
      "\u001b[1m 46/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:26\u001b[0m 466ms/step - accuracy: 0.5111 - binary_io_u_5: 0.5103 - loss: 0.9056Batch 47, Loss Value: 0.9177\n",
      "Batch 47, Gradient Norm: 0.1535\n",
      "\u001b[1m 47/232\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:26\u001b[0m 466ms/step - accuracy: 0.5110 - binary_io_u_5: 0.5101 - loss: 0.9057Batch 48, Loss Value: 0.9239\n",
      "Batch 48, Gradient Norm: 0.1403\n",
      "\u001b[1m 48/232\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:25\u001b[0m 466ms/step - accuracy: 0.5108 - binary_io_u_5: 0.5099 - loss: 0.9057Batch 49, Loss Value: 0.9030\n",
      "Batch 49, Gradient Norm: 0.0294\n",
      "\u001b[1m 49/232\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:25\u001b[0m 466ms/step - accuracy: 0.5107 - binary_io_u_5: 0.5098 - loss: 0.9058Batch 50, Loss Value: 0.9220\n",
      "Batch 50, Gradient Norm: 0.2228\n",
      "\u001b[1m 50/232\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:24\u001b[0m 466ms/step - accuracy: 0.5105 - binary_io_u_5: 0.5096 - loss: 0.9059Batch 51, Loss Value: 0.9224\n",
      "Batch 51, Gradient Norm: 0.3734\n",
      "\u001b[1m 51/232\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:24\u001b[0m 466ms/step - accuracy: 0.5104 - binary_io_u_5: 0.5094 - loss: 0.9060Batch 52, Loss Value: 0.9029\n",
      "Batch 52, Gradient Norm: 0.2572\n",
      "\u001b[1m 52/232\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:23\u001b[0m 466ms/step - accuracy: 0.5103 - binary_io_u_5: 0.5093 - loss: 0.9061Batch 53, Loss Value: 0.8936\n",
      "Batch 53, Gradient Norm: 0.3843\n",
      "\u001b[1m 53/232\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:23\u001b[0m 466ms/step - accuracy: 0.5102 - binary_io_u_5: 0.5092 - loss: 0.9061Batch 54, Loss Value: 0.8773\n",
      "Batch 54, Gradient Norm: 0.1430\n",
      "\u001b[1m 54/232\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:23\u001b[0m 466ms/step - accuracy: 0.5102 - binary_io_u_5: 0.5091 - loss: 0.9062Batch 55, Loss Value: 0.9421\n",
      "Batch 55, Gradient Norm: 0.0001\n",
      "\u001b[1m 55/232\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:22\u001b[0m 466ms/step - accuracy: 0.5101 - binary_io_u_5: 0.5090 - loss: 0.9062Batch 56, Loss Value: 0.9108\n",
      "Batch 56, Gradient Norm: 0.3289\n",
      "\u001b[1m 56/232\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:22\u001b[0m 466ms/step - accuracy: 0.5100 - binary_io_u_5: 0.5088 - loss: 0.9063Batch 57, Loss Value: 0.9114\n",
      "Batch 57, Gradient Norm: 0.1456\n",
      "\u001b[1m 57/232\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:21\u001b[0m 467ms/step - accuracy: 0.5098 - binary_io_u_5: 0.5087 - loss: 0.9064Batch 58, Loss Value: 0.9330\n",
      "Batch 58, Gradient Norm: 0.1101\n",
      "\u001b[1m 58/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:21\u001b[0m 467ms/step - accuracy: 0.5097 - binary_io_u_5: 0.5085 - loss: 0.9065Batch 59, Loss Value: 0.8993\n",
      "Batch 59, Gradient Norm: 0.3877\n",
      "\u001b[1m 59/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:20\u001b[0m 467ms/step - accuracy: 0.5096 - binary_io_u_5: 0.5083 - loss: 0.9065Batch 60, Loss Value: 0.9344\n",
      "Batch 60, Gradient Norm: 0.2567\n",
      "\u001b[1m 60/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:20\u001b[0m 467ms/step - accuracy: 0.5095 - binary_io_u_5: 0.5082 - loss: 0.9066Batch 61, Loss Value: 0.9326\n",
      "Batch 61, Gradient Norm: 0.1207\n",
      "\u001b[1m 61/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:19\u001b[0m 467ms/step - accuracy: 0.5094 - binary_io_u_5: 0.5080 - loss: 0.9067Batch 62, Loss Value: 0.9308\n",
      "Batch 62, Gradient Norm: 0.8232\n",
      "\u001b[1m 62/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:19\u001b[0m 467ms/step - accuracy: 0.5092 - binary_io_u_5: 0.5078 - loss: 0.9068Batch 63, Loss Value: 0.9158\n",
      "Batch 63, Gradient Norm: 0.0627\n",
      "\u001b[1m 63/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:18\u001b[0m 467ms/step - accuracy: 0.5091 - binary_io_u_5: 0.5076 - loss: 0.9069Batch 64, Loss Value: 0.9386\n",
      "Batch 64, Gradient Norm: 0.1325\n",
      "\u001b[1m 64/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:18\u001b[0m 467ms/step - accuracy: 0.5090 - binary_io_u_5: 0.5074 - loss: 0.9070Batch 65, Loss Value: 0.9185\n",
      "Batch 65, Gradient Norm: 0.0682\n",
      "\u001b[1m 65/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:18\u001b[0m 467ms/step - accuracy: 0.5088 - binary_io_u_5: 0.5073 - loss: 0.9071Batch 66, Loss Value: 0.9430\n",
      "Batch 66, Gradient Norm: 0.9367\n",
      "\u001b[1m 66/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:17\u001b[0m 467ms/step - accuracy: 0.5087 - binary_io_u_5: 0.5071 - loss: 0.9071Batch 67, Loss Value: 0.9268\n",
      "Batch 67, Gradient Norm: 0.1764\n",
      "\u001b[1m 67/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:17\u001b[0m 467ms/step - accuracy: 0.5086 - binary_io_u_5: 0.5070 - loss: 0.9072Batch 68, Loss Value: 0.9194\n",
      "Batch 68, Gradient Norm: 0.8220\n",
      "\u001b[1m 68/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:16\u001b[0m 467ms/step - accuracy: 0.5085 - binary_io_u_5: 0.5069 - loss: 0.9073Batch 69, Loss Value: 0.9287\n",
      "Batch 69, Gradient Norm: 0.1449\n",
      "\u001b[1m 69/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:16\u001b[0m 467ms/step - accuracy: 0.5084 - binary_io_u_5: 0.5067 - loss: 0.9074Batch 70, Loss Value: 0.9112\n",
      "Batch 70, Gradient Norm: 1.8021\n",
      "\u001b[1m 70/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:15\u001b[0m 467ms/step - accuracy: 0.5083 - binary_io_u_5: 0.5066 - loss: 0.9074Batch 71, Loss Value: 0.9284\n",
      "Batch 71, Gradient Norm: 0.2481\n",
      "\u001b[1m 71/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:15\u001b[0m 467ms/step - accuracy: 0.5083 - binary_io_u_5: 0.5065 - loss: 0.9075Batch 72, Loss Value: 0.9153\n",
      "Batch 72, Gradient Norm: 0.1494\n",
      "\u001b[1m 72/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:14\u001b[0m 467ms/step - accuracy: 0.5082 - binary_io_u_5: 0.5064 - loss: 0.9075Batch 73, Loss Value: 0.9104\n",
      "Batch 73, Gradient Norm: 0.1450\n",
      "\u001b[1m 73/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:14\u001b[0m 467ms/step - accuracy: 0.5081 - binary_io_u_5: 0.5063 - loss: 0.9076Batch 74, Loss Value: 0.9145\n",
      "Batch 74, Gradient Norm: 0.3382\n",
      "\u001b[1m 74/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:13\u001b[0m 468ms/step - accuracy: 0.5081 - binary_io_u_5: 0.5062 - loss: 0.9077Batch 75, Loss Value: 0.9085\n",
      "Batch 75, Gradient Norm: 0.2177\n",
      "\u001b[1m 75/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:13\u001b[0m 468ms/step - accuracy: 0.5080 - binary_io_u_5: 0.5061 - loss: 0.9077Batch 76, Loss Value: 0.9179\n",
      "Batch 76, Gradient Norm: 0.1740\n",
      "\u001b[1m 76/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:12\u001b[0m 468ms/step - accuracy: 0.5080 - binary_io_u_5: 0.5060 - loss: 0.9077Batch 77, Loss Value: 0.9005\n",
      "Batch 77, Gradient Norm: 0.4657\n",
      "\u001b[1m 77/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:12\u001b[0m 468ms/step - accuracy: 0.5079 - binary_io_u_5: 0.5060 - loss: 0.9078Batch 78, Loss Value: 0.9085\n",
      "Batch 78, Gradient Norm: 0.5022\n",
      "\u001b[1m 78/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:12\u001b[0m 468ms/step - accuracy: 0.5078 - binary_io_u_5: 0.5059 - loss: 0.9078Batch 79, Loss Value: 0.9032\n",
      "Batch 79, Gradient Norm: 0.0862\n",
      "\u001b[1m 79/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:11\u001b[0m 468ms/step - accuracy: 0.5078 - binary_io_u_5: 0.5058 - loss: 0.9079Batch 80, Loss Value: 0.9157\n",
      "Batch 80, Gradient Norm: 0.1372\n",
      "\u001b[1m 80/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:11\u001b[0m 468ms/step - accuracy: 0.5077 - binary_io_u_5: 0.5057 - loss: 0.9079Batch 81, Loss Value: 0.9147\n",
      "Batch 81, Gradient Norm: 0.0144\n",
      "\u001b[1m 81/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:10\u001b[0m 468ms/step - accuracy: 0.5076 - binary_io_u_5: 0.5056 - loss: 0.9080Batch 82, Loss Value: 0.9157\n",
      "Batch 82, Gradient Norm: 0.0592\n",
      "\u001b[1m 82/232\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m1:10\u001b[0m 468ms/step - accuracy: 0.5076 - binary_io_u_5: 0.5055 - loss: 0.9080Batch 83, Loss Value: 0.9152\n",
      "Batch 83, Gradient Norm: 0.2635\n",
      "\u001b[1m 83/232\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m1:09\u001b[0m 468ms/step - accuracy: 0.5075 - binary_io_u_5: 0.5054 - loss: 0.9081Batch 84, Loss Value: 0.9066\n",
      "Batch 84, Gradient Norm: 0.2424\n",
      "\u001b[1m 84/232\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m1:09\u001b[0m 468ms/step - accuracy: 0.5075 - binary_io_u_5: 0.5054 - loss: 0.9081Batch 85, Loss Value: 0.9357\n",
      "Batch 85, Gradient Norm: 0.3857\n",
      "\u001b[1m 85/232\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m1:08\u001b[0m 468ms/step - accuracy: 0.5074 - binary_io_u_5: 0.5053 - loss: 0.9081Batch 86, Loss Value: 0.9118\n",
      "Batch 86, Gradient Norm: 0.2353\n",
      "\u001b[1m 86/232\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m1:08\u001b[0m 468ms/step - accuracy: 0.5074 - binary_io_u_5: 0.5053 - loss: 0.9081Batch 87, Loss Value: 0.8994\n",
      "Batch 87, Gradient Norm: 0.1763\n",
      "\u001b[1m 87/232\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m1:07\u001b[0m 469ms/step - accuracy: 0.5073 - binary_io_u_5: 0.5052 - loss: 0.9082Batch 88, Loss Value: 0.9067\n",
      "Batch 88, Gradient Norm: 0.0541\n",
      "\u001b[1m 88/232\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m1:07\u001b[0m 469ms/step - accuracy: 0.5073 - binary_io_u_5: 0.5051 - loss: 0.9082Batch 89, Loss Value: 0.9025\n",
      "Batch 89, Gradient Norm: 0.8512\n",
      "\u001b[1m 89/232\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m1:07\u001b[0m 469ms/step - accuracy: 0.5073 - binary_io_u_5: 0.5051 - loss: 0.9082Batch 90, Loss Value: 0.9116\n",
      "Batch 90, Gradient Norm: 0.2488\n",
      "\u001b[1m 90/232\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m1:06\u001b[0m 469ms/step - accuracy: 0.5072 - binary_io_u_5: 0.5050 - loss: 0.9082Batch 91, Loss Value: 0.9217\n",
      "Batch 91, Gradient Norm: 0.8259\n",
      "\u001b[1m 91/232\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m1:06\u001b[0m 469ms/step - accuracy: 0.5072 - binary_io_u_5: 0.5050 - loss: 0.9082Batch 92, Loss Value: 0.9127\n",
      "Batch 92, Gradient Norm: 0.1357\n",
      "\u001b[1m 92/232\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m1:05\u001b[0m 469ms/step - accuracy: 0.5071 - binary_io_u_5: 0.5049 - loss: 0.9083Batch 93, Loss Value: 0.9219\n",
      "Batch 93, Gradient Norm: 0.4675\n",
      "\u001b[1m 93/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m1:05\u001b[0m 469ms/step - accuracy: 0.5071 - binary_io_u_5: 0.5049 - loss: 0.9083Batch 94, Loss Value: 0.9117\n",
      "Batch 94, Gradient Norm: 0.3473\n",
      "\u001b[1m 94/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m1:04\u001b[0m 469ms/step - accuracy: 0.5070 - binary_io_u_5: 0.5048 - loss: 0.9083Batch 95, Loss Value: 0.9238\n",
      "Batch 95, Gradient Norm: 0.4388\n",
      "\u001b[1m 95/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m1:04\u001b[0m 469ms/step - accuracy: 0.5070 - binary_io_u_5: 0.5048 - loss: 0.9083Batch 96, Loss Value: 0.9242\n",
      "Batch 96, Gradient Norm: 0.3951\n",
      "\u001b[1m 96/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m1:03\u001b[0m 469ms/step - accuracy: 0.5070 - binary_io_u_5: 0.5047 - loss: 0.9084Batch 97, Loss Value: 0.9078\n",
      "Batch 97, Gradient Norm: 0.5256\n",
      "\u001b[1m 97/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m1:03\u001b[0m 469ms/step - accuracy: 0.5069 - binary_io_u_5: 0.5046 - loss: 0.9084Batch 98, Loss Value: 0.9156\n",
      "Batch 98, Gradient Norm: 0.2646\n",
      "\u001b[1m 98/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m1:02\u001b[0m 469ms/step - accuracy: 0.5069 - binary_io_u_5: 0.5046 - loss: 0.9084Batch 99, Loss Value: 0.9130\n",
      "Batch 99, Gradient Norm: 0.0746\n",
      "\u001b[1m 99/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m1:02\u001b[0m 469ms/step - accuracy: 0.5068 - binary_io_u_5: 0.5045 - loss: 0.9084Batch 100, Loss Value: 0.9194\n",
      "Batch 100, Gradient Norm: 0.3708\n",
      "\u001b[1m100/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m1:01\u001b[0m 469ms/step - accuracy: 0.5067 - binary_io_u_5: 0.5044 - loss: 0.9085Batch 101, Loss Value: 0.9102\n",
      "Batch 101, Gradient Norm: 0.2062\n",
      "\u001b[1m101/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m1:01\u001b[0m 469ms/step - accuracy: 0.5067 - binary_io_u_5: 0.5044 - loss: 0.9085Batch 102, Loss Value: 0.9100\n",
      "Batch 102, Gradient Norm: 0.1493\n",
      "\u001b[1m102/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m1:00\u001b[0m 469ms/step - accuracy: 0.5066 - binary_io_u_5: 0.5043 - loss: 0.9085Batch 103, Loss Value: 0.9126\n",
      "Batch 103, Gradient Norm: 0.1742\n",
      "\u001b[1m103/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m1:00\u001b[0m 469ms/step - accuracy: 0.5066 - binary_io_u_5: 0.5042 - loss: 0.9086Batch 104, Loss Value: 0.9423\n",
      "Batch 104, Gradient Norm: 0.0268\n",
      "\u001b[1m104/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m1:00\u001b[0m 469ms/step - accuracy: 0.5065 - binary_io_u_5: 0.5042 - loss: 0.9086Batch 105, Loss Value: 0.9425\n",
      "Batch 105, Gradient Norm: 0.0603\n",
      "\u001b[1m105/232\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m59s\u001b[0m 469ms/step - accuracy: 0.5064 - binary_io_u_5: 0.5041 - loss: 0.9086 Batch 106, Loss Value: 0.9298\n",
      "Batch 106, Gradient Norm: 0.3786\n",
      "\u001b[1m106/232\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m59s\u001b[0m 469ms/step - accuracy: 0.5064 - binary_io_u_5: 0.5040 - loss: 0.9086Batch 107, Loss Value: 0.9413\n",
      "Batch 107, Gradient Norm: 1.1259\n",
      "\u001b[1m107/232\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m58s\u001b[0m 469ms/step - accuracy: 0.5063 - binary_io_u_5: 0.5040 - loss: 0.9087Batch 108, Loss Value: 0.9137\n",
      "Batch 108, Gradient Norm: 0.6080\n",
      "\u001b[1m108/232\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m58s\u001b[0m 469ms/step - accuracy: 0.5063 - binary_io_u_5: 0.5039 - loss: 0.9087Batch 109, Loss Value: 0.9143\n",
      "Batch 109, Gradient Norm: 0.3966\n",
      "\u001b[1m109/232\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m57s\u001b[0m 469ms/step - accuracy: 0.5062 - binary_io_u_5: 0.5039 - loss: 0.9087Batch 110, Loss Value: 0.9429\n",
      "Batch 110, Gradient Norm: 0.0461\n",
      "\u001b[1m110/232\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m57s\u001b[0m 469ms/step - accuracy: 0.5062 - binary_io_u_5: 0.5038 - loss: 0.9087Batch 111, Loss Value: 0.8940\n",
      "Batch 111, Gradient Norm: 0.7742\n",
      "\u001b[1m111/232\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m56s\u001b[0m 469ms/step - accuracy: 0.5061 - binary_io_u_5: 0.5037 - loss: 0.9088Batch 112, Loss Value: 0.9175\n",
      "Batch 112, Gradient Norm: 0.3311\n",
      "\u001b[1m112/232\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m56s\u001b[0m 469ms/step - accuracy: 0.5061 - binary_io_u_5: 0.5037 - loss: 0.9088Batch 113, Loss Value: 0.9121\n",
      "Batch 113, Gradient Norm: 0.1033\n",
      "\u001b[1m113/232\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m55s\u001b[0m 469ms/step - accuracy: 0.5060 - binary_io_u_5: 0.5036 - loss: 0.9088Batch 114, Loss Value: 0.9145\n",
      "Batch 114, Gradient Norm: 0.1242\n",
      "\u001b[1m114/232\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m55s\u001b[0m 469ms/step - accuracy: 0.5059 - binary_io_u_5: 0.5036 - loss: 0.9089Batch 115, Loss Value: 0.9074\n",
      "Batch 115, Gradient Norm: 0.2381\n",
      "\u001b[1m115/232\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m54s\u001b[0m 469ms/step - accuracy: 0.5059 - binary_io_u_5: 0.5035 - loss: 0.9089Batch 116, Loss Value: 0.9161\n",
      "Batch 116, Gradient Norm: 0.3163\n",
      "\u001b[1m116/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m54s\u001b[0m 469ms/step - accuracy: 0.5058 - binary_io_u_5: 0.5034 - loss: 0.9089Batch 117, Loss Value: 0.9136\n",
      "Batch 117, Gradient Norm: 0.0140\n",
      "\u001b[1m117/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m53s\u001b[0m 469ms/step - accuracy: 0.5058 - binary_io_u_5: 0.5034 - loss: 0.9090Batch 118, Loss Value: 0.9110\n",
      "Batch 118, Gradient Norm: 0.1390\n",
      "\u001b[1m118/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m53s\u001b[0m 468ms/step - accuracy: 0.5057 - binary_io_u_5: 0.5033 - loss: 0.9090Batch 119, Loss Value: 0.9259\n",
      "Batch 119, Gradient Norm: 0.1094\n",
      "\u001b[1m119/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m52s\u001b[0m 468ms/step - accuracy: 0.5057 - binary_io_u_5: 0.5033 - loss: 0.9090Batch 120, Loss Value: 0.9206\n",
      "Batch 120, Gradient Norm: 0.0405\n",
      "\u001b[1m120/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m52s\u001b[0m 468ms/step - accuracy: 0.5056 - binary_io_u_5: 0.5032 - loss: 0.9090Batch 121, Loss Value: 0.9075\n",
      "Batch 121, Gradient Norm: 0.0219\n",
      "\u001b[1m121/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m51s\u001b[0m 468ms/step - accuracy: 0.5056 - binary_io_u_5: 0.5032 - loss: 0.9091Batch 122, Loss Value: 0.9135\n",
      "Batch 122, Gradient Norm: 0.0018\n",
      "\u001b[1m122/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m51s\u001b[0m 468ms/step - accuracy: 0.5055 - binary_io_u_5: 0.5031 - loss: 0.9091Batch 123, Loss Value: 0.9201\n",
      "Batch 123, Gradient Norm: 0.1337\n",
      "\u001b[1m123/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m51s\u001b[0m 468ms/step - accuracy: 0.5055 - binary_io_u_5: 0.5031 - loss: 0.9091Batch 124, Loss Value: 0.9150\n",
      "Batch 124, Gradient Norm: 0.0073\n",
      "\u001b[1m124/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m50s\u001b[0m 468ms/step - accuracy: 0.5054 - binary_io_u_5: 0.5030 - loss: 0.9091Batch 125, Loss Value: 0.9078\n",
      "Batch 125, Gradient Norm: 0.0001\n",
      "\u001b[1m125/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m50s\u001b[0m 468ms/step - accuracy: 0.5054 - binary_io_u_5: 0.5030 - loss: 0.9091Batch 126, Loss Value: 0.9078\n",
      "Batch 126, Gradient Norm: 0.0006\n",
      "\u001b[1m126/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m49s\u001b[0m 468ms/step - accuracy: 0.5054 - binary_io_u_5: 0.5030 - loss: 0.9092Batch 127, Loss Value: 0.9149\n",
      "Batch 127, Gradient Norm: 0.0007\n",
      "\u001b[1m127/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m49s\u001b[0m 468ms/step - accuracy: 0.5053 - binary_io_u_5: 0.5029 - loss: 0.9092Batch 128, Loss Value: 0.9206\n",
      "Batch 128, Gradient Norm: 0.0165\n",
      "\u001b[1m128/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m48s\u001b[0m 468ms/step - accuracy: 0.5053 - binary_io_u_5: 0.5029 - loss: 0.9092Batch 129, Loss Value: 0.9135\n",
      "Batch 129, Gradient Norm: 0.0013\n",
      "\u001b[1m129/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m48s\u001b[0m 468ms/step - accuracy: 0.5052 - binary_io_u_5: 0.5028 - loss: 0.9092Batch 130, Loss Value: 0.9195\n",
      "Batch 130, Gradient Norm: 0.6378\n",
      "\u001b[1m130/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m47s\u001b[0m 468ms/step - accuracy: 0.5052 - binary_io_u_5: 0.5028 - loss: 0.9093Batch 131, Loss Value: 0.9149\n",
      "Batch 131, Gradient Norm: 0.0597\n",
      "\u001b[1m131/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m47s\u001b[0m 468ms/step - accuracy: 0.5051 - binary_io_u_5: 0.5027 - loss: 0.9093Batch 132, Loss Value: 0.9159\n",
      "Batch 132, Gradient Norm: 0.1399\n",
      "\u001b[1m132/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m46s\u001b[0m 468ms/step - accuracy: 0.5051 - binary_io_u_5: 0.5027 - loss: 0.9093Batch 133, Loss Value: 0.9083\n",
      "Batch 133, Gradient Norm: 0.1780\n",
      "\u001b[1m133/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m46s\u001b[0m 468ms/step - accuracy: 0.5051 - binary_io_u_5: 0.5027 - loss: 0.9093Batch 134, Loss Value: 0.9269\n",
      "Batch 134, Gradient Norm: 0.2522\n",
      "\u001b[1m134/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m45s\u001b[0m 468ms/step - accuracy: 0.5050 - binary_io_u_5: 0.5026 - loss: 0.9093Batch 135, Loss Value: 0.9206\n",
      "Batch 135, Gradient Norm: 0.0000\n",
      "\u001b[1m135/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m45s\u001b[0m 468ms/step - accuracy: 0.5050 - binary_io_u_5: 0.5026 - loss: 0.9094Batch 136, Loss Value: 0.9011\n",
      "Batch 136, Gradient Norm: 0.9291\n",
      "\u001b[1m136/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m44s\u001b[0m 468ms/step - accuracy: 0.5050 - binary_io_u_5: 0.5026 - loss: 0.9094Batch 137, Loss Value: 0.9277\n",
      "Batch 137, Gradient Norm: 0.0010\n",
      "\u001b[1m137/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m44s\u001b[0m 468ms/step - accuracy: 0.5049 - binary_io_u_5: 0.5025 - loss: 0.9094Batch 138, Loss Value: 0.9206\n",
      "Batch 138, Gradient Norm: 0.0011\n",
      "\u001b[1m138/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m44s\u001b[0m 468ms/step - accuracy: 0.5049 - binary_io_u_5: 0.5025 - loss: 0.9094Batch 139, Loss Value: 0.9021\n",
      "Batch 139, Gradient Norm: 0.0023\n",
      "\u001b[1m139/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m43s\u001b[0m 468ms/step - accuracy: 0.5049 - binary_io_u_5: 0.5025 - loss: 0.9094Batch 140, Loss Value: 0.9149\n",
      "Batch 140, Gradient Norm: 0.0080\n",
      "\u001b[1m140/232\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m43s\u001b[0m 468ms/step - accuracy: 0.5048 - binary_io_u_5: 0.5024 - loss: 0.9095Batch 141, Loss Value: 0.9220\n",
      "Batch 141, Gradient Norm: 0.0005\n",
      "\u001b[1m141/232\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m42s\u001b[0m 468ms/step - accuracy: 0.5048 - binary_io_u_5: 0.5024 - loss: 0.9095Batch 142, Loss Value: 0.9152\n",
      "Batch 142, Gradient Norm: 0.0496\n",
      "\u001b[1m142/232\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m42s\u001b[0m 468ms/step - accuracy: 0.5048 - binary_io_u_5: 0.5023 - loss: 0.9095Batch 143, Loss Value: 0.9265\n",
      "Batch 143, Gradient Norm: 0.5959\n",
      "\u001b[1m143/232\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m41s\u001b[0m 468ms/step - accuracy: 0.5047 - binary_io_u_5: 0.5023 - loss: 0.9095Batch 144, Loss Value: 0.9235\n",
      "Batch 144, Gradient Norm: 0.1998\n",
      "\u001b[1m144/232\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m41s\u001b[0m 468ms/step - accuracy: 0.5047 - binary_io_u_5: 0.5023 - loss: 0.9095Batch 145, Loss Value: 0.9150\n",
      "Batch 145, Gradient Norm: 0.0159\n",
      "\u001b[1m145/232\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m40s\u001b[0m 468ms/step - accuracy: 0.5047 - binary_io_u_5: 0.5022 - loss: 0.9096Batch 146, Loss Value: 0.9124\n",
      "Batch 146, Gradient Norm: 0.1777\n",
      "\u001b[1m146/232\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m40s\u001b[0m 468ms/step - accuracy: 0.5046 - binary_io_u_5: 0.5022 - loss: 0.9096Batch 147, Loss Value: 0.9022\n",
      "Batch 147, Gradient Norm: 0.0192\n",
      "\u001b[1m147/232\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m39s\u001b[0m 469ms/step - accuracy: 0.5046 - binary_io_u_5: 0.5022 - loss: 0.9096Batch 148, Loss Value: 0.9078\n",
      "Batch 148, Gradient Norm: 0.0302\n",
      "\u001b[1m148/232\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m39s\u001b[0m 468ms/step - accuracy: 0.5045 - binary_io_u_5: 0.5021 - loss: 0.9096Batch 149, Loss Value: 0.9348\n",
      "Batch 149, Gradient Norm: 0.0008\n",
      "\u001b[1m149/232\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m38s\u001b[0m 468ms/step - accuracy: 0.5045 - binary_io_u_5: 0.5021 - loss: 0.9096Batch 150, Loss Value: 0.9078\n",
      "Batch 150, Gradient Norm: 0.0001\n",
      "\u001b[1m150/232\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m38s\u001b[0m 468ms/step - accuracy: 0.5045 - binary_io_u_5: 0.5020 - loss: 0.9097Batch 151, Loss Value: 0.9219\n",
      "Batch 151, Gradient Norm: 0.0404\n",
      "\u001b[1m151/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m37s\u001b[0m 468ms/step - accuracy: 0.5044 - binary_io_u_5: 0.5020 - loss: 0.9097Batch 152, Loss Value: 0.9214\n",
      "Batch 152, Gradient Norm: 0.2401\n",
      "\u001b[1m152/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m37s\u001b[0m 468ms/step - accuracy: 0.5044 - binary_io_u_5: 0.5020 - loss: 0.9097Batch 153, Loss Value: 0.9125\n",
      "Batch 153, Gradient Norm: 0.2893\n",
      "\u001b[1m153/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m37s\u001b[0m 468ms/step - accuracy: 0.5044 - binary_io_u_5: 0.5019 - loss: 0.9097Batch 154, Loss Value: 0.9206\n",
      "Batch 154, Gradient Norm: 0.0003\n",
      "\u001b[1m154/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m36s\u001b[0m 468ms/step - accuracy: 0.5043 - binary_io_u_5: 0.5019 - loss: 0.9097Batch 155, Loss Value: 0.9135\n",
      "Batch 155, Gradient Norm: 0.0009\n",
      "\u001b[1m155/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m36s\u001b[0m 468ms/step - accuracy: 0.5043 - binary_io_u_5: 0.5019 - loss: 0.9098Batch 156, Loss Value: 0.9079\n",
      "Batch 156, Gradient Norm: 0.0114\n",
      "\u001b[1m156/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m35s\u001b[0m 468ms/step - accuracy: 0.5043 - binary_io_u_5: 0.5018 - loss: 0.9098Batch 157, Loss Value: 0.9139\n",
      "Batch 157, Gradient Norm: 0.2425\n",
      "\u001b[1m157/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m35s\u001b[0m 468ms/step - accuracy: 0.5042 - binary_io_u_5: 0.5018 - loss: 0.9098Batch 158, Loss Value: 0.9135\n",
      "Batch 158, Gradient Norm: 0.0034\n",
      "\u001b[1m158/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m34s\u001b[0m 468ms/step - accuracy: 0.5042 - binary_io_u_5: 0.5018 - loss: 0.9098Batch 159, Loss Value: 0.9135\n",
      "Batch 159, Gradient Norm: 0.0000\n",
      "\u001b[1m159/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m34s\u001b[0m 468ms/step - accuracy: 0.5042 - binary_io_u_5: 0.5017 - loss: 0.9098Batch 160, Loss Value: 0.9135\n",
      "Batch 160, Gradient Norm: 0.0001\n",
      "\u001b[1m160/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m33s\u001b[0m 469ms/step - accuracy: 0.5041 - binary_io_u_5: 0.5017 - loss: 0.9099Batch 161, Loss Value: 0.9209\n",
      "Batch 161, Gradient Norm: 0.2472\n",
      "\u001b[1m161/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m33s\u001b[0m 469ms/step - accuracy: 0.5041 - binary_io_u_5: 0.5017 - loss: 0.9099Batch 162, Loss Value: 0.9135\n",
      "Batch 162, Gradient Norm: 0.0119\n",
      "\u001b[1m162/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m32s\u001b[0m 469ms/step - accuracy: 0.5040 - binary_io_u_5: 0.5016 - loss: 0.9099Batch 163, Loss Value: 0.9132\n",
      "Batch 163, Gradient Norm: 0.0223\n",
      "\u001b[1m163/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m32s\u001b[0m 469ms/step - accuracy: 0.5040 - binary_io_u_5: 0.5016 - loss: 0.9099Batch 164, Loss Value: 0.9207\n",
      "Batch 164, Gradient Norm: 0.0032\n",
      "\u001b[1m164/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m31s\u001b[0m 469ms/step - accuracy: 0.5040 - binary_io_u_5: 0.5016 - loss: 0.9099Batch 165, Loss Value: 0.9135\n",
      "Batch 165, Gradient Norm: 0.0005\n",
      "\u001b[1m165/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m31s\u001b[0m 469ms/step - accuracy: 0.5039 - binary_io_u_5: 0.5015 - loss: 0.9100Batch 166, Loss Value: 0.9135\n",
      "Batch 166, Gradient Norm: 0.0001\n",
      "\u001b[1m166/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m30s\u001b[0m 469ms/step - accuracy: 0.5039 - binary_io_u_5: 0.5015 - loss: 0.9100Batch 167, Loss Value: 0.9138\n",
      "Batch 167, Gradient Norm: 0.0188\n",
      "\u001b[1m167/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m30s\u001b[0m 469ms/step - accuracy: 0.5039 - binary_io_u_5: 0.5015 - loss: 0.9100Batch 168, Loss Value: 0.9135\n",
      "Batch 168, Gradient Norm: 0.0003\n",
      "\u001b[1m168/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m29s\u001b[0m 469ms/step - accuracy: 0.5039 - binary_io_u_5: 0.5014 - loss: 0.9100Batch 169, Loss Value: 0.9071\n",
      "Batch 169, Gradient Norm: 0.0562\n",
      "\u001b[1m169/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m29s\u001b[0m 469ms/step - accuracy: 0.5038 - binary_io_u_5: 0.5014 - loss: 0.9100Batch 170, Loss Value: 0.9135\n",
      "Batch 170, Gradient Norm: 0.0020\n",
      "\u001b[1m170/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m29s\u001b[0m 469ms/step - accuracy: 0.5038 - binary_io_u_5: 0.5014 - loss: 0.9100Batch 171, Loss Value: 0.9245\n",
      "Batch 171, Gradient Norm: 0.1028\n",
      "\u001b[1m171/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m28s\u001b[0m 469ms/step - accuracy: 0.5038 - binary_io_u_5: 0.5014 - loss: 0.9100Batch 172, Loss Value: 0.9082\n",
      "Batch 172, Gradient Norm: 0.2253\n",
      "\u001b[1m172/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m28s\u001b[0m 469ms/step - accuracy: 0.5037 - binary_io_u_5: 0.5013 - loss: 0.9101Batch 173, Loss Value: 0.9131\n",
      "Batch 173, Gradient Norm: 0.0208\n",
      "\u001b[1m173/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m27s\u001b[0m 469ms/step - accuracy: 0.5037 - binary_io_u_5: 0.5013 - loss: 0.9101Batch 174, Loss Value: 0.9135\n",
      "Batch 174, Gradient Norm: 0.0002\n",
      "\u001b[1m174/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m27s\u001b[0m 469ms/step - accuracy: 0.5037 - binary_io_u_5: 0.5013 - loss: 0.9101Batch 175, Loss Value: 0.9133\n",
      "Batch 175, Gradient Norm: 0.0534\n",
      "\u001b[1m175/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m26s\u001b[0m 469ms/step - accuracy: 0.5036 - binary_io_u_5: 0.5012 - loss: 0.9101Batch 176, Loss Value: 0.9099\n",
      "Batch 176, Gradient Norm: 0.5352\n",
      "\u001b[1m176/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m26s\u001b[0m 469ms/step - accuracy: 0.5036 - binary_io_u_5: 0.5012 - loss: 0.9101Batch 177, Loss Value: 0.9132\n",
      "Batch 177, Gradient Norm: 0.0211\n",
      "\u001b[1m177/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m25s\u001b[0m 469ms/step - accuracy: 0.5036 - binary_io_u_5: 0.5012 - loss: 0.9101Batch 178, Loss Value: 0.9208\n",
      "Batch 178, Gradient Norm: 0.0397\n",
      "\u001b[1m178/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m25s\u001b[0m 469ms/step - accuracy: 0.5036 - binary_io_u_5: 0.5012 - loss: 0.9102Batch 179, Loss Value: 0.9156\n",
      "Batch 179, Gradient Norm: 0.0977\n",
      "\u001b[1m179/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m24s\u001b[0m 469ms/step - accuracy: 0.5035 - binary_io_u_5: 0.5011 - loss: 0.9102Batch 180, Loss Value: 0.9104\n",
      "Batch 180, Gradient Norm: 0.1036\n",
      "\u001b[1m180/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m24s\u001b[0m 469ms/step - accuracy: 0.5035 - binary_io_u_5: 0.5011 - loss: 0.9102Batch 181, Loss Value: 0.9135\n",
      "Batch 181, Gradient Norm: 0.0068\n",
      "\u001b[1m181/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m23s\u001b[0m 469ms/step - accuracy: 0.5035 - binary_io_u_5: 0.5011 - loss: 0.9102Batch 182, Loss Value: 0.9149\n",
      "Batch 182, Gradient Norm: 0.0018\n",
      "\u001b[1m182/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m23s\u001b[0m 469ms/step - accuracy: 0.5034 - binary_io_u_5: 0.5011 - loss: 0.9102Batch 183, Loss Value: 0.9135\n",
      "Batch 183, Gradient Norm: 0.0030\n",
      "\u001b[1m183/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m22s\u001b[0m 469ms/step - accuracy: 0.5034 - binary_io_u_5: 0.5010 - loss: 0.9102Batch 184, Loss Value: 0.9135\n",
      "Batch 184, Gradient Norm: 0.0007\n",
      "\u001b[1m184/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m22s\u001b[0m 469ms/step - accuracy: 0.5034 - binary_io_u_5: 0.5010 - loss: 0.9102Batch 185, Loss Value: 0.9206\n",
      "Batch 185, Gradient Norm: 0.0019\n",
      "\u001b[1m185/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m22s\u001b[0m 469ms/step - accuracy: 0.5034 - binary_io_u_5: 0.5010 - loss: 0.9103Batch 186, Loss Value: 0.9135\n",
      "Batch 186, Gradient Norm: 0.0000\n",
      "\u001b[1m186/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m21s\u001b[0m 469ms/step - accuracy: 0.5033 - binary_io_u_5: 0.5010 - loss: 0.9103Batch 187, Loss Value: 0.9086\n",
      "Batch 187, Gradient Norm: 0.3618\n",
      "\u001b[1m187/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m21s\u001b[0m 469ms/step - accuracy: 0.5033 - binary_io_u_5: 0.5009 - loss: 0.9103Batch 188, Loss Value: 0.9132\n",
      "Batch 188, Gradient Norm: 0.0241\n",
      "\u001b[1m188/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m20s\u001b[0m 469ms/step - accuracy: 0.5033 - binary_io_u_5: 0.5009 - loss: 0.9103Batch 189, Loss Value: 0.9133\n",
      "Batch 189, Gradient Norm: 0.0131\n",
      "\u001b[1m189/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m20s\u001b[0m 469ms/step - accuracy: 0.5032 - binary_io_u_5: 0.5009 - loss: 0.9103Batch 190, Loss Value: 0.9206\n",
      "Batch 190, Gradient Norm: 0.0020\n",
      "\u001b[1m190/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m19s\u001b[0m 469ms/step - accuracy: 0.5032 - binary_io_u_5: 0.5008 - loss: 0.9103Batch 191, Loss Value: 0.9128\n",
      "Batch 191, Gradient Norm: 0.2678\n",
      "\u001b[1m191/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m19s\u001b[0m 469ms/step - accuracy: 0.5032 - binary_io_u_5: 0.5008 - loss: 0.9104Batch 192, Loss Value: 0.9134\n",
      "Batch 192, Gradient Norm: 0.0080\n",
      "\u001b[1m192/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m18s\u001b[0m 469ms/step - accuracy: 0.5031 - binary_io_u_5: 0.5008 - loss: 0.9104Batch 193, Loss Value: 0.9092\n",
      "Batch 193, Gradient Norm: 0.1187\n",
      "\u001b[1m193/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m18s\u001b[0m 469ms/step - accuracy: 0.5031 - binary_io_u_5: 0.5008 - loss: 0.9104Batch 194, Loss Value: 0.9133\n",
      "Batch 194, Gradient Norm: 0.0151\n",
      "\u001b[1m194/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m17s\u001b[0m 469ms/step - accuracy: 0.5031 - binary_io_u_5: 0.5007 - loss: 0.9104Batch 195, Loss Value: 0.9135\n",
      "Batch 195, Gradient Norm: 0.0018\n",
      "\u001b[1m195/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m17s\u001b[0m 469ms/step - accuracy: 0.5031 - binary_io_u_5: 0.5007 - loss: 0.9104Batch 196, Loss Value: 0.9132\n",
      "Batch 196, Gradient Norm: 0.1419\n",
      "\u001b[1m196/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m16s\u001b[0m 469ms/step - accuracy: 0.5030 - binary_io_u_5: 0.5007 - loss: 0.9104Batch 197, Loss Value: 0.9078\n",
      "Batch 197, Gradient Norm: 0.0004\n",
      "\u001b[1m197/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m16s\u001b[0m 469ms/step - accuracy: 0.5030 - binary_io_u_5: 0.5007 - loss: 0.9104Batch 198, Loss Value: 0.9135\n",
      "Batch 198, Gradient Norm: 0.0001\n",
      "\u001b[1m198/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m15s\u001b[0m 469ms/step - accuracy: 0.5030 - binary_io_u_5: 0.5006 - loss: 0.9105Batch 199, Loss Value: 0.9095\n",
      "Batch 199, Gradient Norm: 0.6143\n",
      "\u001b[1m199/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m15s\u001b[0m 469ms/step - accuracy: 0.5030 - binary_io_u_5: 0.5006 - loss: 0.9105Batch 200, Loss Value: 0.9078\n",
      "Batch 200, Gradient Norm: 0.0042\n",
      "\u001b[1m200/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m15s\u001b[0m 469ms/step - accuracy: 0.5029 - binary_io_u_5: 0.5006 - loss: 0.9105Batch 201, Loss Value: 0.9148\n",
      "Batch 201, Gradient Norm: 0.0167\n",
      "\u001b[1m201/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m14s\u001b[0m 469ms/step - accuracy: 0.5029 - binary_io_u_5: 0.5006 - loss: 0.9105Batch 202, Loss Value: 0.9078\n",
      "Batch 202, Gradient Norm: 0.0040\n",
      "\u001b[1m202/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m14s\u001b[0m 469ms/step - accuracy: 0.5029 - binary_io_u_5: 0.5006 - loss: 0.9105Batch 203, Loss Value: 0.9183\n",
      "Batch 203, Gradient Norm: 0.4892\n",
      "\u001b[1m203/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m13s\u001b[0m 469ms/step - accuracy: 0.5029 - binary_io_u_5: 0.5005 - loss: 0.9105Batch 204, Loss Value: 0.9150\n",
      "Batch 204, Gradient Norm: 0.0086\n",
      "\u001b[1m204/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m13s\u001b[0m 469ms/step - accuracy: 0.5028 - binary_io_u_5: 0.5005 - loss: 0.9105Batch 205, Loss Value: 0.9088\n",
      "Batch 205, Gradient Norm: 0.3459\n",
      "\u001b[1m205/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m12s\u001b[0m 469ms/step - accuracy: 0.5028 - binary_io_u_5: 0.5005 - loss: 0.9105Batch 206, Loss Value: 0.9135\n",
      "Batch 206, Gradient Norm: 0.0019\n",
      "\u001b[1m206/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m12s\u001b[0m 469ms/step - accuracy: 0.5028 - binary_io_u_5: 0.5005 - loss: 0.9105Batch 207, Loss Value: 0.9127\n",
      "Batch 207, Gradient Norm: 0.0694\n",
      "\u001b[1m207/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m11s\u001b[0m 469ms/step - accuracy: 0.5028 - binary_io_u_5: 0.5005 - loss: 0.9105Batch 208, Loss Value: 0.9149\n",
      "Batch 208, Gradient Norm: 0.0109\n",
      "\u001b[1m208/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m11s\u001b[0m 469ms/step - accuracy: 0.5028 - binary_io_u_5: 0.5005 - loss: 0.9106Batch 209, Loss Value: 0.9129\n",
      "Batch 209, Gradient Norm: 0.0411\n",
      "\u001b[1m209/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m10s\u001b[0m 469ms/step - accuracy: 0.5028 - binary_io_u_5: 0.5004 - loss: 0.9106Batch 210, Loss Value: 0.9130\n",
      "Batch 210, Gradient Norm: 0.0423\n",
      "\u001b[1m210/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m10s\u001b[0m 469ms/step - accuracy: 0.5027 - binary_io_u_5: 0.5004 - loss: 0.9106Batch 211, Loss Value: 0.9078\n",
      "Batch 211, Gradient Norm: 0.0232\n",
      "\u001b[1m211/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m9s\u001b[0m 469ms/step - accuracy: 0.5027 - binary_io_u_5: 0.5004 - loss: 0.9106 Batch 212, Loss Value: 0.9135\n",
      "Batch 212, Gradient Norm: 0.0012\n",
      "\u001b[1m212/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m9s\u001b[0m 469ms/step - accuracy: 0.5027 - binary_io_u_5: 0.5004 - loss: 0.9106Batch 213, Loss Value: 0.9206\n",
      "Batch 213, Gradient Norm: 0.3033\n",
      "\u001b[1m213/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m8s\u001b[0m 469ms/step - accuracy: 0.5027 - binary_io_u_5: 0.5004 - loss: 0.9106Batch 214, Loss Value: 0.9111\n",
      "Batch 214, Gradient Norm: 0.1435\n",
      "\u001b[1m214/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m8s\u001b[0m 469ms/step - accuracy: 0.5026 - binary_io_u_5: 0.5004 - loss: 0.9106Batch 215, Loss Value: 0.9197\n",
      "Batch 215, Gradient Norm: 0.2959\n",
      "\u001b[1m215/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m7s\u001b[0m 469ms/step - accuracy: 0.5026 - binary_io_u_5: 0.5003 - loss: 0.9106Batch 216, Loss Value: 0.9131\n",
      "Batch 216, Gradient Norm: 0.0249\n",
      "\u001b[1m216/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m7s\u001b[0m 469ms/step - accuracy: 0.5026 - binary_io_u_5: 0.5003 - loss: 0.9106Batch 217, Loss Value: 0.9078\n",
      "Batch 217, Gradient Norm: 0.1217\n",
      "\u001b[1m217/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m7s\u001b[0m 469ms/step - accuracy: 0.5026 - binary_io_u_5: 0.5003 - loss: 0.9106Batch 218, Loss Value: 0.9195\n",
      "Batch 218, Gradient Norm: 0.3381\n",
      "\u001b[1m218/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m6s\u001b[0m 469ms/step - accuracy: 0.5025 - binary_io_u_5: 0.5003 - loss: 0.9107Batch 219, Loss Value: 0.9138\n",
      "Batch 219, Gradient Norm: 0.0399\n",
      "\u001b[1m219/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m6s\u001b[0m 469ms/step - accuracy: 0.5025 - binary_io_u_5: 0.5003 - loss: 0.9107Batch 220, Loss Value: 0.9140\n",
      "Batch 220, Gradient Norm: 0.1091\n",
      "\u001b[1m220/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m5s\u001b[0m 469ms/step - accuracy: 0.5025 - binary_io_u_5: 0.5002 - loss: 0.9107Batch 221, Loss Value: 0.9122\n",
      "Batch 221, Gradient Norm: 0.1286\n",
      "\u001b[1m221/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m5s\u001b[0m 469ms/step - accuracy: 0.5025 - binary_io_u_5: 0.5002 - loss: 0.9107Batch 222, Loss Value: 0.9155\n",
      "Batch 222, Gradient Norm: 0.0615\n",
      "\u001b[1m222/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m4s\u001b[0m 469ms/step - accuracy: 0.5025 - binary_io_u_5: 0.5002 - loss: 0.9107Batch 223, Loss Value: 0.9212\n",
      "Batch 223, Gradient Norm: 0.1640\n",
      "\u001b[1m223/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m4s\u001b[0m 469ms/step - accuracy: 0.5024 - binary_io_u_5: 0.5002 - loss: 0.9107Batch 224, Loss Value: 0.9232\n",
      "Batch 224, Gradient Norm: 0.1744\n",
      "\u001b[1m224/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m3s\u001b[0m 469ms/step - accuracy: 0.5024 - binary_io_u_5: 0.5002 - loss: 0.9107Batch 225, Loss Value: 0.9159\n",
      "Batch 225, Gradient Norm: 0.1998\n",
      "\u001b[1m225/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m3s\u001b[0m 469ms/step - accuracy: 0.5024 - binary_io_u_5: 0.5001 - loss: 0.9107Batch 226, Loss Value: 0.9179\n",
      "Batch 226, Gradient Norm: 0.0743\n",
      "\u001b[1m226/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m2s\u001b[0m 469ms/step - accuracy: 0.5024 - binary_io_u_5: 0.5001 - loss: 0.9107Batch 227, Loss Value: 0.9106\n",
      "Batch 227, Gradient Norm: 0.1292\n",
      "\u001b[1m227/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m2s\u001b[0m 469ms/step - accuracy: 0.5023 - binary_io_u_5: 0.5001 - loss: 0.9108Batch 228, Loss Value: 0.9310\n",
      "Batch 228, Gradient Norm: 0.3522\n",
      "\u001b[1m228/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m1s\u001b[0m 469ms/step - accuracy: 0.5023 - binary_io_u_5: 0.5001 - loss: 0.9108Batch 229, Loss Value: 0.9151\n",
      "Batch 229, Gradient Norm: 0.1003\n",
      "\u001b[1m229/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m1s\u001b[0m 469ms/step - accuracy: 0.5023 - binary_io_u_5: 0.5000 - loss: 0.9108Batch 230, Loss Value: 0.9105\n",
      "Batch 230, Gradient Norm: 0.0570\n",
      "\u001b[1m230/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 469ms/step - accuracy: 0.5022 - binary_io_u_5: 0.5000 - loss: 0.9108Batch 231, Loss Value: 0.9156\n",
      "Batch 231, Gradient Norm: 0.0513\n",
      "\u001b[1m231/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 469ms/step - accuracy: 0.5022 - binary_io_u_5: 0.5000 - loss: 0.9108Batch 232, Loss Value: 0.9119\n",
      "Batch 232, Gradient Norm: 0.1247\n",
      "\u001b[1m232/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m111s\u001b[0m 478ms/step - accuracy: 0.5022 - binary_io_u_5: 0.5000 - loss: 0.9108 - val_accuracy: 0.4949 - val_binary_io_u_5: 0.4949 - val_loss: 0.9071 - learning_rate: 0.0010\n",
      "Epoch 7/200\n",
      "Batch 1, Loss Value: 0.9306\n",
      "Batch 1, Gradient Norm: 0.2202\n",
      "\u001b[1m  1/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:01\u001b[0m 528ms/step - accuracy: 0.6200 - binary_io_u_5: 0.6200 - loss: 0.8316Batch 2, Loss Value: 0.9342\n",
      "Batch 2, Gradient Norm: 0.0043\n",
      "\u001b[1m  2/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:46\u001b[0m 464ms/step - accuracy: 0.5725 - binary_io_u_5: 0.5725 - loss: 0.8601Batch 3, Loss Value: 0.9354\n",
      "Batch 3, Gradient Norm: 0.0602\n",
      "\u001b[1m  3/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:46\u001b[0m 467ms/step - accuracy: 0.5550 - binary_io_u_5: 0.5550 - loss: 0.8717Batch 4, Loss Value: 0.9298\n",
      "Batch 4, Gradient Norm: 0.1477\n",
      "\u001b[1m  4/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:46\u001b[0m 467ms/step - accuracy: 0.5400 - binary_io_u_5: 0.5400 - loss: 0.8807Batch 5, Loss Value: 0.9314\n",
      "Batch 5, Gradient Norm: 0.1042\n",
      "\u001b[1m  5/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:45\u001b[0m 467ms/step - accuracy: 0.5308 - binary_io_u_5: 0.5308 - loss: 0.8865Batch 6, Loss Value: 0.9337\n",
      "Batch 6, Gradient Norm: 0.0368\n",
      "\u001b[1m  6/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:45\u001b[0m 467ms/step - accuracy: 0.5271 - binary_io_u_5: 0.5271 - loss: 0.8889Batch 7, Loss Value: 0.9340\n",
      "Batch 7, Gradient Norm: 0.0123\n",
      "\u001b[1m  7/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:45\u001b[0m 468ms/step - accuracy: 0.5236 - binary_io_u_5: 0.5236 - loss: 0.8913Batch 8, Loss Value: 0.9341\n",
      "Batch 8, Gradient Norm: 0.0194\n",
      "\u001b[1m  8/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:44\u001b[0m 467ms/step - accuracy: 0.5205 - binary_io_u_5: 0.5205 - loss: 0.8934Batch 9, Loss Value: 0.9412\n",
      "Batch 9, Gradient Norm: 0.0133\n",
      "\u001b[1m  9/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:44\u001b[0m 467ms/step - accuracy: 0.5183 - binary_io_u_5: 0.5183 - loss: 0.8950Batch 10, Loss Value: 0.9342\n",
      "Batch 10, Gradient Norm: 0.0010\n",
      "\u001b[1m 10/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:43\u001b[0m 467ms/step - accuracy: 0.5161 - binary_io_u_5: 0.5161 - loss: 0.8966Batch 11, Loss Value: 0.9412\n",
      "Batch 11, Gradient Norm: 0.0126\n",
      "\u001b[1m 11/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:43\u001b[0m 467ms/step - accuracy: 0.5136 - binary_io_u_5: 0.5136 - loss: 0.8982Batch 12, Loss Value: 0.9300\n",
      "Batch 12, Gradient Norm: 0.2006\n",
      "\u001b[1m 12/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:42\u001b[0m 468ms/step - accuracy: 0.5117 - binary_io_u_5: 0.5117 - loss: 0.8996Batch 13, Loss Value: 0.9332\n",
      "Batch 13, Gradient Norm: 0.0709\n",
      "\u001b[1m 13/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:42\u001b[0m 467ms/step - accuracy: 0.5103 - binary_io_u_5: 0.5103 - loss: 0.9005Batch 14, Loss Value: 0.9434\n",
      "Batch 14, Gradient Norm: 0.0055\n",
      "\u001b[1m 14/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:41\u001b[0m 467ms/step - accuracy: 0.5093 - binary_io_u_5: 0.5093 - loss: 0.9013Batch 15, Loss Value: 0.9333\n",
      "Batch 15, Gradient Norm: 0.0671\n",
      "\u001b[1m 15/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:41\u001b[0m 466ms/step - accuracy: 0.5082 - binary_io_u_5: 0.5082 - loss: 0.9021Batch 16, Loss Value: 0.9330\n",
      "Batch 16, Gradient Norm: 0.0799\n",
      "\u001b[1m 16/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:40\u001b[0m 466ms/step - accuracy: 0.5075 - binary_io_u_5: 0.5075 - loss: 0.9026Batch 17, Loss Value: 0.9203\n",
      "Batch 17, Gradient Norm: 0.1534\n",
      "\u001b[1m 17/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:40\u001b[0m 466ms/step - accuracy: 0.5070 - binary_io_u_5: 0.5070 - loss: 0.9031Batch 18, Loss Value: 0.9411\n",
      "Batch 18, Gradient Norm: 0.0228\n",
      "\u001b[1m 18/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:39\u001b[0m 466ms/step - accuracy: 0.5065 - binary_io_u_5: 0.5065 - loss: 0.9035Batch 19, Loss Value: 0.9285\n",
      "Batch 19, Gradient Norm: 0.0005\n",
      "\u001b[1m 19/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:39\u001b[0m 467ms/step - accuracy: 0.5062 - binary_io_u_5: 0.5062 - loss: 0.9038Batch 20, Loss Value: 0.9413\n",
      "Batch 20, Gradient Norm: 0.0037\n",
      "\u001b[1m 20/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:38\u001b[0m 466ms/step - accuracy: 0.5059 - binary_io_u_5: 0.5059 - loss: 0.9041Batch 21, Loss Value: 0.9341\n",
      "Batch 21, Gradient Norm: 0.0105\n",
      "\u001b[1m 21/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:38\u001b[0m 466ms/step - accuracy: 0.5054 - binary_io_u_5: 0.5054 - loss: 0.9044Batch 22, Loss Value: 0.9285\n",
      "Batch 22, Gradient Norm: 0.0018\n",
      "\u001b[1m 22/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:37\u001b[0m 466ms/step - accuracy: 0.5049 - binary_io_u_5: 0.5049 - loss: 0.9048Batch 23, Loss Value: 0.9293\n",
      "Batch 23, Gradient Norm: 0.4893\n",
      "\u001b[1m 23/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:37\u001b[0m 466ms/step - accuracy: 0.5044 - binary_io_u_5: 0.5044 - loss: 0.9053Batch 24, Loss Value: 0.9412\n",
      "Batch 24, Gradient Norm: 0.0350\n",
      "\u001b[1m 24/232\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:36\u001b[0m 465ms/step - accuracy: 0.5041 - binary_io_u_5: 0.5041 - loss: 0.9056Batch 25, Loss Value: 0.9433\n",
      "Batch 25, Gradient Norm: 0.0077\n",
      "\u001b[1m 25/232\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:36\u001b[0m 465ms/step - accuracy: 0.5038 - binary_io_u_5: 0.5038 - loss: 0.9058Batch 26, Loss Value: 0.9285\n",
      "Batch 26, Gradient Norm: 0.0002\n",
      "\u001b[1m 26/232\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:35\u001b[0m 465ms/step - accuracy: 0.5035 - binary_io_u_5: 0.5035 - loss: 0.9061Batch 27, Loss Value: 0.9231\n",
      "Batch 27, Gradient Norm: 0.0589\n",
      "\u001b[1m 27/232\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:35\u001b[0m 465ms/step - accuracy: 0.5033 - binary_io_u_5: 0.5033 - loss: 0.9063Batch 28, Loss Value: 0.9342\n",
      "Batch 28, Gradient Norm: 0.0031\n",
      "\u001b[1m 28/232\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:34\u001b[0m 465ms/step - accuracy: 0.5032 - binary_io_u_5: 0.5032 - loss: 0.9064Batch 29, Loss Value: 0.9297\n",
      "Batch 29, Gradient Norm: 0.3692\n",
      "\u001b[1m 29/232\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:34\u001b[0m 465ms/step - accuracy: 0.5030 - binary_io_u_5: 0.5029 - loss: 0.9066Batch 30, Loss Value: 0.9413\n",
      "Batch 30, Gradient Norm: 0.0010\n",
      "\u001b[1m 30/232\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:33\u001b[0m 465ms/step - accuracy: 0.5027 - binary_io_u_5: 0.5027 - loss: 0.9068Batch 31, Loss Value: 0.9285\n",
      "Batch 31, Gradient Norm: 0.0028\n",
      "\u001b[1m 31/232\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:33\u001b[0m 465ms/step - accuracy: 0.5025 - binary_io_u_5: 0.5024 - loss: 0.9070Batch 32, Loss Value: 0.9413\n",
      "Batch 32, Gradient Norm: 0.0025\n",
      "\u001b[1m 32/232\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:32\u001b[0m 465ms/step - accuracy: 0.5023 - binary_io_u_5: 0.5022 - loss: 0.9071Batch 33, Loss Value: 0.9341\n",
      "Batch 33, Gradient Norm: 0.0126\n",
      "\u001b[1m 33/232\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:32\u001b[0m 465ms/step - accuracy: 0.5020 - binary_io_u_5: 0.5019 - loss: 0.9073Batch 34, Loss Value: 0.9286\n",
      "Batch 34, Gradient Norm: 0.0009\n",
      "\u001b[1m 34/232\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:32\u001b[0m 465ms/step - accuracy: 0.5016 - binary_io_u_5: 0.5016 - loss: 0.9075Batch 35, Loss Value: 0.9357\n",
      "Batch 35, Gradient Norm: 0.5060\n",
      "\u001b[1m 35/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:31\u001b[0m 465ms/step - accuracy: 0.5013 - binary_io_u_5: 0.5012 - loss: 0.9077Batch 36, Loss Value: 0.9285\n",
      "Batch 36, Gradient Norm: 0.0004\n",
      "\u001b[1m 36/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:31\u001b[0m 465ms/step - accuracy: 0.5011 - binary_io_u_5: 0.5010 - loss: 0.9078Batch 37, Loss Value: 0.9433\n",
      "Batch 37, Gradient Norm: 0.0065\n",
      "\u001b[1m 37/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:30\u001b[0m 465ms/step - accuracy: 0.5009 - binary_io_u_5: 0.5008 - loss: 0.9079Batch 38, Loss Value: 0.9428\n",
      "Batch 38, Gradient Norm: 0.0004\n",
      "\u001b[1m 38/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:30\u001b[0m 465ms/step - accuracy: 0.5007 - binary_io_u_5: 0.5006 - loss: 0.9080Batch 39, Loss Value: 0.9428\n",
      "Batch 39, Gradient Norm: 0.0000\n",
      "\u001b[1m 39/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:29\u001b[0m 465ms/step - accuracy: 0.5006 - binary_io_u_5: 0.5004 - loss: 0.9081Batch 40, Loss Value: 0.9172\n",
      "Batch 40, Gradient Norm: 0.0319\n",
      "\u001b[1m 40/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:29\u001b[0m 465ms/step - accuracy: 0.5004 - binary_io_u_5: 0.5003 - loss: 0.9082Batch 41, Loss Value: 0.9250\n",
      "Batch 41, Gradient Norm: 0.3387\n",
      "\u001b[1m 41/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:28\u001b[0m 465ms/step - accuracy: 0.5003 - binary_io_u_5: 0.5002 - loss: 0.9083Batch 42, Loss Value: 0.9354\n",
      "Batch 42, Gradient Norm: 0.0440\n",
      "\u001b[1m 42/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:28\u001b[0m 465ms/step - accuracy: 0.5002 - binary_io_u_5: 0.5001 - loss: 0.9083Batch 43, Loss Value: 0.9435\n",
      "Batch 43, Gradient Norm: 0.0000\n",
      "\u001b[1m 43/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:27\u001b[0m 465ms/step - accuracy: 0.5001 - binary_io_u_5: 0.5000 - loss: 0.9084Batch 44, Loss Value: 0.9300\n",
      "Batch 44, Gradient Norm: 0.0011\n",
      "\u001b[1m 44/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:27\u001b[0m 465ms/step - accuracy: 0.5001 - binary_io_u_5: 0.4999 - loss: 0.9085Batch 45, Loss Value: 0.9356\n",
      "Batch 45, Gradient Norm: 0.0003\n",
      "\u001b[1m 45/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:26\u001b[0m 465ms/step - accuracy: 0.5000 - binary_io_u_5: 0.4999 - loss: 0.9085Batch 46, Loss Value: 0.9372\n",
      "Batch 46, Gradient Norm: 0.1052\n",
      "\u001b[1m 46/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:26\u001b[0m 465ms/step - accuracy: 0.4999 - binary_io_u_5: 0.4998 - loss: 0.9086Batch 47, Loss Value: 0.9287\n",
      "Batch 47, Gradient Norm: 0.2105\n",
      "\u001b[1m 47/232\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:25\u001b[0m 465ms/step - accuracy: 0.4999 - binary_io_u_5: 0.4997 - loss: 0.9086Batch 48, Loss Value: 0.9434\n",
      "Batch 48, Gradient Norm: 0.0185\n",
      "\u001b[1m 48/232\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:25\u001b[0m 465ms/step - accuracy: 0.4998 - binary_io_u_5: 0.4997 - loss: 0.9087Batch 49, Loss Value: 0.9299\n",
      "Batch 49, Gradient Norm: 0.0115\n",
      "\u001b[1m 49/232\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:25\u001b[0m 465ms/step - accuracy: 0.4997 - binary_io_u_5: 0.4996 - loss: 0.9087Batch 50, Loss Value: 0.9342\n",
      "Batch 50, Gradient Norm: 0.7066\n",
      "\u001b[1m 50/232\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:24\u001b[0m 465ms/step - accuracy: 0.4997 - binary_io_u_5: 0.4995 - loss: 0.9088Batch 51, Loss Value: 0.9299\n",
      "Batch 51, Gradient Norm: 0.0046\n",
      "\u001b[1m 51/232\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:24\u001b[0m 465ms/step - accuracy: 0.4996 - binary_io_u_5: 0.4994 - loss: 0.9089Batch 52, Loss Value: 0.9188\n",
      "Batch 52, Gradient Norm: 0.3156\n",
      "\u001b[1m 52/232\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:23\u001b[0m 465ms/step - accuracy: 0.4996 - binary_io_u_5: 0.4994 - loss: 0.9089Batch 53, Loss Value: 0.9433\n",
      "Batch 53, Gradient Norm: 0.0020\n",
      "\u001b[1m 53/232\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:23\u001b[0m 465ms/step - accuracy: 0.4995 - binary_io_u_5: 0.4993 - loss: 0.9090Batch 54, Loss Value: 0.9427\n",
      "Batch 54, Gradient Norm: 0.0051\n",
      "\u001b[1m 54/232\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:22\u001b[0m 465ms/step - accuracy: 0.4995 - binary_io_u_5: 0.4993 - loss: 0.9090Batch 55, Loss Value: 0.9357\n",
      "Batch 55, Gradient Norm: 0.0000\n",
      "\u001b[1m 55/232\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:22\u001b[0m 465ms/step - accuracy: 0.4995 - binary_io_u_5: 0.4993 - loss: 0.9090Batch 56, Loss Value: 0.9356\n",
      "Batch 56, Gradient Norm: 0.0029\n",
      "\u001b[1m 56/232\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:21\u001b[0m 465ms/step - accuracy: 0.4994 - binary_io_u_5: 0.4993 - loss: 0.9091Batch 57, Loss Value: 0.9115\n",
      "Batch 57, Gradient Norm: 0.0001\n",
      "\u001b[1m 57/232\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:21\u001b[0m 465ms/step - accuracy: 0.4994 - binary_io_u_5: 0.4992 - loss: 0.9091Batch 58, Loss Value: 0.9300\n",
      "Batch 58, Gradient Norm: 0.0001\n",
      "\u001b[1m 58/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:20\u001b[0m 465ms/step - accuracy: 0.4994 - binary_io_u_5: 0.4992 - loss: 0.9091Batch 59, Loss Value: 0.9355\n",
      "Batch 59, Gradient Norm: 0.0321\n",
      "\u001b[1m 59/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:20\u001b[0m 465ms/step - accuracy: 0.4994 - binary_io_u_5: 0.4992 - loss: 0.9092Batch 60, Loss Value: 0.9435\n",
      "Batch 60, Gradient Norm: 0.0000\n",
      "\u001b[1m 60/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:20\u001b[0m 466ms/step - accuracy: 0.4993 - binary_io_u_5: 0.4991 - loss: 0.9092Batch 61, Loss Value: 0.9420\n",
      "Batch 61, Gradient Norm: 0.2861\n",
      "\u001b[1m 61/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:19\u001b[0m 466ms/step - accuracy: 0.4993 - binary_io_u_5: 0.4991 - loss: 0.9092Batch 62, Loss Value: 0.9433\n",
      "Batch 62, Gradient Norm: 0.0140\n",
      "\u001b[1m 62/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:19\u001b[0m 466ms/step - accuracy: 0.4993 - binary_io_u_5: 0.4991 - loss: 0.9093Batch 63, Loss Value: 0.9433\n",
      "Batch 63, Gradient Norm: 0.0000\n",
      "\u001b[1m 63/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:18\u001b[0m 466ms/step - accuracy: 0.4993 - binary_io_u_5: 0.4990 - loss: 0.9093Batch 64, Loss Value: 0.9324\n",
      "Batch 64, Gradient Norm: 0.4376\n",
      "\u001b[1m 64/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:18\u001b[0m 466ms/step - accuracy: 0.4993 - binary_io_u_5: 0.4990 - loss: 0.9093Batch 65, Loss Value: 0.9432\n",
      "Batch 65, Gradient Norm: 0.0280\n",
      "\u001b[1m 65/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:17\u001b[0m 467ms/step - accuracy: 0.4992 - binary_io_u_5: 0.4990 - loss: 0.9093Batch 66, Loss Value: 0.9344\n",
      "Batch 66, Gradient Norm: 0.3693\n",
      "\u001b[1m 66/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:17\u001b[0m 467ms/step - accuracy: 0.4992 - binary_io_u_5: 0.4990 - loss: 0.9094Batch 67, Loss Value: 0.9300\n",
      "Batch 67, Gradient Norm: 0.0000\n",
      "\u001b[1m 67/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:17\u001b[0m 467ms/step - accuracy: 0.4992 - binary_io_u_5: 0.4990 - loss: 0.9094Batch 68, Loss Value: 0.9356\n",
      "Batch 68, Gradient Norm: 0.0003\n",
      "\u001b[1m 68/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:16\u001b[0m 468ms/step - accuracy: 0.4992 - binary_io_u_5: 0.4990 - loss: 0.9094Batch 69, Loss Value: 0.9434\n",
      "Batch 69, Gradient Norm: 0.0010\n",
      "\u001b[1m 69/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:16\u001b[0m 468ms/step - accuracy: 0.4992 - binary_io_u_5: 0.4990 - loss: 0.9094Batch 70, Loss Value: 0.9235\n",
      "Batch 70, Gradient Norm: 0.3898\n",
      "\u001b[1m 70/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:15\u001b[0m 468ms/step - accuracy: 0.4992 - binary_io_u_5: 0.4989 - loss: 0.9095Batch 71, Loss Value: 0.9172\n",
      "Batch 71, Gradient Norm: 0.0003\n",
      "\u001b[1m 71/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:15\u001b[0m 468ms/step - accuracy: 0.4992 - binary_io_u_5: 0.4989 - loss: 0.9095Batch 72, Loss Value: 0.9362\n",
      "Batch 72, Gradient Norm: 0.1090\n",
      "\u001b[1m 72/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:14\u001b[0m 468ms/step - accuracy: 0.4991 - binary_io_u_5: 0.4989 - loss: 0.9095Batch 73, Loss Value: 0.9363\n",
      "Batch 73, Gradient Norm: 0.2211\n",
      "\u001b[1m 73/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:14\u001b[0m 468ms/step - accuracy: 0.4991 - binary_io_u_5: 0.4989 - loss: 0.9096Batch 74, Loss Value: 0.9274\n",
      "Batch 74, Gradient Norm: 0.2535\n",
      "\u001b[1m 74/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:14\u001b[0m 468ms/step - accuracy: 0.4991 - binary_io_u_5: 0.4989 - loss: 0.9096Batch 75, Loss Value: 0.9243\n",
      "Batch 75, Gradient Norm: 0.0096\n",
      "\u001b[1m 75/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:13\u001b[0m 468ms/step - accuracy: 0.4990 - binary_io_u_5: 0.4988 - loss: 0.9096Batch 76, Loss Value: 0.9274\n",
      "Batch 76, Gradient Norm: 0.8143\n",
      "\u001b[1m 76/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:13\u001b[0m 469ms/step - accuracy: 0.4990 - binary_io_u_5: 0.4988 - loss: 0.9097Batch 77, Loss Value: 0.9323\n",
      "Batch 77, Gradient Norm: 0.7145\n",
      "\u001b[1m 77/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:12\u001b[0m 469ms/step - accuracy: 0.4990 - binary_io_u_5: 0.4988 - loss: 0.9097Batch 78, Loss Value: 0.9386\n",
      "Batch 78, Gradient Norm: 0.7204\n",
      "\u001b[1m 78/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:12\u001b[0m 469ms/step - accuracy: 0.4989 - binary_io_u_5: 0.4987 - loss: 0.9098Batch 79, Loss Value: 0.9173\n",
      "Batch 79, Gradient Norm: 0.2040\n",
      "\u001b[1m 79/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:11\u001b[0m 469ms/step - accuracy: 0.4989 - binary_io_u_5: 0.4987 - loss: 0.9098Batch 80, Loss Value: 0.9356\n",
      "Batch 80, Gradient Norm: 0.0002\n",
      "\u001b[1m 80/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:11\u001b[0m 469ms/step - accuracy: 0.4988 - binary_io_u_5: 0.4987 - loss: 0.9098Batch 81, Loss Value: 0.9285\n",
      "Batch 81, Gradient Norm: 0.0009\n",
      "\u001b[1m 81/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:10\u001b[0m 469ms/step - accuracy: 0.4988 - binary_io_u_5: 0.4986 - loss: 0.9099Batch 82, Loss Value: 0.9228\n",
      "Batch 82, Gradient Norm: 0.0000\n",
      "\u001b[1m 82/232\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m1:10\u001b[0m 469ms/step - accuracy: 0.4987 - binary_io_u_5: 0.4986 - loss: 0.9099Batch 83, Loss Value: 0.9285\n",
      "Batch 83, Gradient Norm: 0.0025\n",
      "\u001b[1m 83/232\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m1:09\u001b[0m 469ms/step - accuracy: 0.4987 - binary_io_u_5: 0.4986 - loss: 0.9099Batch 84, Loss Value: 0.9414\n",
      "Batch 84, Gradient Norm: 0.0487\n",
      "\u001b[1m 84/232\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m1:09\u001b[0m 469ms/step - accuracy: 0.4987 - binary_io_u_5: 0.4985 - loss: 0.9100Batch 85, Loss Value: 0.9357\n",
      "Batch 85, Gradient Norm: 0.0343\n",
      "\u001b[1m 85/232\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m1:08\u001b[0m 469ms/step - accuracy: 0.4986 - binary_io_u_5: 0.4985 - loss: 0.9100Batch 86, Loss Value: 0.9433\n",
      "Batch 86, Gradient Norm: 0.0002\n",
      "\u001b[1m 86/232\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m1:08\u001b[0m 469ms/step - accuracy: 0.4986 - binary_io_u_5: 0.4985 - loss: 0.9100Batch 87, Loss Value: 0.9347\n",
      "Batch 87, Gradient Norm: 0.1868\n",
      "\u001b[1m 87/232\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m1:07\u001b[0m 469ms/step - accuracy: 0.4986 - binary_io_u_5: 0.4985 - loss: 0.9101Batch 88, Loss Value: 0.9432\n",
      "Batch 88, Gradient Norm: 0.0079\n",
      "\u001b[1m 88/232\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m1:07\u001b[0m 469ms/step - accuracy: 0.4985 - binary_io_u_5: 0.4985 - loss: 0.9101Batch 89, Loss Value: 0.9291\n",
      "Batch 89, Gradient Norm: 0.3915\n",
      "\u001b[1m 89/232\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m1:07\u001b[0m 469ms/step - accuracy: 0.4985 - binary_io_u_5: 0.4985 - loss: 0.9101Batch 90, Loss Value: 0.9285\n",
      "Batch 90, Gradient Norm: 0.0019\n",
      "\u001b[1m 90/232\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m1:06\u001b[0m 469ms/step - accuracy: 0.4985 - binary_io_u_5: 0.4984 - loss: 0.9101Batch 91, Loss Value: 0.9285\n",
      "Batch 91, Gradient Norm: 0.0004\n",
      "\u001b[1m 91/232\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m1:06\u001b[0m 469ms/step - accuracy: 0.4985 - binary_io_u_5: 0.4984 - loss: 0.9102Batch 92, Loss Value: 0.9287\n",
      "Batch 92, Gradient Norm: 0.0434\n",
      "\u001b[1m 92/232\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m1:05\u001b[0m 469ms/step - accuracy: 0.4985 - binary_io_u_5: 0.4984 - loss: 0.9102Batch 93, Loss Value: 0.9285\n",
      "Batch 93, Gradient Norm: 0.0041\n",
      "\u001b[1m 93/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m1:05\u001b[0m 469ms/step - accuracy: 0.4985 - binary_io_u_5: 0.4984 - loss: 0.9102Batch 94, Loss Value: 0.9414\n",
      "Batch 94, Gradient Norm: 0.0103\n",
      "\u001b[1m 94/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m1:04\u001b[0m 469ms/step - accuracy: 0.4985 - binary_io_u_5: 0.4984 - loss: 0.9102Batch 95, Loss Value: 0.9356\n",
      "Batch 95, Gradient Norm: 0.0025\n",
      "\u001b[1m 95/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m1:04\u001b[0m 469ms/step - accuracy: 0.4985 - binary_io_u_5: 0.4984 - loss: 0.9102Batch 96, Loss Value: 0.9228\n",
      "Batch 96, Gradient Norm: 0.0003\n",
      "\u001b[1m 96/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m1:03\u001b[0m 469ms/step - accuracy: 0.4985 - binary_io_u_5: 0.4984 - loss: 0.9103Batch 97, Loss Value: 0.9412\n",
      "Batch 97, Gradient Norm: 0.0068\n",
      "\u001b[1m 97/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m1:03\u001b[0m 469ms/step - accuracy: 0.4985 - binary_io_u_5: 0.4984 - loss: 0.9103Batch 98, Loss Value: 0.9413\n",
      "Batch 98, Gradient Norm: 0.0002\n",
      "\u001b[1m 98/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m1:02\u001b[0m 469ms/step - accuracy: 0.4985 - binary_io_u_5: 0.4984 - loss: 0.9103Batch 99, Loss Value: 0.9284\n",
      "Batch 99, Gradient Norm: 0.0082\n",
      "\u001b[1m 99/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m1:02\u001b[0m 469ms/step - accuracy: 0.4985 - binary_io_u_5: 0.4984 - loss: 0.9103Batch 100, Loss Value: 0.9433\n",
      "Batch 100, Gradient Norm: 0.0127\n",
      "\u001b[1m100/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m1:01\u001b[0m 469ms/step - accuracy: 0.4984 - binary_io_u_5: 0.4984 - loss: 0.9103Batch 101, Loss Value: 0.9333\n",
      "Batch 101, Gradient Norm: 0.1774\n",
      "\u001b[1m101/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m1:01\u001b[0m 469ms/step - accuracy: 0.4984 - binary_io_u_5: 0.4984 - loss: 0.9103Batch 102, Loss Value: 0.9342\n",
      "Batch 102, Gradient Norm: 0.0000\n",
      "\u001b[1m102/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m1:01\u001b[0m 469ms/step - accuracy: 0.4984 - binary_io_u_5: 0.4984 - loss: 0.9103Batch 103, Loss Value: 0.9356\n",
      "Batch 103, Gradient Norm: 0.0014\n",
      "\u001b[1m103/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m1:00\u001b[0m 469ms/step - accuracy: 0.4984 - binary_io_u_5: 0.4984 - loss: 0.9103Batch 104, Loss Value: 0.9342\n",
      "Batch 104, Gradient Norm: 0.0032\n",
      "\u001b[1m104/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m1:00\u001b[0m 469ms/step - accuracy: 0.4984 - binary_io_u_5: 0.4984 - loss: 0.9104Batch 105, Loss Value: 0.9413\n",
      "Batch 105, Gradient Norm: 0.0003\n",
      "\u001b[1m105/232\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m59s\u001b[0m 469ms/step - accuracy: 0.4984 - binary_io_u_5: 0.4984 - loss: 0.9104 Batch 106, Loss Value: 0.9229\n",
      "Batch 106, Gradient Norm: 0.0321\n",
      "\u001b[1m106/232\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m59s\u001b[0m 469ms/step - accuracy: 0.4984 - binary_io_u_5: 0.4984 - loss: 0.9104Batch 107, Loss Value: 0.9335\n",
      "Batch 107, Gradient Norm: 0.2678\n",
      "\u001b[1m107/232\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m58s\u001b[0m 470ms/step - accuracy: 0.4984 - binary_io_u_5: 0.4984 - loss: 0.9104Batch 108, Loss Value: 0.9296\n",
      "Batch 108, Gradient Norm: 0.1394\n",
      "\u001b[1m108/232\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m58s\u001b[0m 470ms/step - accuracy: 0.4984 - binary_io_u_5: 0.4983 - loss: 0.9104Batch 109, Loss Value: 0.9285\n",
      "Batch 109, Gradient Norm: 0.0054\n",
      "\u001b[1m109/232\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m57s\u001b[0m 470ms/step - accuracy: 0.4984 - binary_io_u_5: 0.4983 - loss: 0.9104Batch 110, Loss Value: 0.9285\n",
      "Batch 110, Gradient Norm: 0.0029\n",
      "\u001b[1m110/232\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m57s\u001b[0m 470ms/step - accuracy: 0.4984 - binary_io_u_5: 0.4983 - loss: 0.9104Batch 111, Loss Value: 0.9413\n",
      "Batch 111, Gradient Norm: 0.0171\n",
      "\u001b[1m111/232\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m56s\u001b[0m 470ms/step - accuracy: 0.4984 - binary_io_u_5: 0.4983 - loss: 0.9104Batch 112, Loss Value: 0.9342\n",
      "Batch 112, Gradient Norm: 0.0005\n",
      "\u001b[1m112/232\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m56s\u001b[0m 470ms/step - accuracy: 0.4984 - binary_io_u_5: 0.4983 - loss: 0.9105Batch 113, Loss Value: 0.9341\n",
      "Batch 113, Gradient Norm: 0.0129\n",
      "\u001b[1m113/232\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m55s\u001b[0m 470ms/step - accuracy: 0.4984 - binary_io_u_5: 0.4983 - loss: 0.9105Batch 114, Loss Value: 0.9376\n",
      "Batch 114, Gradient Norm: 0.3955\n",
      "\u001b[1m114/232\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m55s\u001b[0m 470ms/step - accuracy: 0.4984 - binary_io_u_5: 0.4983 - loss: 0.9105Batch 115, Loss Value: 0.9413\n",
      "Batch 115, Gradient Norm: 0.0016\n",
      "\u001b[1m115/232\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m54s\u001b[0m 470ms/step - accuracy: 0.4984 - binary_io_u_5: 0.4983 - loss: 0.9105Batch 116, Loss Value: 0.9433\n",
      "Batch 116, Gradient Norm: 0.0001\n",
      "\u001b[1m116/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m54s\u001b[0m 470ms/step - accuracy: 0.4984 - binary_io_u_5: 0.4983 - loss: 0.9105Batch 117, Loss Value: 0.9229\n",
      "Batch 117, Gradient Norm: 0.0111\n",
      "\u001b[1m117/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m54s\u001b[0m 470ms/step - accuracy: 0.4984 - binary_io_u_5: 0.4983 - loss: 0.9105Batch 118, Loss Value: 0.9228\n",
      "Batch 118, Gradient Norm: 0.0002\n",
      "\u001b[1m118/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m53s\u001b[0m 470ms/step - accuracy: 0.4984 - binary_io_u_5: 0.4983 - loss: 0.9105Batch 119, Loss Value: 0.9301\n",
      "Batch 119, Gradient Norm: 0.6968\n",
      "\u001b[1m119/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m53s\u001b[0m 470ms/step - accuracy: 0.4983 - binary_io_u_5: 0.4983 - loss: 0.9105Batch 120, Loss Value: 0.9285\n",
      "Batch 120, Gradient Norm: 0.0057\n",
      "\u001b[1m120/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m52s\u001b[0m 470ms/step - accuracy: 0.4983 - binary_io_u_5: 0.4983 - loss: 0.9106Batch 121, Loss Value: 0.9410\n",
      "Batch 121, Gradient Norm: 0.0251\n",
      "\u001b[1m121/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m52s\u001b[0m 470ms/step - accuracy: 0.4983 - binary_io_u_5: 0.4983 - loss: 0.9106Batch 122, Loss Value: 0.9433\n",
      "Batch 122, Gradient Norm: 0.0000\n",
      "\u001b[1m122/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m51s\u001b[0m 470ms/step - accuracy: 0.4983 - binary_io_u_5: 0.4982 - loss: 0.9106Batch 123, Loss Value: 0.9413\n",
      "Batch 123, Gradient Norm: 0.0005\n",
      "\u001b[1m123/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m51s\u001b[0m 471ms/step - accuracy: 0.4983 - binary_io_u_5: 0.4982 - loss: 0.9106Batch 124, Loss Value: 0.9228\n",
      "Batch 124, Gradient Norm: 0.0001\n",
      "\u001b[1m124/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m50s\u001b[0m 471ms/step - accuracy: 0.4983 - binary_io_u_5: 0.4982 - loss: 0.9106Batch 125, Loss Value: 0.9413\n",
      "Batch 125, Gradient Norm: 0.0423\n",
      "\u001b[1m125/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m50s\u001b[0m 471ms/step - accuracy: 0.4983 - binary_io_u_5: 0.4982 - loss: 0.9106Batch 126, Loss Value: 0.9356\n",
      "Batch 126, Gradient Norm: 0.0032\n",
      "\u001b[1m126/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m49s\u001b[0m 471ms/step - accuracy: 0.4983 - binary_io_u_5: 0.4982 - loss: 0.9106Batch 127, Loss Value: 0.9226\n",
      "Batch 127, Gradient Norm: 0.0190\n",
      "\u001b[1m127/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m49s\u001b[0m 471ms/step - accuracy: 0.4983 - binary_io_u_5: 0.4982 - loss: 0.9106Batch 128, Loss Value: 0.9228\n",
      "Batch 128, Gradient Norm: 0.0062\n",
      "\u001b[1m128/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m48s\u001b[0m 471ms/step - accuracy: 0.4983 - binary_io_u_5: 0.4982 - loss: 0.9107Batch 129, Loss Value: 0.9393\n",
      "Batch 129, Gradient Norm: 0.6323\n",
      "\u001b[1m129/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m48s\u001b[0m 471ms/step - accuracy: 0.4983 - binary_io_u_5: 0.4982 - loss: 0.9107Batch 130, Loss Value: 0.9286\n",
      "Batch 130, Gradient Norm: 0.0406\n",
      "\u001b[1m130/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m48s\u001b[0m 471ms/step - accuracy: 0.4982 - binary_io_u_5: 0.4981 - loss: 0.9107Batch 131, Loss Value: 0.9357\n",
      "Batch 131, Gradient Norm: 0.0003\n",
      "\u001b[1m131/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m47s\u001b[0m 471ms/step - accuracy: 0.4982 - binary_io_u_5: 0.4981 - loss: 0.9107Batch 132, Loss Value: 0.9357\n",
      "Batch 132, Gradient Norm: 0.0001\n",
      "\u001b[1m132/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m47s\u001b[0m 471ms/step - accuracy: 0.4982 - binary_io_u_5: 0.4981 - loss: 0.9107Batch 133, Loss Value: 0.9285\n",
      "Batch 133, Gradient Norm: 0.0001\n",
      "\u001b[1m133/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m46s\u001b[0m 471ms/step - accuracy: 0.4982 - binary_io_u_5: 0.4981 - loss: 0.9107Batch 134, Loss Value: 0.9357\n",
      "Batch 134, Gradient Norm: 0.0001\n",
      "\u001b[1m134/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m46s\u001b[0m 470ms/step - accuracy: 0.4982 - binary_io_u_5: 0.4981 - loss: 0.9108Batch 135, Loss Value: 0.9357\n",
      "Batch 135, Gradient Norm: 0.0014\n",
      "\u001b[1m135/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m45s\u001b[0m 470ms/step - accuracy: 0.4982 - binary_io_u_5: 0.4980 - loss: 0.9108Batch 136, Loss Value: 0.9285\n",
      "Batch 136, Gradient Norm: 0.0005\n",
      "\u001b[1m136/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m45s\u001b[0m 470ms/step - accuracy: 0.4981 - binary_io_u_5: 0.4980 - loss: 0.9108Batch 137, Loss Value: 0.9413\n",
      "Batch 137, Gradient Norm: 0.0003\n",
      "\u001b[1m137/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m44s\u001b[0m 470ms/step - accuracy: 0.4981 - binary_io_u_5: 0.4980 - loss: 0.9108Batch 138, Loss Value: 0.9244\n",
      "Batch 138, Gradient Norm: 0.8408\n",
      "\u001b[1m138/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m44s\u001b[0m 470ms/step - accuracy: 0.4981 - binary_io_u_5: 0.4980 - loss: 0.9108Batch 139, Loss Value: 0.9228\n",
      "Batch 139, Gradient Norm: 0.0037\n",
      "\u001b[1m139/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m43s\u001b[0m 470ms/step - accuracy: 0.4981 - binary_io_u_5: 0.4979 - loss: 0.9109Batch 140, Loss Value: 0.9338\n",
      "Batch 140, Gradient Norm: 0.5765\n",
      "\u001b[1m140/232\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m43s\u001b[0m 470ms/step - accuracy: 0.4980 - binary_io_u_5: 0.4979 - loss: 0.9109Batch 141, Loss Value: 0.9371\n",
      "Batch 141, Gradient Norm: 0.0018\n",
      "\u001b[1m141/232\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m42s\u001b[0m 470ms/step - accuracy: 0.4980 - binary_io_u_5: 0.4979 - loss: 0.9109Batch 142, Loss Value: 0.9413\n",
      "Batch 142, Gradient Norm: 0.0000\n",
      "\u001b[1m142/232\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m42s\u001b[0m 470ms/step - accuracy: 0.4980 - binary_io_u_5: 0.4978 - loss: 0.9109Batch 143, Loss Value: 0.9413\n",
      "Batch 143, Gradient Norm: 0.0011\n",
      "\u001b[1m143/232\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m41s\u001b[0m 470ms/step - accuracy: 0.4980 - binary_io_u_5: 0.4978 - loss: 0.9109Batch 144, Loss Value: 0.9299\n",
      "Batch 144, Gradient Norm: 0.0394\n",
      "\u001b[1m144/232\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m41s\u001b[0m 470ms/step - accuracy: 0.4979 - binary_io_u_5: 0.4978 - loss: 0.9110Batch 145, Loss Value: 0.9357\n",
      "Batch 145, Gradient Norm: 0.0002\n",
      "\u001b[1m145/232\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m40s\u001b[0m 470ms/step - accuracy: 0.4979 - binary_io_u_5: 0.4977 - loss: 0.9110Batch 146, Loss Value: 0.9431\n",
      "Batch 146, Gradient Norm: 0.0428\n",
      "\u001b[1m146/232\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m40s\u001b[0m 470ms/step - accuracy: 0.4979 - binary_io_u_5: 0.4977 - loss: 0.9110Batch 147, Loss Value: 0.9271\n",
      "Batch 147, Gradient Norm: 0.3077\n",
      "\u001b[1m147/232\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m39s\u001b[0m 470ms/step - accuracy: 0.4978 - binary_io_u_5: 0.4977 - loss: 0.9110Batch 148, Loss Value: 0.9436\n",
      "Batch 148, Gradient Norm: 0.0000\n",
      "\u001b[1m148/232\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m39s\u001b[0m 470ms/step - accuracy: 0.4978 - binary_io_u_5: 0.4977 - loss: 0.9110Batch 149, Loss Value: 0.9428\n",
      "Batch 149, Gradient Norm: 0.0000\n",
      "\u001b[1m149/232\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m39s\u001b[0m 470ms/step - accuracy: 0.4978 - binary_io_u_5: 0.4976 - loss: 0.9110Batch 150, Loss Value: 0.9369\n",
      "Batch 150, Gradient Norm: 0.0331\n",
      "\u001b[1m150/232\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m38s\u001b[0m 470ms/step - accuracy: 0.4978 - binary_io_u_5: 0.4976 - loss: 0.9111Batch 151, Loss Value: 0.9228\n",
      "Batch 151, Gradient Norm: 0.0000\n",
      "\u001b[1m151/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m38s\u001b[0m 470ms/step - accuracy: 0.4978 - binary_io_u_5: 0.4976 - loss: 0.9111Batch 152, Loss Value: 0.9433\n",
      "Batch 152, Gradient Norm: 0.0000\n",
      "\u001b[1m152/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m37s\u001b[0m 470ms/step - accuracy: 0.4977 - binary_io_u_5: 0.4975 - loss: 0.9111Batch 153, Loss Value: 0.9302\n",
      "Batch 153, Gradient Norm: 0.1087\n",
      "\u001b[1m153/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m37s\u001b[0m 470ms/step - accuracy: 0.4977 - binary_io_u_5: 0.4975 - loss: 0.9111Batch 154, Loss Value: 0.9428\n",
      "Batch 154, Gradient Norm: 0.0000\n",
      "\u001b[1m154/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m36s\u001b[0m 470ms/step - accuracy: 0.4977 - binary_io_u_5: 0.4975 - loss: 0.9111Batch 155, Loss Value: 0.9192\n",
      "Batch 155, Gradient Norm: 0.2501\n",
      "\u001b[1m155/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m36s\u001b[0m 470ms/step - accuracy: 0.4977 - binary_io_u_5: 0.4975 - loss: 0.9111Batch 156, Loss Value: 0.9173\n",
      "Batch 156, Gradient Norm: 0.0358\n",
      "\u001b[1m156/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m35s\u001b[0m 470ms/step - accuracy: 0.4976 - binary_io_u_5: 0.4974 - loss: 0.9112Batch 157, Loss Value: 0.9432\n",
      "Batch 157, Gradient Norm: 0.3010\n",
      "\u001b[1m157/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m35s\u001b[0m 470ms/step - accuracy: 0.4976 - binary_io_u_5: 0.4974 - loss: 0.9112Batch 158, Loss Value: 0.9172\n",
      "Batch 158, Gradient Norm: 0.0000\n",
      "\u001b[1m158/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m34s\u001b[0m 470ms/step - accuracy: 0.4976 - binary_io_u_5: 0.4974 - loss: 0.9112Batch 159, Loss Value: 0.9174\n",
      "Batch 159, Gradient Norm: 0.0829\n",
      "\u001b[1m159/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m34s\u001b[0m 470ms/step - accuracy: 0.4976 - binary_io_u_5: 0.4974 - loss: 0.9112Batch 160, Loss Value: 0.9428\n",
      "Batch 160, Gradient Norm: 0.2816\n",
      "\u001b[1m160/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m33s\u001b[0m 470ms/step - accuracy: 0.4975 - binary_io_u_5: 0.4973 - loss: 0.9112Batch 161, Loss Value: 0.9439\n",
      "Batch 161, Gradient Norm: 0.0258\n",
      "\u001b[1m161/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m33s\u001b[0m 470ms/step - accuracy: 0.4975 - binary_io_u_5: 0.4973 - loss: 0.9112Batch 162, Loss Value: 0.9129\n",
      "Batch 162, Gradient Norm: 0.0011\n",
      "\u001b[1m162/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m32s\u001b[0m 470ms/step - accuracy: 0.4975 - binary_io_u_5: 0.4973 - loss: 0.9113Batch 163, Loss Value: 0.9371\n",
      "Batch 163, Gradient Norm: 0.0000\n",
      "\u001b[1m163/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m32s\u001b[0m 470ms/step - accuracy: 0.4975 - binary_io_u_5: 0.4972 - loss: 0.9113Batch 164, Loss Value: 0.9371\n",
      "Batch 164, Gradient Norm: 0.0024\n",
      "\u001b[1m164/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m31s\u001b[0m 470ms/step - accuracy: 0.4974 - binary_io_u_5: 0.4972 - loss: 0.9113Batch 165, Loss Value: 0.9300\n",
      "Batch 165, Gradient Norm: 0.2852\n",
      "\u001b[1m165/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m31s\u001b[0m 470ms/step - accuracy: 0.4974 - binary_io_u_5: 0.4972 - loss: 0.9113Batch 166, Loss Value: 0.9183\n",
      "Batch 166, Gradient Norm: 0.0330\n",
      "\u001b[1m166/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m30s\u001b[0m 470ms/step - accuracy: 0.4974 - binary_io_u_5: 0.4972 - loss: 0.9113Batch 167, Loss Value: 0.9386\n",
      "Batch 167, Gradient Norm: 0.4567\n",
      "\u001b[1m167/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m30s\u001b[0m 470ms/step - accuracy: 0.4974 - binary_io_u_5: 0.4971 - loss: 0.9113Batch 168, Loss Value: 0.9435\n",
      "Batch 168, Gradient Norm: 0.0066\n",
      "\u001b[1m168/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m30s\u001b[0m 469ms/step - accuracy: 0.4974 - binary_io_u_5: 0.4971 - loss: 0.9114Batch 169, Loss Value: 0.9256\n",
      "Batch 169, Gradient Norm: 0.0137\n",
      "\u001b[1m169/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m29s\u001b[0m 469ms/step - accuracy: 0.4974 - binary_io_u_5: 0.4971 - loss: 0.9114Batch 170, Loss Value: 0.9257\n",
      "Batch 170, Gradient Norm: 0.0000\n",
      "\u001b[1m170/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m29s\u001b[0m 469ms/step - accuracy: 0.4973 - binary_io_u_5: 0.4971 - loss: 0.9114Batch 171, Loss Value: 0.9439\n",
      "Batch 171, Gradient Norm: 0.0396\n",
      "\u001b[1m171/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m28s\u001b[0m 469ms/step - accuracy: 0.4973 - binary_io_u_5: 0.4971 - loss: 0.9114Batch 172, Loss Value: 0.9435\n",
      "Batch 172, Gradient Norm: 0.0132\n",
      "\u001b[1m172/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m28s\u001b[0m 469ms/step - accuracy: 0.4973 - binary_io_u_5: 0.4970 - loss: 0.9114Batch 173, Loss Value: 0.9437\n",
      "Batch 173, Gradient Norm: 0.0009\n",
      "\u001b[1m173/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m27s\u001b[0m 469ms/step - accuracy: 0.4973 - binary_io_u_5: 0.4970 - loss: 0.9114Batch 174, Loss Value: 0.9433\n",
      "Batch 174, Gradient Norm: 0.0058\n",
      "\u001b[1m174/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m27s\u001b[0m 469ms/step - accuracy: 0.4973 - binary_io_u_5: 0.4970 - loss: 0.9114Batch 175, Loss Value: 0.9283\n",
      "Batch 175, Gradient Norm: 0.9321\n",
      "\u001b[1m175/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m26s\u001b[0m 469ms/step - accuracy: 0.4973 - binary_io_u_5: 0.4970 - loss: 0.9115Batch 176, Loss Value: 0.9254\n",
      "Batch 176, Gradient Norm: 0.0715\n",
      "\u001b[1m176/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m26s\u001b[0m 469ms/step - accuracy: 0.4972 - binary_io_u_5: 0.4970 - loss: 0.9115Batch 177, Loss Value: 0.9386\n",
      "Batch 177, Gradient Norm: 0.0454\n",
      "\u001b[1m177/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m25s\u001b[0m 469ms/step - accuracy: 0.4972 - binary_io_u_5: 0.4970 - loss: 0.9115Batch 178, Loss Value: 0.9062\n",
      "Batch 178, Gradient Norm: 0.1080\n",
      "\u001b[1m178/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m25s\u001b[0m 469ms/step - accuracy: 0.4972 - binary_io_u_5: 0.4969 - loss: 0.9115Batch 179, Loss Value: 0.9292\n",
      "Batch 179, Gradient Norm: 0.4907\n",
      "\u001b[1m179/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m24s\u001b[0m 469ms/step - accuracy: 0.4972 - binary_io_u_5: 0.4969 - loss: 0.9115Batch 180, Loss Value: 0.9380\n",
      "Batch 180, Gradient Norm: 0.3624\n",
      "\u001b[1m180/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m24s\u001b[0m 469ms/step - accuracy: 0.4972 - binary_io_u_5: 0.4969 - loss: 0.9115Batch 181, Loss Value: 0.9254\n",
      "Batch 181, Gradient Norm: 0.3853\n",
      "\u001b[1m181/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m23s\u001b[0m 469ms/step - accuracy: 0.4972 - binary_io_u_5: 0.4969 - loss: 0.9115Batch 182, Loss Value: 0.9436\n",
      "Batch 182, Gradient Norm: 0.0000\n",
      "\u001b[1m182/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m23s\u001b[0m 469ms/step - accuracy: 0.4972 - binary_io_u_5: 0.4969 - loss: 0.9115Batch 183, Loss Value: 0.9317\n",
      "Batch 183, Gradient Norm: 0.5329\n",
      "\u001b[1m183/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m22s\u001b[0m 469ms/step - accuracy: 0.4971 - binary_io_u_5: 0.4969 - loss: 0.9115Batch 184, Loss Value: 0.9365\n",
      "Batch 184, Gradient Norm: 17.4168\n",
      "\u001b[1m184/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m22s\u001b[0m 469ms/step - accuracy: 0.4971 - binary_io_u_5: 0.4969 - loss: 0.9116Batch 185, Loss Value: 0.9385\n",
      "Batch 185, Gradient Norm: 0.0001\n",
      "\u001b[1m185/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m22s\u001b[0m 469ms/step - accuracy: 0.4971 - binary_io_u_5: 0.4969 - loss: 0.9116Batch 186, Loss Value: 0.9437\n",
      "Batch 186, Gradient Norm: 0.0205\n",
      "\u001b[1m186/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m21s\u001b[0m 469ms/step - accuracy: 0.4971 - binary_io_u_5: 0.4968 - loss: 0.9116Batch 187, Loss Value: 0.9314\n",
      "Batch 187, Gradient Norm: 0.0000\n",
      "\u001b[1m187/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m21s\u001b[0m 469ms/step - accuracy: 0.4971 - binary_io_u_5: 0.4968 - loss: 0.9116Batch 188, Loss Value: 0.9435\n",
      "Batch 188, Gradient Norm: 0.0002\n",
      "\u001b[1m188/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m20s\u001b[0m 470ms/step - accuracy: 0.4971 - binary_io_u_5: 0.4968 - loss: 0.9116Batch 189, Loss Value: 0.9371\n",
      "Batch 189, Gradient Norm: 0.0019\n",
      "\u001b[1m189/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m20s\u001b[0m 470ms/step - accuracy: 0.4971 - binary_io_u_5: 0.4968 - loss: 0.9116Batch 190, Loss Value: 0.9186\n",
      "Batch 190, Gradient Norm: 0.0156\n",
      "\u001b[1m190/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m19s\u001b[0m 470ms/step - accuracy: 0.4971 - binary_io_u_5: 0.4968 - loss: 0.9116Batch 191, Loss Value: 0.9424\n",
      "Batch 191, Gradient Norm: 0.6782\n",
      "\u001b[1m191/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m19s\u001b[0m 470ms/step - accuracy: 0.4971 - binary_io_u_5: 0.4968 - loss: 0.9116Batch 192, Loss Value: 0.9438\n",
      "Batch 192, Gradient Norm: 0.0074\n",
      "\u001b[1m192/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m18s\u001b[0m 470ms/step - accuracy: 0.4971 - binary_io_u_5: 0.4968 - loss: 0.9116Batch 193, Loss Value: 0.9314\n",
      "Batch 193, Gradient Norm: 0.0004\n",
      "\u001b[1m193/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m18s\u001b[0m 470ms/step - accuracy: 0.4971 - binary_io_u_5: 0.4968 - loss: 0.9116Batch 194, Loss Value: 0.9437\n",
      "Batch 194, Gradient Norm: 0.0000\n",
      "\u001b[1m194/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m17s\u001b[0m 470ms/step - accuracy: 0.4971 - binary_io_u_5: 0.4968 - loss: 0.9116Batch 195, Loss Value: 0.9430\n",
      "Batch 195, Gradient Norm: 0.2659\n",
      "\u001b[1m195/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m17s\u001b[0m 470ms/step - accuracy: 0.4970 - binary_io_u_5: 0.4968 - loss: 0.9117Batch 196, Loss Value: 0.9185\n",
      "Batch 196, Gradient Norm: 0.0325\n",
      "\u001b[1m196/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m16s\u001b[0m 470ms/step - accuracy: 0.4970 - binary_io_u_5: 0.4967 - loss: 0.9117Batch 197, Loss Value: 0.9310\n",
      "Batch 197, Gradient Norm: 0.4525\n",
      "\u001b[1m197/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m16s\u001b[0m 470ms/step - accuracy: 0.4970 - binary_io_u_5: 0.4967 - loss: 0.9117Batch 198, Loss Value: 0.9356\n",
      "Batch 198, Gradient Norm: 0.9064\n",
      "\u001b[1m198/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m15s\u001b[0m 470ms/step - accuracy: 0.4970 - binary_io_u_5: 0.4967 - loss: 0.9117Batch 199, Loss Value: 0.9436\n",
      "Batch 199, Gradient Norm: 0.0004\n",
      "\u001b[1m199/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m15s\u001b[0m 470ms/step - accuracy: 0.4970 - binary_io_u_5: 0.4967 - loss: 0.9117Batch 200, Loss Value: 0.9440\n",
      "Batch 200, Gradient Norm: 0.0829\n",
      "\u001b[1m200/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m15s\u001b[0m 470ms/step - accuracy: 0.4970 - binary_io_u_5: 0.4967 - loss: 0.9117Batch 201, Loss Value: 0.9371\n",
      "Batch 201, Gradient Norm: 0.7860\n",
      "\u001b[1m201/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m14s\u001b[0m 470ms/step - accuracy: 0.4970 - binary_io_u_5: 0.4967 - loss: 0.9117Batch 202, Loss Value: 0.9413\n",
      "Batch 202, Gradient Norm: 0.8115\n",
      "\u001b[1m202/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m14s\u001b[0m 470ms/step - accuracy: 0.4970 - binary_io_u_5: 0.4967 - loss: 0.9117Batch 203, Loss Value: 0.9371\n",
      "Batch 203, Gradient Norm: 0.0000\n",
      "\u001b[1m203/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m13s\u001b[0m 470ms/step - accuracy: 0.4970 - binary_io_u_5: 0.4967 - loss: 0.9117Batch 204, Loss Value: 0.9399\n",
      "Batch 204, Gradient Norm: 0.2420\n",
      "\u001b[1m204/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m13s\u001b[0m 470ms/step - accuracy: 0.4970 - binary_io_u_5: 0.4967 - loss: 0.9118Batch 205, Loss Value: 0.9375\n",
      "Batch 205, Gradient Norm: 0.2472\n",
      "\u001b[1m205/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m12s\u001b[0m 470ms/step - accuracy: 0.4970 - binary_io_u_5: 0.4967 - loss: 0.9118Batch 206, Loss Value: 0.9434\n",
      "Batch 206, Gradient Norm: 0.2278\n",
      "\u001b[1m206/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m12s\u001b[0m 470ms/step - accuracy: 0.4969 - binary_io_u_5: 0.4967 - loss: 0.9118Batch 207, Loss Value: 0.9371\n",
      "Batch 207, Gradient Norm: 0.0012\n",
      "\u001b[1m207/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m11s\u001b[0m 470ms/step - accuracy: 0.4969 - binary_io_u_5: 0.4966 - loss: 0.9118Batch 208, Loss Value: 0.9443\n",
      "Batch 208, Gradient Norm: 0.0228\n",
      "\u001b[1m208/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m11s\u001b[0m 470ms/step - accuracy: 0.4969 - binary_io_u_5: 0.4966 - loss: 0.9118Batch 209, Loss Value: 0.9171\n",
      "Batch 209, Gradient Norm: 0.5219\n",
      "\u001b[1m209/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m10s\u001b[0m 470ms/step - accuracy: 0.4969 - binary_io_u_5: 0.4966 - loss: 0.9118Batch 210, Loss Value: 0.9328\n",
      "Batch 210, Gradient Norm: 0.0001\n",
      "\u001b[1m210/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m10s\u001b[0m 470ms/step - accuracy: 0.4969 - binary_io_u_5: 0.4966 - loss: 0.9118Batch 211, Loss Value: 0.9441\n",
      "Batch 211, Gradient Norm: 0.0143\n",
      "\u001b[1m211/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m9s\u001b[0m 470ms/step - accuracy: 0.4969 - binary_io_u_5: 0.4966 - loss: 0.9118 Batch 212, Loss Value: 0.9297\n",
      "Batch 212, Gradient Norm: 0.8817\n",
      "\u001b[1m212/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m9s\u001b[0m 470ms/step - accuracy: 0.4969 - binary_io_u_5: 0.4966 - loss: 0.9118Batch 213, Loss Value: 0.9438\n",
      "Batch 213, Gradient Norm: 0.0000\n",
      "\u001b[1m213/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m8s\u001b[0m 470ms/step - accuracy: 0.4969 - binary_io_u_5: 0.4966 - loss: 0.9118Batch 214, Loss Value: 0.9293\n",
      "Batch 214, Gradient Norm: 0.0895\n",
      "\u001b[1m214/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m8s\u001b[0m 470ms/step - accuracy: 0.4969 - binary_io_u_5: 0.4966 - loss: 0.9119Batch 215, Loss Value: 0.9277\n",
      "Batch 215, Gradient Norm: 0.2547\n",
      "\u001b[1m215/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m7s\u001b[0m 470ms/step - accuracy: 0.4968 - binary_io_u_5: 0.4965 - loss: 0.9119Batch 216, Loss Value: 0.9400\n",
      "Batch 216, Gradient Norm: 0.0186\n",
      "\u001b[1m216/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m7s\u001b[0m 470ms/step - accuracy: 0.4968 - binary_io_u_5: 0.4965 - loss: 0.9119Batch 217, Loss Value: 0.9414\n",
      "Batch 217, Gradient Norm: 0.0085\n",
      "\u001b[1m217/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m7s\u001b[0m 470ms/step - accuracy: 0.4968 - binary_io_u_5: 0.4965 - loss: 0.9119Batch 218, Loss Value: 0.9373\n",
      "Batch 218, Gradient Norm: 0.1909\n",
      "\u001b[1m218/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m6s\u001b[0m 470ms/step - accuracy: 0.4968 - binary_io_u_5: 0.4965 - loss: 0.9119Batch 219, Loss Value: 0.9441\n",
      "Batch 219, Gradient Norm: 0.0201\n",
      "\u001b[1m219/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m6s\u001b[0m 470ms/step - accuracy: 0.4968 - binary_io_u_5: 0.4965 - loss: 0.9119Batch 220, Loss Value: 0.9371\n",
      "Batch 220, Gradient Norm: 0.1145\n",
      "\u001b[1m220/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m5s\u001b[0m 470ms/step - accuracy: 0.4968 - binary_io_u_5: 0.4965 - loss: 0.9119Batch 221, Loss Value: 0.9438\n",
      "Batch 221, Gradient Norm: 0.0009\n",
      "\u001b[1m221/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m5s\u001b[0m 470ms/step - accuracy: 0.4968 - binary_io_u_5: 0.4965 - loss: 0.9119Batch 222, Loss Value: 0.9089\n",
      "Batch 222, Gradient Norm: 0.0511\n",
      "\u001b[1m222/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m4s\u001b[0m 471ms/step - accuracy: 0.4968 - binary_io_u_5: 0.4965 - loss: 0.9119Batch 223, Loss Value: 0.9127\n",
      "Batch 223, Gradient Norm: 0.0262\n",
      "\u001b[1m223/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m4s\u001b[0m 471ms/step - accuracy: 0.4968 - binary_io_u_5: 0.4965 - loss: 0.9120Batch 224, Loss Value: 0.9437\n",
      "Batch 224, Gradient Norm: 0.0002\n",
      "\u001b[1m224/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m3s\u001b[0m 471ms/step - accuracy: 0.4968 - binary_io_u_5: 0.4965 - loss: 0.9120Batch 225, Loss Value: 0.9143\n",
      "Batch 225, Gradient Norm: 0.0027\n",
      "\u001b[1m225/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m3s\u001b[0m 471ms/step - accuracy: 0.4968 - binary_io_u_5: 0.4965 - loss: 0.9120Batch 226, Loss Value: 0.9273\n",
      "Batch 226, Gradient Norm: 0.0349\n",
      "\u001b[1m226/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m2s\u001b[0m 471ms/step - accuracy: 0.4968 - binary_io_u_5: 0.4964 - loss: 0.9120Batch 227, Loss Value: 0.9439\n",
      "Batch 227, Gradient Norm: 0.0038\n",
      "\u001b[1m227/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m2s\u001b[0m 471ms/step - accuracy: 0.4967 - binary_io_u_5: 0.4964 - loss: 0.9120Batch 228, Loss Value: 0.9436\n",
      "Batch 228, Gradient Norm: 0.0003\n",
      "\u001b[1m228/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m1s\u001b[0m 471ms/step - accuracy: 0.4967 - binary_io_u_5: 0.4964 - loss: 0.9120Batch 229, Loss Value: 0.9394\n",
      "Batch 229, Gradient Norm: 0.0566\n",
      "\u001b[1m229/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m1s\u001b[0m 471ms/step - accuracy: 0.4967 - binary_io_u_5: 0.4964 - loss: 0.9120Batch 230, Loss Value: 0.9418\n",
      "Batch 230, Gradient Norm: 0.6465\n",
      "\u001b[1m230/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 471ms/step - accuracy: 0.4967 - binary_io_u_5: 0.4964 - loss: 0.9120Batch 231, Loss Value: 0.9443\n",
      "Batch 231, Gradient Norm: 0.0119\n",
      "\u001b[1m231/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 471ms/step - accuracy: 0.4967 - binary_io_u_5: 0.4964 - loss: 0.9120Batch 232, Loss Value: 0.9089\n",
      "Batch 232, Gradient Norm: 0.2987\n",
      "\u001b[1m232/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m111s\u001b[0m 476ms/step - accuracy: 0.4967 - binary_io_u_5: 0.4964 - loss: 0.9121 - val_accuracy: 0.5021 - val_binary_io_u_5: 0.4949 - val_loss: 0.9137 - learning_rate: 0.0010\n",
      "Epoch 8/200\n",
      "Batch 1, Loss Value: 0.8457\n",
      "Batch 1, Gradient Norm: 0.4371\n",
      "\u001b[1m  1/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:56\u001b[0m 506ms/step - accuracy: 0.5100 - binary_io_u_5: 0.4800 - loss: 0.9072Batch 2, Loss Value: 0.8785\n",
      "Batch 2, Gradient Norm: 0.1797\n",
      "\u001b[1m  2/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:51\u001b[0m 483ms/step - accuracy: 0.5175 - binary_io_u_5: 0.4900 - loss: 0.9026Batch 3, Loss Value: 0.8874\n",
      "Batch 3, Gradient Norm: 0.0118\n",
      "\u001b[1m  3/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:48\u001b[0m 475ms/step - accuracy: 0.5083 - binary_io_u_5: 0.4867 - loss: 0.9064Batch 4, Loss Value: 0.8973\n",
      "Batch 4, Gradient Norm: 0.0114\n",
      "\u001b[1m  4/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:47\u001b[0m 471ms/step - accuracy: 0.5044 - binary_io_u_5: 0.4850 - loss: 0.9080Batch 5, Loss Value: 0.8991\n",
      "Batch 5, Gradient Norm: 0.9763\n",
      "\u001b[1m  5/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:46\u001b[0m 469ms/step - accuracy: 0.5055 - binary_io_u_5: 0.4868 - loss: 0.9073Batch 6, Loss Value: 0.8770\n",
      "Batch 6, Gradient Norm: 2.5641\n",
      "\u001b[1m  6/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:45\u001b[0m 467ms/step - accuracy: 0.5046 - binary_io_u_5: 0.4876 - loss: 0.9080Batch 7, Loss Value: 0.8920\n",
      "Batch 7, Gradient Norm: 0.3336\n",
      "\u001b[1m  7/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:44\u001b[0m 467ms/step - accuracy: 0.5031 - binary_io_u_5: 0.4873 - loss: 0.9091Batch 8, Loss Value: 0.9327\n",
      "Batch 8, Gradient Norm: 0.3425\n",
      "\u001b[1m  8/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:44\u001b[0m 465ms/step - accuracy: 0.5012 - binary_io_u_5: 0.4860 - loss: 0.9104Batch 9, Loss Value: 0.9314\n",
      "Batch 9, Gradient Norm: 0.0000\n",
      "\u001b[1m  9/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:43\u001b[0m 465ms/step - accuracy: 0.4999 - binary_io_u_5: 0.4852 - loss: 0.9112Batch 10, Loss Value: 0.8733\n",
      "Batch 10, Gradient Norm: 0.0226\n",
      "\u001b[1m 10/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:43\u001b[0m 465ms/step - accuracy: 0.4986 - binary_io_u_5: 0.4847 - loss: 0.9122Batch 11, Loss Value: 0.8706\n",
      "Batch 11, Gradient Norm: 0.3176\n",
      "\u001b[1m 11/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:42\u001b[0m 465ms/step - accuracy: 0.4978 - binary_io_u_5: 0.4842 - loss: 0.9128Batch 12, Loss Value: 0.9101\n",
      "Batch 12, Gradient Norm: 0.0083\n",
      "\u001b[1m 12/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:42\u001b[0m 465ms/step - accuracy: 0.4966 - binary_io_u_5: 0.4837 - loss: 0.9135Batch 13, Loss Value: 0.9102\n",
      "Batch 13, Gradient Norm: 0.0662\n",
      "\u001b[1m 13/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:41\u001b[0m 465ms/step - accuracy: 0.4956 - binary_io_u_5: 0.4833 - loss: 0.9142Batch 14, Loss Value: 0.9247\n",
      "Batch 14, Gradient Norm: 0.3823\n",
      "\u001b[1m 14/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:41\u001b[0m 465ms/step - accuracy: 0.4948 - binary_io_u_5: 0.4831 - loss: 0.9147Batch 15, Loss Value: 0.9097\n",
      "Batch 15, Gradient Norm: 0.0907\n",
      "\u001b[1m 15/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:40\u001b[0m 465ms/step - accuracy: 0.4940 - binary_io_u_5: 0.4828 - loss: 0.9153Batch 16, Loss Value: 0.8775\n",
      "Batch 16, Gradient Norm: 0.0012\n",
      "\u001b[1m 16/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:40\u001b[0m 465ms/step - accuracy: 0.4935 - binary_io_u_5: 0.4826 - loss: 0.9157Batch 17, Loss Value: 0.8661\n",
      "Batch 17, Gradient Norm: 0.0001\n",
      "\u001b[1m 17/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:40\u001b[0m 465ms/step - accuracy: 0.4932 - binary_io_u_5: 0.4827 - loss: 0.9160Batch 18, Loss Value: 0.9260\n",
      "Batch 18, Gradient Norm: 0.1886\n",
      "\u001b[1m 18/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:39\u001b[0m 466ms/step - accuracy: 0.4931 - binary_io_u_5: 0.4829 - loss: 0.9162Batch 19, Loss Value: 0.8783\n",
      "Batch 19, Gradient Norm: 0.2766\n",
      "\u001b[1m 19/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:39\u001b[0m 465ms/step - accuracy: 0.4932 - binary_io_u_5: 0.4833 - loss: 0.9162Batch 20, Loss Value: 0.8860\n",
      "Batch 20, Gradient Norm: 0.0001\n",
      "\u001b[1m 20/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:38\u001b[0m 465ms/step - accuracy: 0.4934 - binary_io_u_5: 0.4837 - loss: 0.9161Batch 21, Loss Value: 0.9090\n",
      "Batch 21, Gradient Norm: 0.0527\n",
      "\u001b[1m 21/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:38\u001b[0m 465ms/step - accuracy: 0.4936 - binary_io_u_5: 0.4840 - loss: 0.9161Batch 22, Loss Value: 0.8786\n",
      "Batch 22, Gradient Norm: 0.1111\n",
      "\u001b[1m 22/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:37\u001b[0m 465ms/step - accuracy: 0.4937 - binary_io_u_5: 0.4843 - loss: 0.9161Batch 23, Loss Value: 0.9096\n",
      "Batch 23, Gradient Norm: 0.0709\n",
      "\u001b[1m 23/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:37\u001b[0m 465ms/step - accuracy: 0.4938 - binary_io_u_5: 0.4845 - loss: 0.9161Batch 24, Loss Value: 0.8832\n",
      "Batch 24, Gradient Norm: 0.0833\n",
      "\u001b[1m 24/232\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:36\u001b[0m 465ms/step - accuracy: 0.4939 - binary_io_u_5: 0.4847 - loss: 0.9162Batch 25, Loss Value: 0.8903\n",
      "Batch 25, Gradient Norm: 0.0002\n",
      "\u001b[1m 25/232\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:36\u001b[0m 466ms/step - accuracy: 0.4939 - binary_io_u_5: 0.4849 - loss: 0.9162Batch 26, Loss Value: 0.9135\n",
      "Batch 26, Gradient Norm: 0.3370\n",
      "\u001b[1m 26/232\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:35\u001b[0m 465ms/step - accuracy: 0.4941 - binary_io_u_5: 0.4852 - loss: 0.9163Batch 27, Loss Value: 0.8967\n",
      "Batch 27, Gradient Norm: 0.0566\n",
      "\u001b[1m 27/232\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:35\u001b[0m 466ms/step - accuracy: 0.4942 - binary_io_u_5: 0.4855 - loss: 0.9162Batch 28, Loss Value: 0.9159\n",
      "Batch 28, Gradient Norm: 0.0253\n",
      "\u001b[1m 28/232\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:34\u001b[0m 466ms/step - accuracy: 0.4944 - binary_io_u_5: 0.4858 - loss: 0.9162Batch 29, Loss Value: 0.9058\n",
      "Batch 29, Gradient Norm: 0.7331\n",
      "\u001b[1m 29/232\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:34\u001b[0m 466ms/step - accuracy: 0.4945 - binary_io_u_5: 0.4860 - loss: 0.9163Batch 30, Loss Value: 0.9018\n",
      "Batch 30, Gradient Norm: 0.4283\n",
      "\u001b[1m 30/232\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:34\u001b[0m 467ms/step - accuracy: 0.4947 - binary_io_u_5: 0.4862 - loss: 0.9163Batch 31, Loss Value: 0.8761\n",
      "Batch 31, Gradient Norm: 0.0001\n",
      "\u001b[1m 31/232\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:33\u001b[0m 467ms/step - accuracy: 0.4947 - binary_io_u_5: 0.4863 - loss: 0.9163Batch 32, Loss Value: 0.8832\n",
      "Batch 32, Gradient Norm: 0.0000\n",
      "\u001b[1m 32/232\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:33\u001b[0m 467ms/step - accuracy: 0.4948 - binary_io_u_5: 0.4864 - loss: 0.9163Batch 33, Loss Value: 0.8945\n",
      "Batch 33, Gradient Norm: 0.0172\n",
      "\u001b[1m 33/232\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:32\u001b[0m 467ms/step - accuracy: 0.4948 - binary_io_u_5: 0.4865 - loss: 0.9164Batch 34, Loss Value: 0.8846\n",
      "Batch 34, Gradient Norm: 1.8880\n",
      "\u001b[1m 34/232\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:32\u001b[0m 468ms/step - accuracy: 0.4948 - binary_io_u_5: 0.4866 - loss: 0.9164Batch 35, Loss Value: 0.8709\n",
      "Batch 35, Gradient Norm: 0.6139\n",
      "\u001b[1m 35/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:32\u001b[0m 468ms/step - accuracy: 0.4948 - binary_io_u_5: 0.4866 - loss: 0.9165Batch 36, Loss Value: 0.9040\n",
      "Batch 36, Gradient Norm: 0.8594\n",
      "\u001b[1m 36/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:31\u001b[0m 469ms/step - accuracy: 0.4947 - binary_io_u_5: 0.4867 - loss: 0.9166Batch 37, Loss Value: 0.8959\n",
      "Batch 37, Gradient Norm: 0.0002\n",
      "\u001b[1m 37/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:31\u001b[0m 469ms/step - accuracy: 0.4947 - binary_io_u_5: 0.4868 - loss: 0.9166Batch 38, Loss Value: 0.8804\n",
      "Batch 38, Gradient Norm: 0.5850\n",
      "\u001b[1m 38/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:30\u001b[0m 469ms/step - accuracy: 0.4947 - binary_io_u_5: 0.4869 - loss: 0.9166Batch 39, Loss Value: 0.8909\n",
      "Batch 39, Gradient Norm: 0.7235\n",
      "\u001b[1m 39/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:30\u001b[0m 469ms/step - accuracy: 0.4947 - binary_io_u_5: 0.4870 - loss: 0.9167Batch 40, Loss Value: 0.8959\n",
      "Batch 40, Gradient Norm: 0.0000\n",
      "\u001b[1m 40/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:30\u001b[0m 469ms/step - accuracy: 0.4947 - binary_io_u_5: 0.4871 - loss: 0.9167Batch 41, Loss Value: 0.9030\n",
      "Batch 41, Gradient Norm: 0.0000\n",
      "\u001b[1m 41/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:29\u001b[0m 470ms/step - accuracy: 0.4948 - binary_io_u_5: 0.4872 - loss: 0.9168Batch 42, Loss Value: 0.8733\n",
      "Batch 42, Gradient Norm: 1.1051\n",
      "\u001b[1m 42/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:29\u001b[0m 470ms/step - accuracy: 0.4947 - binary_io_u_5: 0.4873 - loss: 0.9168Batch 43, Loss Value: 0.8794\n",
      "Batch 43, Gradient Norm: 1.2227\n",
      "\u001b[1m 43/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:28\u001b[0m 470ms/step - accuracy: 0.4948 - binary_io_u_5: 0.4874 - loss: 0.9168Batch 44, Loss Value: 0.8904\n",
      "Batch 44, Gradient Norm: 0.0304\n",
      "\u001b[1m 44/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:28\u001b[0m 470ms/step - accuracy: 0.4948 - binary_io_u_5: 0.4875 - loss: 0.9168Batch 45, Loss Value: 0.8917\n",
      "Batch 45, Gradient Norm: 0.5529\n",
      "\u001b[1m 45/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:27\u001b[0m 470ms/step - accuracy: 0.4948 - binary_io_u_5: 0.4876 - loss: 0.9169Batch 46, Loss Value: 0.8773\n",
      "Batch 46, Gradient Norm: 0.1130\n",
      "\u001b[1m 46/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:27\u001b[0m 470ms/step - accuracy: 0.4949 - binary_io_u_5: 0.4877 - loss: 0.9168Batch 47, Loss Value: 0.8775\n",
      "Batch 47, Gradient Norm: 0.0000\n",
      "\u001b[1m 47/232\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:26\u001b[0m 470ms/step - accuracy: 0.4950 - binary_io_u_5: 0.4879 - loss: 0.9168Batch 48, Loss Value: 0.9058\n",
      "Batch 48, Gradient Norm: 0.7140\n",
      "\u001b[1m 48/232\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:26\u001b[0m 470ms/step - accuracy: 0.4950 - binary_io_u_5: 0.4880 - loss: 0.9168Batch 49, Loss Value: 0.8959\n",
      "Batch 49, Gradient Norm: 0.0129\n",
      "\u001b[1m 49/232\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:26\u001b[0m 470ms/step - accuracy: 0.4951 - binary_io_u_5: 0.4882 - loss: 0.9167Batch 50, Loss Value: 0.9158\n",
      "Batch 50, Gradient Norm: 0.0000\n",
      "\u001b[1m 50/232\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:25\u001b[0m 470ms/step - accuracy: 0.4952 - binary_io_u_5: 0.4883 - loss: 0.9167Batch 51, Loss Value: 0.9096\n",
      "Batch 51, Gradient Norm: 0.3270\n",
      "\u001b[1m 51/232\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:25\u001b[0m 470ms/step - accuracy: 0.4953 - binary_io_u_5: 0.4884 - loss: 0.9167Batch 52, Loss Value: 0.8903\n",
      "Batch 52, Gradient Norm: 0.0000\n",
      "\u001b[1m 52/232\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:24\u001b[0m 471ms/step - accuracy: 0.4954 - binary_io_u_5: 0.4885 - loss: 0.9166Batch 53, Loss Value: 0.8832\n",
      "Batch 53, Gradient Norm: 0.0015\n",
      "\u001b[1m 53/232\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:24\u001b[0m 471ms/step - accuracy: 0.4956 - binary_io_u_5: 0.4887 - loss: 0.9166Batch 54, Loss Value: 0.9096\n",
      "Batch 54, Gradient Norm: 0.1324\n",
      "\u001b[1m 54/232\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:23\u001b[0m 471ms/step - accuracy: 0.4957 - binary_io_u_5: 0.4888 - loss: 0.9166Batch 55, Loss Value: 0.8906\n",
      "Batch 55, Gradient Norm: 0.1588\n",
      "\u001b[1m 55/232\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:23\u001b[0m 471ms/step - accuracy: 0.4957 - binary_io_u_5: 0.4889 - loss: 0.9165Batch 56, Loss Value: 0.8986\n",
      "Batch 56, Gradient Norm: 0.0219\n",
      "\u001b[1m 56/232\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:22\u001b[0m 471ms/step - accuracy: 0.4958 - binary_io_u_5: 0.4890 - loss: 0.9165Batch 57, Loss Value: 0.8847\n",
      "Batch 57, Gradient Norm: 0.0189\n",
      "\u001b[1m 57/232\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:22\u001b[0m 472ms/step - accuracy: 0.4959 - binary_io_u_5: 0.4890 - loss: 0.9165Batch 58, Loss Value: 0.8918\n",
      "Batch 58, Gradient Norm: 0.0295\n",
      "\u001b[1m 58/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:22\u001b[0m 472ms/step - accuracy: 0.4959 - binary_io_u_5: 0.4891 - loss: 0.9165Batch 59, Loss Value: 0.8860\n",
      "Batch 59, Gradient Norm: 0.0013\n",
      "\u001b[1m 59/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:21\u001b[0m 472ms/step - accuracy: 0.4960 - binary_io_u_5: 0.4892 - loss: 0.9165Batch 60, Loss Value: 0.9115\n",
      "Batch 60, Gradient Norm: 0.7970\n",
      "\u001b[1m 60/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:21\u001b[0m 472ms/step - accuracy: 0.4960 - binary_io_u_5: 0.4892 - loss: 0.9165Batch 61, Loss Value: 0.8846\n",
      "Batch 61, Gradient Norm: 0.0001\n",
      "\u001b[1m 61/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:20\u001b[0m 472ms/step - accuracy: 0.4960 - binary_io_u_5: 0.4893 - loss: 0.9165Batch 62, Loss Value: 0.9030\n",
      "Batch 62, Gradient Norm: 0.2815\n",
      "\u001b[1m 62/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:20\u001b[0m 472ms/step - accuracy: 0.4960 - binary_io_u_5: 0.4893 - loss: 0.9164Batch 63, Loss Value: 0.8903\n",
      "Batch 63, Gradient Norm: 0.0257\n",
      "\u001b[1m 63/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:19\u001b[0m 472ms/step - accuracy: 0.4960 - binary_io_u_5: 0.4893 - loss: 0.9165Batch 64, Loss Value: 0.8978\n",
      "Batch 64, Gradient Norm: 0.8603\n",
      "\u001b[1m 64/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:19\u001b[0m 472ms/step - accuracy: 0.4961 - binary_io_u_5: 0.4894 - loss: 0.9165Batch 65, Loss Value: 0.9374\n",
      "Batch 65, Gradient Norm: 0.0668\n",
      "\u001b[1m 65/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:18\u001b[0m 472ms/step - accuracy: 0.4961 - binary_io_u_5: 0.4894 - loss: 0.9164Batch 66, Loss Value: 0.9029\n",
      "Batch 66, Gradient Norm: 0.0493\n",
      "\u001b[1m 66/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:18\u001b[0m 473ms/step - accuracy: 0.4961 - binary_io_u_5: 0.4895 - loss: 0.9164Batch 67, Loss Value: 0.8775\n",
      "Batch 67, Gradient Norm: 0.2937\n",
      "\u001b[1m 67/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:18\u001b[0m 473ms/step - accuracy: 0.4961 - binary_io_u_5: 0.4895 - loss: 0.9164Batch 68, Loss Value: 0.9044\n",
      "Batch 68, Gradient Norm: 0.0000\n",
      "\u001b[1m 68/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:17\u001b[0m 473ms/step - accuracy: 0.4962 - binary_io_u_5: 0.4896 - loss: 0.9164Batch 69, Loss Value: 0.8973\n",
      "Batch 69, Gradient Norm: 0.0008\n",
      "\u001b[1m 69/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:17\u001b[0m 473ms/step - accuracy: 0.4962 - binary_io_u_5: 0.4896 - loss: 0.9164Batch 70, Loss Value: 0.9101\n",
      "Batch 70, Gradient Norm: 0.0001\n",
      "\u001b[1m 70/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:16\u001b[0m 473ms/step - accuracy: 0.4962 - binary_io_u_5: 0.4897 - loss: 0.9164Batch 71, Loss Value: 0.8856\n",
      "Batch 71, Gradient Norm: 0.2938\n",
      "\u001b[1m 71/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:16\u001b[0m 473ms/step - accuracy: 0.4963 - binary_io_u_5: 0.4898 - loss: 0.9164Batch 72, Loss Value: 0.8916\n",
      "Batch 72, Gradient Norm: 0.4894\n",
      "\u001b[1m 72/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:15\u001b[0m 474ms/step - accuracy: 0.4963 - binary_io_u_5: 0.4898 - loss: 0.9164Batch 73, Loss Value: 0.9226\n",
      "Batch 73, Gradient Norm: 0.2132\n",
      "\u001b[1m 73/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:15\u001b[0m 474ms/step - accuracy: 0.4963 - binary_io_u_5: 0.4899 - loss: 0.9164Batch 74, Loss Value: 0.8647\n",
      "Batch 74, Gradient Norm: 0.0044\n",
      "\u001b[1m 74/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:14\u001b[0m 474ms/step - accuracy: 0.4964 - binary_io_u_5: 0.4899 - loss: 0.9164Batch 75, Loss Value: 0.9024\n",
      "Batch 75, Gradient Norm: 0.1704\n",
      "\u001b[1m 75/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:14\u001b[0m 474ms/step - accuracy: 0.4964 - binary_io_u_5: 0.4900 - loss: 0.9164Batch 76, Loss Value: 0.8832\n",
      "Batch 76, Gradient Norm: 0.0108\n",
      "\u001b[1m 76/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:13\u001b[0m 474ms/step - accuracy: 0.4964 - binary_io_u_5: 0.4900 - loss: 0.9163Batch 77, Loss Value: 0.8959\n",
      "Batch 77, Gradient Norm: 0.0000\n",
      "\u001b[1m 77/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:17\u001b[0m 502ms/step - accuracy: 0.4965 - binary_io_u_5: 0.4901 - loss: 0.9163Batch 78, Loss Value: 0.8775\n",
      "Batch 78, Gradient Norm: 0.0090\n",
      "\u001b[1m 78/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:17\u001b[0m 502ms/step - accuracy: 0.4965 - binary_io_u_5: 0.4902 - loss: 0.9163Batch 79, Loss Value: 0.8848\n",
      "Batch 79, Gradient Norm: 0.0326\n",
      "\u001b[1m 79/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:16\u001b[0m 502ms/step - accuracy: 0.4966 - binary_io_u_5: 0.4902 - loss: 0.9163Batch 80, Loss Value: 0.9101\n",
      "Batch 80, Gradient Norm: 0.0000\n",
      "\u001b[1m 80/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:16\u001b[0m 501ms/step - accuracy: 0.4966 - binary_io_u_5: 0.4903 - loss: 0.9163Batch 81, Loss Value: 0.9034\n",
      "Batch 81, Gradient Norm: 0.2804\n",
      "\u001b[1m 81/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:15\u001b[0m 501ms/step - accuracy: 0.4967 - binary_io_u_5: 0.4903 - loss: 0.9163Batch 82, Loss Value: 0.8846\n",
      "Batch 82, Gradient Norm: 0.0000\n",
      "\u001b[1m 82/232\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m1:15\u001b[0m 501ms/step - accuracy: 0.4967 - binary_io_u_5: 0.4904 - loss: 0.9163Batch 83, Loss Value: 0.8845\n",
      "Batch 83, Gradient Norm: 0.0161\n",
      "\u001b[1m 83/232\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m1:14\u001b[0m 501ms/step - accuracy: 0.4967 - binary_io_u_5: 0.4904 - loss: 0.9162Batch 84, Loss Value: 0.9100\n",
      "Batch 84, Gradient Norm: 0.0972\n",
      "\u001b[1m 84/232\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m1:14\u001b[0m 501ms/step - accuracy: 0.4968 - binary_io_u_5: 0.4905 - loss: 0.9162Batch 85, Loss Value: 0.8801\n",
      "Batch 85, Gradient Norm: 0.4292\n",
      "\u001b[1m 85/232\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m1:13\u001b[0m 501ms/step - accuracy: 0.4968 - binary_io_u_5: 0.4906 - loss: 0.9162Batch 86, Loss Value: 0.9106\n",
      "Batch 86, Gradient Norm: 0.1996\n",
      "\u001b[1m 86/232\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m1:13\u001b[0m 501ms/step - accuracy: 0.4969 - binary_io_u_5: 0.4906 - loss: 0.9162Batch 87, Loss Value: 0.8775\n",
      "Batch 87, Gradient Norm: 0.0000\n",
      "\u001b[1m 87/232\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m1:12\u001b[0m 501ms/step - accuracy: 0.4969 - binary_io_u_5: 0.4907 - loss: 0.9162Batch 88, Loss Value: 0.8974\n",
      "Batch 88, Gradient Norm: 0.0099\n",
      "\u001b[1m 88/232\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m1:12\u001b[0m 501ms/step - accuracy: 0.4970 - binary_io_u_5: 0.4907 - loss: 0.9162Batch 89, Loss Value: 0.9026\n",
      "Batch 89, Gradient Norm: 0.4882\n",
      "\u001b[1m 89/232\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m1:11\u001b[0m 501ms/step - accuracy: 0.4970 - binary_io_u_5: 0.4908 - loss: 0.9161Batch 90, Loss Value: 0.8772\n",
      "Batch 90, Gradient Norm: 0.2675\n",
      "\u001b[1m 90/232\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m1:11\u001b[0m 501ms/step - accuracy: 0.4971 - binary_io_u_5: 0.4909 - loss: 0.9161Batch 91, Loss Value: 0.8914\n",
      "Batch 91, Gradient Norm: 0.0948\n",
      "\u001b[1m 91/232\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m1:10\u001b[0m 501ms/step - accuracy: 0.4971 - binary_io_u_5: 0.4909 - loss: 0.9161Batch 92, Loss Value: 0.9357\n",
      "Batch 92, Gradient Norm: 0.0111\n",
      "\u001b[1m 92/232\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m1:10\u001b[0m 501ms/step - accuracy: 0.4972 - binary_io_u_5: 0.4910 - loss: 0.9161Batch 93, Loss Value: 0.9106\n",
      "Batch 93, Gradient Norm: 0.0499\n",
      "\u001b[1m 93/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m1:09\u001b[0m 501ms/step - accuracy: 0.4972 - binary_io_u_5: 0.4910 - loss: 0.9161Batch 94, Loss Value: 0.9108\n",
      "Batch 94, Gradient Norm: 0.1552\n",
      "\u001b[1m 94/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m1:09\u001b[0m 501ms/step - accuracy: 0.4972 - binary_io_u_5: 0.4911 - loss: 0.9161Batch 95, Loss Value: 0.8733\n",
      "Batch 95, Gradient Norm: 0.0113\n",
      "\u001b[1m 95/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m1:08\u001b[0m 501ms/step - accuracy: 0.4973 - binary_io_u_5: 0.4911 - loss: 0.9161Batch 96, Loss Value: 0.8617\n",
      "Batch 96, Gradient Norm: 0.3509\n",
      "\u001b[1m 96/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m1:08\u001b[0m 501ms/step - accuracy: 0.4973 - binary_io_u_5: 0.4911 - loss: 0.9161Batch 97, Loss Value: 0.9026\n",
      "Batch 97, Gradient Norm: 0.0586\n",
      "\u001b[1m 97/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m1:07\u001b[0m 501ms/step - accuracy: 0.4973 - binary_io_u_5: 0.4911 - loss: 0.9161Batch 98, Loss Value: 0.8927\n",
      "Batch 98, Gradient Norm: 0.4033\n",
      "\u001b[1m 98/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m1:07\u001b[0m 501ms/step - accuracy: 0.4973 - binary_io_u_5: 0.4912 - loss: 0.9161Batch 99, Loss Value: 0.8916\n",
      "Batch 99, Gradient Norm: 0.0096\n",
      "\u001b[1m 99/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m1:06\u001b[0m 501ms/step - accuracy: 0.4973 - binary_io_u_5: 0.4912 - loss: 0.9161Batch 100, Loss Value: 0.9115\n",
      "Batch 100, Gradient Norm: 0.0000\n",
      "\u001b[1m100/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m1:06\u001b[0m 501ms/step - accuracy: 0.4973 - binary_io_u_5: 0.4912 - loss: 0.9161Batch 101, Loss Value: 0.8910\n",
      "Batch 101, Gradient Norm: 0.1965\n",
      "\u001b[1m101/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m1:05\u001b[0m 501ms/step - accuracy: 0.4973 - binary_io_u_5: 0.4912 - loss: 0.9161Batch 102, Loss Value: 0.8812\n",
      "Batch 102, Gradient Norm: 0.6594\n",
      "\u001b[1m102/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m1:05\u001b[0m 500ms/step - accuracy: 0.4973 - binary_io_u_5: 0.4912 - loss: 0.9161Batch 103, Loss Value: 0.9276\n",
      "Batch 103, Gradient Norm: 0.3848\n",
      "\u001b[1m103/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m1:04\u001b[0m 500ms/step - accuracy: 0.4973 - binary_io_u_5: 0.4912 - loss: 0.9161Batch 104, Loss Value: 0.8784\n",
      "Batch 104, Gradient Norm: 0.4893\n",
      "\u001b[1m104/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m1:04\u001b[0m 500ms/step - accuracy: 0.4974 - binary_io_u_5: 0.4913 - loss: 0.9161Batch 105, Loss Value: 0.8888\n",
      "Batch 105, Gradient Norm: 0.0468\n",
      "\u001b[1m105/232\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m1:03\u001b[0m 500ms/step - accuracy: 0.4974 - binary_io_u_5: 0.4913 - loss: 0.9161Batch 106, Loss Value: 0.9402\n",
      "Batch 106, Gradient Norm: 0.0001\n",
      "\u001b[1m106/232\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m1:02\u001b[0m 500ms/step - accuracy: 0.4974 - binary_io_u_5: 0.4913 - loss: 0.9161Batch 107, Loss Value: 0.9248\n",
      "Batch 107, Gradient Norm: 0.5321\n",
      "\u001b[1m107/232\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m1:02\u001b[0m 500ms/step - accuracy: 0.4974 - binary_io_u_5: 0.4913 - loss: 0.9161Batch 108, Loss Value: 0.9058\n",
      "Batch 108, Gradient Norm: 0.0074\n",
      "\u001b[1m108/232\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m1:01\u001b[0m 500ms/step - accuracy: 0.4974 - binary_io_u_5: 0.4913 - loss: 0.9161Batch 109, Loss Value: 0.9057\n",
      "Batch 109, Gradient Norm: 0.3797\n",
      "\u001b[1m109/232\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m1:01\u001b[0m 500ms/step - accuracy: 0.4974 - binary_io_u_5: 0.4914 - loss: 0.9161Batch 110, Loss Value: 0.9182\n",
      "Batch 110, Gradient Norm: 0.0984\n",
      "\u001b[1m110/232\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m1:00\u001b[0m 499ms/step - accuracy: 0.4974 - binary_io_u_5: 0.4914 - loss: 0.9161Batch 111, Loss Value: 0.8719\n",
      "Batch 111, Gradient Norm: 2.3327\n",
      "\u001b[1m111/232\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m1:00\u001b[0m 500ms/step - accuracy: 0.4974 - binary_io_u_5: 0.4914 - loss: 0.9161Batch 112, Loss Value: 0.9008\n",
      "Batch 112, Gradient Norm: 0.1245\n",
      "\u001b[1m112/232\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m59s\u001b[0m 500ms/step - accuracy: 0.4974 - binary_io_u_5: 0.4914 - loss: 0.9161 Batch 113, Loss Value: 0.9001\n",
      "Batch 113, Gradient Norm: 0.5126\n",
      "\u001b[1m113/232\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m59s\u001b[0m 500ms/step - accuracy: 0.4974 - binary_io_u_5: 0.4914 - loss: 0.9161Batch 114, Loss Value: 0.9401\n",
      "Batch 114, Gradient Norm: 0.0238\n",
      "\u001b[1m114/232\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m58s\u001b[0m 500ms/step - accuracy: 0.4974 - binary_io_u_5: 0.4914 - loss: 0.9161Batch 115, Loss Value: 0.9001\n",
      "Batch 115, Gradient Norm: 0.0148\n",
      "\u001b[1m115/232\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m58s\u001b[0m 500ms/step - accuracy: 0.4974 - binary_io_u_5: 0.4915 - loss: 0.9161Batch 116, Loss Value: 0.8834\n",
      "Batch 116, Gradient Norm: 0.3964\n",
      "\u001b[1m116/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m57s\u001b[0m 500ms/step - accuracy: 0.4974 - binary_io_u_5: 0.4915 - loss: 0.9161Batch 117, Loss Value: 0.8991\n",
      "Batch 117, Gradient Norm: 0.2460\n",
      "\u001b[1m117/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m57s\u001b[0m 500ms/step - accuracy: 0.4974 - binary_io_u_5: 0.4915 - loss: 0.9161Batch 118, Loss Value: 0.8966\n",
      "Batch 118, Gradient Norm: 0.4456\n",
      "\u001b[1m118/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m56s\u001b[0m 500ms/step - accuracy: 0.4974 - binary_io_u_5: 0.4915 - loss: 0.9161Batch 119, Loss Value: 0.9137\n",
      "Batch 119, Gradient Norm: 0.0932\n",
      "\u001b[1m119/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m56s\u001b[0m 499ms/step - accuracy: 0.4974 - binary_io_u_5: 0.4915 - loss: 0.9161Batch 120, Loss Value: 0.9159\n",
      "Batch 120, Gradient Norm: 0.9631\n",
      "\u001b[1m120/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m55s\u001b[0m 500ms/step - accuracy: 0.4975 - binary_io_u_5: 0.4915 - loss: 0.9161Batch 121, Loss Value: 0.9187\n",
      "Batch 121, Gradient Norm: 0.2974\n",
      "\u001b[1m121/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m55s\u001b[0m 500ms/step - accuracy: 0.4975 - binary_io_u_5: 0.4915 - loss: 0.9162Batch 122, Loss Value: 0.9039\n",
      "Batch 122, Gradient Norm: 0.6848\n",
      "\u001b[1m122/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m54s\u001b[0m 500ms/step - accuracy: 0.4975 - binary_io_u_5: 0.4915 - loss: 0.9162Batch 123, Loss Value: 0.8986\n",
      "Batch 123, Gradient Norm: 0.0177\n",
      "\u001b[1m123/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m54s\u001b[0m 500ms/step - accuracy: 0.4975 - binary_io_u_5: 0.4915 - loss: 0.9161Batch 124, Loss Value: 0.9340\n",
      "Batch 124, Gradient Norm: 0.0345\n",
      "\u001b[1m124/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m53s\u001b[0m 500ms/step - accuracy: 0.4975 - binary_io_u_5: 0.4916 - loss: 0.9161Batch 125, Loss Value: 0.9043\n",
      "Batch 125, Gradient Norm: 0.2997\n",
      "\u001b[1m125/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m53s\u001b[0m 500ms/step - accuracy: 0.4975 - binary_io_u_5: 0.4916 - loss: 0.9161Batch 126, Loss Value: 0.8879\n",
      "Batch 126, Gradient Norm: 0.6589\n",
      "\u001b[1m126/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m52s\u001b[0m 499ms/step - accuracy: 0.4975 - binary_io_u_5: 0.4916 - loss: 0.9161Batch 127, Loss Value: 0.9125\n",
      "Batch 127, Gradient Norm: 1.1847\n",
      "\u001b[1m127/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m52s\u001b[0m 499ms/step - accuracy: 0.4975 - binary_io_u_5: 0.4916 - loss: 0.9161Batch 128, Loss Value: 0.9115\n",
      "Batch 128, Gradient Norm: 0.0161\n",
      "\u001b[1m128/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m51s\u001b[0m 499ms/step - accuracy: 0.4976 - binary_io_u_5: 0.4916 - loss: 0.9161Batch 129, Loss Value: 0.9071\n",
      "Batch 129, Gradient Norm: 0.0414\n",
      "\u001b[1m129/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m51s\u001b[0m 499ms/step - accuracy: 0.4976 - binary_io_u_5: 0.4916 - loss: 0.9161Batch 130, Loss Value: 0.9270\n",
      "Batch 130, Gradient Norm: 0.0753\n",
      "\u001b[1m130/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m50s\u001b[0m 499ms/step - accuracy: 0.4976 - binary_io_u_5: 0.4917 - loss: 0.9161Batch 131, Loss Value: 0.8747\n",
      "Batch 131, Gradient Norm: 0.2427\n",
      "\u001b[1m131/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m50s\u001b[0m 499ms/step - accuracy: 0.4976 - binary_io_u_5: 0.4917 - loss: 0.9161Batch 132, Loss Value: 0.9001\n",
      "Batch 132, Gradient Norm: 0.0010\n",
      "\u001b[1m132/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m49s\u001b[0m 499ms/step - accuracy: 0.4976 - binary_io_u_5: 0.4917 - loss: 0.9161Batch 133, Loss Value: 0.9407\n",
      "Batch 133, Gradient Norm: 0.0012\n",
      "\u001b[1m133/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m49s\u001b[0m 499ms/step - accuracy: 0.4977 - binary_io_u_5: 0.4917 - loss: 0.9161Batch 134, Loss Value: 0.8992\n",
      "Batch 134, Gradient Norm: 0.3555\n",
      "\u001b[1m134/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m48s\u001b[0m 499ms/step - accuracy: 0.4977 - binary_io_u_5: 0.4917 - loss: 0.9161Batch 135, Loss Value: 0.9214\n",
      "Batch 135, Gradient Norm: 0.0000\n",
      "\u001b[1m135/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m48s\u001b[0m 499ms/step - accuracy: 0.4977 - binary_io_u_5: 0.4917 - loss: 0.9161Batch 136, Loss Value: 0.9229\n",
      "Batch 136, Gradient Norm: 0.1985\n",
      "\u001b[1m136/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m47s\u001b[0m 499ms/step - accuracy: 0.4977 - binary_io_u_5: 0.4918 - loss: 0.9161Batch 137, Loss Value: 0.8987\n",
      "Batch 137, Gradient Norm: 0.0217\n",
      "\u001b[1m137/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m47s\u001b[0m 499ms/step - accuracy: 0.4978 - binary_io_u_5: 0.4918 - loss: 0.9161Batch 138, Loss Value: 0.9243\n",
      "Batch 138, Gradient Norm: 0.6792\n",
      "\u001b[1m138/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m46s\u001b[0m 499ms/step - accuracy: 0.4978 - binary_io_u_5: 0.4918 - loss: 0.9161Batch 139, Loss Value: 0.8870\n",
      "Batch 139, Gradient Norm: 0.3985\n",
      "\u001b[1m139/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m46s\u001b[0m 499ms/step - accuracy: 0.4978 - binary_io_u_5: 0.4918 - loss: 0.9161Batch 140, Loss Value: 0.9271\n",
      "Batch 140, Gradient Norm: 0.0000\n",
      "\u001b[1m140/232\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m45s\u001b[0m 499ms/step - accuracy: 0.4978 - binary_io_u_5: 0.4918 - loss: 0.9161Batch 141, Loss Value: 0.8896\n",
      "Batch 141, Gradient Norm: 2.0298\n",
      "\u001b[1m141/232\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m45s\u001b[0m 499ms/step - accuracy: 0.4978 - binary_io_u_5: 0.4919 - loss: 0.9160Batch 142, Loss Value: 0.9173\n",
      "Batch 142, Gradient Norm: 0.3986\n",
      "\u001b[1m142/232\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m44s\u001b[0m 499ms/step - accuracy: 0.4979 - binary_io_u_5: 0.4919 - loss: 0.9160Batch 143, Loss Value: 0.9258\n",
      "Batch 143, Gradient Norm: 0.0109\n",
      "\u001b[1m143/232\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m44s\u001b[0m 499ms/step - accuracy: 0.4979 - binary_io_u_5: 0.4919 - loss: 0.9160Batch 144, Loss Value: 0.9408\n",
      "Batch 144, Gradient Norm: 0.0033\n",
      "\u001b[1m144/232\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m43s\u001b[0m 499ms/step - accuracy: 0.4979 - binary_io_u_5: 0.4919 - loss: 0.9160Batch 145, Loss Value: 0.8689\n",
      "Batch 145, Gradient Norm: 0.0014\n",
      "\u001b[1m145/232\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m43s\u001b[0m 499ms/step - accuracy: 0.4979 - binary_io_u_5: 0.4919 - loss: 0.9160Batch 146, Loss Value: 0.9182\n",
      "Batch 146, Gradient Norm: 0.4720\n",
      "\u001b[1m146/232\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m42s\u001b[0m 499ms/step - accuracy: 0.4979 - binary_io_u_5: 0.4920 - loss: 0.9160Batch 147, Loss Value: 0.8848\n",
      "Batch 147, Gradient Norm: 0.0925\n",
      "\u001b[1m147/232\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m42s\u001b[0m 499ms/step - accuracy: 0.4979 - binary_io_u_5: 0.4920 - loss: 0.9160Batch 148, Loss Value: 0.9137\n",
      "Batch 148, Gradient Norm: 0.3947\n",
      "\u001b[1m148/232\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m41s\u001b[0m 499ms/step - accuracy: 0.4979 - binary_io_u_5: 0.4920 - loss: 0.9160Batch 149, Loss Value: 0.8857\n",
      "Batch 149, Gradient Norm: 0.0823\n",
      "\u001b[1m149/232\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m41s\u001b[0m 499ms/step - accuracy: 0.4980 - binary_io_u_5: 0.4920 - loss: 0.9160Batch 150, Loss Value: 0.8731\n",
      "Batch 150, Gradient Norm: 0.0043\n",
      "\u001b[1m150/232\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m40s\u001b[0m 499ms/step - accuracy: 0.4980 - binary_io_u_5: 0.4920 - loss: 0.9160Batch 151, Loss Value: 0.8914\n",
      "Batch 151, Gradient Norm: 0.2352\n",
      "\u001b[1m151/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m40s\u001b[0m 499ms/step - accuracy: 0.4980 - binary_io_u_5: 0.4920 - loss: 0.9160Batch 152, Loss Value: 0.9075\n",
      "Batch 152, Gradient Norm: 0.1358\n",
      "\u001b[1m152/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m39s\u001b[0m 499ms/step - accuracy: 0.4980 - binary_io_u_5: 0.4921 - loss: 0.9160Batch 153, Loss Value: 0.8841\n",
      "Batch 153, Gradient Norm: 0.8802\n",
      "\u001b[1m153/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m39s\u001b[0m 499ms/step - accuracy: 0.4980 - binary_io_u_5: 0.4921 - loss: 0.9160Batch 154, Loss Value: 0.8929\n",
      "Batch 154, Gradient Norm: 0.0518\n",
      "\u001b[1m154/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m38s\u001b[0m 499ms/step - accuracy: 0.4980 - binary_io_u_5: 0.4921 - loss: 0.9160Batch 155, Loss Value: 0.8952\n",
      "Batch 155, Gradient Norm: 0.1957\n",
      "\u001b[1m155/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m38s\u001b[0m 499ms/step - accuracy: 0.4980 - binary_io_u_5: 0.4921 - loss: 0.9160Batch 156, Loss Value: 0.9403\n",
      "Batch 156, Gradient Norm: 0.0124\n",
      "\u001b[1m156/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m37s\u001b[0m 499ms/step - accuracy: 0.4980 - binary_io_u_5: 0.4921 - loss: 0.9160Batch 157, Loss Value: 0.8832\n",
      "Batch 157, Gradient Norm: 0.0001\n",
      "\u001b[1m157/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m37s\u001b[0m 499ms/step - accuracy: 0.4981 - binary_io_u_5: 0.4921 - loss: 0.9160Batch 158, Loss Value: 0.9070\n",
      "Batch 158, Gradient Norm: 0.4085\n",
      "\u001b[1m158/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m36s\u001b[0m 499ms/step - accuracy: 0.4981 - binary_io_u_5: 0.4921 - loss: 0.9160Batch 159, Loss Value: 0.8661\n",
      "Batch 159, Gradient Norm: 0.0002\n",
      "\u001b[1m159/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m36s\u001b[0m 499ms/step - accuracy: 0.4981 - binary_io_u_5: 0.4922 - loss: 0.9160Batch 160, Loss Value: 0.8884\n",
      "Batch 160, Gradient Norm: 0.2524\n",
      "\u001b[1m160/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m35s\u001b[0m 499ms/step - accuracy: 0.4981 - binary_io_u_5: 0.4922 - loss: 0.9160Batch 161, Loss Value: 0.8917\n",
      "Batch 161, Gradient Norm: 0.0020\n",
      "\u001b[1m161/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m35s\u001b[0m 499ms/step - accuracy: 0.4981 - binary_io_u_5: 0.4922 - loss: 0.9160Batch 162, Loss Value: 0.8999\n",
      "Batch 162, Gradient Norm: 0.0523\n",
      "\u001b[1m162/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m34s\u001b[0m 499ms/step - accuracy: 0.4981 - binary_io_u_5: 0.4922 - loss: 0.9160Batch 163, Loss Value: 0.8803\n",
      "Batch 163, Gradient Norm: 1.0289\n",
      "\u001b[1m163/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m34s\u001b[0m 499ms/step - accuracy: 0.4982 - binary_io_u_5: 0.4922 - loss: 0.9160Batch 164, Loss Value: 0.9124\n",
      "Batch 164, Gradient Norm: 0.1135\n",
      "\u001b[1m164/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m33s\u001b[0m 499ms/step - accuracy: 0.4982 - binary_io_u_5: 0.4923 - loss: 0.9160Batch 165, Loss Value: 0.8827\n",
      "Batch 165, Gradient Norm: 0.8594\n",
      "\u001b[1m165/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m33s\u001b[0m 499ms/step - accuracy: 0.4982 - binary_io_u_5: 0.4923 - loss: 0.9160Batch 166, Loss Value: 0.8616\n",
      "Batch 166, Gradient Norm: 0.1311\n",
      "\u001b[1m166/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m32s\u001b[0m 499ms/step - accuracy: 0.4982 - binary_io_u_5: 0.4923 - loss: 0.9160Batch 167, Loss Value: 0.8800\n",
      "Batch 167, Gradient Norm: 0.0656\n",
      "\u001b[1m167/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m32s\u001b[0m 499ms/step - accuracy: 0.4982 - binary_io_u_5: 0.4923 - loss: 0.9160Batch 168, Loss Value: 0.8941\n",
      "Batch 168, Gradient Norm: 0.4882\n",
      "\u001b[1m168/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m31s\u001b[0m 499ms/step - accuracy: 0.4982 - binary_io_u_5: 0.4923 - loss: 0.9160Batch 169, Loss Value: 0.9374\n",
      "Batch 169, Gradient Norm: 0.0329\n",
      "\u001b[1m169/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m31s\u001b[0m 499ms/step - accuracy: 0.4982 - binary_io_u_5: 0.4923 - loss: 0.9160Batch 170, Loss Value: 0.9223\n",
      "Batch 170, Gradient Norm: 0.2666\n",
      "\u001b[1m170/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m30s\u001b[0m 499ms/step - accuracy: 0.4983 - binary_io_u_5: 0.4923 - loss: 0.9160Batch 171, Loss Value: 0.9126\n",
      "Batch 171, Gradient Norm: 0.0394\n",
      "\u001b[1m171/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m30s\u001b[0m 499ms/step - accuracy: 0.4983 - binary_io_u_5: 0.4924 - loss: 0.9160Batch 172, Loss Value: 0.9087\n",
      "Batch 172, Gradient Norm: 0.0031\n",
      "\u001b[1m172/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m29s\u001b[0m 499ms/step - accuracy: 0.4983 - binary_io_u_5: 0.4924 - loss: 0.9160Batch 173, Loss Value: 0.9007\n",
      "Batch 173, Gradient Norm: 0.6676\n",
      "\u001b[1m173/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m29s\u001b[0m 499ms/step - accuracy: 0.4983 - binary_io_u_5: 0.4924 - loss: 0.9160Batch 174, Loss Value: 0.8899\n",
      "Batch 174, Gradient Norm: 0.3303\n",
      "\u001b[1m174/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m28s\u001b[0m 499ms/step - accuracy: 0.4983 - binary_io_u_5: 0.4924 - loss: 0.9160Batch 175, Loss Value: 0.9044\n",
      "Batch 175, Gradient Norm: 0.0066\n",
      "\u001b[1m175/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m28s\u001b[0m 499ms/step - accuracy: 0.4983 - binary_io_u_5: 0.4924 - loss: 0.9160Batch 176, Loss Value: 0.9115\n",
      "Batch 176, Gradient Norm: 0.0002\n",
      "\u001b[1m176/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m27s\u001b[0m 499ms/step - accuracy: 0.4983 - binary_io_u_5: 0.4924 - loss: 0.9160Batch 177, Loss Value: 0.8987\n",
      "Batch 177, Gradient Norm: 0.0006\n",
      "\u001b[1m177/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m27s\u001b[0m 499ms/step - accuracy: 0.4983 - binary_io_u_5: 0.4924 - loss: 0.9160Batch 178, Loss Value: 0.8988\n",
      "Batch 178, Gradient Norm: 0.0095\n",
      "\u001b[1m178/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m26s\u001b[0m 499ms/step - accuracy: 0.4984 - binary_io_u_5: 0.4924 - loss: 0.9160Batch 179, Loss Value: 0.9046\n",
      "Batch 179, Gradient Norm: 1.1202\n",
      "\u001b[1m179/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m26s\u001b[0m 499ms/step - accuracy: 0.4984 - binary_io_u_5: 0.4925 - loss: 0.9160Batch 180, Loss Value: 0.8789\n",
      "Batch 180, Gradient Norm: 0.0165\n",
      "\u001b[1m180/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m25s\u001b[0m 499ms/step - accuracy: 0.4984 - binary_io_u_5: 0.4925 - loss: 0.9160Batch 181, Loss Value: 0.9067\n",
      "Batch 181, Gradient Norm: 0.0658\n",
      "\u001b[1m181/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m25s\u001b[0m 499ms/step - accuracy: 0.4984 - binary_io_u_5: 0.4925 - loss: 0.9160Batch 182, Loss Value: 0.8478\n",
      "Batch 182, Gradient Norm: 0.3342\n",
      "\u001b[1m182/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m24s\u001b[0m 499ms/step - accuracy: 0.4984 - binary_io_u_5: 0.4925 - loss: 0.9160Batch 183, Loss Value: 0.8776\n",
      "Batch 183, Gradient Norm: 0.0511\n",
      "\u001b[1m183/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m24s\u001b[0m 499ms/step - accuracy: 0.4984 - binary_io_u_5: 0.4925 - loss: 0.9160Batch 184, Loss Value: 0.9187\n",
      "Batch 184, Gradient Norm: 0.4848\n",
      "\u001b[1m184/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m23s\u001b[0m 499ms/step - accuracy: 0.4984 - binary_io_u_5: 0.4925 - loss: 0.9160Batch 185, Loss Value: 0.8723\n",
      "Batch 185, Gradient Norm: 3.6251\n",
      "\u001b[1m185/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m23s\u001b[0m 499ms/step - accuracy: 0.4984 - binary_io_u_5: 0.4925 - loss: 0.9160Batch 186, Loss Value: 0.8855\n",
      "Batch 186, Gradient Norm: 0.7001\n",
      "\u001b[1m186/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m22s\u001b[0m 499ms/step - accuracy: 0.4985 - binary_io_u_5: 0.4925 - loss: 0.9160Batch 187, Loss Value: 0.9010\n",
      "Batch 187, Gradient Norm: 0.2700\n",
      "\u001b[1m187/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m22s\u001b[0m 499ms/step - accuracy: 0.4985 - binary_io_u_5: 0.4926 - loss: 0.9160Batch 188, Loss Value: 0.8648\n",
      "Batch 188, Gradient Norm: 0.3337\n",
      "\u001b[1m188/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m21s\u001b[0m 499ms/step - accuracy: 0.4985 - binary_io_u_5: 0.4926 - loss: 0.9160Batch 189, Loss Value: 0.8846\n",
      "Batch 189, Gradient Norm: 0.0278\n",
      "\u001b[1m189/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m21s\u001b[0m 499ms/step - accuracy: 0.4985 - binary_io_u_5: 0.4926 - loss: 0.9160Batch 190, Loss Value: 0.8964\n",
      "Batch 190, Gradient Norm: 0.0857\n",
      "\u001b[1m190/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m20s\u001b[0m 499ms/step - accuracy: 0.4985 - binary_io_u_5: 0.4926 - loss: 0.9160Batch 191, Loss Value: 0.9229\n",
      "Batch 191, Gradient Norm: 0.0004\n",
      "\u001b[1m191/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m20s\u001b[0m 499ms/step - accuracy: 0.4985 - binary_io_u_5: 0.4926 - loss: 0.9160Batch 192, Loss Value: 0.8654\n",
      "Batch 192, Gradient Norm: 0.3133\n",
      "\u001b[1m192/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m19s\u001b[0m 499ms/step - accuracy: 0.4985 - binary_io_u_5: 0.4926 - loss: 0.9160Batch 193, Loss Value: 0.9118\n",
      "Batch 193, Gradient Norm: 0.3998\n",
      "\u001b[1m193/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m19s\u001b[0m 499ms/step - accuracy: 0.4985 - binary_io_u_5: 0.4926 - loss: 0.9160Batch 194, Loss Value: 0.9045\n",
      "Batch 194, Gradient Norm: 0.0064\n",
      "\u001b[1m194/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m18s\u001b[0m 499ms/step - accuracy: 0.4985 - binary_io_u_5: 0.4926 - loss: 0.9160Batch 195, Loss Value: 0.9059\n",
      "Batch 195, Gradient Norm: 0.7224\n",
      "\u001b[1m195/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m18s\u001b[0m 499ms/step - accuracy: 0.4985 - binary_io_u_5: 0.4926 - loss: 0.9160Batch 196, Loss Value: 0.8631\n",
      "Batch 196, Gradient Norm: 0.4968\n",
      "\u001b[1m196/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m17s\u001b[0m 499ms/step - accuracy: 0.4985 - binary_io_u_5: 0.4926 - loss: 0.9160Batch 197, Loss Value: 0.8920\n",
      "Batch 197, Gradient Norm: 0.5793\n",
      "\u001b[1m197/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m17s\u001b[0m 499ms/step - accuracy: 0.4985 - binary_io_u_5: 0.4927 - loss: 0.9160Batch 198, Loss Value: 0.9030\n",
      "Batch 198, Gradient Norm: 0.0200\n",
      "\u001b[1m198/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m16s\u001b[0m 499ms/step - accuracy: 0.4986 - binary_io_u_5: 0.4927 - loss: 0.9160Batch 199, Loss Value: 0.9101\n",
      "Batch 199, Gradient Norm: 0.5863\n",
      "\u001b[1m199/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m16s\u001b[0m 499ms/step - accuracy: 0.4986 - binary_io_u_5: 0.4927 - loss: 0.9160Batch 200, Loss Value: 0.8718\n",
      "Batch 200, Gradient Norm: 0.6864\n",
      "\u001b[1m200/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m15s\u001b[0m 499ms/step - accuracy: 0.4986 - binary_io_u_5: 0.4927 - loss: 0.9160Batch 201, Loss Value: 0.9154\n",
      "Batch 201, Gradient Norm: 0.0965\n",
      "\u001b[1m201/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m15s\u001b[0m 499ms/step - accuracy: 0.4986 - binary_io_u_5: 0.4927 - loss: 0.9160Batch 202, Loss Value: 0.8875\n",
      "Batch 202, Gradient Norm: 0.0187\n",
      "\u001b[1m202/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m14s\u001b[0m 499ms/step - accuracy: 0.4986 - binary_io_u_5: 0.4927 - loss: 0.9160Batch 203, Loss Value: 0.8774\n",
      "Batch 203, Gradient Norm: 0.0195\n",
      "\u001b[1m203/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m14s\u001b[0m 499ms/step - accuracy: 0.4986 - binary_io_u_5: 0.4927 - loss: 0.9160Batch 204, Loss Value: 0.9016\n",
      "Batch 204, Gradient Norm: 0.0010\n",
      "\u001b[1m204/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m13s\u001b[0m 499ms/step - accuracy: 0.4986 - binary_io_u_5: 0.4927 - loss: 0.9160Batch 205, Loss Value: 0.8903\n",
      "Batch 205, Gradient Norm: 0.0004\n",
      "\u001b[1m205/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m13s\u001b[0m 499ms/step - accuracy: 0.4986 - binary_io_u_5: 0.4928 - loss: 0.9160Batch 206, Loss Value: 0.9144\n",
      "Batch 206, Gradient Norm: 0.0002\n",
      "\u001b[1m206/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m12s\u001b[0m 499ms/step - accuracy: 0.4986 - binary_io_u_5: 0.4928 - loss: 0.9160Batch 207, Loss Value: 0.9286\n",
      "Batch 207, Gradient Norm: 0.0000\n",
      "\u001b[1m207/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m12s\u001b[0m 499ms/step - accuracy: 0.4986 - binary_io_u_5: 0.4928 - loss: 0.9160Batch 208, Loss Value: 0.8916\n",
      "Batch 208, Gradient Norm: 0.7111\n",
      "\u001b[1m208/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m11s\u001b[0m 499ms/step - accuracy: 0.4986 - binary_io_u_5: 0.4928 - loss: 0.9160Batch 209, Loss Value: 0.8961\n",
      "Batch 209, Gradient Norm: 0.0471\n",
      "\u001b[1m209/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m11s\u001b[0m 499ms/step - accuracy: 0.4986 - binary_io_u_5: 0.4928 - loss: 0.9160Batch 210, Loss Value: 0.8802\n",
      "Batch 210, Gradient Norm: 0.3148\n",
      "\u001b[1m210/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m10s\u001b[0m 498ms/step - accuracy: 0.4986 - binary_io_u_5: 0.4928 - loss: 0.9160Batch 211, Loss Value: 0.8677\n",
      "Batch 211, Gradient Norm: 0.6130\n",
      "\u001b[1m211/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m10s\u001b[0m 498ms/step - accuracy: 0.4987 - binary_io_u_5: 0.4928 - loss: 0.9160Batch 212, Loss Value: 0.9101\n",
      "Batch 212, Gradient Norm: 0.0029\n",
      "\u001b[1m212/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m9s\u001b[0m 498ms/step - accuracy: 0.4987 - binary_io_u_5: 0.4928 - loss: 0.9160 Batch 213, Loss Value: 0.8993\n",
      "Batch 213, Gradient Norm: 0.5331\n",
      "\u001b[1m213/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m9s\u001b[0m 498ms/step - accuracy: 0.4987 - binary_io_u_5: 0.4928 - loss: 0.9160Batch 214, Loss Value: 0.9042\n",
      "Batch 214, Gradient Norm: 0.1037\n",
      "\u001b[1m214/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m8s\u001b[0m 498ms/step - accuracy: 0.4987 - binary_io_u_5: 0.4929 - loss: 0.9160Batch 215, Loss Value: 0.8662\n",
      "Batch 215, Gradient Norm: 0.0193\n",
      "\u001b[1m215/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m8s\u001b[0m 498ms/step - accuracy: 0.4987 - binary_io_u_5: 0.4929 - loss: 0.9160Batch 216, Loss Value: 0.8886\n",
      "Batch 216, Gradient Norm: 0.3571\n",
      "\u001b[1m216/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m7s\u001b[0m 498ms/step - accuracy: 0.4987 - binary_io_u_5: 0.4929 - loss: 0.9160Batch 217, Loss Value: 0.8984\n",
      "Batch 217, Gradient Norm: 1.1452\n",
      "\u001b[1m217/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m7s\u001b[0m 498ms/step - accuracy: 0.4987 - binary_io_u_5: 0.4929 - loss: 0.9160Batch 218, Loss Value: 0.9162\n",
      "Batch 218, Gradient Norm: 0.0731\n",
      "\u001b[1m218/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m6s\u001b[0m 498ms/step - accuracy: 0.4987 - binary_io_u_5: 0.4929 - loss: 0.9160Batch 219, Loss Value: 0.9208\n",
      "Batch 219, Gradient Norm: 0.6570\n",
      "\u001b[1m219/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m6s\u001b[0m 498ms/step - accuracy: 0.4987 - binary_io_u_5: 0.4929 - loss: 0.9160Batch 220, Loss Value: 0.9167\n",
      "Batch 220, Gradient Norm: 0.1278\n",
      "\u001b[1m220/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m5s\u001b[0m 498ms/step - accuracy: 0.4987 - binary_io_u_5: 0.4929 - loss: 0.9160Batch 221, Loss Value: 0.9112\n",
      "Batch 221, Gradient Norm: 0.1305\n",
      "\u001b[1m221/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m5s\u001b[0m 498ms/step - accuracy: 0.4987 - binary_io_u_5: 0.4929 - loss: 0.9160Batch 222, Loss Value: 0.9141\n",
      "Batch 222, Gradient Norm: 0.2134\n",
      "\u001b[1m222/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m4s\u001b[0m 498ms/step - accuracy: 0.4987 - binary_io_u_5: 0.4929 - loss: 0.9160Batch 223, Loss Value: 0.9029\n",
      "Batch 223, Gradient Norm: 0.0149\n",
      "\u001b[1m223/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m4s\u001b[0m 498ms/step - accuracy: 0.4987 - binary_io_u_5: 0.4929 - loss: 0.9160Batch 224, Loss Value: 0.8907\n",
      "Batch 224, Gradient Norm: 0.1413\n",
      "\u001b[1m224/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m3s\u001b[0m 497ms/step - accuracy: 0.4987 - binary_io_u_5: 0.4929 - loss: 0.9160Batch 225, Loss Value: 0.9102\n",
      "Batch 225, Gradient Norm: 0.0122\n",
      "\u001b[1m225/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m3s\u001b[0m 497ms/step - accuracy: 0.4988 - binary_io_u_5: 0.4929 - loss: 0.9160Batch 226, Loss Value: 0.8996\n",
      "Batch 226, Gradient Norm: 0.3813\n",
      "\u001b[1m226/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m2s\u001b[0m 497ms/step - accuracy: 0.4988 - binary_io_u_5: 0.4930 - loss: 0.9160Batch 227, Loss Value: 0.8957\n",
      "Batch 227, Gradient Norm: 0.0956\n",
      "\u001b[1m227/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m2s\u001b[0m 497ms/step - accuracy: 0.4988 - binary_io_u_5: 0.4930 - loss: 0.9160Batch 228, Loss Value: 0.9279\n",
      "Batch 228, Gradient Norm: 0.2107\n",
      "\u001b[1m228/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m1s\u001b[0m 497ms/step - accuracy: 0.4988 - binary_io_u_5: 0.4930 - loss: 0.9160Batch 229, Loss Value: 0.8821\n",
      "Batch 229, Gradient Norm: 0.7408\n",
      "\u001b[1m229/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m1s\u001b[0m 497ms/step - accuracy: 0.4988 - binary_io_u_5: 0.4930 - loss: 0.9160Batch 230, Loss Value: 0.8902\n",
      "Batch 230, Gradient Norm: 0.0059\n",
      "\u001b[1m230/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 497ms/step - accuracy: 0.4988 - binary_io_u_5: 0.4930 - loss: 0.9160Batch 231, Loss Value: 0.8987\n",
      "Batch 231, Gradient Norm: 0.0152\n",
      "\u001b[1m231/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 497ms/step - accuracy: 0.4988 - binary_io_u_5: 0.4930 - loss: 0.9160Batch 232, Loss Value: 0.8760\n",
      "Batch 232, Gradient Norm: 0.6496\n",
      "\u001b[1m232/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m117s\u001b[0m 503ms/step - accuracy: 0.4988 - binary_io_u_5: 0.4930 - loss: 0.9160 - val_accuracy: 0.4999 - val_binary_io_u_5: 0.4949 - val_loss: 0.9171 - learning_rate: 0.0010\n",
      "Epoch 9/200\n",
      "Batch 1, Loss Value: 0.9423\n",
      "Batch 1, Gradient Norm: 0.0000\n",
      "\u001b[1m  1/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:57\u001b[0m 507ms/step - accuracy: 0.5300 - binary_io_u_5: 0.4800 - loss: 0.9014Batch 2, Loss Value: 0.9190\n",
      "Batch 2, Gradient Norm: 0.0009\n",
      "\u001b[1m  2/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:58\u001b[0m 775ms/step - accuracy: 0.5250 - binary_io_u_5: 0.4900 - loss: 0.9034Batch 3, Loss Value: 0.9426\n",
      "Batch 3, Gradient Norm: 0.0002\n",
      "\u001b[1m  3/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:23\u001b[0m 626ms/step - accuracy: 0.5278 - binary_io_u_5: 0.5011 - loss: 0.9009Batch 4, Loss Value: 0.9423\n",
      "Batch 4, Gradient Norm: 0.0164\n",
      "\u001b[1m  4/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:11\u001b[0m 576ms/step - accuracy: 0.5271 - binary_io_u_5: 0.5052 - loss: 0.9006Batch 5, Loss Value: 0.9175\n",
      "Batch 5, Gradient Norm: 0.0000\n",
      "\u001b[1m  5/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:05\u001b[0m 553ms/step - accuracy: 0.5277 - binary_io_u_5: 0.5082 - loss: 0.9005Batch 6, Loss Value: 0.9303\n",
      "Batch 6, Gradient Norm: 0.0000\n",
      "\u001b[1m  6/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:02\u001b[0m 542ms/step - accuracy: 0.5303 - binary_io_u_5: 0.5126 - loss: 0.8990Batch 7, Loss Value: 0.9232\n",
      "Batch 7, Gradient Norm: 0.0010\n",
      "\u001b[1m  7/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:00\u001b[0m 534ms/step - accuracy: 0.5306 - binary_io_u_5: 0.5143 - loss: 0.8989Batch 8, Loss Value: 0.9232\n",
      "Batch 8, Gradient Norm: 0.0030\n",
      "\u001b[1m  8/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:57\u001b[0m 526ms/step - accuracy: 0.5303 - binary_io_u_5: 0.5153 - loss: 0.8991Batch 9, Loss Value: 0.9047\n",
      "Batch 9, Gradient Norm: 0.0002\n",
      "\u001b[1m  9/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:56\u001b[0m 521ms/step - accuracy: 0.5297 - binary_io_u_5: 0.5163 - loss: 0.8995Batch 10, Loss Value: 0.9033\n",
      "Batch 10, Gradient Norm: 0.0018\n",
      "\u001b[1m 10/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:54\u001b[0m 516ms/step - accuracy: 0.5287 - binary_io_u_5: 0.5165 - loss: 0.9002Batch 11, Loss Value: 0.9226\n",
      "Batch 11, Gradient Norm: 1.8676\n",
      "\u001b[1m 11/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:53\u001b[0m 512ms/step - accuracy: 0.5280 - binary_io_u_5: 0.5168 - loss: 0.9006Batch 12, Loss Value: 0.9232\n",
      "Batch 12, Gradient Norm: 0.0083\n",
      "\u001b[1m 12/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:51\u001b[0m 509ms/step - accuracy: 0.5276 - binary_io_u_5: 0.5173 - loss: 0.9009Batch 13, Loss Value: 0.9208\n",
      "Batch 13, Gradient Norm: 7.1701\n",
      "\u001b[1m 13/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:50\u001b[0m 506ms/step - accuracy: 0.5272 - binary_io_u_5: 0.5178 - loss: 0.9011Batch 14, Loss Value: 0.9297\n",
      "Batch 14, Gradient Norm: 0.2156\n",
      "\u001b[1m 14/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:49\u001b[0m 504ms/step - accuracy: 0.5273 - binary_io_u_5: 0.5185 - loss: 0.9010Batch 15, Loss Value: 0.9346\n",
      "Batch 15, Gradient Norm: 0.0011\n",
      "\u001b[1m 15/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:48\u001b[0m 502ms/step - accuracy: 0.5268 - binary_io_u_5: 0.5186 - loss: 0.9012Batch 16, Loss Value: 0.9283\n",
      "Batch 16, Gradient Norm: 0.9927\n",
      "\u001b[1m 16/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:48\u001b[0m 500ms/step - accuracy: 0.5263 - binary_io_u_5: 0.5183 - loss: 0.9014Batch 17, Loss Value: 0.9346\n",
      "Batch 17, Gradient Norm: 0.0001\n",
      "\u001b[1m 17/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:47\u001b[0m 500ms/step - accuracy: 0.5261 - binary_io_u_5: 0.5184 - loss: 0.9015Batch 18, Loss Value: 0.9346\n",
      "Batch 18, Gradient Norm: 0.0202\n",
      "\u001b[1m 18/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:46\u001b[0m 499ms/step - accuracy: 0.5260 - binary_io_u_5: 0.5187 - loss: 0.9014Batch 19, Loss Value: 0.9275\n",
      "Batch 19, Gradient Norm: 0.0018\n",
      "\u001b[1m 19/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:50\u001b[0m 518ms/step - accuracy: 0.5258 - binary_io_u_5: 0.5187 - loss: 0.9015Batch 20, Loss Value: 0.9346\n",
      "Batch 20, Gradient Norm: 0.0095\n",
      "\u001b[1m 20/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:55\u001b[0m 543ms/step - accuracy: 0.5258 - binary_io_u_5: 0.5191 - loss: 0.9013Batch 21, Loss Value: 0.9346\n",
      "Batch 21, Gradient Norm: 0.0016\n",
      "\u001b[1m 21/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:59\u001b[0m 565ms/step - accuracy: 0.5259 - binary_io_u_5: 0.5194 - loss: 0.9012Batch 22, Loss Value: 0.9204\n",
      "Batch 22, Gradient Norm: 0.0003\n",
      "\u001b[1m 22/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:02\u001b[0m 586ms/step - accuracy: 0.5259 - binary_io_u_5: 0.5196 - loss: 0.9011Batch 23, Loss Value: 0.9295\n",
      "Batch 23, Gradient Norm: 0.5041\n",
      "\u001b[1m 23/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:06\u001b[0m 603ms/step - accuracy: 0.5259 - binary_io_u_5: 0.5199 - loss: 0.9010Batch 24, Loss Value: 0.9203\n",
      "Batch 24, Gradient Norm: 0.0102\n",
      "\u001b[1m 24/232\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:08\u001b[0m 620ms/step - accuracy: 0.5259 - binary_io_u_5: 0.5200 - loss: 0.9010Batch 25, Loss Value: 0.9270\n",
      "Batch 25, Gradient Norm: 0.0465\n",
      "\u001b[1m 25/232\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:11\u001b[0m 635ms/step - accuracy: 0.5258 - binary_io_u_5: 0.5201 - loss: 0.9010Batch 26, Loss Value: 0.9241\n",
      "Batch 26, Gradient Norm: 0.4018\n",
      "\u001b[1m 26/232\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:13\u001b[0m 649ms/step - accuracy: 0.5255 - binary_io_u_5: 0.5200 - loss: 0.9010Batch 27, Loss Value: 0.9203\n",
      "Batch 27, Gradient Norm: 0.0051\n",
      "\u001b[1m 27/232\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:15\u001b[0m 663ms/step - accuracy: 0.5252 - binary_io_u_5: 0.5199 - loss: 0.9011Batch 28, Loss Value: 0.9204\n",
      "Batch 28, Gradient Norm: 0.0004\n",
      "\u001b[1m 28/232\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:17\u001b[0m 675ms/step - accuracy: 0.5250 - binary_io_u_5: 0.5198 - loss: 0.9012Batch 29, Loss Value: 0.9148\n",
      "Batch 29, Gradient Norm: 0.0052\n",
      "\u001b[1m 29/232\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:19\u001b[0m 687ms/step - accuracy: 0.5247 - binary_io_u_5: 0.5197 - loss: 0.9012Batch 30, Loss Value: 0.9204\n",
      "Batch 30, Gradient Norm: 0.0005\n",
      "\u001b[1m 30/232\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:20\u001b[0m 698ms/step - accuracy: 0.5244 - binary_io_u_5: 0.5195 - loss: 0.9013Batch 31, Loss Value: 0.9204\n",
      "Batch 31, Gradient Norm: 0.0003\n",
      "\u001b[1m 31/232\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:22\u001b[0m 708ms/step - accuracy: 0.5241 - binary_io_u_5: 0.5193 - loss: 0.9014Batch 32, Loss Value: 0.9204\n",
      "Batch 32, Gradient Norm: 0.0005\n",
      "\u001b[1m 32/232\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:23\u001b[0m 719ms/step - accuracy: 0.5237 - binary_io_u_5: 0.5190 - loss: 0.9016Batch 33, Loss Value: 0.9204\n",
      "Batch 33, Gradient Norm: 0.0001\n",
      "\u001b[1m 33/232\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:24\u001b[0m 728ms/step - accuracy: 0.5233 - binary_io_u_5: 0.5187 - loss: 0.9018Batch 34, Loss Value: 0.9277\n",
      "Batch 34, Gradient Norm: 0.0745\n",
      "\u001b[1m 34/232\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:22\u001b[0m 720ms/step - accuracy: 0.5228 - binary_io_u_5: 0.5183 - loss: 0.9019Batch 35, Loss Value: 0.9204\n",
      "Batch 35, Gradient Norm: 0.0003\n",
      "\u001b[1m 35/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:21\u001b[0m 718ms/step - accuracy: 0.5224 - binary_io_u_5: 0.5180 - loss: 0.9021Batch 36, Loss Value: 0.9199\n",
      "Batch 36, Gradient Norm: 0.0420\n",
      "\u001b[1m 36/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:21\u001b[0m 721ms/step - accuracy: 0.5220 - binary_io_u_5: 0.5177 - loss: 0.9023Batch 37, Loss Value: 0.9204\n",
      "Batch 37, Gradient Norm: 0.0045\n",
      "\u001b[1m 37/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:20\u001b[0m 720ms/step - accuracy: 0.5216 - binary_io_u_5: 0.5174 - loss: 0.9024Batch 38, Loss Value: 0.9204\n",
      "Batch 38, Gradient Norm: 0.0001\n",
      "\u001b[1m 38/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:21\u001b[0m 727ms/step - accuracy: 0.5212 - binary_io_u_5: 0.5171 - loss: 0.9025Batch 39, Loss Value: 0.9264\n",
      "Batch 39, Gradient Norm: 0.0889\n",
      "\u001b[1m 39/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:21\u001b[0m 734ms/step - accuracy: 0.5209 - binary_io_u_5: 0.5169 - loss: 0.9026Batch 40, Loss Value: 0.9201\n",
      "Batch 40, Gradient Norm: 0.0246\n",
      "\u001b[1m 40/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:22\u001b[0m 742ms/step - accuracy: 0.5206 - binary_io_u_5: 0.5166 - loss: 0.9027Batch 41, Loss Value: 0.9262\n",
      "Batch 41, Gradient Norm: 0.4576\n",
      "\u001b[1m 41/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:22\u001b[0m 748ms/step - accuracy: 0.5204 - binary_io_u_5: 0.5165 - loss: 0.9028Batch 42, Loss Value: 0.9216\n",
      "Batch 42, Gradient Norm: 0.0187\n",
      "\u001b[1m 42/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:23\u001b[0m 754ms/step - accuracy: 0.5201 - binary_io_u_5: 0.5163 - loss: 0.9029Batch 43, Loss Value: 0.9206\n",
      "Batch 43, Gradient Norm: 0.0202\n",
      "\u001b[1m 43/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:23\u001b[0m 760ms/step - accuracy: 0.5198 - binary_io_u_5: 0.5160 - loss: 0.9030Batch 44, Loss Value: 0.9202\n",
      "Batch 44, Gradient Norm: 0.0191\n",
      "\u001b[1m 44/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:23\u001b[0m 766ms/step - accuracy: 0.5194 - binary_io_u_5: 0.5157 - loss: 0.9031Batch 45, Loss Value: 0.9268\n",
      "Batch 45, Gradient Norm: 0.1116\n",
      "\u001b[1m 45/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:23\u001b[0m 770ms/step - accuracy: 0.5191 - binary_io_u_5: 0.5155 - loss: 0.9032Batch 46, Loss Value: 0.9169\n",
      "Batch 46, Gradient Norm: 0.7360\n",
      "\u001b[1m 46/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:24\u001b[0m 775ms/step - accuracy: 0.5188 - binary_io_u_5: 0.5152 - loss: 0.9033Batch 47, Loss Value: 0.9206\n",
      "Batch 47, Gradient Norm: 0.0129\n",
      "\u001b[1m 47/232\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:24\u001b[0m 779ms/step - accuracy: 0.5186 - binary_io_u_5: 0.5150 - loss: 0.9033Batch 48, Loss Value: 0.9204\n",
      "Batch 48, Gradient Norm: 0.0014\n",
      "\u001b[1m 48/232\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:24\u001b[0m 785ms/step - accuracy: 0.5183 - binary_io_u_5: 0.5148 - loss: 0.9034Batch 49, Loss Value: 0.9202\n",
      "Batch 49, Gradient Norm: 0.0130\n",
      "\u001b[1m 49/232\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:24\u001b[0m 789ms/step - accuracy: 0.5180 - binary_io_u_5: 0.5146 - loss: 0.9035Batch 50, Loss Value: 0.9204\n",
      "Batch 50, Gradient Norm: 0.0001\n",
      "\u001b[1m 50/232\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:24\u001b[0m 794ms/step - accuracy: 0.5177 - binary_io_u_5: 0.5143 - loss: 0.9036Batch 51, Loss Value: 0.9204\n",
      "Batch 51, Gradient Norm: 0.0035\n",
      "\u001b[1m 51/232\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:24\u001b[0m 798ms/step - accuracy: 0.5174 - binary_io_u_5: 0.5141 - loss: 0.9037Batch 52, Loss Value: 0.9218\n",
      "Batch 52, Gradient Norm: 0.0099\n",
      "\u001b[1m 52/232\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:23\u001b[0m 799ms/step - accuracy: 0.5171 - binary_io_u_5: 0.5138 - loss: 0.9038Batch 53, Loss Value: 0.9204\n",
      "Batch 53, Gradient Norm: 0.0042\n",
      "\u001b[1m 53/232\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:22\u001b[0m 797ms/step - accuracy: 0.5168 - binary_io_u_5: 0.5136 - loss: 0.9039Batch 54, Loss Value: 0.9275\n",
      "Batch 54, Gradient Norm: 0.0181\n",
      "\u001b[1m 54/232\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:22\u001b[0m 799ms/step - accuracy: 0.5166 - binary_io_u_5: 0.5134 - loss: 0.9039Batch 55, Loss Value: 0.9218\n",
      "Batch 55, Gradient Norm: 0.0015\n",
      "\u001b[1m 55/232\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:22\u001b[0m 803ms/step - accuracy: 0.5164 - binary_io_u_5: 0.5132 - loss: 0.9040Batch 56, Loss Value: 0.9204\n",
      "Batch 56, Gradient Norm: 0.0000\n",
      "\u001b[1m 56/232\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:21\u001b[0m 806ms/step - accuracy: 0.5161 - binary_io_u_5: 0.5131 - loss: 0.9041Batch 57, Loss Value: 0.9204\n",
      "Batch 57, Gradient Norm: 0.0010\n",
      "\u001b[1m 57/232\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:21\u001b[0m 809ms/step - accuracy: 0.5159 - binary_io_u_5: 0.5128 - loss: 0.9042Batch 58, Loss Value: 0.9274\n",
      "Batch 58, Gradient Norm: 1.3597\n",
      "\u001b[1m 58/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:21\u001b[0m 812ms/step - accuracy: 0.5156 - binary_io_u_5: 0.5126 - loss: 0.9042Batch 59, Loss Value: 0.9204\n",
      "Batch 59, Gradient Norm: 0.0001\n",
      "\u001b[1m 59/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:21\u001b[0m 815ms/step - accuracy: 0.5154 - binary_io_u_5: 0.5125 - loss: 0.9043Batch 60, Loss Value: 0.9177\n",
      "Batch 60, Gradient Norm: 0.1479\n",
      "\u001b[1m 60/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:20\u001b[0m 819ms/step - accuracy: 0.5152 - binary_io_u_5: 0.5123 - loss: 0.9044Batch 61, Loss Value: 0.9201\n",
      "Batch 61, Gradient Norm: 0.0229\n",
      "\u001b[1m 61/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:20\u001b[0m 822ms/step - accuracy: 0.5150 - binary_io_u_5: 0.5121 - loss: 0.9045Batch 62, Loss Value: 0.9143\n",
      "Batch 62, Gradient Norm: 0.0273\n",
      "\u001b[1m 62/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:20\u001b[0m 826ms/step - accuracy: 0.5148 - binary_io_u_5: 0.5119 - loss: 0.9046Batch 63, Loss Value: 0.9205\n",
      "Batch 63, Gradient Norm: 0.0199\n",
      "\u001b[1m 63/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:19\u001b[0m 828ms/step - accuracy: 0.5146 - binary_io_u_5: 0.5117 - loss: 0.9046Batch 64, Loss Value: 0.9197\n",
      "Batch 64, Gradient Norm: 0.0464\n",
      "\u001b[1m 64/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:19\u001b[0m 831ms/step - accuracy: 0.5144 - binary_io_u_5: 0.5116 - loss: 0.9047Batch 65, Loss Value: 0.9203\n",
      "Batch 65, Gradient Norm: 0.0108\n",
      "\u001b[1m 65/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:19\u001b[0m 834ms/step - accuracy: 0.5142 - binary_io_u_5: 0.5114 - loss: 0.9048Batch 66, Loss Value: 0.9204\n",
      "Batch 66, Gradient Norm: 0.0016\n",
      "\u001b[1m 66/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:18\u001b[0m 836ms/step - accuracy: 0.5140 - binary_io_u_5: 0.5113 - loss: 0.9048Batch 67, Loss Value: 0.9204\n",
      "Batch 67, Gradient Norm: 0.0013\n",
      "\u001b[1m 67/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:18\u001b[0m 838ms/step - accuracy: 0.5138 - binary_io_u_5: 0.5111 - loss: 0.9049Batch 68, Loss Value: 0.9203\n",
      "Batch 68, Gradient Norm: 0.0061\n",
      "\u001b[1m 68/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:17\u001b[0m 840ms/step - accuracy: 0.5135 - binary_io_u_5: 0.5109 - loss: 0.9050Batch 69, Loss Value: 0.9207\n",
      "Batch 69, Gradient Norm: 0.0353\n",
      "\u001b[1m 69/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:16\u001b[0m 839ms/step - accuracy: 0.5133 - binary_io_u_5: 0.5107 - loss: 0.9051Batch 70, Loss Value: 0.9196\n",
      "Batch 70, Gradient Norm: 0.0671\n",
      "\u001b[1m 70/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:15\u001b[0m 839ms/step - accuracy: 0.5131 - binary_io_u_5: 0.5105 - loss: 0.9052Batch 71, Loss Value: 0.9229\n",
      "Batch 71, Gradient Norm: 0.1832\n",
      "\u001b[1m 71/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:15\u001b[0m 842ms/step - accuracy: 0.5129 - binary_io_u_5: 0.5104 - loss: 0.9052Batch 72, Loss Value: 0.9204\n",
      "Batch 72, Gradient Norm: 0.0000\n",
      "\u001b[1m 72/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:14\u001b[0m 843ms/step - accuracy: 0.5128 - binary_io_u_5: 0.5102 - loss: 0.9053Batch 73, Loss Value: 0.9204\n",
      "Batch 73, Gradient Norm: 0.0002\n",
      "\u001b[1m 73/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:14\u001b[0m 845ms/step - accuracy: 0.5126 - binary_io_u_5: 0.5101 - loss: 0.9054Batch 74, Loss Value: 0.9203\n",
      "Batch 74, Gradient Norm: 0.0071\n",
      "\u001b[1m 74/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:13\u001b[0m 848ms/step - accuracy: 0.5124 - binary_io_u_5: 0.5099 - loss: 0.9055Batch 75, Loss Value: 0.9204\n",
      "Batch 75, Gradient Norm: 0.0007\n",
      "\u001b[1m 75/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:13\u001b[0m 849ms/step - accuracy: 0.5122 - binary_io_u_5: 0.5097 - loss: 0.9056Batch 76, Loss Value: 0.9201\n",
      "Batch 76, Gradient Norm: 0.0270\n",
      "\u001b[1m 76/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:12\u001b[0m 852ms/step - accuracy: 0.5120 - binary_io_u_5: 0.5096 - loss: 0.9056Batch 77, Loss Value: 0.9204\n",
      "Batch 77, Gradient Norm: 0.0006\n",
      "\u001b[1m 77/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:12\u001b[0m 854ms/step - accuracy: 0.5118 - binary_io_u_5: 0.5094 - loss: 0.9057Batch 78, Loss Value: 0.9157\n",
      "Batch 78, Gradient Norm: 0.2883\n",
      "\u001b[1m 78/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:11\u001b[0m 856ms/step - accuracy: 0.5116 - binary_io_u_5: 0.5093 - loss: 0.9058Batch 79, Loss Value: 0.9204\n",
      "Batch 79, Gradient Norm: 0.0010\n",
      "\u001b[1m 79/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:11\u001b[0m 857ms/step - accuracy: 0.5115 - binary_io_u_5: 0.5091 - loss: 0.9059Batch 80, Loss Value: 0.9197\n",
      "Batch 80, Gradient Norm: 0.0488\n",
      "\u001b[1m 80/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:10\u001b[0m 859ms/step - accuracy: 0.5113 - binary_io_u_5: 0.5090 - loss: 0.9059Batch 81, Loss Value: 0.9204\n",
      "Batch 81, Gradient Norm: 0.0027\n",
      "\u001b[1m 81/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:09\u001b[0m 860ms/step - accuracy: 0.5111 - binary_io_u_5: 0.5088 - loss: 0.9060Batch 82, Loss Value: 0.9204\n",
      "Batch 82, Gradient Norm: 0.0023\n",
      "\u001b[1m 82/232\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m2:09\u001b[0m 862ms/step - accuracy: 0.5110 - binary_io_u_5: 0.5087 - loss: 0.9061Batch 83, Loss Value: 0.9203\n",
      "Batch 83, Gradient Norm: 0.0079\n",
      "\u001b[1m 83/232\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m2:08\u001b[0m 864ms/step - accuracy: 0.5108 - binary_io_u_5: 0.5085 - loss: 0.9062Batch 84, Loss Value: 0.9288\n",
      "Batch 84, Gradient Norm: 0.4909\n",
      "\u001b[1m 84/232\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m2:08\u001b[0m 865ms/step - accuracy: 0.5106 - binary_io_u_5: 0.5084 - loss: 0.9062Batch 85, Loss Value: 0.9203\n",
      "Batch 85, Gradient Norm: 0.0045\n",
      "\u001b[1m 85/232\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m2:07\u001b[0m 865ms/step - accuracy: 0.5105 - binary_io_u_5: 0.5082 - loss: 0.9063Batch 86, Loss Value: 0.9203\n",
      "Batch 86, Gradient Norm: 0.0081\n",
      "\u001b[1m 86/232\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m2:06\u001b[0m 864ms/step - accuracy: 0.5103 - binary_io_u_5: 0.5081 - loss: 0.9064Batch 87, Loss Value: 0.9204\n",
      "Batch 87, Gradient Norm: 0.0009\n",
      "\u001b[1m 87/232\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m2:05\u001b[0m 865ms/step - accuracy: 0.5101 - binary_io_u_5: 0.5080 - loss: 0.9064Batch 88, Loss Value: 0.9204\n",
      "Batch 88, Gradient Norm: 0.0001\n",
      "\u001b[1m 88/232\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m2:04\u001b[0m 866ms/step - accuracy: 0.5100 - binary_io_u_5: 0.5078 - loss: 0.9065Batch 89, Loss Value: 0.9204\n",
      "Batch 89, Gradient Norm: 0.0002\n",
      "\u001b[1m 89/232\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m2:04\u001b[0m 868ms/step - accuracy: 0.5098 - binary_io_u_5: 0.5077 - loss: 0.9066Batch 90, Loss Value: 0.9204\n",
      "Batch 90, Gradient Norm: 0.0006\n",
      "\u001b[1m 90/232\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m2:03\u001b[0m 869ms/step - accuracy: 0.5097 - binary_io_u_5: 0.5075 - loss: 0.9067Batch 91, Loss Value: 0.9204\n",
      "Batch 91, Gradient Norm: 0.0000\n",
      "\u001b[1m 91/232\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m2:02\u001b[0m 870ms/step - accuracy: 0.5095 - binary_io_u_5: 0.5074 - loss: 0.9067Batch 92, Loss Value: 0.9204\n",
      "Batch 92, Gradient Norm: 0.0017\n",
      "\u001b[1m 92/232\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m2:02\u001b[0m 872ms/step - accuracy: 0.5094 - binary_io_u_5: 0.5073 - loss: 0.9068Batch 93, Loss Value: 0.9204\n",
      "Batch 93, Gradient Norm: 0.0003\n",
      "\u001b[1m 93/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m2:01\u001b[0m 873ms/step - accuracy: 0.5092 - binary_io_u_5: 0.5072 - loss: 0.9068Batch 94, Loss Value: 0.9203\n",
      "Batch 94, Gradient Norm: 0.0045\n",
      "\u001b[1m 94/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m2:00\u001b[0m 874ms/step - accuracy: 0.5091 - binary_io_u_5: 0.5071 - loss: 0.9069Batch 95, Loss Value: 0.9204\n",
      "Batch 95, Gradient Norm: 0.0001\n",
      "\u001b[1m 95/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m1:59\u001b[0m 876ms/step - accuracy: 0.5090 - binary_io_u_5: 0.5069 - loss: 0.9070Batch 96, Loss Value: 0.9202\n",
      "Batch 96, Gradient Norm: 0.0160\n",
      "\u001b[1m 96/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m1:59\u001b[0m 877ms/step - accuracy: 0.5088 - binary_io_u_5: 0.5068 - loss: 0.9070Batch 97, Loss Value: 0.9204\n",
      "Batch 97, Gradient Norm: 0.0011\n",
      "\u001b[1m 97/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m1:58\u001b[0m 878ms/step - accuracy: 0.5087 - binary_io_u_5: 0.5067 - loss: 0.9071Batch 98, Loss Value: 0.9204\n",
      "Batch 98, Gradient Norm: 0.0016\n",
      "\u001b[1m 98/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m1:57\u001b[0m 879ms/step - accuracy: 0.5086 - binary_io_u_5: 0.5066 - loss: 0.9071Batch 99, Loss Value: 0.9165\n",
      "Batch 99, Gradient Norm: 0.2522\n",
      "\u001b[1m 99/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m1:57\u001b[0m 880ms/step - accuracy: 0.5085 - binary_io_u_5: 0.5065 - loss: 0.9072Batch 100, Loss Value: 0.9196\n",
      "Batch 100, Gradient Norm: 0.0692\n",
      "\u001b[1m100/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m1:56\u001b[0m 881ms/step - accuracy: 0.5083 - binary_io_u_5: 0.5064 - loss: 0.9073Batch 101, Loss Value: 0.9201\n",
      "Batch 101, Gradient Norm: 0.1697\n",
      "\u001b[1m101/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m1:55\u001b[0m 882ms/step - accuracy: 0.5082 - binary_io_u_5: 0.5063 - loss: 0.9073Batch 102, Loss Value: 0.9204\n",
      "Batch 102, Gradient Norm: 0.0002\n",
      "\u001b[1m102/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m1:54\u001b[0m 881ms/step - accuracy: 0.5081 - binary_io_u_5: 0.5062 - loss: 0.9074Batch 103, Loss Value: 0.9204\n",
      "Batch 103, Gradient Norm: 0.0001\n",
      "\u001b[1m103/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m1:53\u001b[0m 881ms/step - accuracy: 0.5080 - binary_io_u_5: 0.5060 - loss: 0.9074Batch 104, Loss Value: 0.9204\n",
      "Batch 104, Gradient Norm: 0.0002\n",
      "\u001b[1m104/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m1:52\u001b[0m 882ms/step - accuracy: 0.5078 - binary_io_u_5: 0.5059 - loss: 0.9075Batch 105, Loss Value: 0.9204\n",
      "Batch 105, Gradient Norm: 0.0002\n",
      "\u001b[1m105/232\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m1:52\u001b[0m 883ms/step - accuracy: 0.5077 - binary_io_u_5: 0.5058 - loss: 0.9075Batch 106, Loss Value: 0.9204\n",
      "Batch 106, Gradient Norm: 0.0006\n",
      "\u001b[1m106/232\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m1:51\u001b[0m 884ms/step - accuracy: 0.5076 - binary_io_u_5: 0.5057 - loss: 0.9076Batch 107, Loss Value: 0.9202\n",
      "Batch 107, Gradient Norm: 0.0179\n",
      "\u001b[1m107/232\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m1:50\u001b[0m 886ms/step - accuracy: 0.5075 - binary_io_u_5: 0.5056 - loss: 0.9077Batch 108, Loss Value: 0.9191\n",
      "Batch 108, Gradient Norm: 0.0921\n",
      "\u001b[1m108/232\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m1:49\u001b[0m 887ms/step - accuracy: 0.5074 - binary_io_u_5: 0.5055 - loss: 0.9077Batch 109, Loss Value: 0.9204\n",
      "Batch 109, Gradient Norm: 0.0002\n",
      "\u001b[1m109/232\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m1:49\u001b[0m 887ms/step - accuracy: 0.5073 - binary_io_u_5: 0.5055 - loss: 0.9078Batch 110, Loss Value: 0.9202\n",
      "Batch 110, Gradient Norm: 0.0160\n",
      "\u001b[1m110/232\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m1:48\u001b[0m 889ms/step - accuracy: 0.5072 - binary_io_u_5: 0.5054 - loss: 0.9078Batch 111, Loss Value: 0.9204\n",
      "Batch 111, Gradient Norm: 0.0005\n",
      "\u001b[1m111/232\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m1:47\u001b[0m 889ms/step - accuracy: 0.5071 - binary_io_u_5: 0.5053 - loss: 0.9079Batch 112, Loss Value: 0.9203\n",
      "Batch 112, Gradient Norm: 0.0059\n",
      "\u001b[1m112/232\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m1:46\u001b[0m 890ms/step - accuracy: 0.5070 - binary_io_u_5: 0.5052 - loss: 0.9079Batch 113, Loss Value: 0.9204\n",
      "Batch 113, Gradient Norm: 0.0000\n",
      "\u001b[1m113/232\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m1:46\u001b[0m 891ms/step - accuracy: 0.5069 - binary_io_u_5: 0.5051 - loss: 0.9080Batch 114, Loss Value: 0.9204\n",
      "Batch 114, Gradient Norm: 0.0004\n",
      "\u001b[1m114/232\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m1:45\u001b[0m 892ms/step - accuracy: 0.5068 - binary_io_u_5: 0.5050 - loss: 0.9080Batch 115, Loss Value: 0.9204\n",
      "Batch 115, Gradient Norm: 0.0004\n",
      "\u001b[1m115/232\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m1:44\u001b[0m 893ms/step - accuracy: 0.5067 - binary_io_u_5: 0.5049 - loss: 0.9081Batch 116, Loss Value: 0.9196\n",
      "Batch 116, Gradient Norm: 0.0568\n",
      "\u001b[1m116/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m1:43\u001b[0m 894ms/step - accuracy: 0.5066 - binary_io_u_5: 0.5048 - loss: 0.9081Batch 117, Loss Value: 0.9203\n",
      "Batch 117, Gradient Norm: 0.0096\n",
      "\u001b[1m117/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m1:42\u001b[0m 895ms/step - accuracy: 0.5065 - binary_io_u_5: 0.5048 - loss: 0.9081Batch 118, Loss Value: 0.9204\n",
      "Batch 118, Gradient Norm: 0.0001\n",
      "\u001b[1m118/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m1:41\u001b[0m 894ms/step - accuracy: 0.5064 - binary_io_u_5: 0.5047 - loss: 0.9082Batch 119, Loss Value: 0.9204\n",
      "Batch 119, Gradient Norm: 0.0006\n",
      "\u001b[1m119/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m1:40\u001b[0m 893ms/step - accuracy: 0.5063 - binary_io_u_5: 0.5046 - loss: 0.9082Batch 120, Loss Value: 0.9204\n",
      "Batch 120, Gradient Norm: 0.0003\n",
      "\u001b[1m120/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m1:40\u001b[0m 894ms/step - accuracy: 0.5062 - binary_io_u_5: 0.5046 - loss: 0.9083Batch 121, Loss Value: 0.9204\n",
      "Batch 121, Gradient Norm: 0.0002\n",
      "\u001b[1m121/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m1:39\u001b[0m 895ms/step - accuracy: 0.5062 - binary_io_u_5: 0.5045 - loss: 0.9083Batch 122, Loss Value: 0.9204\n",
      "Batch 122, Gradient Norm: 0.0000\n",
      "\u001b[1m122/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m1:38\u001b[0m 896ms/step - accuracy: 0.5061 - binary_io_u_5: 0.5044 - loss: 0.9083Batch 123, Loss Value: 0.9178\n",
      "Batch 123, Gradient Norm: 0.6854\n",
      "\u001b[1m123/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m1:37\u001b[0m 897ms/step - accuracy: 0.5060 - binary_io_u_5: 0.5044 - loss: 0.9084Batch 124, Loss Value: 0.9204\n",
      "Batch 124, Gradient Norm: 0.0014\n",
      "\u001b[1m124/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m1:36\u001b[0m 897ms/step - accuracy: 0.5060 - binary_io_u_5: 0.5043 - loss: 0.9084Batch 125, Loss Value: 0.9202\n",
      "Batch 125, Gradient Norm: 0.0119\n",
      "\u001b[1m125/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m1:36\u001b[0m 898ms/step - accuracy: 0.5059 - binary_io_u_5: 0.5043 - loss: 0.9084Batch 126, Loss Value: 0.9204\n",
      "Batch 126, Gradient Norm: 0.0049\n",
      "\u001b[1m126/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m1:35\u001b[0m 899ms/step - accuracy: 0.5058 - binary_io_u_5: 0.5042 - loss: 0.9085Batch 127, Loss Value: 0.9204\n",
      "Batch 127, Gradient Norm: 0.0001\n",
      "\u001b[1m127/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m1:34\u001b[0m 900ms/step - accuracy: 0.5058 - binary_io_u_5: 0.5042 - loss: 0.9085Batch 128, Loss Value: 0.9204\n",
      "Batch 128, Gradient Norm: 0.0008\n",
      "\u001b[1m128/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m1:33\u001b[0m 901ms/step - accuracy: 0.5057 - binary_io_u_5: 0.5041 - loss: 0.9085Batch 129, Loss Value: 0.9201\n",
      "Batch 129, Gradient Norm: 0.0189\n",
      "\u001b[1m129/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m1:32\u001b[0m 901ms/step - accuracy: 0.5056 - binary_io_u_5: 0.5041 - loss: 0.9085Batch 130, Loss Value: 0.9204\n",
      "Batch 130, Gradient Norm: 0.0025\n",
      "\u001b[1m130/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m1:32\u001b[0m 902ms/step - accuracy: 0.5056 - binary_io_u_5: 0.5040 - loss: 0.9086Batch 131, Loss Value: 0.9200\n",
      "Batch 131, Gradient Norm: 0.0342\n",
      "\u001b[1m131/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m1:31\u001b[0m 903ms/step - accuracy: 0.5055 - binary_io_u_5: 0.5039 - loss: 0.9086Batch 132, Loss Value: 0.9204\n",
      "Batch 132, Gradient Norm: 0.0030\n",
      "\u001b[1m132/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m1:30\u001b[0m 904ms/step - accuracy: 0.5054 - binary_io_u_5: 0.5039 - loss: 0.9086Batch 133, Loss Value: 0.9203\n",
      "Batch 133, Gradient Norm: 0.0115\n",
      "\u001b[1m133/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m1:29\u001b[0m 904ms/step - accuracy: 0.5054 - binary_io_u_5: 0.5038 - loss: 0.9087Batch 134, Loss Value: 0.9203\n",
      "Batch 134, Gradient Norm: 0.0072\n",
      "\u001b[1m134/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m1:28\u001b[0m 904ms/step - accuracy: 0.5053 - binary_io_u_5: 0.5038 - loss: 0.9087Batch 135, Loss Value: 0.9204\n",
      "Batch 135, Gradient Norm: 0.0002\n",
      "\u001b[1m135/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m1:27\u001b[0m 904ms/step - accuracy: 0.5052 - binary_io_u_5: 0.5037 - loss: 0.9087Batch 136, Loss Value: 0.9249\n",
      "Batch 136, Gradient Norm: 0.2912\n",
      "\u001b[1m136/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m1:26\u001b[0m 903ms/step - accuracy: 0.5052 - binary_io_u_5: 0.5036 - loss: 0.9088Batch 137, Loss Value: 0.9193\n",
      "Batch 137, Gradient Norm: 0.0619\n",
      "\u001b[1m137/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m1:25\u001b[0m 904ms/step - accuracy: 0.5051 - binary_io_u_5: 0.5036 - loss: 0.9088Batch 138, Loss Value: 0.9181\n",
      "Batch 138, Gradient Norm: 0.1183\n",
      "\u001b[1m138/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m1:25\u001b[0m 905ms/step - accuracy: 0.5050 - binary_io_u_5: 0.5035 - loss: 0.9088Batch 139, Loss Value: 0.9201\n",
      "Batch 139, Gradient Norm: 0.0257\n",
      "\u001b[1m139/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m1:24\u001b[0m 906ms/step - accuracy: 0.5050 - binary_io_u_5: 0.5035 - loss: 0.9088Batch 140, Loss Value: 0.9204\n",
      "Batch 140, Gradient Norm: 0.0015\n",
      "\u001b[1m140/232\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m1:23\u001b[0m 906ms/step - accuracy: 0.5049 - binary_io_u_5: 0.5034 - loss: 0.9089Batch 141, Loss Value: 0.9182\n",
      "Batch 141, Gradient Norm: 0.1456\n",
      "\u001b[1m141/232\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m1:22\u001b[0m 907ms/step - accuracy: 0.5048 - binary_io_u_5: 0.5034 - loss: 0.9089Batch 142, Loss Value: 0.9199\n",
      "Batch 142, Gradient Norm: 0.0348\n",
      "\u001b[1m142/232\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m1:21\u001b[0m 908ms/step - accuracy: 0.5048 - binary_io_u_5: 0.5033 - loss: 0.9089Batch 143, Loss Value: 0.9204\n",
      "Batch 143, Gradient Norm: 0.0041\n",
      "\u001b[1m143/232\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m1:20\u001b[0m 908ms/step - accuracy: 0.5047 - binary_io_u_5: 0.5032 - loss: 0.9090Batch 144, Loss Value: 0.9199\n",
      "Batch 144, Gradient Norm: 0.0362\n",
      "\u001b[1m144/232\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m1:19\u001b[0m 909ms/step - accuracy: 0.5047 - binary_io_u_5: 0.5032 - loss: 0.9090Batch 145, Loss Value: 0.9085\n",
      "Batch 145, Gradient Norm: 0.2956\n",
      "\u001b[1m145/232\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m1:19\u001b[0m 909ms/step - accuracy: 0.5046 - binary_io_u_5: 0.5031 - loss: 0.9090Batch 146, Loss Value: 0.9188\n",
      "Batch 146, Gradient Norm: 0.1011\n",
      "\u001b[1m146/232\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m1:18\u001b[0m 910ms/step - accuracy: 0.5045 - binary_io_u_5: 0.5031 - loss: 0.9090Batch 147, Loss Value: 0.9173\n",
      "Batch 147, Gradient Norm: 0.0741\n",
      "\u001b[1m147/232\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m1:17\u001b[0m 910ms/step - accuracy: 0.5045 - binary_io_u_5: 0.5030 - loss: 0.9091Batch 148, Loss Value: 0.9143\n",
      "Batch 148, Gradient Norm: 0.2548\n",
      "\u001b[1m148/232\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m1:16\u001b[0m 911ms/step - accuracy: 0.5044 - binary_io_u_5: 0.5030 - loss: 0.9091Batch 149, Loss Value: 0.9363\n",
      "Batch 149, Gradient Norm: 0.2119\n",
      "\u001b[1m149/232\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m1:15\u001b[0m 911ms/step - accuracy: 0.5044 - binary_io_u_5: 0.5029 - loss: 0.9091Batch 150, Loss Value: 0.9191\n",
      "Batch 150, Gradient Norm: 0.0925\n",
      "\u001b[1m150/232\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m1:14\u001b[0m 912ms/step - accuracy: 0.5043 - binary_io_u_5: 0.5029 - loss: 0.9091Batch 151, Loss Value: 0.9245\n",
      "Batch 151, Gradient Norm: 0.2573\n",
      "\u001b[1m151/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m1:13\u001b[0m 912ms/step - accuracy: 0.5042 - binary_io_u_5: 0.5028 - loss: 0.9092Batch 152, Loss Value: 0.9213\n",
      "Batch 152, Gradient Norm: 0.0979\n",
      "\u001b[1m152/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m1:12\u001b[0m 911ms/step - accuracy: 0.5042 - binary_io_u_5: 0.5028 - loss: 0.9092Batch 153, Loss Value: 0.8979\n",
      "Batch 153, Gradient Norm: 0.0894\n",
      "\u001b[1m153/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m1:11\u001b[0m 911ms/step - accuracy: 0.5041 - binary_io_u_5: 0.5027 - loss: 0.9092Batch 154, Loss Value: 0.9060\n",
      "Batch 154, Gradient Norm: 0.4536\n",
      "\u001b[1m154/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m1:11\u001b[0m 912ms/step - accuracy: 0.5040 - binary_io_u_5: 0.5027 - loss: 0.9092Batch 155, Loss Value: 0.9213\n",
      "Batch 155, Gradient Norm: 0.0465\n",
      "\u001b[1m155/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m1:10\u001b[0m 912ms/step - accuracy: 0.5040 - binary_io_u_5: 0.5026 - loss: 0.9093Batch 156, Loss Value: 0.9198\n",
      "Batch 156, Gradient Norm: 0.0354\n",
      "\u001b[1m156/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m1:09\u001b[0m 913ms/step - accuracy: 0.5039 - binary_io_u_5: 0.5026 - loss: 0.9093Batch 157, Loss Value: 0.9136\n",
      "Batch 157, Gradient Norm: 0.1642\n",
      "\u001b[1m157/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m1:08\u001b[0m 913ms/step - accuracy: 0.5039 - binary_io_u_5: 0.5025 - loss: 0.9093Batch 158, Loss Value: 0.9187\n",
      "Batch 158, Gradient Norm: 0.7328\n",
      "\u001b[1m158/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m1:07\u001b[0m 914ms/step - accuracy: 0.5038 - binary_io_u_5: 0.5025 - loss: 0.9093Batch 159, Loss Value: 0.9244\n",
      "Batch 159, Gradient Norm: 0.1463\n",
      "\u001b[1m159/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m1:06\u001b[0m 915ms/step - accuracy: 0.5038 - binary_io_u_5: 0.5024 - loss: 0.9094Batch 160, Loss Value: 0.9177\n",
      "Batch 160, Gradient Norm: 0.1627\n",
      "\u001b[1m160/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m1:05\u001b[0m 915ms/step - accuracy: 0.5037 - binary_io_u_5: 0.5024 - loss: 0.9094Batch 161, Loss Value: 0.9109\n",
      "Batch 161, Gradient Norm: 0.2177\n",
      "\u001b[1m161/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m1:05\u001b[0m 916ms/step - accuracy: 0.5037 - binary_io_u_5: 0.5023 - loss: 0.9094Batch 162, Loss Value: 0.9155\n",
      "Batch 162, Gradient Norm: 0.2800\n",
      "\u001b[1m162/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m1:04\u001b[0m 917ms/step - accuracy: 0.5036 - binary_io_u_5: 0.5023 - loss: 0.9094Batch 163, Loss Value: 0.9225\n",
      "Batch 163, Gradient Norm: 0.2402\n",
      "\u001b[1m163/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m1:03\u001b[0m 918ms/step - accuracy: 0.5036 - binary_io_u_5: 0.5022 - loss: 0.9095Batch 164, Loss Value: 0.9176\n",
      "Batch 164, Gradient Norm: 0.1636\n",
      "\u001b[1m164/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m1:02\u001b[0m 918ms/step - accuracy: 0.5035 - binary_io_u_5: 0.5022 - loss: 0.9095Batch 165, Loss Value: 0.9201\n",
      "Batch 165, Gradient Norm: 0.0212\n",
      "\u001b[1m165/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m1:01\u001b[0m 919ms/step - accuracy: 0.5035 - binary_io_u_5: 0.5022 - loss: 0.9095Batch 166, Loss Value: 0.9195\n",
      "Batch 166, Gradient Norm: 0.0576\n",
      "\u001b[1m166/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m1:00\u001b[0m 919ms/step - accuracy: 0.5034 - binary_io_u_5: 0.5021 - loss: 0.9095Batch 167, Loss Value: 0.9204\n",
      "Batch 167, Gradient Norm: 0.0034\n",
      "\u001b[1m167/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m59s\u001b[0m 920ms/step - accuracy: 0.5034 - binary_io_u_5: 0.5021 - loss: 0.9095 Batch 168, Loss Value: 0.9133\n",
      "Batch 168, Gradient Norm: 0.4021\n",
      "\u001b[1m168/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m58s\u001b[0m 920ms/step - accuracy: 0.5033 - binary_io_u_5: 0.5020 - loss: 0.9096Batch 169, Loss Value: 0.9204\n",
      "Batch 169, Gradient Norm: 0.0012\n",
      "\u001b[1m169/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m57s\u001b[0m 919ms/step - accuracy: 0.5033 - binary_io_u_5: 0.5020 - loss: 0.9096Batch 170, Loss Value: 0.9115\n",
      "Batch 170, Gradient Norm: 0.2694\n",
      "\u001b[1m170/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m56s\u001b[0m 919ms/step - accuracy: 0.5032 - binary_io_u_5: 0.5019 - loss: 0.9096Batch 171, Loss Value: 0.9253\n",
      "Batch 171, Gradient Norm: 0.2025\n",
      "\u001b[1m171/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m56s\u001b[0m 920ms/step - accuracy: 0.5032 - binary_io_u_5: 0.5019 - loss: 0.9096Batch 172, Loss Value: 0.9200\n",
      "Batch 172, Gradient Norm: 0.0684\n",
      "\u001b[1m172/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m55s\u001b[0m 920ms/step - accuracy: 0.5031 - binary_io_u_5: 0.5018 - loss: 0.9097Batch 173, Loss Value: 0.9185\n",
      "Batch 173, Gradient Norm: 0.1115\n",
      "\u001b[1m173/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m54s\u001b[0m 920ms/step - accuracy: 0.5031 - binary_io_u_5: 0.5018 - loss: 0.9097Batch 174, Loss Value: 0.9198\n",
      "Batch 174, Gradient Norm: 0.0391\n",
      "\u001b[1m174/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m53s\u001b[0m 921ms/step - accuracy: 0.5030 - binary_io_u_5: 0.5018 - loss: 0.9097Batch 175, Loss Value: 0.9147\n",
      "Batch 175, Gradient Norm: 0.2877\n",
      "\u001b[1m175/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m52s\u001b[0m 921ms/step - accuracy: 0.5030 - binary_io_u_5: 0.5017 - loss: 0.9097Batch 176, Loss Value: 0.9204\n",
      "Batch 176, Gradient Norm: 0.0016\n",
      "\u001b[1m176/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m51s\u001b[0m 921ms/step - accuracy: 0.5029 - binary_io_u_5: 0.5017 - loss: 0.9097Batch 177, Loss Value: 0.9204\n",
      "Batch 177, Gradient Norm: 0.0006\n",
      "\u001b[1m177/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m50s\u001b[0m 922ms/step - accuracy: 0.5029 - binary_io_u_5: 0.5016 - loss: 0.9098Batch 178, Loss Value: 0.9203\n",
      "Batch 178, Gradient Norm: 0.0076\n",
      "\u001b[1m178/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m49s\u001b[0m 922ms/step - accuracy: 0.5028 - binary_io_u_5: 0.5016 - loss: 0.9098Batch 179, Loss Value: 0.9204\n",
      "Batch 179, Gradient Norm: 0.0012\n",
      "\u001b[1m179/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m48s\u001b[0m 922ms/step - accuracy: 0.5028 - binary_io_u_5: 0.5016 - loss: 0.9098Batch 180, Loss Value: 0.9041\n",
      "Batch 180, Gradient Norm: 0.4181\n",
      "\u001b[1m180/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m47s\u001b[0m 923ms/step - accuracy: 0.5028 - binary_io_u_5: 0.5015 - loss: 0.9098Batch 181, Loss Value: 0.9154\n",
      "Batch 181, Gradient Norm: 0.2512\n",
      "\u001b[1m181/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m47s\u001b[0m 923ms/step - accuracy: 0.5027 - binary_io_u_5: 0.5015 - loss: 0.9098Batch 182, Loss Value: 0.9203\n",
      "Batch 182, Gradient Norm: 0.0070\n",
      "\u001b[1m182/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m46s\u001b[0m 923ms/step - accuracy: 0.5027 - binary_io_u_5: 0.5015 - loss: 0.9099Batch 183, Loss Value: 0.9092\n",
      "Batch 183, Gradient Norm: 0.0626\n",
      "\u001b[1m183/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m45s\u001b[0m 924ms/step - accuracy: 0.5026 - binary_io_u_5: 0.5014 - loss: 0.9099Batch 184, Loss Value: 0.9189\n",
      "Batch 184, Gradient Norm: 0.0968\n",
      "\u001b[1m184/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m44s\u001b[0m 924ms/step - accuracy: 0.5026 - binary_io_u_5: 0.5014 - loss: 0.9099Batch 185, Loss Value: 0.9184\n",
      "Batch 185, Gradient Norm: 0.1180\n",
      "\u001b[1m185/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m43s\u001b[0m 923ms/step - accuracy: 0.5025 - binary_io_u_5: 0.5013 - loss: 0.9099Batch 186, Loss Value: 0.9183\n",
      "Batch 186, Gradient Norm: 0.0517\n",
      "\u001b[1m186/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m42s\u001b[0m 923ms/step - accuracy: 0.5025 - binary_io_u_5: 0.5013 - loss: 0.9099Batch 187, Loss Value: 0.9199\n",
      "Batch 187, Gradient Norm: 0.0330\n",
      "\u001b[1m187/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m41s\u001b[0m 923ms/step - accuracy: 0.5025 - binary_io_u_5: 0.5013 - loss: 0.9100Batch 188, Loss Value: 0.9148\n",
      "Batch 188, Gradient Norm: 0.0712\n",
      "\u001b[1m188/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m40s\u001b[0m 924ms/step - accuracy: 0.5024 - binary_io_u_5: 0.5012 - loss: 0.9100Batch 189, Loss Value: 0.9177\n",
      "Batch 189, Gradient Norm: 0.2595\n",
      "\u001b[1m189/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m39s\u001b[0m 924ms/step - accuracy: 0.5024 - binary_io_u_5: 0.5012 - loss: 0.9100Batch 190, Loss Value: 0.9131\n",
      "Batch 190, Gradient Norm: 0.2633\n",
      "\u001b[1m190/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m38s\u001b[0m 925ms/step - accuracy: 0.5023 - binary_io_u_5: 0.5012 - loss: 0.9100Batch 191, Loss Value: 0.8990\n",
      "Batch 191, Gradient Norm: 0.0558\n",
      "\u001b[1m191/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m37s\u001b[0m 925ms/step - accuracy: 0.5023 - binary_io_u_5: 0.5011 - loss: 0.9100Batch 192, Loss Value: 0.9158\n",
      "Batch 192, Gradient Norm: 0.1710\n",
      "\u001b[1m192/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m37s\u001b[0m 926ms/step - accuracy: 0.5022 - binary_io_u_5: 0.5011 - loss: 0.9101Batch 193, Loss Value: 0.9122\n",
      "Batch 193, Gradient Norm: 0.3315\n",
      "\u001b[1m193/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m36s\u001b[0m 926ms/step - accuracy: 0.5022 - binary_io_u_5: 0.5010 - loss: 0.9101Batch 194, Loss Value: 0.9223\n",
      "Batch 194, Gradient Norm: 0.2232\n",
      "\u001b[1m194/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m35s\u001b[0m 926ms/step - accuracy: 0.5022 - binary_io_u_5: 0.5010 - loss: 0.9101Batch 195, Loss Value: 0.9207\n",
      "Batch 195, Gradient Norm: 0.2349\n",
      "\u001b[1m195/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m34s\u001b[0m 927ms/step - accuracy: 0.5021 - binary_io_u_5: 0.5010 - loss: 0.9101Batch 196, Loss Value: 0.9194\n",
      "Batch 196, Gradient Norm: 0.0644\n",
      "\u001b[1m196/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m33s\u001b[0m 927ms/step - accuracy: 0.5021 - binary_io_u_5: 0.5009 - loss: 0.9101Batch 197, Loss Value: 0.9059\n",
      "Batch 197, Gradient Norm: 0.2290\n",
      "\u001b[1m197/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m32s\u001b[0m 928ms/step - accuracy: 0.5020 - binary_io_u_5: 0.5009 - loss: 0.9102Batch 198, Loss Value: 0.9201\n",
      "Batch 198, Gradient Norm: 0.0208\n",
      "\u001b[1m198/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m31s\u001b[0m 928ms/step - accuracy: 0.5020 - binary_io_u_5: 0.5009 - loss: 0.9102Batch 199, Loss Value: 0.9193\n",
      "Batch 199, Gradient Norm: 0.0667\n",
      "\u001b[1m199/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m30s\u001b[0m 928ms/step - accuracy: 0.5020 - binary_io_u_5: 0.5009 - loss: 0.9102Batch 200, Loss Value: 0.9203\n",
      "Batch 200, Gradient Norm: 0.0085\n",
      "\u001b[1m200/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m29s\u001b[0m 928ms/step - accuracy: 0.5019 - binary_io_u_5: 0.5008 - loss: 0.9102Batch 201, Loss Value: 0.9111\n",
      "Batch 201, Gradient Norm: 0.3799\n",
      "\u001b[1m201/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m28s\u001b[0m 928ms/step - accuracy: 0.5019 - binary_io_u_5: 0.5008 - loss: 0.9102Batch 202, Loss Value: 0.9177\n",
      "Batch 202, Gradient Norm: 0.1693\n",
      "\u001b[1m202/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m27s\u001b[0m 928ms/step - accuracy: 0.5019 - binary_io_u_5: 0.5008 - loss: 0.9102Batch 203, Loss Value: 0.9204\n",
      "Batch 203, Gradient Norm: 0.0017\n",
      "\u001b[1m203/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m26s\u001b[0m 926ms/step - accuracy: 0.5018 - binary_io_u_5: 0.5007 - loss: 0.9103Batch 204, Loss Value: 0.9128\n",
      "Batch 204, Gradient Norm: 0.3497\n",
      "\u001b[1m204/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m25s\u001b[0m 927ms/step - accuracy: 0.5018 - binary_io_u_5: 0.5007 - loss: 0.9103Batch 205, Loss Value: 0.9199\n",
      "Batch 205, Gradient Norm: 0.0793\n",
      "\u001b[1m205/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m25s\u001b[0m 927ms/step - accuracy: 0.5018 - binary_io_u_5: 0.5007 - loss: 0.9103Batch 206, Loss Value: 0.9199\n",
      "Batch 206, Gradient Norm: 0.0335\n",
      "\u001b[1m206/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m24s\u001b[0m 927ms/step - accuracy: 0.5017 - binary_io_u_5: 0.5007 - loss: 0.9103Batch 207, Loss Value: 0.9193\n",
      "Batch 207, Gradient Norm: 0.0745\n",
      "\u001b[1m207/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m23s\u001b[0m 928ms/step - accuracy: 0.5017 - binary_io_u_5: 0.5006 - loss: 0.9103Batch 208, Loss Value: 0.9172\n",
      "Batch 208, Gradient Norm: 0.0985\n",
      "\u001b[1m208/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m22s\u001b[0m 928ms/step - accuracy: 0.5017 - binary_io_u_5: 0.5006 - loss: 0.9103Batch 209, Loss Value: 0.9170\n",
      "Batch 209, Gradient Norm: 0.2038\n",
      "\u001b[1m209/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m21s\u001b[0m 928ms/step - accuracy: 0.5017 - binary_io_u_5: 0.5006 - loss: 0.9103Batch 210, Loss Value: 0.9079\n",
      "Batch 210, Gradient Norm: 0.2094\n",
      "\u001b[1m210/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m20s\u001b[0m 928ms/step - accuracy: 0.5016 - binary_io_u_5: 0.5006 - loss: 0.9103Batch 211, Loss Value: 0.9418\n",
      "Batch 211, Gradient Norm: 0.2057\n",
      "\u001b[1m211/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m19s\u001b[0m 929ms/step - accuracy: 0.5016 - binary_io_u_5: 0.5005 - loss: 0.9104Batch 212, Loss Value: 0.9058\n",
      "Batch 212, Gradient Norm: 0.2603\n",
      "\u001b[1m212/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m18s\u001b[0m 929ms/step - accuracy: 0.5016 - binary_io_u_5: 0.5005 - loss: 0.9104Batch 213, Loss Value: 0.9157\n",
      "Batch 213, Gradient Norm: 0.1200\n",
      "\u001b[1m213/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m17s\u001b[0m 929ms/step - accuracy: 0.5015 - binary_io_u_5: 0.5005 - loss: 0.9104Batch 214, Loss Value: 0.9042\n",
      "Batch 214, Gradient Norm: 0.3153\n",
      "\u001b[1m214/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m16s\u001b[0m 929ms/step - accuracy: 0.5015 - binary_io_u_5: 0.5005 - loss: 0.9104Batch 215, Loss Value: 0.9229\n",
      "Batch 215, Gradient Norm: 0.3425\n",
      "\u001b[1m215/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m15s\u001b[0m 929ms/step - accuracy: 0.5015 - binary_io_u_5: 0.5004 - loss: 0.9104Batch 216, Loss Value: 0.8984\n",
      "Batch 216, Gradient Norm: 0.5852\n",
      "\u001b[1m216/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m14s\u001b[0m 930ms/step - accuracy: 0.5015 - binary_io_u_5: 0.5004 - loss: 0.9104Batch 217, Loss Value: 0.9379\n",
      "Batch 217, Gradient Norm: 0.3885\n",
      "\u001b[1m217/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m13s\u001b[0m 930ms/step - accuracy: 0.5014 - binary_io_u_5: 0.5004 - loss: 0.9104Batch 218, Loss Value: 0.8968\n",
      "Batch 218, Gradient Norm: 0.3614\n",
      "\u001b[1m218/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m13s\u001b[0m 930ms/step - accuracy: 0.5014 - binary_io_u_5: 0.5004 - loss: 0.9104Batch 219, Loss Value: 0.9414\n",
      "Batch 219, Gradient Norm: 0.3845\n",
      "\u001b[1m219/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m12s\u001b[0m 929ms/step - accuracy: 0.5014 - binary_io_u_5: 0.5004 - loss: 0.9105Batch 220, Loss Value: 0.9253\n",
      "Batch 220, Gradient Norm: 0.3445\n",
      "\u001b[1m220/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m11s\u001b[0m 929ms/step - accuracy: 0.5014 - binary_io_u_5: 0.5003 - loss: 0.9105Batch 221, Loss Value: 0.8979\n",
      "Batch 221, Gradient Norm: 0.3290\n",
      "\u001b[1m221/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m10s\u001b[0m 929ms/step - accuracy: 0.5013 - binary_io_u_5: 0.5003 - loss: 0.9105Batch 222, Loss Value: 0.9051\n",
      "Batch 222, Gradient Norm: 0.1039\n",
      "\u001b[1m222/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m9s\u001b[0m 929ms/step - accuracy: 0.5013 - binary_io_u_5: 0.5003 - loss: 0.9105 Batch 223, Loss Value: 0.9379\n",
      "Batch 223, Gradient Norm: 0.2691\n",
      "\u001b[1m223/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m8s\u001b[0m 930ms/step - accuracy: 0.5013 - binary_io_u_5: 0.5003 - loss: 0.9105Batch 224, Loss Value: 0.9321\n",
      "Batch 224, Gradient Norm: 0.1331\n",
      "\u001b[1m224/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m7s\u001b[0m 930ms/step - accuracy: 0.5013 - binary_io_u_5: 0.5002 - loss: 0.9105Batch 225, Loss Value: 0.9127\n",
      "Batch 225, Gradient Norm: 0.1396\n",
      "\u001b[1m225/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m6s\u001b[0m 930ms/step - accuracy: 0.5012 - binary_io_u_5: 0.5002 - loss: 0.9105Batch 226, Loss Value: 0.8957\n",
      "Batch 226, Gradient Norm: 0.6796\n",
      "\u001b[1m226/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m5s\u001b[0m 930ms/step - accuracy: 0.5012 - binary_io_u_5: 0.5002 - loss: 0.9105Batch 227, Loss Value: 0.9389\n",
      "Batch 227, Gradient Norm: 0.0681\n",
      "\u001b[1m227/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m4s\u001b[0m 930ms/step - accuracy: 0.5012 - binary_io_u_5: 0.5002 - loss: 0.9105Batch 228, Loss Value: 0.9241\n",
      "Batch 228, Gradient Norm: 0.4450\n",
      "\u001b[1m228/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m3s\u001b[0m 930ms/step - accuracy: 0.5011 - binary_io_u_5: 0.5001 - loss: 0.9106Batch 229, Loss Value: 0.9383\n",
      "Batch 229, Gradient Norm: 0.2053\n",
      "\u001b[1m229/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m2s\u001b[0m 931ms/step - accuracy: 0.5011 - binary_io_u_5: 0.5001 - loss: 0.9106Batch 230, Loss Value: 0.9047\n",
      "Batch 230, Gradient Norm: 0.1641\n",
      "\u001b[1m230/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m1s\u001b[0m 931ms/step - accuracy: 0.5011 - binary_io_u_5: 0.5001 - loss: 0.9106Batch 231, Loss Value: 0.8972\n",
      "Batch 231, Gradient Norm: 0.0666\n",
      "\u001b[1m231/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 931ms/step - accuracy: 0.5011 - binary_io_u_5: 0.5001 - loss: 0.9106Batch 232, Loss Value: 0.9183\n",
      "Batch 232, Gradient Norm: 0.0455\n",
      "\u001b[1m232/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m218s\u001b[0m 942ms/step - accuracy: 0.5010 - binary_io_u_5: 0.5000 - loss: 0.9106 - val_accuracy: 0.4996 - val_binary_io_u_5: 0.4949 - val_loss: 0.9238 - learning_rate: 0.0010\n",
      "Epoch 10/200\n",
      "Batch 1, Loss Value: 0.9092\n",
      "Batch 1, Gradient Norm: 0.1497\n",
      "\u001b[1m  1/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4:01\u001b[0m 1s/step - accuracy: 0.5000 - binary_io_u_5: 0.5000 - loss: 0.9026Batch 2, Loss Value: 0.9002\n",
      "Batch 2, Gradient Norm: 0.1653\n",
      "\u001b[1m  2/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:11\u001b[0m 831ms/step - accuracy: 0.5000 - binary_io_u_5: 0.5000 - loss: 0.9029Batch 3, Loss Value: 0.9050\n",
      "Batch 3, Gradient Norm: 0.1194\n",
      "\u001b[1m  3/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:09\u001b[0m 826ms/step - accuracy: 0.4967 - binary_io_u_5: 0.4967 - loss: 0.9060Batch 4, Loss Value: 0.9003\n",
      "Batch 4, Gradient Norm: 0.2846\n",
      "\u001b[1m  4/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:21\u001b[0m 884ms/step - accuracy: 0.4981 - binary_io_u_5: 0.4981 - loss: 0.9049Batch 5, Loss Value: 0.9129\n",
      "Batch 5, Gradient Norm: 0.0848\n",
      "\u001b[1m  5/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:25\u001b[0m 906ms/step - accuracy: 0.4981 - binary_io_u_5: 0.4981 - loss: 0.9055Batch 6, Loss Value: 0.9032\n",
      "Batch 6, Gradient Norm: 0.0368\n",
      "\u001b[1m  6/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:27\u001b[0m 920ms/step - accuracy: 0.4995 - binary_io_u_5: 0.4995 - loss: 0.9050Batch 7, Loss Value: 0.9052\n",
      "Batch 7, Gradient Norm: 0.0621\n",
      "\u001b[1m  7/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:29\u001b[0m 929ms/step - accuracy: 0.4990 - binary_io_u_5: 0.4990 - loss: 0.9055Batch 8, Loss Value: 0.9074\n",
      "Batch 8, Gradient Norm: 0.0492\n",
      "\u001b[1m  8/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:31\u001b[0m 942ms/step - accuracy: 0.4975 - binary_io_u_5: 0.4975 - loss: 0.9065Batch 9, Loss Value: 0.9091\n",
      "Batch 9, Gradient Norm: 0.0944\n",
      "\u001b[1m  9/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:31\u001b[0m 950ms/step - accuracy: 0.4981 - binary_io_u_5: 0.4981 - loss: 0.9063Batch 10, Loss Value: 0.9055\n",
      "Batch 10, Gradient Norm: 0.0682\n",
      "\u001b[1m 10/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:31\u001b[0m 951ms/step - accuracy: 0.4990 - binary_io_u_5: 0.4990 - loss: 0.9059Batch 11, Loss Value: 0.9043\n",
      "Batch 11, Gradient Norm: 0.1386\n",
      "\u001b[1m 11/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:30\u001b[0m 954ms/step - accuracy: 0.5001 - binary_io_u_5: 0.5001 - loss: 0.9054Batch 12, Loss Value: 0.9046\n",
      "Batch 12, Gradient Norm: 0.1129\n",
      "\u001b[1m 12/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:31\u001b[0m 959ms/step - accuracy: 0.5005 - binary_io_u_5: 0.5005 - loss: 0.9052Batch 13, Loss Value: 0.9085\n",
      "Batch 13, Gradient Norm: 0.1141\n",
      "\u001b[1m 13/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:31\u001b[0m 964ms/step - accuracy: 0.5003 - binary_io_u_5: 0.5003 - loss: 0.9054Batch 14, Loss Value: 0.9064\n",
      "Batch 14, Gradient Norm: 0.0124\n",
      "\u001b[1m 14/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:30\u001b[0m 968ms/step - accuracy: 0.4997 - binary_io_u_5: 0.4997 - loss: 0.9057Batch 15, Loss Value: 0.9104\n",
      "Batch 15, Gradient Norm: 0.1591\n",
      "\u001b[1m 15/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:29\u001b[0m 967ms/step - accuracy: 0.4991 - binary_io_u_5: 0.4991 - loss: 0.9061Batch 16, Loss Value: 0.9066\n",
      "Batch 16, Gradient Norm: 0.0029\n",
      "\u001b[1m 16/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:29\u001b[0m 968ms/step - accuracy: 0.4984 - binary_io_u_5: 0.4984 - loss: 0.9066Batch 17, Loss Value: 0.9062\n",
      "Batch 17, Gradient Norm: 0.0300\n",
      "\u001b[1m 17/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:28\u001b[0m 969ms/step - accuracy: 0.4981 - binary_io_u_5: 0.4981 - loss: 0.9068Batch 18, Loss Value: 0.9066\n",
      "Batch 18, Gradient Norm: 0.0062\n",
      "\u001b[1m 18/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:26\u001b[0m 966ms/step - accuracy: 0.4977 - binary_io_u_5: 0.4977 - loss: 0.9071Batch 19, Loss Value: 0.9064\n",
      "Batch 19, Gradient Norm: 0.0170\n",
      "\u001b[1m 19/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:24\u001b[0m 959ms/step - accuracy: 0.4973 - binary_io_u_5: 0.4973 - loss: 0.9074Batch 20, Loss Value: 0.9049\n",
      "Batch 20, Gradient Norm: 0.1127\n",
      "\u001b[1m 20/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:22\u001b[0m 956ms/step - accuracy: 0.4969 - binary_io_u_5: 0.4969 - loss: 0.9078Batch 21, Loss Value: 0.9065\n",
      "Batch 21, Gradient Norm: 0.0091\n",
      "\u001b[1m 21/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:22\u001b[0m 960ms/step - accuracy: 0.4966 - binary_io_u_5: 0.4966 - loss: 0.9080Batch 22, Loss Value: 0.9051\n",
      "Batch 22, Gradient Norm: 0.1012\n",
      "\u001b[1m 22/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:21\u001b[0m 961ms/step - accuracy: 0.4962 - binary_io_u_5: 0.4962 - loss: 0.9083Batch 23, Loss Value: 0.9065\n",
      "Batch 23, Gradient Norm: 0.0087\n",
      "\u001b[1m 23/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:21\u001b[0m 962ms/step - accuracy: 0.4958 - binary_io_u_5: 0.4958 - loss: 0.9086Batch 24, Loss Value: 0.9164\n",
      "Batch 24, Gradient Norm: 0.1099\n",
      "\u001b[1m 24/232\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:20\u001b[0m 964ms/step - accuracy: 0.4955 - binary_io_u_5: 0.4955 - loss: 0.9089Batch 25, Loss Value: 0.9067\n",
      "Batch 25, Gradient Norm: 0.0028\n",
      "\u001b[1m 25/232\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:20\u001b[0m 967ms/step - accuracy: 0.4952 - binary_io_u_5: 0.4953 - loss: 0.9092Batch 26, Loss Value: 0.9066\n",
      "Batch 26, Gradient Norm: 0.0016\n",
      "\u001b[1m 26/232\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:19\u001b[0m 967ms/step - accuracy: 0.4951 - binary_io_u_5: 0.4951 - loss: 0.9094Batch 27, Loss Value: 0.9066\n",
      "Batch 27, Gradient Norm: 0.0053\n",
      "\u001b[1m 27/232\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:18\u001b[0m 969ms/step - accuracy: 0.4949 - binary_io_u_5: 0.4949 - loss: 0.9096Batch 28, Loss Value: 0.9062\n",
      "Batch 28, Gradient Norm: 0.0325\n",
      "\u001b[1m 28/232\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:18\u001b[0m 971ms/step - accuracy: 0.4947 - binary_io_u_5: 0.4948 - loss: 0.9098Batch 29, Loss Value: 0.9023\n",
      "Batch 29, Gradient Norm: 0.1666\n",
      "\u001b[1m 29/232\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:17\u001b[0m 972ms/step - accuracy: 0.4947 - binary_io_u_5: 0.4948 - loss: 0.9099Batch 30, Loss Value: 0.9061\n",
      "Batch 30, Gradient Norm: 0.0350\n",
      "\u001b[1m 30/232\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:16\u001b[0m 971ms/step - accuracy: 0.4947 - binary_io_u_5: 0.4948 - loss: 0.9099Batch 31, Loss Value: 0.9065\n",
      "Batch 31, Gradient Norm: 0.0335\n",
      "\u001b[1m 31/232\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:15\u001b[0m 972ms/step - accuracy: 0.4947 - binary_io_u_5: 0.4949 - loss: 0.9100Batch 32, Loss Value: 0.9065\n",
      "Batch 32, Gradient Norm: 0.0075\n",
      "\u001b[1m 32/232\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:14\u001b[0m 972ms/step - accuracy: 0.4948 - binary_io_u_5: 0.4950 - loss: 0.9101Batch 33, Loss Value: 0.9065\n",
      "Batch 33, Gradient Norm: 0.0108\n",
      "\u001b[1m 33/232\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:13\u001b[0m 972ms/step - accuracy: 0.4948 - binary_io_u_5: 0.4950 - loss: 0.9101Batch 34, Loss Value: 0.9067\n",
      "Batch 34, Gradient Norm: 0.0026\n",
      "\u001b[1m 34/232\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:12\u001b[0m 973ms/step - accuracy: 0.4949 - binary_io_u_5: 0.4951 - loss: 0.9102Batch 35, Loss Value: 0.9047\n",
      "Batch 35, Gradient Norm: 0.0708\n",
      "\u001b[1m 35/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:10\u001b[0m 969ms/step - accuracy: 0.4949 - binary_io_u_5: 0.4951 - loss: 0.9103Batch 36, Loss Value: 0.9063\n",
      "Batch 36, Gradient Norm: 0.0223\n",
      "\u001b[1m 36/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:08\u001b[0m 964ms/step - accuracy: 0.4950 - binary_io_u_5: 0.4952 - loss: 0.9103Batch 37, Loss Value: 0.9100\n",
      "Batch 37, Gradient Norm: 0.1834\n",
      "\u001b[1m 37/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:08\u001b[0m 966ms/step - accuracy: 0.4951 - binary_io_u_5: 0.4953 - loss: 0.9103Batch 38, Loss Value: 0.9072\n",
      "Batch 38, Gradient Norm: 0.0505\n",
      "\u001b[1m 38/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:07\u001b[0m 966ms/step - accuracy: 0.4951 - binary_io_u_5: 0.4954 - loss: 0.9103Batch 39, Loss Value: 0.9068\n",
      "Batch 39, Gradient Norm: 0.0260\n",
      "\u001b[1m 39/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:06\u001b[0m 967ms/step - accuracy: 0.4951 - binary_io_u_5: 0.4954 - loss: 0.9104Batch 40, Loss Value: 0.9063\n",
      "Batch 40, Gradient Norm: 0.0244\n",
      "\u001b[1m 40/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:05\u001b[0m 968ms/step - accuracy: 0.4951 - binary_io_u_5: 0.4954 - loss: 0.9105Batch 41, Loss Value: 0.9062\n",
      "Batch 41, Gradient Norm: 0.0340\n",
      "\u001b[1m 41/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:05\u001b[0m 969ms/step - accuracy: 0.4950 - binary_io_u_5: 0.4953 - loss: 0.9106Batch 42, Loss Value: 0.9065\n",
      "Batch 42, Gradient Norm: 0.0074\n",
      "\u001b[1m 42/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:04\u001b[0m 970ms/step - accuracy: 0.4950 - binary_io_u_5: 0.4952 - loss: 0.9107Batch 43, Loss Value: 0.9060\n",
      "Batch 43, Gradient Norm: 0.0354\n",
      "\u001b[1m 43/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:03\u001b[0m 971ms/step - accuracy: 0.4949 - binary_io_u_5: 0.4951 - loss: 0.9108Batch 44, Loss Value: 0.9067\n",
      "Batch 44, Gradient Norm: 0.0045\n",
      "\u001b[1m 44/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:02\u001b[0m 971ms/step - accuracy: 0.4948 - binary_io_u_5: 0.4951 - loss: 0.9109Batch 45, Loss Value: 0.9062\n",
      "Batch 45, Gradient Norm: 0.0332\n",
      "\u001b[1m 45/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:01\u001b[0m 971ms/step - accuracy: 0.4947 - binary_io_u_5: 0.4950 - loss: 0.9110Batch 46, Loss Value: 0.9061\n",
      "Batch 46, Gradient Norm: 0.0372\n",
      "\u001b[1m 46/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:00\u001b[0m 972ms/step - accuracy: 0.4946 - binary_io_u_5: 0.4949 - loss: 0.9111Batch 47, Loss Value: 0.9112\n",
      "Batch 47, Gradient Norm: 0.2338\n",
      "\u001b[1m 47/232\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:59\u001b[0m 973ms/step - accuracy: 0.4946 - binary_io_u_5: 0.4949 - loss: 0.9111Batch 48, Loss Value: 0.9064\n",
      "Batch 48, Gradient Norm: 0.0173\n",
      "\u001b[1m 48/232\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:58\u001b[0m 973ms/step - accuracy: 0.4946 - binary_io_u_5: 0.4949 - loss: 0.9112Batch 49, Loss Value: 0.9089\n",
      "Batch 49, Gradient Norm: 0.2776\n",
      "\u001b[1m 49/232\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:58\u001b[0m 973ms/step - accuracy: 0.4946 - binary_io_u_5: 0.4948 - loss: 0.9112Batch 50, Loss Value: 0.9078\n",
      "Batch 50, Gradient Norm: 0.1196\n",
      "\u001b[1m 50/232\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:57\u001b[0m 973ms/step - accuracy: 0.4945 - binary_io_u_5: 0.4948 - loss: 0.9112Batch 51, Loss Value: 0.9086\n",
      "Batch 51, Gradient Norm: 0.1593\n",
      "\u001b[1m 51/232\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:55\u001b[0m 972ms/step - accuracy: 0.4945 - binary_io_u_5: 0.4948 - loss: 0.9113Batch 52, Loss Value: 0.9104\n",
      "Batch 52, Gradient Norm: 0.1568\n",
      "\u001b[1m 52/232\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:54\u001b[0m 969ms/step - accuracy: 0.4945 - binary_io_u_5: 0.4948 - loss: 0.9113Batch 53, Loss Value: 0.8984\n",
      "Batch 53, Gradient Norm: 0.0769\n",
      "\u001b[1m 53/232\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:53\u001b[0m 967ms/step - accuracy: 0.4944 - binary_io_u_5: 0.4947 - loss: 0.9114Batch 54, Loss Value: 0.9182\n",
      "Batch 54, Gradient Norm: 0.4720\n",
      "\u001b[1m 54/232\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:52\u001b[0m 968ms/step - accuracy: 0.4944 - binary_io_u_5: 0.4947 - loss: 0.9114Batch 55, Loss Value: 0.9158\n",
      "Batch 55, Gradient Norm: 0.2986\n",
      "\u001b[1m 55/232\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:51\u001b[0m 968ms/step - accuracy: 0.4944 - binary_io_u_5: 0.4947 - loss: 0.9114Batch 56, Loss Value: 0.9213\n",
      "Batch 56, Gradient Norm: 0.3733\n",
      "\u001b[1m 56/232\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:50\u001b[0m 969ms/step - accuracy: 0.4944 - binary_io_u_5: 0.4947 - loss: 0.9114Batch 57, Loss Value: 0.9130\n",
      "Batch 57, Gradient Norm: 0.1013\n",
      "\u001b[1m 57/232\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:49\u001b[0m 970ms/step - accuracy: 0.4944 - binary_io_u_5: 0.4947 - loss: 0.9115Batch 58, Loss Value: 0.9186\n",
      "Batch 58, Gradient Norm: 0.2532\n",
      "\u001b[1m 58/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:48\u001b[0m 970ms/step - accuracy: 0.4943 - binary_io_u_5: 0.4946 - loss: 0.9115Batch 59, Loss Value: 0.9128\n",
      "Batch 59, Gradient Norm: 0.4207\n",
      "\u001b[1m 59/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:47\u001b[0m 971ms/step - accuracy: 0.4943 - binary_io_u_5: 0.4946 - loss: 0.9115Batch 60, Loss Value: 0.9379\n",
      "Batch 60, Gradient Norm: 0.1275\n",
      "\u001b[1m 60/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:47\u001b[0m 971ms/step - accuracy: 0.4943 - binary_io_u_5: 0.4946 - loss: 0.9115Batch 61, Loss Value: 0.8942\n",
      "Batch 61, Gradient Norm: 0.2617\n",
      "\u001b[1m 61/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:46\u001b[0m 972ms/step - accuracy: 0.4942 - binary_io_u_5: 0.4945 - loss: 0.9116Batch 62, Loss Value: 0.9342\n",
      "Batch 62, Gradient Norm: 0.1084\n",
      "\u001b[1m 62/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:45\u001b[0m 972ms/step - accuracy: 0.4942 - binary_io_u_5: 0.4945 - loss: 0.9116Batch 63, Loss Value: 0.9101\n",
      "Batch 63, Gradient Norm: 0.3766\n",
      "\u001b[1m 63/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:44\u001b[0m 973ms/step - accuracy: 0.4942 - binary_io_u_5: 0.4945 - loss: 0.9116Batch 64, Loss Value: 0.9081\n",
      "Batch 64, Gradient Norm: 0.2163\n",
      "\u001b[1m 64/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:43\u001b[0m 973ms/step - accuracy: 0.4942 - binary_io_u_5: 0.4945 - loss: 0.9117Batch 65, Loss Value: 0.9181\n",
      "Batch 65, Gradient Norm: 0.3651\n",
      "\u001b[1m 65/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:42\u001b[0m 974ms/step - accuracy: 0.4941 - binary_io_u_5: 0.4944 - loss: 0.9117Batch 66, Loss Value: 0.8953\n",
      "Batch 66, Gradient Norm: 0.4559\n",
      "\u001b[1m 66/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:41\u001b[0m 974ms/step - accuracy: 0.4941 - binary_io_u_5: 0.4944 - loss: 0.9117Batch 67, Loss Value: 0.8984\n",
      "Batch 67, Gradient Norm: 0.3712\n",
      "\u001b[1m 67/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:40\u001b[0m 974ms/step - accuracy: 0.4941 - binary_io_u_5: 0.4944 - loss: 0.9117Batch 68, Loss Value: 0.9074\n",
      "Batch 68, Gradient Norm: 0.0467\n",
      "\u001b[1m 68/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:39\u001b[0m 973ms/step - accuracy: 0.4941 - binary_io_u_5: 0.4944 - loss: 0.9117Batch 69, Loss Value: 0.9080\n",
      "Batch 69, Gradient Norm: 0.0937\n",
      "\u001b[1m 69/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:38\u001b[0m 970ms/step - accuracy: 0.4941 - binary_io_u_5: 0.4944 - loss: 0.9118Batch 70, Loss Value: 0.9059\n",
      "Batch 70, Gradient Norm: 0.0533\n",
      "\u001b[1m 70/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:36\u001b[0m 969ms/step - accuracy: 0.4941 - binary_io_u_5: 0.4944 - loss: 0.9118Batch 71, Loss Value: 0.9066\n",
      "Batch 71, Gradient Norm: 0.0044\n",
      "\u001b[1m 71/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:36\u001b[0m 970ms/step - accuracy: 0.4941 - binary_io_u_5: 0.4944 - loss: 0.9118Batch 72, Loss Value: 0.9066\n",
      "Batch 72, Gradient Norm: 0.0121\n",
      "\u001b[1m 72/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:35\u001b[0m 970ms/step - accuracy: 0.4940 - binary_io_u_5: 0.4943 - loss: 0.9118Batch 73, Loss Value: 0.9066\n",
      "Batch 73, Gradient Norm: 0.0005\n",
      "\u001b[1m 73/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:34\u001b[0m 970ms/step - accuracy: 0.4940 - binary_io_u_5: 0.4943 - loss: 0.9118Batch 74, Loss Value: 0.9067\n",
      "Batch 74, Gradient Norm: 0.0080\n",
      "\u001b[1m 74/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:33\u001b[0m 970ms/step - accuracy: 0.4940 - binary_io_u_5: 0.4943 - loss: 0.9119Batch 75, Loss Value: 0.9067\n",
      "Batch 75, Gradient Norm: 0.0032\n",
      "\u001b[1m 75/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:32\u001b[0m 970ms/step - accuracy: 0.4939 - binary_io_u_5: 0.4942 - loss: 0.9119Batch 76, Loss Value: 0.9066\n",
      "Batch 76, Gradient Norm: 0.0069\n",
      "\u001b[1m 76/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:31\u001b[0m 970ms/step - accuracy: 0.4939 - binary_io_u_5: 0.4942 - loss: 0.9119Batch 77, Loss Value: 0.9066\n",
      "Batch 77, Gradient Norm: 0.0013\n",
      "\u001b[1m 77/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:30\u001b[0m 970ms/step - accuracy: 0.4939 - binary_io_u_5: 0.4942 - loss: 0.9119Batch 78, Loss Value: 0.9067\n",
      "Batch 78, Gradient Norm: 0.0027\n",
      "\u001b[1m 78/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:29\u001b[0m 970ms/step - accuracy: 0.4938 - binary_io_u_5: 0.4941 - loss: 0.9119Batch 79, Loss Value: 0.9066\n",
      "Batch 79, Gradient Norm: 0.0008\n",
      "\u001b[1m 79/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:28\u001b[0m 971ms/step - accuracy: 0.4938 - binary_io_u_5: 0.4941 - loss: 0.9120Batch 80, Loss Value: 0.9066\n",
      "Batch 80, Gradient Norm: 0.0002\n",
      "\u001b[1m 80/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:27\u001b[0m 972ms/step - accuracy: 0.4937 - binary_io_u_5: 0.4940 - loss: 0.9120Batch 81, Loss Value: 0.9066\n",
      "Batch 81, Gradient Norm: 0.0033\n",
      "\u001b[1m 81/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:26\u001b[0m 972ms/step - accuracy: 0.4937 - binary_io_u_5: 0.4940 - loss: 0.9120Batch 82, Loss Value: 0.9067\n",
      "Batch 82, Gradient Norm: 0.0027\n",
      "\u001b[1m 82/232\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m2:25\u001b[0m 972ms/step - accuracy: 0.4936 - binary_io_u_5: 0.4939 - loss: 0.9121Batch 83, Loss Value: 0.9066\n",
      "Batch 83, Gradient Norm: 0.0003\n",
      "\u001b[1m 83/232\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m2:24\u001b[0m 972ms/step - accuracy: 0.4936 - binary_io_u_5: 0.4939 - loss: 0.9121Batch 84, Loss Value: 0.9066\n",
      "Batch 84, Gradient Norm: 0.0011\n",
      "\u001b[1m 84/232\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m2:23\u001b[0m 973ms/step - accuracy: 0.4935 - binary_io_u_5: 0.4938 - loss: 0.9121Batch 85, Loss Value: 0.9066\n",
      "Batch 85, Gradient Norm: 0.0028\n",
      "\u001b[1m 85/232\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m2:22\u001b[0m 970ms/step - accuracy: 0.4935 - binary_io_u_5: 0.4937 - loss: 0.9122Batch 86, Loss Value: 0.9066\n",
      "Batch 86, Gradient Norm: 0.0009\n",
      "\u001b[1m 86/232\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m2:21\u001b[0m 968ms/step - accuracy: 0.4934 - binary_io_u_5: 0.4937 - loss: 0.9122Batch 87, Loss Value: 0.9066\n",
      "Batch 87, Gradient Norm: 0.0003\n",
      "\u001b[1m 87/232\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m2:20\u001b[0m 968ms/step - accuracy: 0.4934 - binary_io_u_5: 0.4936 - loss: 0.9122Batch 88, Loss Value: 0.9067\n",
      "Batch 88, Gradient Norm: 0.0048\n",
      "\u001b[1m 88/232\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m2:19\u001b[0m 969ms/step - accuracy: 0.4933 - binary_io_u_5: 0.4936 - loss: 0.9123Batch 89, Loss Value: 0.9066\n",
      "Batch 89, Gradient Norm: 0.0003\n",
      "\u001b[1m 89/232\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m2:18\u001b[0m 970ms/step - accuracy: 0.4933 - binary_io_u_5: 0.4936 - loss: 0.9123Batch 90, Loss Value: 0.9067\n",
      "Batch 90, Gradient Norm: 0.0011\n",
      "\u001b[1m 90/232\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m2:17\u001b[0m 970ms/step - accuracy: 0.4932 - binary_io_u_5: 0.4935 - loss: 0.9123Batch 91, Loss Value: 0.9066\n",
      "Batch 91, Gradient Norm: 0.0003\n",
      "\u001b[1m 91/232\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m2:16\u001b[0m 970ms/step - accuracy: 0.4932 - binary_io_u_5: 0.4935 - loss: 0.9123Batch 92, Loss Value: 0.9066\n",
      "Batch 92, Gradient Norm: 0.0005\n",
      "\u001b[1m 92/232\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m2:15\u001b[0m 970ms/step - accuracy: 0.4932 - binary_io_u_5: 0.4934 - loss: 0.9124Batch 93, Loss Value: 0.9066\n",
      "Batch 93, Gradient Norm: 0.0056\n",
      "\u001b[1m 93/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m2:14\u001b[0m 969ms/step - accuracy: 0.4931 - binary_io_u_5: 0.4934 - loss: 0.9124Batch 94, Loss Value: 0.9066\n",
      "Batch 94, Gradient Norm: 0.0039\n",
      "\u001b[1m 94/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m2:13\u001b[0m 969ms/step - accuracy: 0.4931 - binary_io_u_5: 0.4934 - loss: 0.9124Batch 95, Loss Value: 0.9066\n",
      "Batch 95, Gradient Norm: 0.0004\n",
      "\u001b[1m 95/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m2:12\u001b[0m 969ms/step - accuracy: 0.4931 - binary_io_u_5: 0.4934 - loss: 0.9124Batch 96, Loss Value: 0.9066\n",
      "Batch 96, Gradient Norm: 0.0009\n",
      "\u001b[1m 96/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m2:11\u001b[0m 970ms/step - accuracy: 0.4931 - binary_io_u_5: 0.4933 - loss: 0.9124Batch 97, Loss Value: 0.9066\n",
      "Batch 97, Gradient Norm: 0.0001\n",
      "\u001b[1m 97/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m2:10\u001b[0m 970ms/step - accuracy: 0.4930 - binary_io_u_5: 0.4933 - loss: 0.9125Batch 98, Loss Value: 0.9066\n",
      "Batch 98, Gradient Norm: 0.0004\n",
      "\u001b[1m 98/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m2:10\u001b[0m 971ms/step - accuracy: 0.4930 - binary_io_u_5: 0.4932 - loss: 0.9125Batch 99, Loss Value: 0.9066\n",
      "Batch 99, Gradient Norm: 0.0007\n",
      "\u001b[1m 99/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m2:09\u001b[0m 971ms/step - accuracy: 0.4929 - binary_io_u_5: 0.4932 - loss: 0.9125Batch 100, Loss Value: 0.9066\n",
      "Batch 100, Gradient Norm: 0.0005\n",
      "\u001b[1m100/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m2:08\u001b[0m 972ms/step - accuracy: 0.4928 - binary_io_u_5: 0.4931 - loss: 0.9125Batch 101, Loss Value: 0.9066\n",
      "Batch 101, Gradient Norm: 0.0000\n",
      "\u001b[1m101/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m2:07\u001b[0m 972ms/step - accuracy: 0.4928 - binary_io_u_5: 0.4931 - loss: 0.9126Batch 102, Loss Value: 0.9066\n",
      "Batch 102, Gradient Norm: 0.0071\n",
      "\u001b[1m102/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m2:06\u001b[0m 970ms/step - accuracy: 0.4927 - binary_io_u_5: 0.4930 - loss: 0.9126Batch 103, Loss Value: 0.9066\n",
      "Batch 103, Gradient Norm: 0.0002\n",
      "\u001b[1m103/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m2:04\u001b[0m 968ms/step - accuracy: 0.4927 - binary_io_u_5: 0.4930 - loss: 0.9126Batch 104, Loss Value: 0.9066\n",
      "Batch 104, Gradient Norm: 0.0000\n",
      "\u001b[1m104/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m2:03\u001b[0m 968ms/step - accuracy: 0.4926 - binary_io_u_5: 0.4929 - loss: 0.9126Batch 105, Loss Value: 0.9066\n",
      "Batch 105, Gradient Norm: 0.0001\n",
      "\u001b[1m105/232\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m2:03\u001b[0m 969ms/step - accuracy: 0.4926 - binary_io_u_5: 0.4929 - loss: 0.9126Batch 106, Loss Value: 0.9066\n",
      "Batch 106, Gradient Norm: 0.0001\n",
      "\u001b[1m106/232\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m2:02\u001b[0m 969ms/step - accuracy: 0.4926 - binary_io_u_5: 0.4929 - loss: 0.9127Batch 107, Loss Value: 0.9066\n",
      "Batch 107, Gradient Norm: 0.0017\n",
      "\u001b[1m107/232\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m2:01\u001b[0m 969ms/step - accuracy: 0.4925 - binary_io_u_5: 0.4928 - loss: 0.9127Batch 108, Loss Value: 0.9066\n",
      "Batch 108, Gradient Norm: 0.0003\n",
      "\u001b[1m108/232\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m2:00\u001b[0m 969ms/step - accuracy: 0.4925 - binary_io_u_5: 0.4928 - loss: 0.9127Batch 109, Loss Value: 0.9066\n",
      "Batch 109, Gradient Norm: 0.0001\n",
      "\u001b[1m109/232\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m1:59\u001b[0m 969ms/step - accuracy: 0.4925 - binary_io_u_5: 0.4927 - loss: 0.9127Batch 110, Loss Value: 0.9066\n",
      "Batch 110, Gradient Norm: 0.0001\n",
      "\u001b[1m110/232\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m1:58\u001b[0m 969ms/step - accuracy: 0.4924 - binary_io_u_5: 0.4927 - loss: 0.9127Batch 111, Loss Value: 0.9066\n",
      "Batch 111, Gradient Norm: 0.0024\n",
      "\u001b[1m111/232\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m1:57\u001b[0m 969ms/step - accuracy: 0.4924 - binary_io_u_5: 0.4927 - loss: 0.9128Batch 112, Loss Value: 0.9066\n",
      "Batch 112, Gradient Norm: 0.0005\n",
      "\u001b[1m112/232\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m1:56\u001b[0m 969ms/step - accuracy: 0.4923 - binary_io_u_5: 0.4926 - loss: 0.9128Batch 113, Loss Value: 0.9066\n",
      "Batch 113, Gradient Norm: 0.0001\n",
      "\u001b[1m113/232\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m1:55\u001b[0m 969ms/step - accuracy: 0.4923 - binary_io_u_5: 0.4926 - loss: 0.9128Batch 114, Loss Value: 0.9066\n",
      "Batch 114, Gradient Norm: 0.0009\n",
      "\u001b[1m114/232\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m1:54\u001b[0m 970ms/step - accuracy: 0.4923 - binary_io_u_5: 0.4925 - loss: 0.9128Batch 115, Loss Value: 0.9066\n",
      "Batch 115, Gradient Norm: 0.0000\n",
      "\u001b[1m115/232\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m1:53\u001b[0m 970ms/step - accuracy: 0.4922 - binary_io_u_5: 0.4925 - loss: 0.9129Batch 116, Loss Value: 0.9066\n",
      "Batch 116, Gradient Norm: 0.0037\n",
      "\u001b[1m116/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m1:52\u001b[0m 970ms/step - accuracy: 0.4922 - binary_io_u_5: 0.4925 - loss: 0.9129Batch 117, Loss Value: 0.9066\n",
      "Batch 117, Gradient Norm: 0.0008\n",
      "\u001b[1m117/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m1:51\u001b[0m 970ms/step - accuracy: 0.4922 - binary_io_u_5: 0.4924 - loss: 0.9129Batch 118, Loss Value: 0.9067\n",
      "Batch 118, Gradient Norm: 0.0073\n",
      "\u001b[1m118/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m1:50\u001b[0m 970ms/step - accuracy: 0.4921 - binary_io_u_5: 0.4924 - loss: 0.9129Batch 119, Loss Value: 0.9062\n",
      "Batch 119, Gradient Norm: 0.0293\n",
      "\u001b[1m119/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m1:49\u001b[0m 969ms/step - accuracy: 0.4921 - binary_io_u_5: 0.4924 - loss: 0.9129Batch 120, Loss Value: 0.9066\n",
      "Batch 120, Gradient Norm: 0.0003\n",
      "\u001b[1m120/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m1:48\u001b[0m 967ms/step - accuracy: 0.4921 - binary_io_u_5: 0.4923 - loss: 0.9130Batch 121, Loss Value: 0.9066\n",
      "Batch 121, Gradient Norm: 0.0019\n",
      "\u001b[1m121/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m1:47\u001b[0m 967ms/step - accuracy: 0.4920 - binary_io_u_5: 0.4923 - loss: 0.9130Batch 122, Loss Value: 0.9066\n",
      "Batch 122, Gradient Norm: 0.0001\n",
      "\u001b[1m122/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m1:46\u001b[0m 967ms/step - accuracy: 0.4920 - binary_io_u_5: 0.4923 - loss: 0.9130Batch 123, Loss Value: 0.9065\n",
      "Batch 123, Gradient Norm: 0.0085\n",
      "\u001b[1m123/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m1:45\u001b[0m 967ms/step - accuracy: 0.4920 - binary_io_u_5: 0.4923 - loss: 0.9130Batch 124, Loss Value: 0.9066\n",
      "Batch 124, Gradient Norm: 0.0014\n",
      "\u001b[1m124/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m1:44\u001b[0m 967ms/step - accuracy: 0.4920 - binary_io_u_5: 0.4922 - loss: 0.9130Batch 125, Loss Value: 0.9163\n",
      "Batch 125, Gradient Norm: 0.3716\n",
      "\u001b[1m125/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m1:43\u001b[0m 967ms/step - accuracy: 0.4919 - binary_io_u_5: 0.4922 - loss: 0.9130Batch 126, Loss Value: 0.9066\n",
      "Batch 126, Gradient Norm: 0.0036\n",
      "\u001b[1m126/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m1:42\u001b[0m 968ms/step - accuracy: 0.4919 - binary_io_u_5: 0.4922 - loss: 0.9131Batch 127, Loss Value: 0.9064\n",
      "Batch 127, Gradient Norm: 0.0161\n",
      "\u001b[1m127/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m1:41\u001b[0m 968ms/step - accuracy: 0.4919 - binary_io_u_5: 0.4922 - loss: 0.9131Batch 128, Loss Value: 0.9067\n",
      "Batch 128, Gradient Norm: 0.0010\n",
      "\u001b[1m128/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m1:40\u001b[0m 968ms/step - accuracy: 0.4919 - binary_io_u_5: 0.4921 - loss: 0.9131Batch 129, Loss Value: 0.9066\n",
      "Batch 129, Gradient Norm: 0.0017\n",
      "\u001b[1m129/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m1:39\u001b[0m 968ms/step - accuracy: 0.4919 - binary_io_u_5: 0.4921 - loss: 0.9131Batch 130, Loss Value: 0.9067\n",
      "Batch 130, Gradient Norm: 0.0157\n",
      "\u001b[1m130/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m1:38\u001b[0m 967ms/step - accuracy: 0.4919 - binary_io_u_5: 0.4921 - loss: 0.9131Batch 131, Loss Value: 0.9035\n",
      "Batch 131, Gradient Norm: 0.1885\n",
      "\u001b[1m131/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m1:37\u001b[0m 967ms/step - accuracy: 0.4919 - binary_io_u_5: 0.4921 - loss: 0.9131Batch 132, Loss Value: 0.9075\n",
      "Batch 132, Gradient Norm: 0.0549\n",
      "\u001b[1m132/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m1:36\u001b[0m 968ms/step - accuracy: 0.4918 - binary_io_u_5: 0.4921 - loss: 0.9131Batch 133, Loss Value: 0.9066\n",
      "Batch 133, Gradient Norm: 0.0007\n",
      "\u001b[1m133/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m1:35\u001b[0m 968ms/step - accuracy: 0.4918 - binary_io_u_5: 0.4921 - loss: 0.9131Batch 134, Loss Value: 0.9066\n",
      "Batch 134, Gradient Norm: 0.0005\n",
      "\u001b[1m134/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m1:34\u001b[0m 968ms/step - accuracy: 0.4918 - binary_io_u_5: 0.4921 - loss: 0.9131Batch 135, Loss Value: 0.9073\n",
      "Batch 135, Gradient Norm: 0.0449\n",
      "\u001b[1m135/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m1:33\u001b[0m 968ms/step - accuracy: 0.4918 - binary_io_u_5: 0.4921 - loss: 0.9131Batch 136, Loss Value: 0.9061\n",
      "Batch 136, Gradient Norm: 0.0348\n",
      "\u001b[1m136/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m1:32\u001b[0m 967ms/step - accuracy: 0.4918 - binary_io_u_5: 0.4921 - loss: 0.9131Batch 137, Loss Value: 0.9061\n",
      "Batch 137, Gradient Norm: 0.0368\n",
      "\u001b[1m137/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m1:31\u001b[0m 966ms/step - accuracy: 0.4918 - binary_io_u_5: 0.4921 - loss: 0.9132Batch 138, Loss Value: 0.9067\n",
      "Batch 138, Gradient Norm: 0.0022\n",
      "\u001b[1m138/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m1:30\u001b[0m 965ms/step - accuracy: 0.4918 - binary_io_u_5: 0.4921 - loss: 0.9132Batch 139, Loss Value: 0.9147\n",
      "Batch 139, Gradient Norm: 0.3934\n",
      "\u001b[1m139/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m1:29\u001b[0m 965ms/step - accuracy: 0.4918 - binary_io_u_5: 0.4920 - loss: 0.9132Batch 140, Loss Value: 0.9049\n",
      "Batch 140, Gradient Norm: 0.1115\n",
      "\u001b[1m140/232\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m1:28\u001b[0m 965ms/step - accuracy: 0.4918 - binary_io_u_5: 0.4920 - loss: 0.9132Batch 141, Loss Value: 0.9065\n",
      "Batch 141, Gradient Norm: 0.0247\n",
      "\u001b[1m141/232\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m1:27\u001b[0m 966ms/step - accuracy: 0.4918 - binary_io_u_5: 0.4920 - loss: 0.9132Batch 142, Loss Value: 0.9066\n",
      "Batch 142, Gradient Norm: 0.0026\n",
      "\u001b[1m142/232\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m1:26\u001b[0m 966ms/step - accuracy: 0.4918 - binary_io_u_5: 0.4920 - loss: 0.9132Batch 143, Loss Value: 0.9066\n",
      "Batch 143, Gradient Norm: 0.0009\n",
      "\u001b[1m143/232\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m1:25\u001b[0m 965ms/step - accuracy: 0.4918 - binary_io_u_5: 0.4920 - loss: 0.9132Batch 144, Loss Value: 0.9067\n",
      "Batch 144, Gradient Norm: 0.0086\n",
      "\u001b[1m144/232\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m1:24\u001b[0m 965ms/step - accuracy: 0.4918 - binary_io_u_5: 0.4920 - loss: 0.9132Batch 145, Loss Value: 0.9066\n",
      "Batch 145, Gradient Norm: 0.0017\n",
      "\u001b[1m145/232\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m1:23\u001b[0m 965ms/step - accuracy: 0.4918 - binary_io_u_5: 0.4920 - loss: 0.9132Batch 146, Loss Value: 0.9066\n",
      "Batch 146, Gradient Norm: 0.0005\n",
      "\u001b[1m146/232\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m1:22\u001b[0m 965ms/step - accuracy: 0.4918 - binary_io_u_5: 0.4920 - loss: 0.9132Batch 147, Loss Value: 0.9066\n",
      "Batch 147, Gradient Norm: 0.0002\n",
      "\u001b[1m147/232\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m1:22\u001b[0m 965ms/step - accuracy: 0.4918 - binary_io_u_5: 0.4920 - loss: 0.9132Batch 148, Loss Value: 0.9067\n",
      "Batch 148, Gradient Norm: 0.0192\n",
      "\u001b[1m148/232\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m1:21\u001b[0m 965ms/step - accuracy: 0.4917 - binary_io_u_5: 0.4920 - loss: 0.9132Batch 149, Loss Value: 0.9169\n",
      "Batch 149, Gradient Norm: 0.5008\n",
      "\u001b[1m149/232\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m1:20\u001b[0m 965ms/step - accuracy: 0.4917 - binary_io_u_5: 0.4920 - loss: 0.9132Batch 150, Loss Value: 0.9057\n",
      "Batch 150, Gradient Norm: 0.0711\n",
      "\u001b[1m150/232\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m1:19\u001b[0m 965ms/step - accuracy: 0.4917 - binary_io_u_5: 0.4920 - loss: 0.9132Batch 151, Loss Value: 0.9066\n",
      "Batch 151, Gradient Norm: 0.0007\n",
      "\u001b[1m151/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m1:18\u001b[0m 965ms/step - accuracy: 0.4917 - binary_io_u_5: 0.4920 - loss: 0.9133Batch 152, Loss Value: 0.9065\n",
      "Batch 152, Gradient Norm: 0.0115\n",
      "\u001b[1m152/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m1:17\u001b[0m 965ms/step - accuracy: 0.4917 - binary_io_u_5: 0.4920 - loss: 0.9133Batch 153, Loss Value: 0.9066\n",
      "Batch 153, Gradient Norm: 0.0013\n",
      "\u001b[1m153/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m1:16\u001b[0m 964ms/step - accuracy: 0.4918 - binary_io_u_5: 0.4920 - loss: 0.9133Batch 154, Loss Value: 0.9066\n",
      "Batch 154, Gradient Norm: 0.0019\n",
      "\u001b[1m154/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m1:15\u001b[0m 963ms/step - accuracy: 0.4918 - binary_io_u_5: 0.4920 - loss: 0.9133Batch 155, Loss Value: 0.9067\n",
      "Batch 155, Gradient Norm: 0.0028\n",
      "\u001b[1m155/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m1:14\u001b[0m 962ms/step - accuracy: 0.4918 - binary_io_u_5: 0.4920 - loss: 0.9133Batch 156, Loss Value: 0.9066\n",
      "Batch 156, Gradient Norm: 0.0012\n",
      "\u001b[1m156/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m1:13\u001b[0m 962ms/step - accuracy: 0.4917 - binary_io_u_5: 0.4920 - loss: 0.9133Batch 157, Loss Value: 0.9038\n",
      "Batch 157, Gradient Norm: 0.1928\n",
      "\u001b[1m157/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m1:12\u001b[0m 962ms/step - accuracy: 0.4917 - binary_io_u_5: 0.4920 - loss: 0.9133Batch 158, Loss Value: 0.9067\n",
      "Batch 158, Gradient Norm: 0.0018\n",
      "\u001b[1m158/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m1:11\u001b[0m 962ms/step - accuracy: 0.4917 - binary_io_u_5: 0.4920 - loss: 0.9133Batch 159, Loss Value: 0.9063\n",
      "Batch 159, Gradient Norm: 0.0223\n",
      "\u001b[1m159/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m1:10\u001b[0m 962ms/step - accuracy: 0.4917 - binary_io_u_5: 0.4920 - loss: 0.9133Batch 160, Loss Value: 0.9065\n",
      "Batch 160, Gradient Norm: 0.0113\n",
      "\u001b[1m160/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m1:09\u001b[0m 962ms/step - accuracy: 0.4917 - binary_io_u_5: 0.4920 - loss: 0.9133Batch 161, Loss Value: 0.9064\n",
      "Batch 161, Gradient Norm: 0.0173\n",
      "\u001b[1m161/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m1:08\u001b[0m 962ms/step - accuracy: 0.4917 - binary_io_u_5: 0.4920 - loss: 0.9133Batch 162, Loss Value: 0.9056\n",
      "Batch 162, Gradient Norm: 0.0618\n",
      "\u001b[1m162/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m1:07\u001b[0m 962ms/step - accuracy: 0.4917 - binary_io_u_5: 0.4920 - loss: 0.9133Batch 163, Loss Value: 0.9048\n",
      "Batch 163, Gradient Norm: 0.1162\n",
      "\u001b[1m163/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m1:06\u001b[0m 962ms/step - accuracy: 0.4917 - binary_io_u_5: 0.4920 - loss: 0.9133Batch 164, Loss Value: 0.9046\n",
      "Batch 164, Gradient Norm: 0.1210\n",
      "\u001b[1m164/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m1:05\u001b[0m 961ms/step - accuracy: 0.4917 - binary_io_u_5: 0.4919 - loss: 0.9133Batch 165, Loss Value: 0.9078\n",
      "Batch 165, Gradient Norm: 0.0669\n",
      "\u001b[1m165/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m1:04\u001b[0m 961ms/step - accuracy: 0.4917 - binary_io_u_5: 0.4919 - loss: 0.9133Batch 166, Loss Value: 0.9066\n",
      "Batch 166, Gradient Norm: 0.0050\n",
      "\u001b[1m166/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m1:03\u001b[0m 962ms/step - accuracy: 0.4917 - binary_io_u_5: 0.4919 - loss: 0.9134Batch 167, Loss Value: 0.9061\n",
      "Batch 167, Gradient Norm: 0.0568\n",
      "\u001b[1m167/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m1:02\u001b[0m 962ms/step - accuracy: 0.4917 - binary_io_u_5: 0.4919 - loss: 0.9134Batch 168, Loss Value: 0.9047\n",
      "Batch 168, Gradient Norm: 0.1204\n",
      "\u001b[1m168/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m1:01\u001b[0m 962ms/step - accuracy: 0.4917 - binary_io_u_5: 0.4919 - loss: 0.9134Batch 169, Loss Value: 0.9151\n",
      "Batch 169, Gradient Norm: 0.0955\n",
      "\u001b[1m169/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m1:00\u001b[0m 962ms/step - accuracy: 0.4917 - binary_io_u_5: 0.4919 - loss: 0.9134Batch 170, Loss Value: 0.9061\n",
      "Batch 170, Gradient Norm: 0.0324\n",
      "\u001b[1m170/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m59s\u001b[0m 962ms/step - accuracy: 0.4917 - binary_io_u_5: 0.4919 - loss: 0.9134 Batch 171, Loss Value: 0.9042\n",
      "Batch 171, Gradient Norm: 0.1554\n",
      "\u001b[1m171/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m58s\u001b[0m 961ms/step - accuracy: 0.4917 - binary_io_u_5: 0.4919 - loss: 0.9134Batch 172, Loss Value: 0.9071\n",
      "Batch 172, Gradient Norm: 0.0362\n",
      "\u001b[1m172/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m57s\u001b[0m 959ms/step - accuracy: 0.4917 - binary_io_u_5: 0.4919 - loss: 0.9134Batch 173, Loss Value: 0.9299\n",
      "Batch 173, Gradient Norm: 0.3765\n",
      "\u001b[1m173/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m56s\u001b[0m 959ms/step - accuracy: 0.4917 - binary_io_u_5: 0.4919 - loss: 0.9134Batch 174, Loss Value: 0.9089\n",
      "Batch 174, Gradient Norm: 0.1377\n",
      "\u001b[1m174/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m55s\u001b[0m 959ms/step - accuracy: 0.4917 - binary_io_u_5: 0.4919 - loss: 0.9134Batch 175, Loss Value: 0.9045\n",
      "Batch 175, Gradient Norm: 0.1101\n",
      "\u001b[1m175/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m54s\u001b[0m 959ms/step - accuracy: 0.4917 - binary_io_u_5: 0.4919 - loss: 0.9134Batch 176, Loss Value: 0.8922\n",
      "Batch 176, Gradient Norm: 0.4554\n",
      "\u001b[1m176/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m53s\u001b[0m 959ms/step - accuracy: 0.4917 - binary_io_u_5: 0.4919 - loss: 0.9134Batch 177, Loss Value: 0.9043\n",
      "Batch 177, Gradient Norm: 0.1073\n",
      "\u001b[1m177/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m52s\u001b[0m 959ms/step - accuracy: 0.4917 - binary_io_u_5: 0.4919 - loss: 0.9134Batch 178, Loss Value: 0.9087\n",
      "Batch 178, Gradient Norm: 0.1413\n",
      "\u001b[1m178/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m51s\u001b[0m 958ms/step - accuracy: 0.4916 - binary_io_u_5: 0.4919 - loss: 0.9134Batch 179, Loss Value: 0.9058\n",
      "Batch 179, Gradient Norm: 0.0424\n",
      "\u001b[1m179/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m50s\u001b[0m 958ms/step - accuracy: 0.4916 - binary_io_u_5: 0.4919 - loss: 0.9134Batch 180, Loss Value: 0.9035\n",
      "Batch 180, Gradient Norm: 0.1368\n",
      "\u001b[1m180/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m49s\u001b[0m 958ms/step - accuracy: 0.4916 - binary_io_u_5: 0.4919 - loss: 0.9134Batch 181, Loss Value: 0.8888\n",
      "Batch 181, Gradient Norm: 0.2927\n",
      "\u001b[1m181/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m48s\u001b[0m 958ms/step - accuracy: 0.4916 - binary_io_u_5: 0.4919 - loss: 0.9134Batch 182, Loss Value: 0.9026\n",
      "Batch 182, Gradient Norm: 0.2104\n",
      "\u001b[1m182/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m47s\u001b[0m 958ms/step - accuracy: 0.4916 - binary_io_u_5: 0.4919 - loss: 0.9134Batch 183, Loss Value: 0.9075\n",
      "Batch 183, Gradient Norm: 0.0565\n",
      "\u001b[1m183/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m46s\u001b[0m 958ms/step - accuracy: 0.4916 - binary_io_u_5: 0.4918 - loss: 0.9135Batch 184, Loss Value: 0.8984\n",
      "Batch 184, Gradient Norm: 0.1759\n",
      "\u001b[1m184/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m45s\u001b[0m 958ms/step - accuracy: 0.4916 - binary_io_u_5: 0.4918 - loss: 0.9135Batch 185, Loss Value: 0.9078\n",
      "Batch 185, Gradient Norm: 0.0783\n",
      "\u001b[1m185/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m45s\u001b[0m 958ms/step - accuracy: 0.4916 - binary_io_u_5: 0.4918 - loss: 0.9135Batch 186, Loss Value: 0.9007\n",
      "Batch 186, Gradient Norm: 0.2212\n",
      "\u001b[1m186/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m44s\u001b[0m 957ms/step - accuracy: 0.4916 - binary_io_u_5: 0.4918 - loss: 0.9135Batch 187, Loss Value: 0.9035\n",
      "Batch 187, Gradient Norm: 0.0583\n",
      "\u001b[1m187/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m43s\u001b[0m 957ms/step - accuracy: 0.4916 - binary_io_u_5: 0.4918 - loss: 0.9135Batch 188, Loss Value: 0.9076\n",
      "Batch 188, Gradient Norm: 0.0885\n",
      "\u001b[1m188/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m42s\u001b[0m 957ms/step - accuracy: 0.4916 - binary_io_u_5: 0.4918 - loss: 0.9135Batch 189, Loss Value: 0.8968\n",
      "Batch 189, Gradient Norm: 0.3024\n",
      "\u001b[1m189/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m41s\u001b[0m 956ms/step - accuracy: 0.4916 - binary_io_u_5: 0.4918 - loss: 0.9135Batch 190, Loss Value: 0.9034\n",
      "Batch 190, Gradient Norm: 0.0993\n",
      "\u001b[1m190/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m40s\u001b[0m 956ms/step - accuracy: 0.4916 - binary_io_u_5: 0.4918 - loss: 0.9135Batch 191, Loss Value: 0.8922\n",
      "Batch 191, Gradient Norm: 0.1602\n",
      "\u001b[1m191/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m39s\u001b[0m 955ms/step - accuracy: 0.4916 - binary_io_u_5: 0.4918 - loss: 0.9135Batch 192, Loss Value: 0.9064\n",
      "Batch 192, Gradient Norm: 0.1902\n",
      "\u001b[1m192/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m38s\u001b[0m 955ms/step - accuracy: 0.4916 - binary_io_u_5: 0.4918 - loss: 0.9135Batch 193, Loss Value: 0.8972\n",
      "Batch 193, Gradient Norm: 0.4016\n",
      "\u001b[1m193/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m37s\u001b[0m 955ms/step - accuracy: 0.4916 - binary_io_u_5: 0.4918 - loss: 0.9135Batch 194, Loss Value: 0.9256\n",
      "Batch 194, Gradient Norm: 0.2048\n",
      "\u001b[1m194/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m36s\u001b[0m 955ms/step - accuracy: 0.4916 - binary_io_u_5: 0.4918 - loss: 0.9135Batch 195, Loss Value: 0.9052\n",
      "Batch 195, Gradient Norm: 0.1196\n",
      "\u001b[1m195/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m35s\u001b[0m 955ms/step - accuracy: 0.4916 - binary_io_u_5: 0.4918 - loss: 0.9135Batch 196, Loss Value: 0.9274\n",
      "Batch 196, Gradient Norm: 0.2886\n",
      "\u001b[1m196/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m34s\u001b[0m 955ms/step - accuracy: 0.4916 - binary_io_u_5: 0.4918 - loss: 0.9135Batch 197, Loss Value: 0.9028\n",
      "Batch 197, Gradient Norm: 0.1919\n",
      "\u001b[1m197/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m33s\u001b[0m 955ms/step - accuracy: 0.4916 - binary_io_u_5: 0.4918 - loss: 0.9135Batch 198, Loss Value: 0.9172\n",
      "Batch 198, Gradient Norm: 0.1585\n",
      "\u001b[1m198/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m32s\u001b[0m 955ms/step - accuracy: 0.4916 - binary_io_u_5: 0.4918 - loss: 0.9135Batch 199, Loss Value: 0.9067\n",
      "Batch 199, Gradient Norm: 0.0599\n",
      "\u001b[1m199/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m31s\u001b[0m 955ms/step - accuracy: 0.4916 - binary_io_u_5: 0.4918 - loss: 0.9135Batch 200, Loss Value: 0.9073\n",
      "Batch 200, Gradient Norm: 0.0488\n",
      "\u001b[1m200/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m30s\u001b[0m 955ms/step - accuracy: 0.4916 - binary_io_u_5: 0.4918 - loss: 0.9135Batch 201, Loss Value: 0.9040\n",
      "Batch 201, Gradient Norm: 0.0884\n",
      "\u001b[1m201/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m29s\u001b[0m 955ms/step - accuracy: 0.4916 - binary_io_u_5: 0.4918 - loss: 0.9135Batch 202, Loss Value: 0.9072\n",
      "Batch 202, Gradient Norm: 0.1162\n",
      "\u001b[1m202/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m28s\u001b[0m 955ms/step - accuracy: 0.4916 - binary_io_u_5: 0.4918 - loss: 0.9135Batch 203, Loss Value: 0.8878\n",
      "Batch 203, Gradient Norm: 0.0703\n",
      "\u001b[1m203/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m27s\u001b[0m 955ms/step - accuracy: 0.4916 - binary_io_u_5: 0.4918 - loss: 0.9135Batch 204, Loss Value: 0.9222\n",
      "Batch 204, Gradient Norm: 0.3540\n",
      "\u001b[1m204/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m26s\u001b[0m 955ms/step - accuracy: 0.4916 - binary_io_u_5: 0.4918 - loss: 0.9135Batch 205, Loss Value: 0.8916\n",
      "Batch 205, Gradient Norm: 0.4501\n",
      "\u001b[1m205/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m25s\u001b[0m 955ms/step - accuracy: 0.4916 - binary_io_u_5: 0.4918 - loss: 0.9135Batch 206, Loss Value: 0.9071\n",
      "Batch 206, Gradient Norm: 0.0281\n",
      "\u001b[1m206/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m24s\u001b[0m 955ms/step - accuracy: 0.4916 - binary_io_u_5: 0.4918 - loss: 0.9135Batch 207, Loss Value: 0.9228\n",
      "Batch 207, Gradient Norm: 0.1507\n",
      "\u001b[1m207/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m23s\u001b[0m 954ms/step - accuracy: 0.4916 - binary_io_u_5: 0.4918 - loss: 0.9135Batch 208, Loss Value: 0.9034\n",
      "Batch 208, Gradient Norm: 0.1921\n",
      "\u001b[1m208/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m22s\u001b[0m 953ms/step - accuracy: 0.4916 - binary_io_u_5: 0.4919 - loss: 0.9135Batch 209, Loss Value: 0.9094\n",
      "Batch 209, Gradient Norm: 0.2413\n",
      "\u001b[1m209/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m21s\u001b[0m 953ms/step - accuracy: 0.4917 - binary_io_u_5: 0.4919 - loss: 0.9135Batch 210, Loss Value: 0.9056\n",
      "Batch 210, Gradient Norm: 0.0644\n",
      "\u001b[1m210/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m20s\u001b[0m 953ms/step - accuracy: 0.4917 - binary_io_u_5: 0.4919 - loss: 0.9135Batch 211, Loss Value: 0.9006\n",
      "Batch 211, Gradient Norm: 0.2497\n",
      "\u001b[1m211/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m20s\u001b[0m 953ms/step - accuracy: 0.4917 - binary_io_u_5: 0.4919 - loss: 0.9135Batch 212, Loss Value: 0.9055\n",
      "Batch 212, Gradient Norm: 0.0702\n",
      "\u001b[1m212/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m19s\u001b[0m 953ms/step - accuracy: 0.4917 - binary_io_u_5: 0.4919 - loss: 0.9135Batch 213, Loss Value: 0.9313\n",
      "Batch 213, Gradient Norm: 0.1701\n",
      "\u001b[1m213/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m18s\u001b[0m 953ms/step - accuracy: 0.4917 - binary_io_u_5: 0.4919 - loss: 0.9135Batch 214, Loss Value: 0.8907\n",
      "Batch 214, Gradient Norm: 0.2764\n",
      "\u001b[1m214/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m17s\u001b[0m 953ms/step - accuracy: 0.4917 - binary_io_u_5: 0.4919 - loss: 0.9135Batch 215, Loss Value: 0.9000\n",
      "Batch 215, Gradient Norm: 0.0784\n",
      "\u001b[1m215/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m16s\u001b[0m 953ms/step - accuracy: 0.4917 - binary_io_u_5: 0.4919 - loss: 0.9135Batch 216, Loss Value: 0.9012\n",
      "Batch 216, Gradient Norm: 0.2065\n",
      "\u001b[1m216/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m15s\u001b[0m 953ms/step - accuracy: 0.4917 - binary_io_u_5: 0.4919 - loss: 0.9135Batch 217, Loss Value: 0.9078\n",
      "Batch 217, Gradient Norm: 0.0977\n",
      "\u001b[1m217/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m14s\u001b[0m 953ms/step - accuracy: 0.4917 - binary_io_u_5: 0.4919 - loss: 0.9135Batch 218, Loss Value: 0.9007\n",
      "Batch 218, Gradient Norm: 0.0818\n",
      "\u001b[1m218/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m13s\u001b[0m 953ms/step - accuracy: 0.4917 - binary_io_u_5: 0.4919 - loss: 0.9135Batch 219, Loss Value: 0.8924\n",
      "Batch 219, Gradient Norm: 0.6223\n",
      "\u001b[1m219/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m12s\u001b[0m 953ms/step - accuracy: 0.4917 - binary_io_u_5: 0.4919 - loss: 0.9135Batch 220, Loss Value: 0.9092\n",
      "Batch 220, Gradient Norm: 0.2184\n",
      "\u001b[1m220/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m11s\u001b[0m 953ms/step - accuracy: 0.4918 - binary_io_u_5: 0.4920 - loss: 0.9135Batch 221, Loss Value: 0.9022\n",
      "Batch 221, Gradient Norm: 0.3370\n",
      "\u001b[1m221/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m10s\u001b[0m 953ms/step - accuracy: 0.4918 - binary_io_u_5: 0.4920 - loss: 0.9135Batch 222, Loss Value: 0.9041\n",
      "Batch 222, Gradient Norm: 0.1107\n",
      "\u001b[1m222/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m9s\u001b[0m 952ms/step - accuracy: 0.4918 - binary_io_u_5: 0.4920 - loss: 0.9135 Batch 223, Loss Value: 0.8998\n",
      "Batch 223, Gradient Norm: 0.1406\n",
      "\u001b[1m223/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m8s\u001b[0m 952ms/step - accuracy: 0.4918 - binary_io_u_5: 0.4920 - loss: 0.9135Batch 224, Loss Value: 0.9095\n",
      "Batch 224, Gradient Norm: 0.1801\n",
      "\u001b[1m224/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m7s\u001b[0m 952ms/step - accuracy: 0.4918 - binary_io_u_5: 0.4920 - loss: 0.9135Batch 225, Loss Value: 0.9101\n",
      "Batch 225, Gradient Norm: 0.0405\n",
      "\u001b[1m225/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m6s\u001b[0m 951ms/step - accuracy: 0.4918 - binary_io_u_5: 0.4920 - loss: 0.9135Batch 226, Loss Value: 0.9160\n",
      "Batch 226, Gradient Norm: 0.2713\n",
      "\u001b[1m226/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m5s\u001b[0m 950ms/step - accuracy: 0.4918 - binary_io_u_5: 0.4920 - loss: 0.9135Batch 227, Loss Value: 0.9067\n",
      "Batch 227, Gradient Norm: 0.0023\n",
      "\u001b[1m227/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m4s\u001b[0m 950ms/step - accuracy: 0.4918 - binary_io_u_5: 0.4920 - loss: 0.9135Batch 228, Loss Value: 0.9082\n",
      "Batch 228, Gradient Norm: 0.1597\n",
      "\u001b[1m228/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m3s\u001b[0m 950ms/step - accuracy: 0.4919 - binary_io_u_5: 0.4921 - loss: 0.9135Batch 229, Loss Value: 0.9066\n",
      "Batch 229, Gradient Norm: 0.0180\n",
      "\u001b[1m229/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m2s\u001b[0m 950ms/step - accuracy: 0.4919 - binary_io_u_5: 0.4921 - loss: 0.9135Batch 230, Loss Value: 0.9067\n",
      "Batch 230, Gradient Norm: 0.0033\n",
      "\u001b[1m230/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m1s\u001b[0m 950ms/step - accuracy: 0.4919 - binary_io_u_5: 0.4921 - loss: 0.9135Batch 231, Loss Value: 0.9065\n",
      "Batch 231, Gradient Norm: 0.0094\n",
      "\u001b[1m231/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 950ms/step - accuracy: 0.4919 - binary_io_u_5: 0.4921 - loss: 0.9135Batch 232, Loss Value: 0.9066\n",
      "Batch 232, Gradient Norm: 0.0001\n",
      "\u001b[1m232/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m223s\u001b[0m 960ms/step - accuracy: 0.4919 - binary_io_u_5: 0.4921 - loss: 0.9134 - val_accuracy: 0.4973 - val_binary_io_u_5: 0.4949 - val_loss: 0.9072 - learning_rate: 0.0010\n",
      "Epoch 11/200\n",
      "Batch 1, Loss Value: 0.8861\n",
      "Batch 1, Gradient Norm: 0.0008\n",
      "\u001b[1m  1/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4:00\u001b[0m 1s/step - accuracy: 0.5300 - binary_io_u_5: 0.5300 - loss: 0.9083Batch 2, Loss Value: 0.8827\n",
      "Batch 2, Gradient Norm: 0.1860\n",
      "\u001b[1m  2/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:35\u001b[0m 935ms/step - accuracy: 0.5150 - binary_io_u_5: 0.5150 - loss: 0.9148Batch 3, Loss Value: 0.8860\n",
      "Batch 3, Gradient Norm: 0.0010\n",
      "\u001b[1m  3/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:35\u001b[0m 941ms/step - accuracy: 0.5067 - binary_io_u_5: 0.5067 - loss: 0.9184Batch 4, Loss Value: 0.8861\n",
      "Batch 4, Gradient Norm: 0.0007\n",
      "\u001b[1m  4/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:33\u001b[0m 936ms/step - accuracy: 0.5063 - binary_io_u_5: 0.5062 - loss: 0.9173Batch 5, Loss Value: 0.8860\n",
      "Batch 5, Gradient Norm: 0.0015\n",
      "\u001b[1m  5/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:31\u001b[0m 931ms/step - accuracy: 0.5062 - binary_io_u_5: 0.5062 - loss: 0.9163Batch 6, Loss Value: 0.8861\n",
      "Batch 6, Gradient Norm: 0.0000\n",
      "\u001b[1m  6/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:30\u001b[0m 933ms/step - accuracy: 0.5066 - binary_io_u_5: 0.5066 - loss: 0.9153Batch 7, Loss Value: 0.8860\n",
      "Batch 7, Gradient Norm: 0.0011\n",
      "\u001b[1m  7/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:28\u001b[0m 929ms/step - accuracy: 0.5060 - binary_io_u_5: 0.5060 - loss: 0.9151Batch 8, Loss Value: 0.8861\n",
      "Batch 8, Gradient Norm: 0.0001\n",
      "\u001b[1m  8/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:26\u001b[0m 920ms/step - accuracy: 0.5057 - binary_io_u_5: 0.5057 - loss: 0.9148Batch 9, Loss Value: 0.8861\n",
      "Batch 9, Gradient Norm: 0.0001\n",
      "\u001b[1m  9/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:21\u001b[0m 902ms/step - accuracy: 0.5062 - binary_io_u_5: 0.5062 - loss: 0.9140Batch 10, Loss Value: 0.8861\n",
      "Batch 10, Gradient Norm: 0.0000\n",
      "\u001b[1m 10/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:18\u001b[0m 892ms/step - accuracy: 0.5065 - binary_io_u_5: 0.5065 - loss: 0.9133Batch 11, Loss Value: 0.8861\n",
      "Batch 11, Gradient Norm: 0.0001\n",
      "\u001b[1m 11/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:19\u001b[0m 901ms/step - accuracy: 0.5069 - binary_io_u_5: 0.5069 - loss: 0.9125Batch 12, Loss Value: 0.8861\n",
      "Batch 12, Gradient Norm: 0.0008\n",
      "\u001b[1m 12/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:19\u001b[0m 905ms/step - accuracy: 0.5073 - binary_io_u_5: 0.5073 - loss: 0.9119Batch 13, Loss Value: 0.8861\n",
      "Batch 13, Gradient Norm: 0.0000\n",
      "\u001b[1m 13/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:18\u001b[0m 909ms/step - accuracy: 0.5083 - binary_io_u_5: 0.5083 - loss: 0.9109Batch 14, Loss Value: 0.8860\n",
      "Batch 14, Gradient Norm: 0.0022\n",
      "\u001b[1m 14/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:18\u001b[0m 913ms/step - accuracy: 0.5088 - binary_io_u_5: 0.5088 - loss: 0.9103Batch 15, Loss Value: 0.8851\n",
      "Batch 15, Gradient Norm: 0.0573\n",
      "\u001b[1m 15/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:17\u001b[0m 911ms/step - accuracy: 0.5088 - binary_io_u_5: 0.5088 - loss: 0.9099Batch 16, Loss Value: 0.8860\n",
      "Batch 16, Gradient Norm: 0.0014\n",
      "\u001b[1m 16/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:17\u001b[0m 914ms/step - accuracy: 0.5091 - binary_io_u_5: 0.5091 - loss: 0.9095Batch 17, Loss Value: 0.8784\n",
      "Batch 17, Gradient Norm: 0.4107\n",
      "\u001b[1m 17/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:16\u001b[0m 916ms/step - accuracy: 0.5093 - binary_io_u_5: 0.5093 - loss: 0.9091Batch 18, Loss Value: 0.8860\n",
      "Batch 18, Gradient Norm: 0.0062\n",
      "\u001b[1m 18/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:16\u001b[0m 918ms/step - accuracy: 0.5093 - binary_io_u_5: 0.5093 - loss: 0.9089Batch 19, Loss Value: 0.8862\n",
      "Batch 19, Gradient Norm: 0.0118\n",
      "\u001b[1m 19/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:15\u001b[0m 920ms/step - accuracy: 0.5093 - binary_io_u_5: 0.5093 - loss: 0.9086Batch 20, Loss Value: 0.8860\n",
      "Batch 20, Gradient Norm: 0.0054\n",
      "\u001b[1m 20/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:15\u001b[0m 922ms/step - accuracy: 0.5094 - binary_io_u_5: 0.5094 - loss: 0.9084Batch 21, Loss Value: 0.8860\n",
      "Batch 21, Gradient Norm: 0.0079\n",
      "\u001b[1m 21/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:14\u001b[0m 923ms/step - accuracy: 0.5093 - binary_io_u_5: 0.5093 - loss: 0.9082Batch 22, Loss Value: 0.8861\n",
      "Batch 22, Gradient Norm: 0.0002\n",
      "\u001b[1m 22/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:13\u001b[0m 923ms/step - accuracy: 0.5094 - binary_io_u_5: 0.5094 - loss: 0.9080Batch 23, Loss Value: 0.8861\n",
      "Batch 23, Gradient Norm: 0.0005\n",
      "\u001b[1m 23/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:13\u001b[0m 925ms/step - accuracy: 0.5095 - binary_io_u_5: 0.5095 - loss: 0.9078Batch 24, Loss Value: 0.8860\n",
      "Batch 24, Gradient Norm: 0.0037\n",
      "\u001b[1m 24/232\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:12\u001b[0m 925ms/step - accuracy: 0.5096 - binary_io_u_5: 0.5096 - loss: 0.9075Batch 25, Loss Value: 0.8861\n",
      "Batch 25, Gradient Norm: 0.0001\n",
      "\u001b[1m 25/232\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:11\u001b[0m 925ms/step - accuracy: 0.5097 - binary_io_u_5: 0.5097 - loss: 0.9074Batch 26, Loss Value: 0.8861\n",
      "Batch 26, Gradient Norm: 0.0002\n",
      "\u001b[1m 26/232\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:09\u001b[0m 920ms/step - accuracy: 0.5096 - binary_io_u_5: 0.5096 - loss: 0.9072Batch 27, Loss Value: 0.8861\n",
      "Batch 27, Gradient Norm: 0.0004\n",
      "\u001b[1m 27/232\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:07\u001b[0m 913ms/step - accuracy: 0.5095 - binary_io_u_5: 0.5095 - loss: 0.9071Batch 28, Loss Value: 0.8858\n",
      "Batch 28, Gradient Norm: 0.0233\n",
      "\u001b[1m 28/232\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:06\u001b[0m 913ms/step - accuracy: 0.5094 - binary_io_u_5: 0.5094 - loss: 0.9070Batch 29, Loss Value: 0.8862\n",
      "Batch 29, Gradient Norm: 0.0120\n",
      "\u001b[1m 29/232\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:05\u001b[0m 913ms/step - accuracy: 0.5094 - binary_io_u_5: 0.5094 - loss: 0.9070Batch 30, Loss Value: 0.8861\n",
      "Batch 30, Gradient Norm: 0.0004\n",
      "\u001b[1m 30/232\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:04\u001b[0m 913ms/step - accuracy: 0.5092 - binary_io_u_5: 0.5092 - loss: 0.9069Batch 31, Loss Value: 0.8843\n",
      "Batch 31, Gradient Norm: 0.1082\n",
      "\u001b[1m 31/232\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:03\u001b[0m 913ms/step - accuracy: 0.5092 - binary_io_u_5: 0.5092 - loss: 0.9068Batch 32, Loss Value: 0.8578\n",
      "Batch 32, Gradient Norm: 0.6174\n",
      "\u001b[1m 32/232\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:02\u001b[0m 912ms/step - accuracy: 0.5092 - binary_io_u_5: 0.5092 - loss: 0.9068Batch 33, Loss Value: 0.8861\n",
      "Batch 33, Gradient Norm: 0.0065\n",
      "\u001b[1m 33/232\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:01\u001b[0m 912ms/step - accuracy: 0.5091 - binary_io_u_5: 0.5091 - loss: 0.9067Batch 34, Loss Value: 0.8861\n",
      "Batch 34, Gradient Norm: 0.0005\n",
      "\u001b[1m 34/232\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:00\u001b[0m 911ms/step - accuracy: 0.5090 - binary_io_u_5: 0.5090 - loss: 0.9067Batch 35, Loss Value: 0.8860\n",
      "Batch 35, Gradient Norm: 0.0021\n",
      "\u001b[1m 35/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:59\u001b[0m 910ms/step - accuracy: 0.5089 - binary_io_u_5: 0.5089 - loss: 0.9067Batch 36, Loss Value: 0.8861\n",
      "Batch 36, Gradient Norm: 0.0021\n",
      "\u001b[1m 36/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:58\u001b[0m 910ms/step - accuracy: 0.5088 - binary_io_u_5: 0.5088 - loss: 0.9067Batch 37, Loss Value: 0.8860\n",
      "Batch 37, Gradient Norm: 0.0035\n",
      "\u001b[1m 37/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:57\u001b[0m 910ms/step - accuracy: 0.5088 - binary_io_u_5: 0.5088 - loss: 0.9067Batch 38, Loss Value: 0.8861\n",
      "Batch 38, Gradient Norm: 0.0011\n",
      "\u001b[1m 38/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:56\u001b[0m 910ms/step - accuracy: 0.5087 - binary_io_u_5: 0.5087 - loss: 0.9067Batch 39, Loss Value: 0.8861\n",
      "Batch 39, Gradient Norm: 0.0004\n",
      "\u001b[1m 39/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:55\u001b[0m 910ms/step - accuracy: 0.5085 - binary_io_u_5: 0.5085 - loss: 0.9067Batch 40, Loss Value: 0.8797\n",
      "Batch 40, Gradient Norm: 0.3794\n",
      "\u001b[1m 40/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:55\u001b[0m 912ms/step - accuracy: 0.5084 - binary_io_u_5: 0.5084 - loss: 0.9068Batch 41, Loss Value: 0.8854\n",
      "Batch 41, Gradient Norm: 0.0492\n",
      "\u001b[1m 41/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:54\u001b[0m 914ms/step - accuracy: 0.5083 - binary_io_u_5: 0.5083 - loss: 0.9068Batch 42, Loss Value: 0.8863\n",
      "Batch 42, Gradient Norm: 0.0184\n",
      "\u001b[1m 42/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:54\u001b[0m 916ms/step - accuracy: 0.5082 - binary_io_u_5: 0.5082 - loss: 0.9068Batch 43, Loss Value: 0.8861\n",
      "Batch 43, Gradient Norm: 0.0030\n",
      "\u001b[1m 43/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:53\u001b[0m 919ms/step - accuracy: 0.5081 - binary_io_u_5: 0.5081 - loss: 0.9068Batch 44, Loss Value: 0.8868\n",
      "Batch 44, Gradient Norm: 0.0515\n",
      "\u001b[1m 44/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:52\u001b[0m 916ms/step - accuracy: 0.5080 - binary_io_u_5: 0.5080 - loss: 0.9068Batch 45, Loss Value: 0.8860\n",
      "Batch 45, Gradient Norm: 0.0020\n",
      "\u001b[1m 45/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:50\u001b[0m 913ms/step - accuracy: 0.5078 - binary_io_u_5: 0.5078 - loss: 0.9069Batch 46, Loss Value: 0.8861\n",
      "Batch 46, Gradient Norm: 0.0033\n",
      "\u001b[1m 46/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:50\u001b[0m 914ms/step - accuracy: 0.5076 - binary_io_u_5: 0.5076 - loss: 0.9069Batch 47, Loss Value: 0.8588\n",
      "Batch 47, Gradient Norm: 0.0732\n",
      "\u001b[1m 47/232\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:49\u001b[0m 915ms/step - accuracy: 0.5074 - binary_io_u_5: 0.5074 - loss: 0.9070Batch 48, Loss Value: 0.8754\n",
      "Batch 48, Gradient Norm: 0.4138\n",
      "\u001b[1m 48/232\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:48\u001b[0m 917ms/step - accuracy: 0.5072 - binary_io_u_5: 0.5072 - loss: 0.9071Batch 49, Loss Value: 0.8834\n",
      "Batch 49, Gradient Norm: 0.1456\n",
      "\u001b[1m 49/232\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:48\u001b[0m 919ms/step - accuracy: 0.5070 - binary_io_u_5: 0.5070 - loss: 0.9071Batch 50, Loss Value: 0.8866\n",
      "Batch 50, Gradient Norm: 0.1567\n",
      "\u001b[1m 50/232\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:47\u001b[0m 920ms/step - accuracy: 0.5069 - binary_io_u_5: 0.5069 - loss: 0.9072Batch 51, Loss Value: 0.8537\n",
      "Batch 51, Gradient Norm: 0.3538\n",
      "\u001b[1m 51/232\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:46\u001b[0m 921ms/step - accuracy: 0.5067 - binary_io_u_5: 0.5067 - loss: 0.9072Batch 52, Loss Value: 0.8738\n",
      "Batch 52, Gradient Norm: 0.3978\n",
      "\u001b[1m 52/232\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:46\u001b[0m 923ms/step - accuracy: 0.5066 - binary_io_u_5: 0.5066 - loss: 0.9073Batch 53, Loss Value: 0.8863\n",
      "Batch 53, Gradient Norm: 0.0130\n",
      "\u001b[1m 53/232\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:45\u001b[0m 924ms/step - accuracy: 0.5065 - binary_io_u_5: 0.5065 - loss: 0.9073Batch 54, Loss Value: 0.8857\n",
      "Batch 54, Gradient Norm: 0.0230\n",
      "\u001b[1m 54/232\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:44\u001b[0m 925ms/step - accuracy: 0.5064 - binary_io_u_5: 0.5064 - loss: 0.9073Batch 55, Loss Value: 0.8923\n",
      "Batch 55, Gradient Norm: 0.1504\n",
      "\u001b[1m 55/232\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:43\u001b[0m 925ms/step - accuracy: 0.5063 - binary_io_u_5: 0.5063 - loss: 0.9073Batch 56, Loss Value: 0.8597\n",
      "Batch 56, Gradient Norm: 0.4698\n",
      "\u001b[1m 56/232\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:42\u001b[0m 926ms/step - accuracy: 0.5062 - binary_io_u_5: 0.5062 - loss: 0.9074Batch 57, Loss Value: 0.8848\n",
      "Batch 57, Gradient Norm: 0.0859\n",
      "\u001b[1m 57/232\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:42\u001b[0m 927ms/step - accuracy: 0.5061 - binary_io_u_5: 0.5061 - loss: 0.9074Batch 58, Loss Value: 0.8901\n",
      "Batch 58, Gradient Norm: 0.1603\n",
      "\u001b[1m 58/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:41\u001b[0m 928ms/step - accuracy: 0.5060 - binary_io_u_5: 0.5060 - loss: 0.9074Batch 59, Loss Value: 0.8862\n",
      "Batch 59, Gradient Norm: 0.0123\n",
      "\u001b[1m 59/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:40\u001b[0m 928ms/step - accuracy: 0.5060 - binary_io_u_5: 0.5060 - loss: 0.9074Batch 60, Loss Value: 0.8649\n",
      "Batch 60, Gradient Norm: 0.5637\n",
      "\u001b[1m 60/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:39\u001b[0m 930ms/step - accuracy: 0.5059 - binary_io_u_5: 0.5059 - loss: 0.9074Batch 61, Loss Value: 0.8687\n",
      "Batch 61, Gradient Norm: 0.1729\n",
      "\u001b[1m 61/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:38\u001b[0m 928ms/step - accuracy: 0.5058 - binary_io_u_5: 0.5058 - loss: 0.9075Batch 62, Loss Value: 0.8682\n",
      "Batch 62, Gradient Norm: 0.1786\n",
      "\u001b[1m 62/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:37\u001b[0m 925ms/step - accuracy: 0.5057 - binary_io_u_5: 0.5057 - loss: 0.9075Batch 63, Loss Value: 0.8957\n",
      "Batch 63, Gradient Norm: 0.3183\n",
      "\u001b[1m 63/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:36\u001b[0m 926ms/step - accuracy: 0.5056 - binary_io_u_5: 0.5056 - loss: 0.9075Batch 64, Loss Value: 0.8818\n",
      "Batch 64, Gradient Norm: 0.2068\n",
      "\u001b[1m 64/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:35\u001b[0m 927ms/step - accuracy: 0.5055 - binary_io_u_5: 0.5055 - loss: 0.9075Batch 65, Loss Value: 0.8513\n",
      "Batch 65, Gradient Norm: 0.2276\n",
      "\u001b[1m 65/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:34\u001b[0m 928ms/step - accuracy: 0.5055 - binary_io_u_5: 0.5055 - loss: 0.9075Batch 66, Loss Value: 0.9018\n",
      "Batch 66, Gradient Norm: 0.2847\n",
      "\u001b[1m 66/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:34\u001b[0m 929ms/step - accuracy: 0.5054 - binary_io_u_5: 0.5054 - loss: 0.9075Batch 67, Loss Value: 0.8703\n",
      "Batch 67, Gradient Norm: 0.3842\n",
      "\u001b[1m 67/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:33\u001b[0m 930ms/step - accuracy: 0.5053 - binary_io_u_5: 0.5053 - loss: 0.9075Batch 68, Loss Value: 0.8673\n",
      "Batch 68, Gradient Norm: 0.0552\n",
      "\u001b[1m 68/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:32\u001b[0m 931ms/step - accuracy: 0.5052 - binary_io_u_5: 0.5052 - loss: 0.9076Batch 69, Loss Value: 0.8730\n",
      "Batch 69, Gradient Norm: 0.6513\n",
      "\u001b[1m 69/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:31\u001b[0m 932ms/step - accuracy: 0.5051 - binary_io_u_5: 0.5051 - loss: 0.9076Batch 70, Loss Value: 0.8867\n",
      "Batch 70, Gradient Norm: 0.1020\n",
      "\u001b[1m 70/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:31\u001b[0m 933ms/step - accuracy: 0.5051 - binary_io_u_5: 0.5051 - loss: 0.9076Batch 71, Loss Value: 0.9323\n",
      "Batch 71, Gradient Norm: 0.0560\n",
      "\u001b[1m 71/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:30\u001b[0m 934ms/step - accuracy: 0.5050 - binary_io_u_5: 0.5050 - loss: 0.9076Batch 72, Loss Value: 0.9417\n",
      "Batch 72, Gradient Norm: 0.0526\n",
      "\u001b[1m 72/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:29\u001b[0m 935ms/step - accuracy: 0.5049 - binary_io_u_5: 0.5049 - loss: 0.9075Batch 73, Loss Value: 0.8628\n",
      "Batch 73, Gradient Norm: 0.3243\n",
      "\u001b[1m 73/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:28\u001b[0m 935ms/step - accuracy: 0.5049 - binary_io_u_5: 0.5049 - loss: 0.9075Batch 74, Loss Value: 0.9243\n",
      "Batch 74, Gradient Norm: 0.4305\n",
      "\u001b[1m 74/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:27\u001b[0m 936ms/step - accuracy: 0.5048 - binary_io_u_5: 0.5048 - loss: 0.9076Batch 75, Loss Value: 0.9019\n",
      "Batch 75, Gradient Norm: 0.1283\n",
      "\u001b[1m 75/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:27\u001b[0m 936ms/step - accuracy: 0.5047 - binary_io_u_5: 0.5047 - loss: 0.9076Batch 76, Loss Value: 0.9396\n",
      "Batch 76, Gradient Norm: 0.5724\n",
      "\u001b[1m 76/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:26\u001b[0m 937ms/step - accuracy: 0.5047 - binary_io_u_5: 0.5046 - loss: 0.9076Batch 77, Loss Value: 0.8865\n",
      "Batch 77, Gradient Norm: 0.0811\n",
      "\u001b[1m 77/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:25\u001b[0m 937ms/step - accuracy: 0.5046 - binary_io_u_5: 0.5045 - loss: 0.9076Batch 78, Loss Value: 0.8774\n",
      "Batch 78, Gradient Norm: 0.2332\n",
      "\u001b[1m 78/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:24\u001b[0m 935ms/step - accuracy: 0.5045 - binary_io_u_5: 0.5045 - loss: 0.9076Batch 79, Loss Value: 0.9354\n",
      "Batch 79, Gradient Norm: 0.0517\n",
      "\u001b[1m 79/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:22\u001b[0m 932ms/step - accuracy: 0.5045 - binary_io_u_5: 0.5044 - loss: 0.9076Batch 80, Loss Value: 0.8601\n",
      "Batch 80, Gradient Norm: 0.1023\n",
      "\u001b[1m 80/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:21\u001b[0m 932ms/step - accuracy: 0.5044 - binary_io_u_5: 0.5044 - loss: 0.9076Batch 81, Loss Value: 0.9289\n",
      "Batch 81, Gradient Norm: 0.3848\n",
      "\u001b[1m 81/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:20\u001b[0m 933ms/step - accuracy: 0.5044 - binary_io_u_5: 0.5043 - loss: 0.9077Batch 82, Loss Value: 0.9043\n",
      "Batch 82, Gradient Norm: 0.7144\n",
      "\u001b[1m 82/232\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m2:20\u001b[0m 934ms/step - accuracy: 0.5044 - binary_io_u_5: 0.5043 - loss: 0.9077Batch 83, Loss Value: 0.9012\n",
      "Batch 83, Gradient Norm: 0.6121\n",
      "\u001b[1m 83/232\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m2:19\u001b[0m 935ms/step - accuracy: 0.5043 - binary_io_u_5: 0.5042 - loss: 0.9077Batch 84, Loss Value: 0.9249\n",
      "Batch 84, Gradient Norm: 0.4204\n",
      "\u001b[1m 84/232\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m2:18\u001b[0m 935ms/step - accuracy: 0.5043 - binary_io_u_5: 0.5041 - loss: 0.9078Batch 85, Loss Value: 0.8787\n",
      "Batch 85, Gradient Norm: 0.2964\n",
      "\u001b[1m 85/232\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m2:17\u001b[0m 936ms/step - accuracy: 0.5043 - binary_io_u_5: 0.5041 - loss: 0.9078Batch 86, Loss Value: 0.9444\n",
      "Batch 86, Gradient Norm: 0.0257\n",
      "\u001b[1m 86/232\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m2:16\u001b[0m 937ms/step - accuracy: 0.5042 - binary_io_u_5: 0.5041 - loss: 0.9078Batch 87, Loss Value: 0.8919\n",
      "Batch 87, Gradient Norm: 0.3571\n",
      "\u001b[1m 87/232\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m2:15\u001b[0m 937ms/step - accuracy: 0.5042 - binary_io_u_5: 0.5040 - loss: 0.9079Batch 88, Loss Value: 0.8772\n",
      "Batch 88, Gradient Norm: 0.1301\n",
      "\u001b[1m 88/232\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m2:14\u001b[0m 937ms/step - accuracy: 0.5042 - binary_io_u_5: 0.5040 - loss: 0.9079Batch 89, Loss Value: 0.9091\n",
      "Batch 89, Gradient Norm: 0.1628\n",
      "\u001b[1m 89/232\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m2:14\u001b[0m 937ms/step - accuracy: 0.5042 - binary_io_u_5: 0.5039 - loss: 0.9079Batch 90, Loss Value: 0.9336\n",
      "Batch 90, Gradient Norm: 0.1889\n",
      "\u001b[1m 90/232\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m2:13\u001b[0m 938ms/step - accuracy: 0.5042 - binary_io_u_5: 0.5039 - loss: 0.9080Batch 91, Loss Value: 0.9459\n",
      "Batch 91, Gradient Norm: 0.0117\n",
      "\u001b[1m 91/232\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m2:12\u001b[0m 938ms/step - accuracy: 0.5041 - binary_io_u_5: 0.5039 - loss: 0.9080Batch 92, Loss Value: 0.9146\n",
      "Batch 92, Gradient Norm: 0.2840\n",
      "\u001b[1m 92/232\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m2:11\u001b[0m 938ms/step - accuracy: 0.5041 - binary_io_u_5: 0.5039 - loss: 0.9080Batch 93, Loss Value: 0.8985\n",
      "Batch 93, Gradient Norm: 0.9496\n",
      "\u001b[1m 93/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m2:10\u001b[0m 939ms/step - accuracy: 0.5041 - binary_io_u_5: 0.5038 - loss: 0.9081Batch 94, Loss Value: 0.9287\n",
      "Batch 94, Gradient Norm: 0.1924\n",
      "\u001b[1m 94/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m2:09\u001b[0m 939ms/step - accuracy: 0.5041 - binary_io_u_5: 0.5038 - loss: 0.9081Batch 95, Loss Value: 0.9052\n",
      "Batch 95, Gradient Norm: 0.4166\n",
      "\u001b[1m 95/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m2:08\u001b[0m 938ms/step - accuracy: 0.5040 - binary_io_u_5: 0.5037 - loss: 0.9082Batch 96, Loss Value: 0.9112\n",
      "Batch 96, Gradient Norm: 0.2236\n",
      "\u001b[1m 96/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m2:07\u001b[0m 937ms/step - accuracy: 0.5040 - binary_io_u_5: 0.5037 - loss: 0.9082Batch 97, Loss Value: 0.8897\n",
      "Batch 97, Gradient Norm: 0.3409\n",
      "\u001b[1m 97/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m2:06\u001b[0m 937ms/step - accuracy: 0.5040 - binary_io_u_5: 0.5037 - loss: 0.9083Batch 98, Loss Value: 0.9132\n",
      "Batch 98, Gradient Norm: 0.5283\n",
      "\u001b[1m 98/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m2:05\u001b[0m 937ms/step - accuracy: 0.5040 - binary_io_u_5: 0.5036 - loss: 0.9083Batch 99, Loss Value: 0.8885\n",
      "Batch 99, Gradient Norm: 0.4057\n",
      "\u001b[1m 99/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m2:04\u001b[0m 937ms/step - accuracy: 0.5040 - binary_io_u_5: 0.5036 - loss: 0.9084Batch 100, Loss Value: 0.9410\n",
      "Batch 100, Gradient Norm: 0.7504\n",
      "\u001b[1m100/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m2:03\u001b[0m 937ms/step - accuracy: 0.5040 - binary_io_u_5: 0.5036 - loss: 0.9084Batch 101, Loss Value: 0.8821\n",
      "Batch 101, Gradient Norm: 0.1299\n",
      "\u001b[1m101/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m2:02\u001b[0m 938ms/step - accuracy: 0.5039 - binary_io_u_5: 0.5035 - loss: 0.9084Batch 102, Loss Value: 0.9216\n",
      "Batch 102, Gradient Norm: 0.5084\n",
      "\u001b[1m102/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m2:01\u001b[0m 938ms/step - accuracy: 0.5039 - binary_io_u_5: 0.5035 - loss: 0.9085Batch 103, Loss Value: 0.9401\n",
      "Batch 103, Gradient Norm: 0.7838\n",
      "\u001b[1m103/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m2:01\u001b[0m 938ms/step - accuracy: 0.5039 - binary_io_u_5: 0.5034 - loss: 0.9085Batch 104, Loss Value: 0.8907\n",
      "Batch 104, Gradient Norm: 0.6445\n",
      "\u001b[1m104/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m2:00\u001b[0m 939ms/step - accuracy: 0.5039 - binary_io_u_5: 0.5034 - loss: 0.9085Batch 105, Loss Value: 0.9428\n",
      "Batch 105, Gradient Norm: 0.0086\n",
      "\u001b[1m105/232\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m1:59\u001b[0m 940ms/step - accuracy: 0.5039 - binary_io_u_5: 0.5034 - loss: 0.9086Batch 106, Loss Value: 0.9124\n",
      "Batch 106, Gradient Norm: 0.2674\n",
      "\u001b[1m106/232\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m1:58\u001b[0m 940ms/step - accuracy: 0.5039 - binary_io_u_5: 0.5034 - loss: 0.9086Batch 107, Loss Value: 0.9056\n",
      "Batch 107, Gradient Norm: 0.0026\n",
      "\u001b[1m107/232\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m1:57\u001b[0m 941ms/step - accuracy: 0.5039 - binary_io_u_5: 0.5033 - loss: 0.9086Batch 108, Loss Value: 0.9041\n",
      "Batch 108, Gradient Norm: 0.6332\n",
      "\u001b[1m108/232\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m1:56\u001b[0m 942ms/step - accuracy: 0.5039 - binary_io_u_5: 0.5033 - loss: 0.9087Batch 109, Loss Value: 0.9015\n",
      "Batch 109, Gradient Norm: 0.2272\n",
      "\u001b[1m109/232\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m1:55\u001b[0m 942ms/step - accuracy: 0.5039 - binary_io_u_5: 0.5033 - loss: 0.9087Batch 110, Loss Value: 0.8834\n",
      "Batch 110, Gradient Norm: 0.2818\n",
      "\u001b[1m110/232\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m1:55\u001b[0m 943ms/step - accuracy: 0.5039 - binary_io_u_5: 0.5032 - loss: 0.9087Batch 111, Loss Value: 0.9226\n",
      "Batch 111, Gradient Norm: 0.3408\n",
      "\u001b[1m111/232\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m1:54\u001b[0m 943ms/step - accuracy: 0.5039 - binary_io_u_5: 0.5032 - loss: 0.9088Batch 112, Loss Value: 0.8895\n",
      "Batch 112, Gradient Norm: 0.7066\n",
      "\u001b[1m112/232\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m1:53\u001b[0m 943ms/step - accuracy: 0.5039 - binary_io_u_5: 0.5031 - loss: 0.9088Batch 113, Loss Value: 0.9045\n",
      "Batch 113, Gradient Norm: 0.2614\n",
      "\u001b[1m113/232\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m1:52\u001b[0m 942ms/step - accuracy: 0.5039 - binary_io_u_5: 0.5031 - loss: 0.9089Batch 114, Loss Value: 0.9424\n",
      "Batch 114, Gradient Norm: 0.0311\n",
      "\u001b[1m114/232\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m1:50\u001b[0m 940ms/step - accuracy: 0.5038 - binary_io_u_5: 0.5030 - loss: 0.9089Batch 115, Loss Value: 0.9011\n",
      "Batch 115, Gradient Norm: 0.2143\n",
      "\u001b[1m115/232\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m1:50\u001b[0m 941ms/step - accuracy: 0.5038 - binary_io_u_5: 0.5030 - loss: 0.9090Batch 116, Loss Value: 0.8665\n",
      "Batch 116, Gradient Norm: 0.1710\n",
      "\u001b[1m116/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m1:49\u001b[0m 941ms/step - accuracy: 0.5038 - binary_io_u_5: 0.5029 - loss: 0.9090Batch 117, Loss Value: 0.8878\n",
      "Batch 117, Gradient Norm: 0.2927\n",
      "\u001b[1m117/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m1:48\u001b[0m 941ms/step - accuracy: 0.5038 - binary_io_u_5: 0.5029 - loss: 0.9091Batch 118, Loss Value: 0.9075\n",
      "Batch 118, Gradient Norm: 0.3169\n",
      "\u001b[1m118/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m1:47\u001b[0m 941ms/step - accuracy: 0.5037 - binary_io_u_5: 0.5028 - loss: 0.9091Batch 119, Loss Value: 0.8972\n",
      "Batch 119, Gradient Norm: 0.0087\n",
      "\u001b[1m119/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m1:46\u001b[0m 942ms/step - accuracy: 0.5037 - binary_io_u_5: 0.5027 - loss: 0.9091Batch 120, Loss Value: 0.8698\n",
      "Batch 120, Gradient Norm: 0.1297\n",
      "\u001b[1m120/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m1:45\u001b[0m 942ms/step - accuracy: 0.5037 - binary_io_u_5: 0.5027 - loss: 0.9092Batch 121, Loss Value: 0.8899\n",
      "Batch 121, Gradient Norm: 0.4118\n",
      "\u001b[1m121/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m1:44\u001b[0m 942ms/step - accuracy: 0.5036 - binary_io_u_5: 0.5026 - loss: 0.9092Batch 122, Loss Value: 0.9130\n",
      "Batch 122, Gradient Norm: 0.2726\n",
      "\u001b[1m122/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m1:43\u001b[0m 943ms/step - accuracy: 0.5036 - binary_io_u_5: 0.5025 - loss: 0.9093Batch 123, Loss Value: 0.9146\n",
      "Batch 123, Gradient Norm: 0.3138\n",
      "\u001b[1m123/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m1:42\u001b[0m 943ms/step - accuracy: 0.5036 - binary_io_u_5: 0.5025 - loss: 0.9093Batch 124, Loss Value: 0.8670\n",
      "Batch 124, Gradient Norm: 0.5859\n",
      "\u001b[1m124/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m1:41\u001b[0m 943ms/step - accuracy: 0.5036 - binary_io_u_5: 0.5024 - loss: 0.9093Batch 125, Loss Value: 0.8745\n",
      "Batch 125, Gradient Norm: 0.0034\n",
      "\u001b[1m125/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m1:40\u001b[0m 944ms/step - accuracy: 0.5035 - binary_io_u_5: 0.5024 - loss: 0.9094Batch 126, Loss Value: 0.9166\n",
      "Batch 126, Gradient Norm: 0.1078\n",
      "\u001b[1m126/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m1:40\u001b[0m 944ms/step - accuracy: 0.5035 - binary_io_u_5: 0.5023 - loss: 0.9094Batch 127, Loss Value: 0.8941\n",
      "Batch 127, Gradient Norm: 0.6690\n",
      "\u001b[1m127/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m1:39\u001b[0m 944ms/step - accuracy: 0.5035 - binary_io_u_5: 0.5023 - loss: 0.9095Batch 128, Loss Value: 0.8594\n",
      "Batch 128, Gradient Norm: 0.1569\n",
      "\u001b[1m128/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m1:38\u001b[0m 945ms/step - accuracy: 0.5034 - binary_io_u_5: 0.5022 - loss: 0.9095Batch 129, Loss Value: 0.8822\n",
      "Batch 129, Gradient Norm: 0.1720\n",
      "\u001b[1m129/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m1:37\u001b[0m 945ms/step - accuracy: 0.5034 - binary_io_u_5: 0.5021 - loss: 0.9095Batch 130, Loss Value: 0.8747\n",
      "Batch 130, Gradient Norm: 0.3048\n",
      "\u001b[1m130/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m1:36\u001b[0m 944ms/step - accuracy: 0.5034 - binary_io_u_5: 0.5021 - loss: 0.9096Batch 131, Loss Value: 0.8734\n",
      "Batch 131, Gradient Norm: 0.0433\n",
      "\u001b[1m131/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m1:35\u001b[0m 943ms/step - accuracy: 0.5034 - binary_io_u_5: 0.5020 - loss: 0.9096Batch 132, Loss Value: 0.8635\n",
      "Batch 132, Gradient Norm: 0.4603\n",
      "\u001b[1m132/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m1:34\u001b[0m 943ms/step - accuracy: 0.5034 - binary_io_u_5: 0.5020 - loss: 0.9097Batch 133, Loss Value: 0.8821\n",
      "Batch 133, Gradient Norm: 0.1695\n",
      "\u001b[1m133/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m1:33\u001b[0m 944ms/step - accuracy: 0.5033 - binary_io_u_5: 0.5020 - loss: 0.9097Batch 134, Loss Value: 0.9022\n",
      "Batch 134, Gradient Norm: 0.1378\n",
      "\u001b[1m134/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m1:32\u001b[0m 944ms/step - accuracy: 0.5033 - binary_io_u_5: 0.5019 - loss: 0.9097Batch 135, Loss Value: 0.8797\n",
      "Batch 135, Gradient Norm: 0.1761\n",
      "\u001b[1m135/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m1:31\u001b[0m 945ms/step - accuracy: 0.5033 - binary_io_u_5: 0.5019 - loss: 0.9098Batch 136, Loss Value: 0.9018\n",
      "Batch 136, Gradient Norm: 0.4309\n",
      "\u001b[1m136/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m1:30\u001b[0m 945ms/step - accuracy: 0.5033 - binary_io_u_5: 0.5018 - loss: 0.9098Batch 137, Loss Value: 0.8932\n",
      "Batch 137, Gradient Norm: 0.2508\n",
      "\u001b[1m137/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m1:29\u001b[0m 945ms/step - accuracy: 0.5033 - binary_io_u_5: 0.5018 - loss: 0.9098Batch 138, Loss Value: 0.8759\n",
      "Batch 138, Gradient Norm: 0.0500\n",
      "\u001b[1m138/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m1:28\u001b[0m 945ms/step - accuracy: 0.5032 - binary_io_u_5: 0.5017 - loss: 0.9099Batch 139, Loss Value: 0.8739\n",
      "Batch 139, Gradient Norm: 0.3757\n",
      "\u001b[1m139/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m1:27\u001b[0m 946ms/step - accuracy: 0.5032 - binary_io_u_5: 0.5017 - loss: 0.9099Batch 140, Loss Value: 0.8838\n",
      "Batch 140, Gradient Norm: 0.1221\n",
      "\u001b[1m140/232\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m1:27\u001b[0m 946ms/step - accuracy: 0.5032 - binary_io_u_5: 0.5016 - loss: 0.9099Batch 141, Loss Value: 0.8692\n",
      "Batch 141, Gradient Norm: 0.1730\n",
      "\u001b[1m141/232\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m1:26\u001b[0m 946ms/step - accuracy: 0.5032 - binary_io_u_5: 0.5016 - loss: 0.9100Batch 142, Loss Value: 0.8942\n",
      "Batch 142, Gradient Norm: 0.4061\n",
      "\u001b[1m142/232\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m1:25\u001b[0m 947ms/step - accuracy: 0.5032 - binary_io_u_5: 0.5015 - loss: 0.9100Batch 143, Loss Value: 0.9068\n",
      "Batch 143, Gradient Norm: 0.2402\n",
      "\u001b[1m143/232\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m1:24\u001b[0m 947ms/step - accuracy: 0.5031 - binary_io_u_5: 0.5015 - loss: 0.9100Batch 144, Loss Value: 0.8927\n",
      "Batch 144, Gradient Norm: 0.0705\n",
      "\u001b[1m144/232\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m1:23\u001b[0m 947ms/step - accuracy: 0.5031 - binary_io_u_5: 0.5014 - loss: 0.9101Batch 145, Loss Value: 0.8809\n",
      "Batch 145, Gradient Norm: 0.1051\n",
      "\u001b[1m145/232\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m1:22\u001b[0m 948ms/step - accuracy: 0.5031 - binary_io_u_5: 0.5014 - loss: 0.9101Batch 146, Loss Value: 0.8904\n",
      "Batch 146, Gradient Norm: 0.1835\n",
      "\u001b[1m146/232\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m1:21\u001b[0m 948ms/step - accuracy: 0.5030 - binary_io_u_5: 0.5013 - loss: 0.9101Batch 147, Loss Value: 0.8720\n",
      "Batch 147, Gradient Norm: 0.2868\n",
      "\u001b[1m147/232\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m1:20\u001b[0m 947ms/step - accuracy: 0.5030 - binary_io_u_5: 0.5013 - loss: 0.9102Batch 148, Loss Value: 0.9114\n",
      "Batch 148, Gradient Norm: 0.2509\n",
      "\u001b[1m148/232\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m1:19\u001b[0m 946ms/step - accuracy: 0.5030 - binary_io_u_5: 0.5012 - loss: 0.9102Batch 149, Loss Value: 0.8820\n",
      "Batch 149, Gradient Norm: 0.3409\n",
      "\u001b[1m149/232\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m1:18\u001b[0m 946ms/step - accuracy: 0.5029 - binary_io_u_5: 0.5012 - loss: 0.9102Batch 150, Loss Value: 0.8549\n",
      "Batch 150, Gradient Norm: 0.1464\n",
      "\u001b[1m150/232\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m1:17\u001b[0m 947ms/step - accuracy: 0.5029 - binary_io_u_5: 0.5011 - loss: 0.9103Batch 151, Loss Value: 0.8992\n",
      "Batch 151, Gradient Norm: 0.3207\n",
      "\u001b[1m151/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m1:16\u001b[0m 947ms/step - accuracy: 0.5029 - binary_io_u_5: 0.5011 - loss: 0.9103Batch 152, Loss Value: 0.8645\n",
      "Batch 152, Gradient Norm: 0.0080\n",
      "\u001b[1m152/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m1:15\u001b[0m 947ms/step - accuracy: 0.5028 - binary_io_u_5: 0.5010 - loss: 0.9103Batch 153, Loss Value: 0.8857\n",
      "Batch 153, Gradient Norm: 0.0297\n",
      "\u001b[1m153/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m1:14\u001b[0m 947ms/step - accuracy: 0.5028 - binary_io_u_5: 0.5010 - loss: 0.9104Batch 154, Loss Value: 0.8842\n",
      "Batch 154, Gradient Norm: 0.0861\n",
      "\u001b[1m154/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m1:13\u001b[0m 948ms/step - accuracy: 0.5028 - binary_io_u_5: 0.5009 - loss: 0.9104Batch 155, Loss Value: 0.8857\n",
      "Batch 155, Gradient Norm: 0.0225\n",
      "\u001b[1m155/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m1:12\u001b[0m 948ms/step - accuracy: 0.5028 - binary_io_u_5: 0.5009 - loss: 0.9104Batch 156, Loss Value: 0.8858\n",
      "Batch 156, Gradient Norm: 0.0172\n",
      "\u001b[1m156/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m1:12\u001b[0m 948ms/step - accuracy: 0.5027 - binary_io_u_5: 0.5008 - loss: 0.9105Batch 157, Loss Value: 0.8902\n",
      "Batch 157, Gradient Norm: 0.2989\n",
      "\u001b[1m157/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m1:11\u001b[0m 949ms/step - accuracy: 0.5027 - binary_io_u_5: 0.5008 - loss: 0.9105Batch 158, Loss Value: 0.8859\n",
      "Batch 158, Gradient Norm: 0.0110\n",
      "\u001b[1m158/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m1:10\u001b[0m 949ms/step - accuracy: 0.5027 - binary_io_u_5: 0.5007 - loss: 0.9105Batch 159, Loss Value: 0.8861\n",
      "Batch 159, Gradient Norm: 0.0002\n",
      "\u001b[1m159/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m1:09\u001b[0m 949ms/step - accuracy: 0.5026 - binary_io_u_5: 0.5007 - loss: 0.9105Batch 160, Loss Value: 0.8857\n",
      "Batch 160, Gradient Norm: 0.0265\n",
      "\u001b[1m160/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m1:08\u001b[0m 949ms/step - accuracy: 0.5026 - binary_io_u_5: 0.5007 - loss: 0.9106Batch 161, Loss Value: 0.8799\n",
      "Batch 161, Gradient Norm: 0.1590\n",
      "\u001b[1m161/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m1:07\u001b[0m 950ms/step - accuracy: 0.5026 - binary_io_u_5: 0.5006 - loss: 0.9106Batch 162, Loss Value: 0.8877\n",
      "Batch 162, Gradient Norm: 0.1519\n",
      "\u001b[1m162/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m1:06\u001b[0m 950ms/step - accuracy: 0.5026 - binary_io_u_5: 0.5006 - loss: 0.9106Batch 163, Loss Value: 0.8864\n",
      "Batch 163, Gradient Norm: 0.0940\n",
      "\u001b[1m163/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m1:05\u001b[0m 949ms/step - accuracy: 0.5025 - binary_io_u_5: 0.5005 - loss: 0.9106Batch 164, Loss Value: 0.8861\n",
      "Batch 164, Gradient Norm: 0.0011\n",
      "\u001b[1m164/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m1:04\u001b[0m 948ms/step - accuracy: 0.5025 - binary_io_u_5: 0.5005 - loss: 0.9107Batch 165, Loss Value: 0.8861\n",
      "Batch 165, Gradient Norm: 0.0032\n",
      "\u001b[1m165/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m1:03\u001b[0m 949ms/step - accuracy: 0.5025 - binary_io_u_5: 0.5005 - loss: 0.9107Batch 166, Loss Value: 0.8861\n",
      "Batch 166, Gradient Norm: 0.0003\n",
      "\u001b[1m166/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m1:02\u001b[0m 949ms/step - accuracy: 0.5025 - binary_io_u_5: 0.5004 - loss: 0.9107Batch 167, Loss Value: 0.8895\n",
      "Batch 167, Gradient Norm: 0.0647\n",
      "\u001b[1m167/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m1:01\u001b[0m 949ms/step - accuracy: 0.5024 - binary_io_u_5: 0.5004 - loss: 0.9107Batch 168, Loss Value: 0.8824\n",
      "Batch 168, Gradient Norm: 0.0387\n",
      "\u001b[1m168/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m1:00\u001b[0m 949ms/step - accuracy: 0.5024 - binary_io_u_5: 0.5004 - loss: 0.9107Batch 169, Loss Value: 0.8817\n",
      "Batch 169, Gradient Norm: 0.0861\n",
      "\u001b[1m169/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m59s\u001b[0m 950ms/step - accuracy: 0.5024 - binary_io_u_5: 0.5003 - loss: 0.9108 Batch 170, Loss Value: 0.8858\n",
      "Batch 170, Gradient Norm: 0.0179\n",
      "\u001b[1m170/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m58s\u001b[0m 950ms/step - accuracy: 0.5024 - binary_io_u_5: 0.5003 - loss: 0.9108Batch 171, Loss Value: 0.8858\n",
      "Batch 171, Gradient Norm: 0.0220\n",
      "\u001b[1m171/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m57s\u001b[0m 950ms/step - accuracy: 0.5024 - binary_io_u_5: 0.5003 - loss: 0.9108Batch 172, Loss Value: 0.8844\n",
      "Batch 172, Gradient Norm: 0.1469\n",
      "\u001b[1m172/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m57s\u001b[0m 950ms/step - accuracy: 0.5023 - binary_io_u_5: 0.5002 - loss: 0.9108Batch 173, Loss Value: 0.8730\n",
      "Batch 173, Gradient Norm: 0.2652\n",
      "\u001b[1m173/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m56s\u001b[0m 950ms/step - accuracy: 0.5023 - binary_io_u_5: 0.5002 - loss: 0.9108Batch 174, Loss Value: 0.8854\n",
      "Batch 174, Gradient Norm: 0.0425\n",
      "\u001b[1m174/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m55s\u001b[0m 951ms/step - accuracy: 0.5023 - binary_io_u_5: 0.5002 - loss: 0.9109Batch 175, Loss Value: 0.8860\n",
      "Batch 175, Gradient Norm: 0.0010\n",
      "\u001b[1m175/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m54s\u001b[0m 950ms/step - accuracy: 0.5023 - binary_io_u_5: 0.5001 - loss: 0.9109Batch 176, Loss Value: 0.8860\n",
      "Batch 176, Gradient Norm: 0.0035\n",
      "\u001b[1m176/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m53s\u001b[0m 950ms/step - accuracy: 0.5022 - binary_io_u_5: 0.5001 - loss: 0.9109Batch 177, Loss Value: 0.8860\n",
      "Batch 177, Gradient Norm: 0.0059\n",
      "\u001b[1m177/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m52s\u001b[0m 950ms/step - accuracy: 0.5022 - binary_io_u_5: 0.5001 - loss: 0.9109Batch 178, Loss Value: 0.8849\n",
      "Batch 178, Gradient Norm: 0.1051\n",
      "\u001b[1m178/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m51s\u001b[0m 950ms/step - accuracy: 0.5022 - binary_io_u_5: 0.5000 - loss: 0.9109Batch 179, Loss Value: 0.8960\n",
      "Batch 179, Gradient Norm: 0.3384\n",
      "\u001b[1m179/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m50s\u001b[0m 951ms/step - accuracy: 0.5022 - binary_io_u_5: 0.5000 - loss: 0.9110Batch 180, Loss Value: 0.8861\n",
      "Batch 180, Gradient Norm: 0.0012\n",
      "\u001b[1m180/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m49s\u001b[0m 950ms/step - accuracy: 0.5022 - binary_io_u_5: 0.5000 - loss: 0.9110Batch 181, Loss Value: 0.8861\n",
      "Batch 181, Gradient Norm: 0.0027\n",
      "\u001b[1m181/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m48s\u001b[0m 949ms/step - accuracy: 0.5021 - binary_io_u_5: 0.4999 - loss: 0.9110Batch 182, Loss Value: 0.8861\n",
      "Batch 182, Gradient Norm: 0.0020\n",
      "\u001b[1m182/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m47s\u001b[0m 949ms/step - accuracy: 0.5021 - binary_io_u_5: 0.4999 - loss: 0.9110Batch 183, Loss Value: 0.8787\n",
      "Batch 183, Gradient Norm: 0.4213\n",
      "\u001b[1m183/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m46s\u001b[0m 949ms/step - accuracy: 0.5021 - binary_io_u_5: 0.4999 - loss: 0.9110Batch 184, Loss Value: 0.8893\n",
      "Batch 184, Gradient Norm: 0.1444\n",
      "\u001b[1m184/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m45s\u001b[0m 950ms/step - accuracy: 0.5021 - binary_io_u_5: 0.4998 - loss: 0.9111Batch 185, Loss Value: 0.8930\n",
      "Batch 185, Gradient Norm: 0.0211\n",
      "\u001b[1m185/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m44s\u001b[0m 950ms/step - accuracy: 0.5020 - binary_io_u_5: 0.4998 - loss: 0.9111Batch 186, Loss Value: 0.8820\n",
      "Batch 186, Gradient Norm: 0.0486\n",
      "\u001b[1m186/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m43s\u001b[0m 950ms/step - accuracy: 0.5020 - binary_io_u_5: 0.4998 - loss: 0.9111Batch 187, Loss Value: 0.8869\n",
      "Batch 187, Gradient Norm: 0.0799\n",
      "\u001b[1m187/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m42s\u001b[0m 950ms/step - accuracy: 0.5020 - binary_io_u_5: 0.4998 - loss: 0.9111Batch 188, Loss Value: 0.8846\n",
      "Batch 188, Gradient Norm: 0.3537\n",
      "\u001b[1m188/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m41s\u001b[0m 950ms/step - accuracy: 0.5020 - binary_io_u_5: 0.4997 - loss: 0.9111Batch 189, Loss Value: 0.8854\n",
      "Batch 189, Gradient Norm: 0.0519\n",
      "\u001b[1m189/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m40s\u001b[0m 951ms/step - accuracy: 0.5020 - binary_io_u_5: 0.4997 - loss: 0.9111Batch 190, Loss Value: 0.8843\n",
      "Batch 190, Gradient Norm: 0.1375\n",
      "\u001b[1m190/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m39s\u001b[0m 951ms/step - accuracy: 0.5019 - binary_io_u_5: 0.4997 - loss: 0.9112Batch 191, Loss Value: 0.8856\n",
      "Batch 191, Gradient Norm: 0.0348\n",
      "\u001b[1m191/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m38s\u001b[0m 951ms/step - accuracy: 0.5019 - binary_io_u_5: 0.4997 - loss: 0.9112Batch 192, Loss Value: 0.8804\n",
      "Batch 192, Gradient Norm: 0.0071\n",
      "\u001b[1m192/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m38s\u001b[0m 951ms/step - accuracy: 0.5019 - binary_io_u_5: 0.4996 - loss: 0.9112Batch 193, Loss Value: 0.8804\n",
      "Batch 193, Gradient Norm: 0.0037\n",
      "\u001b[1m193/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m37s\u001b[0m 951ms/step - accuracy: 0.5019 - binary_io_u_5: 0.4996 - loss: 0.9112Batch 194, Loss Value: 0.8882\n",
      "Batch 194, Gradient Norm: 0.1227\n",
      "\u001b[1m194/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m36s\u001b[0m 951ms/step - accuracy: 0.5019 - binary_io_u_5: 0.4996 - loss: 0.9112Batch 195, Loss Value: 0.8894\n",
      "Batch 195, Gradient Norm: 0.6764\n",
      "\u001b[1m195/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m35s\u001b[0m 952ms/step - accuracy: 0.5019 - binary_io_u_5: 0.4996 - loss: 0.9112Batch 196, Loss Value: 0.8861\n",
      "Batch 196, Gradient Norm: 0.0002\n",
      "\u001b[1m196/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m34s\u001b[0m 952ms/step - accuracy: 0.5018 - binary_io_u_5: 0.4995 - loss: 0.9112Batch 197, Loss Value: 0.8861\n",
      "Batch 197, Gradient Norm: 0.0002\n",
      "\u001b[1m197/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m33s\u001b[0m 951ms/step - accuracy: 0.5018 - binary_io_u_5: 0.4995 - loss: 0.9113Batch 198, Loss Value: 0.8860\n",
      "Batch 198, Gradient Norm: 0.0056\n",
      "\u001b[1m198/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m32s\u001b[0m 951ms/step - accuracy: 0.5018 - binary_io_u_5: 0.4995 - loss: 0.9113Batch 199, Loss Value: 0.8860\n",
      "Batch 199, Gradient Norm: 0.0085\n",
      "\u001b[1m199/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m31s\u001b[0m 951ms/step - accuracy: 0.5018 - binary_io_u_5: 0.4995 - loss: 0.9113Batch 200, Loss Value: 0.8861\n",
      "Batch 200, Gradient Norm: 0.0063\n",
      "\u001b[1m200/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m30s\u001b[0m 951ms/step - accuracy: 0.5018 - binary_io_u_5: 0.4995 - loss: 0.9113Batch 201, Loss Value: 0.8861\n",
      "Batch 201, Gradient Norm: 0.0000\n",
      "\u001b[1m201/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m29s\u001b[0m 952ms/step - accuracy: 0.5018 - binary_io_u_5: 0.4994 - loss: 0.9113Batch 202, Loss Value: 0.8828\n",
      "Batch 202, Gradient Norm: 0.3951\n",
      "\u001b[1m202/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m28s\u001b[0m 952ms/step - accuracy: 0.5018 - binary_io_u_5: 0.4994 - loss: 0.9113Batch 203, Loss Value: 0.8861\n",
      "Batch 203, Gradient Norm: 0.0000\n",
      "\u001b[1m203/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m27s\u001b[0m 952ms/step - accuracy: 0.5017 - binary_io_u_5: 0.4994 - loss: 0.9113Batch 204, Loss Value: 0.8861\n",
      "Batch 204, Gradient Norm: 0.0001\n",
      "\u001b[1m204/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m26s\u001b[0m 952ms/step - accuracy: 0.5017 - binary_io_u_5: 0.4994 - loss: 0.9113Batch 205, Loss Value: 0.8861\n",
      "Batch 205, Gradient Norm: 0.0000\n",
      "\u001b[1m205/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m25s\u001b[0m 952ms/step - accuracy: 0.5017 - binary_io_u_5: 0.4994 - loss: 0.9114Batch 206, Loss Value: 0.8861\n",
      "Batch 206, Gradient Norm: 0.0000\n",
      "\u001b[1m206/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m24s\u001b[0m 953ms/step - accuracy: 0.5017 - binary_io_u_5: 0.4993 - loss: 0.9114Batch 207, Loss Value: 0.8720\n",
      "Batch 207, Gradient Norm: 0.6138\n",
      "\u001b[1m207/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m23s\u001b[0m 953ms/step - accuracy: 0.5017 - binary_io_u_5: 0.4993 - loss: 0.9114Batch 208, Loss Value: 0.8861\n",
      "Batch 208, Gradient Norm: 0.0000\n",
      "\u001b[1m208/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m22s\u001b[0m 953ms/step - accuracy: 0.5017 - binary_io_u_5: 0.4993 - loss: 0.9114Batch 209, Loss Value: 0.8870\n",
      "Batch 209, Gradient Norm: 0.3160\n",
      "\u001b[1m209/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m21s\u001b[0m 953ms/step - accuracy: 0.5017 - binary_io_u_5: 0.4993 - loss: 0.9114Batch 210, Loss Value: 0.8861\n",
      "Batch 210, Gradient Norm: 0.0072\n",
      "\u001b[1m210/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m20s\u001b[0m 953ms/step - accuracy: 0.5017 - binary_io_u_5: 0.4993 - loss: 0.9114Batch 211, Loss Value: 0.8861\n",
      "Batch 211, Gradient Norm: 0.0000\n",
      "\u001b[1m211/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m20s\u001b[0m 954ms/step - accuracy: 0.5017 - binary_io_u_5: 0.4993 - loss: 0.9114Batch 212, Loss Value: 0.8861\n",
      "Batch 212, Gradient Norm: 0.0000\n",
      "\u001b[1m212/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m19s\u001b[0m 954ms/step - accuracy: 0.5016 - binary_io_u_5: 0.4993 - loss: 0.9114Batch 213, Loss Value: 0.8860\n",
      "Batch 213, Gradient Norm: 0.0067\n",
      "\u001b[1m213/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m18s\u001b[0m 954ms/step - accuracy: 0.5016 - binary_io_u_5: 0.4992 - loss: 0.9114Batch 214, Loss Value: 0.8767\n",
      "Batch 214, Gradient Norm: 0.5305\n",
      "\u001b[1m214/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m17s\u001b[0m 953ms/step - accuracy: 0.5016 - binary_io_u_5: 0.4992 - loss: 0.9114Batch 215, Loss Value: 0.8861\n",
      "Batch 215, Gradient Norm: 0.0001\n",
      "\u001b[1m215/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m16s\u001b[0m 952ms/step - accuracy: 0.5016 - binary_io_u_5: 0.4992 - loss: 0.9114Batch 216, Loss Value: 0.8861\n",
      "Batch 216, Gradient Norm: 0.0001\n",
      "\u001b[1m216/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m15s\u001b[0m 953ms/step - accuracy: 0.5016 - binary_io_u_5: 0.4992 - loss: 0.9115Batch 217, Loss Value: 0.8773\n",
      "Batch 217, Gradient Norm: 0.4812\n",
      "\u001b[1m217/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m14s\u001b[0m 953ms/step - accuracy: 0.5016 - binary_io_u_5: 0.4992 - loss: 0.9115Batch 218, Loss Value: 0.8861\n",
      "Batch 218, Gradient Norm: 0.0001\n",
      "\u001b[1m218/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m13s\u001b[0m 953ms/step - accuracy: 0.5016 - binary_io_u_5: 0.4992 - loss: 0.9115Batch 219, Loss Value: 0.8861\n",
      "Batch 219, Gradient Norm: 0.0000\n",
      "\u001b[1m219/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m12s\u001b[0m 953ms/step - accuracy: 0.5016 - binary_io_u_5: 0.4992 - loss: 0.9115Batch 220, Loss Value: 0.8861\n",
      "Batch 220, Gradient Norm: 0.0000\n",
      "\u001b[1m220/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m11s\u001b[0m 953ms/step - accuracy: 0.5016 - binary_io_u_5: 0.4992 - loss: 0.9115Batch 221, Loss Value: 0.8861\n",
      "Batch 221, Gradient Norm: 0.0001\n",
      "\u001b[1m221/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m10s\u001b[0m 953ms/step - accuracy: 0.5015 - binary_io_u_5: 0.4991 - loss: 0.9115Batch 222, Loss Value: 0.8832\n",
      "Batch 222, Gradient Norm: 0.1965\n",
      "\u001b[1m222/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m9s\u001b[0m 954ms/step - accuracy: 0.5015 - binary_io_u_5: 0.4991 - loss: 0.9115 Batch 223, Loss Value: 0.8879\n",
      "Batch 223, Gradient Norm: 0.0880\n",
      "\u001b[1m223/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m8s\u001b[0m 954ms/step - accuracy: 0.5015 - binary_io_u_5: 0.4991 - loss: 0.9115Batch 224, Loss Value: 0.8860\n",
      "Batch 224, Gradient Norm: 0.0013\n",
      "\u001b[1m224/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m7s\u001b[0m 954ms/step - accuracy: 0.5015 - binary_io_u_5: 0.4991 - loss: 0.9115Batch 225, Loss Value: 0.8861\n",
      "Batch 225, Gradient Norm: 0.0000\n",
      "\u001b[1m225/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m6s\u001b[0m 954ms/step - accuracy: 0.5015 - binary_io_u_5: 0.4991 - loss: 0.9115Batch 226, Loss Value: 0.8826\n",
      "Batch 226, Gradient Norm: 0.2539\n",
      "\u001b[1m226/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m5s\u001b[0m 954ms/step - accuracy: 0.5015 - binary_io_u_5: 0.4990 - loss: 0.9116Batch 227, Loss Value: 0.8861\n",
      "Batch 227, Gradient Norm: 0.0002\n",
      "\u001b[1m227/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m4s\u001b[0m 954ms/step - accuracy: 0.5015 - binary_io_u_5: 0.4990 - loss: 0.9116Batch 228, Loss Value: 0.8861\n",
      "Batch 228, Gradient Norm: 0.0000\n",
      "\u001b[1m228/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m3s\u001b[0m 955ms/step - accuracy: 0.5014 - binary_io_u_5: 0.4990 - loss: 0.9116Batch 229, Loss Value: 0.8861\n",
      "Batch 229, Gradient Norm: 0.0000\n",
      "\u001b[1m229/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m2s\u001b[0m 955ms/step - accuracy: 0.5014 - binary_io_u_5: 0.4990 - loss: 0.9116Batch 230, Loss Value: 0.8861\n",
      "Batch 230, Gradient Norm: 0.0005\n",
      "\u001b[1m230/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m1s\u001b[0m 955ms/step - accuracy: 0.5014 - binary_io_u_5: 0.4990 - loss: 0.9116Batch 231, Loss Value: 0.8861\n",
      "Batch 231, Gradient Norm: 0.0000\n",
      "\u001b[1m231/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 954ms/step - accuracy: 0.5014 - binary_io_u_5: 0.4990 - loss: 0.9116Batch 232, Loss Value: 0.8861\n",
      "Batch 232, Gradient Norm: 0.0002\n",
      "\u001b[1m232/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m224s\u001b[0m 964ms/step - accuracy: 0.5014 - binary_io_u_5: 0.4989 - loss: 0.9116 - val_accuracy: 0.4953 - val_binary_io_u_5: 0.4949 - val_loss: 0.9137 - learning_rate: 0.0010\n",
      "Epoch 12/200\n",
      "Batch 1, Loss Value: 0.8861\n",
      "Batch 1, Gradient Norm: 0.0006\n",
      "\u001b[1m  1/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4:12\u001b[0m 1s/step - accuracy: 0.4600 - binary_io_u_5: 0.4600 - loss: 0.9412Batch 2, Loss Value: 0.8861\n",
      "Batch 2, Gradient Norm: 0.0001\n",
      "\u001b[1m  2/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:50\u001b[0m 1s/step - accuracy: 0.4675 - binary_io_u_5: 0.4675 - loss: 0.9360Batch 3, Loss Value: 0.8861\n",
      "Batch 3, Gradient Norm: 0.0001\n",
      "\u001b[1m  3/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:48\u001b[0m 999ms/step - accuracy: 0.4728 - binary_io_u_5: 0.4728 - loss: 0.9323Batch 4, Loss Value: 0.8766\n",
      "Batch 4, Gradient Norm: 0.2593\n",
      "\u001b[1m  4/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:46\u001b[0m 995ms/step - accuracy: 0.4765 - binary_io_u_5: 0.4765 - loss: 0.9298Batch 5, Loss Value: 0.8861\n",
      "Batch 5, Gradient Norm: 0.0001\n",
      "\u001b[1m  5/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:43\u001b[0m 986ms/step - accuracy: 0.4784 - binary_io_u_5: 0.4784 - loss: 0.9284Batch 6, Loss Value: 0.8846\n",
      "Batch 6, Gradient Norm: 0.0891\n",
      "\u001b[1m  6/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:43\u001b[0m 990ms/step - accuracy: 0.4789 - binary_io_u_5: 0.4789 - loss: 0.9281Batch 7, Loss Value: 0.8839\n",
      "Batch 7, Gradient Norm: 0.1143\n",
      "\u001b[1m  7/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:43\u001b[0m 991ms/step - accuracy: 0.4781 - binary_io_u_5: 0.4781 - loss: 0.9282Batch 8, Loss Value: 0.8861\n",
      "Batch 8, Gradient Norm: 0.0000\n",
      "\u001b[1m  8/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:42\u001b[0m 992ms/step - accuracy: 0.4774 - binary_io_u_5: 0.4774 - loss: 0.9284Batch 9, Loss Value: 0.8863\n",
      "Batch 9, Gradient Norm: 0.0258\n",
      "\u001b[1m  9/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:40\u001b[0m 989ms/step - accuracy: 0.4765 - binary_io_u_5: 0.4765 - loss: 0.9287Batch 10, Loss Value: 0.8851\n",
      "Batch 10, Gradient Norm: 0.0624\n",
      "\u001b[1m 10/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:40\u001b[0m 992ms/step - accuracy: 0.4762 - binary_io_u_5: 0.4762 - loss: 0.9288Batch 11, Loss Value: 0.8860\n",
      "Batch 11, Gradient Norm: 0.0051\n",
      "\u001b[1m 11/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:39\u001b[0m 991ms/step - accuracy: 0.4761 - binary_io_u_5: 0.4761 - loss: 0.9287Batch 12, Loss Value: 0.8861\n",
      "Batch 12, Gradient Norm: 0.0001\n",
      "\u001b[1m 12/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:38\u001b[0m 994ms/step - accuracy: 0.4764 - binary_io_u_5: 0.4764 - loss: 0.9285Batch 13, Loss Value: 0.8835\n",
      "Batch 13, Gradient Norm: 0.1761\n",
      "\u001b[1m 13/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:34\u001b[0m 979ms/step - accuracy: 0.4765 - binary_io_u_5: 0.4765 - loss: 0.9283Batch 14, Loss Value: 0.8861\n",
      "Batch 14, Gradient Norm: 0.0019\n",
      "\u001b[1m 14/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:29\u001b[0m 963ms/step - accuracy: 0.4769 - binary_io_u_5: 0.4769 - loss: 0.9280Batch 15, Loss Value: 0.8861\n",
      "Batch 15, Gradient Norm: 0.0003\n",
      "\u001b[1m 15/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:29\u001b[0m 966ms/step - accuracy: 0.4771 - binary_io_u_5: 0.4771 - loss: 0.9278Batch 16, Loss Value: 0.8859\n",
      "Batch 16, Gradient Norm: 0.0128\n",
      "\u001b[1m 16/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:28\u001b[0m 967ms/step - accuracy: 0.4775 - binary_io_u_5: 0.4775 - loss: 0.9276Batch 17, Loss Value: 0.8867\n",
      "Batch 17, Gradient Norm: 0.0485\n",
      "\u001b[1m 17/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:28\u001b[0m 972ms/step - accuracy: 0.4777 - binary_io_u_5: 0.4777 - loss: 0.9274Batch 18, Loss Value: 0.8861\n",
      "Batch 18, Gradient Norm: 0.0014\n",
      "\u001b[1m 18/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:28\u001b[0m 974ms/step - accuracy: 0.4783 - binary_io_u_5: 0.4783 - loss: 0.9270Batch 19, Loss Value: 0.8866\n",
      "Batch 19, Gradient Norm: 0.0425\n",
      "\u001b[1m 19/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:27\u001b[0m 975ms/step - accuracy: 0.4788 - binary_io_u_5: 0.4788 - loss: 0.9266Batch 20, Loss Value: 0.8862\n",
      "Batch 20, Gradient Norm: 0.0813\n",
      "\u001b[1m 20/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:26\u001b[0m 975ms/step - accuracy: 0.4795 - binary_io_u_5: 0.4795 - loss: 0.9262Batch 21, Loss Value: 0.8861\n",
      "Batch 21, Gradient Norm: 0.0003\n",
      "\u001b[1m 21/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:25\u001b[0m 975ms/step - accuracy: 0.4798 - binary_io_u_5: 0.4798 - loss: 0.9258Batch 22, Loss Value: 0.8903\n",
      "Batch 22, Gradient Norm: 0.1369\n",
      "\u001b[1m 22/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:24\u001b[0m 976ms/step - accuracy: 0.4801 - binary_io_u_5: 0.4801 - loss: 0.9255Batch 23, Loss Value: 0.8861\n",
      "Batch 23, Gradient Norm: 0.0012\n",
      "\u001b[1m 23/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:24\u001b[0m 977ms/step - accuracy: 0.4803 - binary_io_u_5: 0.4803 - loss: 0.9253Batch 24, Loss Value: 0.8861\n",
      "Batch 24, Gradient Norm: 0.0000\n",
      "\u001b[1m 24/232\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:23\u001b[0m 978ms/step - accuracy: 0.4805 - binary_io_u_5: 0.4805 - loss: 0.9251Batch 25, Loss Value: 0.8861\n",
      "Batch 25, Gradient Norm: 0.0000\n",
      "\u001b[1m 25/232\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:22\u001b[0m 978ms/step - accuracy: 0.4808 - binary_io_u_5: 0.4808 - loss: 0.9248Batch 26, Loss Value: 0.8846\n",
      "Batch 26, Gradient Norm: 0.1008\n",
      "\u001b[1m 26/232\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:21\u001b[0m 976ms/step - accuracy: 0.4810 - binary_io_u_5: 0.4810 - loss: 0.9247Batch 27, Loss Value: 0.8861\n",
      "Batch 27, Gradient Norm: 0.0008\n",
      "\u001b[1m 27/232\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:20\u001b[0m 976ms/step - accuracy: 0.4813 - binary_io_u_5: 0.4813 - loss: 0.9244Batch 28, Loss Value: 0.8861\n",
      "Batch 28, Gradient Norm: 0.0004\n",
      "\u001b[1m 28/232\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:18\u001b[0m 974ms/step - accuracy: 0.4816 - binary_io_u_5: 0.4816 - loss: 0.9242Batch 29, Loss Value: 0.8857\n",
      "Batch 29, Gradient Norm: 0.0260\n",
      "\u001b[1m 29/232\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:17\u001b[0m 975ms/step - accuracy: 0.4819 - binary_io_u_5: 0.4819 - loss: 0.9239Batch 30, Loss Value: 0.8860\n",
      "Batch 30, Gradient Norm: 0.0027\n",
      "\u001b[1m 30/232\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:15\u001b[0m 969ms/step - accuracy: 0.4823 - binary_io_u_5: 0.4823 - loss: 0.9237Batch 31, Loss Value: 0.8861\n",
      "Batch 31, Gradient Norm: 0.0005\n",
      "\u001b[1m 31/232\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:13\u001b[0m 963ms/step - accuracy: 0.4824 - binary_io_u_5: 0.4824 - loss: 0.9235Batch 32, Loss Value: 0.8861\n",
      "Batch 32, Gradient Norm: 0.0000\n",
      "\u001b[1m 32/232\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:12\u001b[0m 963ms/step - accuracy: 0.4827 - binary_io_u_5: 0.4827 - loss: 0.9233Batch 33, Loss Value: 0.8861\n",
      "Batch 33, Gradient Norm: 0.0004\n",
      "\u001b[1m 33/232\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:11\u001b[0m 964ms/step - accuracy: 0.4828 - binary_io_u_5: 0.4828 - loss: 0.9232Batch 34, Loss Value: 0.8883\n",
      "Batch 34, Gradient Norm: 0.0964\n",
      "\u001b[1m 34/232\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:11\u001b[0m 965ms/step - accuracy: 0.4829 - binary_io_u_5: 0.4829 - loss: 0.9230Batch 35, Loss Value: 0.8860\n",
      "Batch 35, Gradient Norm: 0.0064\n",
      "\u001b[1m 35/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:10\u001b[0m 966ms/step - accuracy: 0.4829 - binary_io_u_5: 0.4829 - loss: 0.9229Batch 36, Loss Value: 0.8864\n",
      "Batch 36, Gradient Norm: 0.0354\n",
      "\u001b[1m 36/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:09\u001b[0m 967ms/step - accuracy: 0.4830 - binary_io_u_5: 0.4830 - loss: 0.9229Batch 37, Loss Value: 0.8861\n",
      "Batch 37, Gradient Norm: 0.0010\n",
      "\u001b[1m 37/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:08\u001b[0m 968ms/step - accuracy: 0.4830 - binary_io_u_5: 0.4830 - loss: 0.9228Batch 38, Loss Value: 0.8861\n",
      "Batch 38, Gradient Norm: 0.0006\n",
      "\u001b[1m 38/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:07\u001b[0m 968ms/step - accuracy: 0.4831 - binary_io_u_5: 0.4831 - loss: 0.9226Batch 39, Loss Value: 0.8861\n",
      "Batch 39, Gradient Norm: 0.0000\n",
      "\u001b[1m 39/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:07\u001b[0m 969ms/step - accuracy: 0.4832 - binary_io_u_5: 0.4832 - loss: 0.9225Batch 40, Loss Value: 0.8860\n",
      "Batch 40, Gradient Norm: 0.0021\n",
      "\u001b[1m 40/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:06\u001b[0m 970ms/step - accuracy: 0.4833 - binary_io_u_5: 0.4833 - loss: 0.9224Batch 41, Loss Value: 0.8861\n",
      "Batch 41, Gradient Norm: 0.0004\n",
      "\u001b[1m 41/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:05\u001b[0m 972ms/step - accuracy: 0.4835 - binary_io_u_5: 0.4835 - loss: 0.9223Batch 42, Loss Value: 0.8861\n",
      "Batch 42, Gradient Norm: 0.0024\n",
      "\u001b[1m 42/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:04\u001b[0m 972ms/step - accuracy: 0.4838 - binary_io_u_5: 0.4838 - loss: 0.9221Batch 43, Loss Value: 0.8848\n",
      "Batch 43, Gradient Norm: 0.0869\n",
      "\u001b[1m 43/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:03\u001b[0m 973ms/step - accuracy: 0.4840 - binary_io_u_5: 0.4840 - loss: 0.9219Batch 44, Loss Value: 0.8833\n",
      "Batch 44, Gradient Norm: 0.1909\n",
      "\u001b[1m 44/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:03\u001b[0m 974ms/step - accuracy: 0.4842 - binary_io_u_5: 0.4842 - loss: 0.9217Batch 45, Loss Value: 0.8861\n",
      "Batch 45, Gradient Norm: 0.0003\n",
      "\u001b[1m 45/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:02\u001b[0m 975ms/step - accuracy: 0.4844 - binary_io_u_5: 0.4844 - loss: 0.9215Batch 46, Loss Value: 0.8861\n",
      "Batch 46, Gradient Norm: 0.0008\n",
      "\u001b[1m 46/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:01\u001b[0m 973ms/step - accuracy: 0.4846 - binary_io_u_5: 0.4846 - loss: 0.9214Batch 47, Loss Value: 0.8862\n",
      "Batch 47, Gradient Norm: 0.0107\n",
      "\u001b[1m 47/232\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:59\u001b[0m 970ms/step - accuracy: 0.4847 - binary_io_u_5: 0.4847 - loss: 0.9213Batch 48, Loss Value: 0.8863\n",
      "Batch 48, Gradient Norm: 0.3068\n",
      "\u001b[1m 48/232\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:58\u001b[0m 968ms/step - accuracy: 0.4849 - binary_io_u_5: 0.4849 - loss: 0.9211Batch 49, Loss Value: 0.8861\n",
      "Batch 49, Gradient Norm: 0.0002\n",
      "\u001b[1m 49/232\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:57\u001b[0m 968ms/step - accuracy: 0.4850 - binary_io_u_5: 0.4850 - loss: 0.9210Batch 50, Loss Value: 0.8861\n",
      "Batch 50, Gradient Norm: 0.0013\n",
      "\u001b[1m 50/232\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:56\u001b[0m 968ms/step - accuracy: 0.4852 - binary_io_u_5: 0.4852 - loss: 0.9209Batch 51, Loss Value: 0.8861\n",
      "Batch 51, Gradient Norm: 0.0001\n",
      "\u001b[1m 51/232\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:55\u001b[0m 967ms/step - accuracy: 0.4853 - binary_io_u_5: 0.4853 - loss: 0.9208Batch 52, Loss Value: 0.8872\n",
      "Batch 52, Gradient Norm: 0.1658\n",
      "\u001b[1m 52/232\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:54\u001b[0m 968ms/step - accuracy: 0.4854 - binary_io_u_5: 0.4854 - loss: 0.9207Batch 53, Loss Value: 0.8886\n",
      "Batch 53, Gradient Norm: 0.3597\n",
      "\u001b[1m 53/232\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:53\u001b[0m 968ms/step - accuracy: 0.4855 - binary_io_u_5: 0.4855 - loss: 0.9206Batch 54, Loss Value: 0.8943\n",
      "Batch 54, Gradient Norm: 0.0520\n",
      "\u001b[1m 54/232\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:52\u001b[0m 969ms/step - accuracy: 0.4857 - binary_io_u_5: 0.4857 - loss: 0.9205Batch 55, Loss Value: 0.8861\n",
      "Batch 55, Gradient Norm: 0.0045\n",
      "\u001b[1m 55/232\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:51\u001b[0m 969ms/step - accuracy: 0.4858 - binary_io_u_5: 0.4858 - loss: 0.9204Batch 56, Loss Value: 0.8861\n",
      "Batch 56, Gradient Norm: 0.0005\n",
      "\u001b[1m 56/232\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:50\u001b[0m 970ms/step - accuracy: 0.4860 - binary_io_u_5: 0.4860 - loss: 0.9203Batch 57, Loss Value: 0.8861\n",
      "Batch 57, Gradient Norm: 0.0000\n",
      "\u001b[1m 57/232\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:49\u001b[0m 971ms/step - accuracy: 0.4861 - binary_io_u_5: 0.4861 - loss: 0.9202Batch 58, Loss Value: 0.8860\n",
      "Batch 58, Gradient Norm: 0.0018\n",
      "\u001b[1m 58/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:49\u001b[0m 971ms/step - accuracy: 0.4862 - binary_io_u_5: 0.4862 - loss: 0.9201Batch 59, Loss Value: 0.8822\n",
      "Batch 59, Gradient Norm: 0.1320\n",
      "\u001b[1m 59/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:48\u001b[0m 974ms/step - accuracy: 0.4863 - binary_io_u_5: 0.4863 - loss: 0.9200Batch 60, Loss Value: 0.9111\n",
      "Batch 60, Gradient Norm: 0.3886\n",
      "\u001b[1m 60/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:48\u001b[0m 977ms/step - accuracy: 0.4865 - binary_io_u_5: 0.4865 - loss: 0.9199Batch 61, Loss Value: 0.8938\n",
      "Batch 61, Gradient Norm: 0.4954\n",
      "\u001b[1m 61/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:47\u001b[0m 980ms/step - accuracy: 0.4866 - binary_io_u_5: 0.4866 - loss: 0.9198Batch 62, Loss Value: 0.8859\n",
      "Batch 62, Gradient Norm: 0.0145\n",
      "\u001b[1m 62/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:47\u001b[0m 983ms/step - accuracy: 0.4867 - binary_io_u_5: 0.4867 - loss: 0.9197Batch 63, Loss Value: 0.8911\n",
      "Batch 63, Gradient Norm: 0.5287\n",
      "\u001b[1m 63/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:46\u001b[0m 985ms/step - accuracy: 0.4868 - binary_io_u_5: 0.4868 - loss: 0.9197Batch 64, Loss Value: 0.8815\n",
      "Batch 64, Gradient Norm: 0.0631\n",
      "\u001b[1m 64/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:45\u001b[0m 987ms/step - accuracy: 0.4869 - binary_io_u_5: 0.4869 - loss: 0.9196Batch 65, Loss Value: 0.8847\n",
      "Batch 65, Gradient Norm: 0.0870\n",
      "\u001b[1m 65/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:44\u001b[0m 987ms/step - accuracy: 0.4871 - binary_io_u_5: 0.4871 - loss: 0.9195Batch 66, Loss Value: 0.8904\n",
      "Batch 66, Gradient Norm: 0.2941\n",
      "\u001b[1m 66/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:44\u001b[0m 991ms/step - accuracy: 0.4872 - binary_io_u_5: 0.4872 - loss: 0.9194Batch 67, Loss Value: 0.8861\n",
      "Batch 67, Gradient Norm: 0.0001\n",
      "\u001b[1m 67/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:43\u001b[0m 993ms/step - accuracy: 0.4873 - binary_io_u_5: 0.4873 - loss: 0.9193Batch 68, Loss Value: 0.8864\n",
      "Batch 68, Gradient Norm: 0.0289\n",
      "\u001b[1m 68/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:43\u001b[0m 994ms/step - accuracy: 0.4874 - binary_io_u_5: 0.4874 - loss: 0.9192Batch 69, Loss Value: 0.8882\n",
      "Batch 69, Gradient Norm: 0.4602\n",
      "\u001b[1m 69/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:42\u001b[0m 997ms/step - accuracy: 0.4875 - binary_io_u_5: 0.4875 - loss: 0.9192Batch 70, Loss Value: 0.8883\n",
      "Batch 70, Gradient Norm: 0.1294\n",
      "\u001b[1m 70/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:41\u001b[0m 997ms/step - accuracy: 0.4877 - binary_io_u_5: 0.4877 - loss: 0.9191Batch 71, Loss Value: 0.8843\n",
      "Batch 71, Gradient Norm: 0.2141\n",
      "\u001b[1m 71/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:40\u001b[0m 998ms/step - accuracy: 0.4878 - binary_io_u_5: 0.4878 - loss: 0.9190Batch 72, Loss Value: 0.8750\n",
      "Batch 72, Gradient Norm: 0.4529\n",
      "\u001b[1m 72/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:40\u001b[0m 1s/step - accuracy: 0.4879 - binary_io_u_5: 0.4879 - loss: 0.9189   Batch 73, Loss Value: 0.8882\n",
      "Batch 73, Gradient Norm: 0.1365\n",
      "\u001b[1m 73/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:39\u001b[0m 1s/step - accuracy: 0.4880 - binary_io_u_5: 0.4880 - loss: 0.9189Batch 74, Loss Value: 0.8897\n",
      "Batch 74, Gradient Norm: 0.2218\n",
      "\u001b[1m 74/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:38\u001b[0m 1s/step - accuracy: 0.4881 - binary_io_u_5: 0.4881 - loss: 0.9188Batch 75, Loss Value: 0.8862\n",
      "Batch 75, Gradient Norm: 0.0117\n",
      "\u001b[1m 75/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:37\u001b[0m 1s/step - accuracy: 0.4882 - binary_io_u_5: 0.4882 - loss: 0.9187Batch 76, Loss Value: 0.8971\n",
      "Batch 76, Gradient Norm: 0.1147\n",
      "\u001b[1m 76/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:37\u001b[0m 1s/step - accuracy: 0.4883 - binary_io_u_5: 0.4883 - loss: 0.9187Batch 77, Loss Value: 0.8921\n",
      "Batch 77, Gradient Norm: 0.2013\n",
      "\u001b[1m 77/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:36\u001b[0m 1s/step - accuracy: 0.4884 - binary_io_u_5: 0.4884 - loss: 0.9186Batch 78, Loss Value: 0.8993\n",
      "Batch 78, Gradient Norm: 0.1066\n",
      "\u001b[1m 78/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:35\u001b[0m 1s/step - accuracy: 0.4885 - binary_io_u_5: 0.4885 - loss: 0.9186Batch 79, Loss Value: 0.8861\n",
      "Batch 79, Gradient Norm: 0.1090\n",
      "\u001b[1m 79/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:34\u001b[0m 1s/step - accuracy: 0.4885 - binary_io_u_5: 0.4885 - loss: 0.9185Batch 80, Loss Value: 0.8653\n",
      "Batch 80, Gradient Norm: 0.5050\n",
      "\u001b[1m 80/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:34\u001b[0m 1s/step - accuracy: 0.4886 - binary_io_u_5: 0.4886 - loss: 0.9185Batch 81, Loss Value: 0.8828\n",
      "Batch 81, Gradient Norm: 0.2361\n",
      "\u001b[1m 81/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:33\u001b[0m 1s/step - accuracy: 0.4887 - binary_io_u_5: 0.4887 - loss: 0.9184Batch 82, Loss Value: 0.8755\n",
      "Batch 82, Gradient Norm: 0.0869\n",
      "\u001b[1m 82/232\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m2:32\u001b[0m 1s/step - accuracy: 0.4888 - binary_io_u_5: 0.4888 - loss: 0.9183Batch 83, Loss Value: 0.8860\n",
      "Batch 83, Gradient Norm: 0.0075\n",
      "\u001b[1m 83/232\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m2:31\u001b[0m 1s/step - accuracy: 0.4889 - binary_io_u_5: 0.4889 - loss: 0.9183Batch 84, Loss Value: 0.8855\n",
      "Batch 84, Gradient Norm: 0.0445\n",
      "\u001b[1m 84/232\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m2:30\u001b[0m 1s/step - accuracy: 0.4890 - binary_io_u_5: 0.4890 - loss: 0.9182Batch 85, Loss Value: 0.8786\n",
      "Batch 85, Gradient Norm: 0.3250\n",
      "\u001b[1m 85/232\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m2:29\u001b[0m 1s/step - accuracy: 0.4890 - binary_io_u_5: 0.4890 - loss: 0.9182Batch 86, Loss Value: 0.9001\n",
      "Batch 86, Gradient Norm: 0.2069\n",
      "\u001b[1m 86/232\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m2:28\u001b[0m 1s/step - accuracy: 0.4891 - binary_io_u_5: 0.4891 - loss: 0.9181Batch 87, Loss Value: 0.8860\n",
      "Batch 87, Gradient Norm: 0.0917\n",
      "\u001b[1m 87/232\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m2:27\u001b[0m 1s/step - accuracy: 0.4892 - binary_io_u_5: 0.4892 - loss: 0.9181Batch 88, Loss Value: 0.8789\n",
      "Batch 88, Gradient Norm: 0.1269\n",
      "\u001b[1m 88/232\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m2:27\u001b[0m 1s/step - accuracy: 0.4893 - binary_io_u_5: 0.4893 - loss: 0.9180Batch 89, Loss Value: 0.8823\n",
      "Batch 89, Gradient Norm: 0.2276\n",
      "\u001b[1m 89/232\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m2:26\u001b[0m 1s/step - accuracy: 0.4894 - binary_io_u_5: 0.4894 - loss: 0.9180Batch 90, Loss Value: 0.8848\n",
      "Batch 90, Gradient Norm: 1.6644\n",
      "\u001b[1m 90/232\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m2:25\u001b[0m 1s/step - accuracy: 0.4894 - binary_io_u_5: 0.4894 - loss: 0.9179Batch 91, Loss Value: 0.8873\n",
      "Batch 91, Gradient Norm: 0.2828\n",
      "\u001b[1m 91/232\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m2:24\u001b[0m 1s/step - accuracy: 0.4895 - binary_io_u_5: 0.4895 - loss: 0.9179Batch 92, Loss Value: 0.8895\n",
      "Batch 92, Gradient Norm: 0.3769\n",
      "\u001b[1m 92/232\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m2:23\u001b[0m 1s/step - accuracy: 0.4896 - binary_io_u_5: 0.4896 - loss: 0.9178Batch 93, Loss Value: 0.8771\n",
      "Batch 93, Gradient Norm: 0.2658\n",
      "\u001b[1m 93/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m2:23\u001b[0m 1s/step - accuracy: 0.4897 - binary_io_u_5: 0.4897 - loss: 0.9178Batch 94, Loss Value: 0.8812\n",
      "Batch 94, Gradient Norm: 0.4966\n",
      "\u001b[1m 94/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m2:22\u001b[0m 1s/step - accuracy: 0.4897 - binary_io_u_5: 0.4897 - loss: 0.9177Batch 95, Loss Value: 0.8899\n",
      "Batch 95, Gradient Norm: 0.4782\n",
      "\u001b[1m 95/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m2:21\u001b[0m 1s/step - accuracy: 0.4898 - binary_io_u_5: 0.4898 - loss: 0.9177Batch 96, Loss Value: 0.8821\n",
      "Batch 96, Gradient Norm: 0.2172\n",
      "\u001b[1m 96/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m2:20\u001b[0m 1s/step - accuracy: 0.4899 - binary_io_u_5: 0.4899 - loss: 0.9176Batch 97, Loss Value: 0.9024\n",
      "Batch 97, Gradient Norm: 0.2745\n",
      "\u001b[1m 97/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m2:19\u001b[0m 1s/step - accuracy: 0.4899 - binary_io_u_5: 0.4899 - loss: 0.9176Batch 98, Loss Value: 0.9030\n",
      "Batch 98, Gradient Norm: 0.1797\n",
      "\u001b[1m 98/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m2:18\u001b[0m 1s/step - accuracy: 0.4900 - binary_io_u_5: 0.4900 - loss: 0.9175Batch 99, Loss Value: 0.9180\n",
      "Batch 99, Gradient Norm: 0.2691\n",
      "\u001b[1m 99/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m2:17\u001b[0m 1s/step - accuracy: 0.4901 - binary_io_u_5: 0.4901 - loss: 0.9175Batch 100, Loss Value: 0.8993\n",
      "Batch 100, Gradient Norm: 0.3356\n",
      "\u001b[1m100/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m2:16\u001b[0m 1s/step - accuracy: 0.4901 - binary_io_u_5: 0.4901 - loss: 0.9174Batch 101, Loss Value: 0.8662\n",
      "Batch 101, Gradient Norm: 0.0866\n",
      "\u001b[1m101/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m2:15\u001b[0m 1s/step - accuracy: 0.4902 - binary_io_u_5: 0.4902 - loss: 0.9174Batch 102, Loss Value: 0.8846\n",
      "Batch 102, Gradient Norm: 0.0955\n",
      "\u001b[1m102/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m2:14\u001b[0m 1s/step - accuracy: 0.4903 - binary_io_u_5: 0.4903 - loss: 0.9174Batch 103, Loss Value: 0.8866\n",
      "Batch 103, Gradient Norm: 0.0398\n",
      "\u001b[1m103/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m2:13\u001b[0m 1s/step - accuracy: 0.4903 - binary_io_u_5: 0.4903 - loss: 0.9173Batch 104, Loss Value: 0.8978\n",
      "Batch 104, Gradient Norm: 0.2451\n",
      "\u001b[1m104/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m2:12\u001b[0m 1s/step - accuracy: 0.4904 - binary_io_u_5: 0.4904 - loss: 0.9173Batch 105, Loss Value: 0.8816\n",
      "Batch 105, Gradient Norm: 0.1961\n",
      "\u001b[1m105/232\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m2:11\u001b[0m 1s/step - accuracy: 0.4905 - binary_io_u_5: 0.4905 - loss: 0.9172Batch 106, Loss Value: 0.8899\n",
      "Batch 106, Gradient Norm: 0.1607\n",
      "\u001b[1m106/232\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m2:10\u001b[0m 1s/step - accuracy: 0.4905 - binary_io_u_5: 0.4905 - loss: 0.9172Batch 107, Loss Value: 0.9016\n",
      "Batch 107, Gradient Norm: 0.3857\n",
      "\u001b[1m107/232\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m2:08\u001b[0m 1s/step - accuracy: 0.4906 - binary_io_u_5: 0.4906 - loss: 0.9171Batch 108, Loss Value: 0.8964\n",
      "Batch 108, Gradient Norm: 0.1175\n",
      "\u001b[1m108/232\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m2:07\u001b[0m 1s/step - accuracy: 0.4906 - binary_io_u_5: 0.4906 - loss: 0.9171Batch 109, Loss Value: 0.8871\n",
      "Batch 109, Gradient Norm: 0.0893\n",
      "\u001b[1m109/232\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m2:05\u001b[0m 1s/step - accuracy: 0.4907 - binary_io_u_5: 0.4907 - loss: 0.9171Batch 110, Loss Value: 0.9149\n",
      "Batch 110, Gradient Norm: 0.1696\n",
      "\u001b[1m110/232\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m2:04\u001b[0m 1s/step - accuracy: 0.4908 - binary_io_u_5: 0.4908 - loss: 0.9170Batch 111, Loss Value: 0.8825\n",
      "Batch 111, Gradient Norm: 0.3200\n",
      "\u001b[1m111/232\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m2:03\u001b[0m 1s/step - accuracy: 0.4908 - binary_io_u_5: 0.4908 - loss: 0.9170Batch 112, Loss Value: 0.9162\n",
      "Batch 112, Gradient Norm: 0.1374\n",
      "\u001b[1m112/232\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m2:01\u001b[0m 1s/step - accuracy: 0.4909 - binary_io_u_5: 0.4909 - loss: 0.9169Batch 113, Loss Value: 0.9096\n",
      "Batch 113, Gradient Norm: 0.0821\n",
      "\u001b[1m113/232\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m2:00\u001b[0m 1s/step - accuracy: 0.4909 - binary_io_u_5: 0.4909 - loss: 0.9169Batch 114, Loss Value: 0.8811\n",
      "Batch 114, Gradient Norm: 0.5982\n",
      "\u001b[1m114/232\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m1:59\u001b[0m 1s/step - accuracy: 0.4910 - binary_io_u_5: 0.4910 - loss: 0.9169Batch 115, Loss Value: 0.8630\n",
      "Batch 115, Gradient Norm: 0.1248\n",
      "\u001b[1m115/232\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m1:57\u001b[0m 1s/step - accuracy: 0.4910 - binary_io_u_5: 0.4910 - loss: 0.9168Batch 116, Loss Value: 0.8824\n",
      "Batch 116, Gradient Norm: 0.3626\n",
      "\u001b[1m116/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m1:56\u001b[0m 1s/step - accuracy: 0.4911 - binary_io_u_5: 0.4911 - loss: 0.9168Batch 117, Loss Value: 0.8858\n",
      "Batch 117, Gradient Norm: 0.3899\n",
      "\u001b[1m117/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m1:55\u001b[0m 1s/step - accuracy: 0.4911 - binary_io_u_5: 0.4911 - loss: 0.9168Batch 118, Loss Value: 0.8908\n",
      "Batch 118, Gradient Norm: 0.0429\n",
      "\u001b[1m118/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m1:53\u001b[0m 997ms/step - accuracy: 0.4912 - binary_io_u_5: 0.4912 - loss: 0.9167Batch 119, Loss Value: 0.9098\n",
      "Batch 119, Gradient Norm: 0.0958\n",
      "\u001b[1m119/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m1:52\u001b[0m 995ms/step - accuracy: 0.4912 - binary_io_u_5: 0.4912 - loss: 0.9167Batch 120, Loss Value: 0.8774\n",
      "Batch 120, Gradient Norm: 0.1177\n",
      "\u001b[1m120/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m1:51\u001b[0m 992ms/step - accuracy: 0.4913 - binary_io_u_5: 0.4913 - loss: 0.9167Batch 121, Loss Value: 0.8895\n",
      "Batch 121, Gradient Norm: 0.1433\n",
      "\u001b[1m121/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m1:49\u001b[0m 990ms/step - accuracy: 0.4913 - binary_io_u_5: 0.4913 - loss: 0.9166Batch 122, Loss Value: 0.9198\n",
      "Batch 122, Gradient Norm: 0.0477\n",
      "\u001b[1m122/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m1:48\u001b[0m 988ms/step - accuracy: 0.4914 - binary_io_u_5: 0.4914 - loss: 0.9166Batch 123, Loss Value: 0.8965\n",
      "Batch 123, Gradient Norm: 0.5628\n",
      "\u001b[1m123/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m1:47\u001b[0m 985ms/step - accuracy: 0.4914 - binary_io_u_5: 0.4914 - loss: 0.9166Batch 124, Loss Value: 0.9031\n",
      "Batch 124, Gradient Norm: 0.1566\n",
      "\u001b[1m124/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m1:46\u001b[0m 983ms/step - accuracy: 0.4915 - binary_io_u_5: 0.4915 - loss: 0.9165Batch 125, Loss Value: 0.8972\n",
      "Batch 125, Gradient Norm: 0.1190\n",
      "\u001b[1m125/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m1:44\u001b[0m 980ms/step - accuracy: 0.4915 - binary_io_u_5: 0.4915 - loss: 0.9165Batch 126, Loss Value: 0.9113\n",
      "Batch 126, Gradient Norm: 1.1455\n",
      "\u001b[1m126/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m1:43\u001b[0m 978ms/step - accuracy: 0.4916 - binary_io_u_5: 0.4916 - loss: 0.9165Batch 127, Loss Value: 0.8835\n",
      "Batch 127, Gradient Norm: 0.2652\n",
      "\u001b[1m127/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m1:42\u001b[0m 976ms/step - accuracy: 0.4916 - binary_io_u_5: 0.4916 - loss: 0.9164Batch 128, Loss Value: 0.8754\n",
      "Batch 128, Gradient Norm: 0.3116\n",
      "\u001b[1m128/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m1:41\u001b[0m 974ms/step - accuracy: 0.4916 - binary_io_u_5: 0.4916 - loss: 0.9164Batch 129, Loss Value: 0.9004\n",
      "Batch 129, Gradient Norm: 0.3729\n",
      "\u001b[1m129/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m1:40\u001b[0m 971ms/step - accuracy: 0.4917 - binary_io_u_5: 0.4917 - loss: 0.9164Batch 130, Loss Value: 0.8802\n",
      "Batch 130, Gradient Norm: 0.2351\n",
      "\u001b[1m130/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m1:38\u001b[0m 969ms/step - accuracy: 0.4917 - binary_io_u_5: 0.4917 - loss: 0.9163Batch 131, Loss Value: 0.8827\n",
      "Batch 131, Gradient Norm: 0.2516\n",
      "\u001b[1m131/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m1:37\u001b[0m 967ms/step - accuracy: 0.4917 - binary_io_u_5: 0.4917 - loss: 0.9163Batch 132, Loss Value: 0.8983\n",
      "Batch 132, Gradient Norm: 0.0967\n",
      "\u001b[1m132/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m1:36\u001b[0m 965ms/step - accuracy: 0.4918 - binary_io_u_5: 0.4918 - loss: 0.9163Batch 133, Loss Value: 0.9268\n",
      "Batch 133, Gradient Norm: 0.3435\n",
      "\u001b[1m133/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m1:35\u001b[0m 963ms/step - accuracy: 0.4918 - binary_io_u_5: 0.4918 - loss: 0.9162Batch 134, Loss Value: 0.9050\n",
      "Batch 134, Gradient Norm: 0.2740\n",
      "\u001b[1m134/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m1:34\u001b[0m 961ms/step - accuracy: 0.4918 - binary_io_u_5: 0.4918 - loss: 0.9162Batch 135, Loss Value: 0.8726\n",
      "Batch 135, Gradient Norm: 0.2231\n",
      "\u001b[1m135/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m1:33\u001b[0m 959ms/step - accuracy: 0.4919 - binary_io_u_5: 0.4919 - loss: 0.9162Batch 136, Loss Value: 0.9055\n",
      "Batch 136, Gradient Norm: 0.0782\n",
      "\u001b[1m136/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m1:31\u001b[0m 957ms/step - accuracy: 0.4919 - binary_io_u_5: 0.4919 - loss: 0.9162Batch 137, Loss Value: 0.8908\n",
      "Batch 137, Gradient Norm: 0.2654\n",
      "\u001b[1m137/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m1:30\u001b[0m 955ms/step - accuracy: 0.4919 - binary_io_u_5: 0.4919 - loss: 0.9161Batch 138, Loss Value: 0.8886\n",
      "Batch 138, Gradient Norm: 0.0933\n",
      "\u001b[1m138/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m1:29\u001b[0m 953ms/step - accuracy: 0.4920 - binary_io_u_5: 0.4920 - loss: 0.9161Batch 139, Loss Value: 0.8992\n",
      "Batch 139, Gradient Norm: 0.3107\n",
      "\u001b[1m139/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m1:28\u001b[0m 951ms/step - accuracy: 0.4920 - binary_io_u_5: 0.4920 - loss: 0.9161Batch 140, Loss Value: 0.9043\n",
      "Batch 140, Gradient Norm: 0.1348\n",
      "\u001b[1m140/232\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m1:27\u001b[0m 949ms/step - accuracy: 0.4920 - binary_io_u_5: 0.4920 - loss: 0.9161Batch 141, Loss Value: 0.9238\n",
      "Batch 141, Gradient Norm: 0.3451\n",
      "\u001b[1m141/232\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m1:26\u001b[0m 947ms/step - accuracy: 0.4920 - binary_io_u_5: 0.4920 - loss: 0.9160Batch 142, Loss Value: 0.8881\n",
      "Batch 142, Gradient Norm: 0.0697\n",
      "\u001b[1m142/232\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m1:25\u001b[0m 945ms/step - accuracy: 0.4921 - binary_io_u_5: 0.4920 - loss: 0.9160Batch 143, Loss Value: 0.9048\n",
      "Batch 143, Gradient Norm: 0.5128\n",
      "\u001b[1m143/232\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m1:23\u001b[0m 944ms/step - accuracy: 0.4921 - binary_io_u_5: 0.4921 - loss: 0.9160Batch 144, Loss Value: 0.8948\n",
      "Batch 144, Gradient Norm: 0.2573\n",
      "\u001b[1m144/232\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m1:22\u001b[0m 942ms/step - accuracy: 0.4921 - binary_io_u_5: 0.4921 - loss: 0.9160Batch 145, Loss Value: 0.9334\n",
      "Batch 145, Gradient Norm: 0.2510\n",
      "\u001b[1m145/232\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m1:21\u001b[0m 940ms/step - accuracy: 0.4921 - binary_io_u_5: 0.4921 - loss: 0.9160Batch 146, Loss Value: 0.8695\n",
      "Batch 146, Gradient Norm: 0.1682\n",
      "\u001b[1m146/232\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m1:20\u001b[0m 939ms/step - accuracy: 0.4922 - binary_io_u_5: 0.4921 - loss: 0.9159Batch 147, Loss Value: 0.9054\n",
      "Batch 147, Gradient Norm: 0.3485\n",
      "\u001b[1m147/232\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m1:19\u001b[0m 937ms/step - accuracy: 0.4922 - binary_io_u_5: 0.4922 - loss: 0.9159Batch 148, Loss Value: 0.9027\n",
      "Batch 148, Gradient Norm: 0.1785\n",
      "\u001b[1m148/232\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m1:18\u001b[0m 936ms/step - accuracy: 0.4922 - binary_io_u_5: 0.4922 - loss: 0.9159Batch 149, Loss Value: 0.9323\n",
      "Batch 149, Gradient Norm: 0.0513\n",
      "\u001b[1m149/232\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m1:17\u001b[0m 934ms/step - accuracy: 0.4922 - binary_io_u_5: 0.4922 - loss: 0.9159Batch 150, Loss Value: 0.9042\n",
      "Batch 150, Gradient Norm: 0.2649\n",
      "\u001b[1m150/232\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m1:16\u001b[0m 932ms/step - accuracy: 0.4923 - binary_io_u_5: 0.4922 - loss: 0.9159Batch 151, Loss Value: 0.9207\n",
      "Batch 151, Gradient Norm: 0.1534\n",
      "\u001b[1m151/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m1:15\u001b[0m 931ms/step - accuracy: 0.4923 - binary_io_u_5: 0.4923 - loss: 0.9158Batch 152, Loss Value: 0.8819\n",
      "Batch 152, Gradient Norm: 0.1396\n",
      "\u001b[1m152/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m1:14\u001b[0m 929ms/step - accuracy: 0.4923 - binary_io_u_5: 0.4923 - loss: 0.9158Batch 153, Loss Value: 0.9101\n",
      "Batch 153, Gradient Norm: 0.0019\n",
      "\u001b[1m153/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m1:13\u001b[0m 928ms/step - accuracy: 0.4923 - binary_io_u_5: 0.4923 - loss: 0.9158Batch 154, Loss Value: 0.9153\n",
      "Batch 154, Gradient Norm: 0.3252\n",
      "\u001b[1m154/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m1:12\u001b[0m 927ms/step - accuracy: 0.4924 - binary_io_u_5: 0.4923 - loss: 0.9158Batch 155, Loss Value: 0.8627\n",
      "Batch 155, Gradient Norm: 0.5380\n",
      "\u001b[1m155/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m1:11\u001b[0m 926ms/step - accuracy: 0.4924 - binary_io_u_5: 0.4924 - loss: 0.9158Batch 156, Loss Value: 0.8896\n",
      "Batch 156, Gradient Norm: 2.1692\n",
      "\u001b[1m156/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m1:10\u001b[0m 924ms/step - accuracy: 0.4924 - binary_io_u_5: 0.4924 - loss: 0.9158Batch 157, Loss Value: 0.8833\n",
      "Batch 157, Gradient Norm: 0.0167\n",
      "\u001b[1m157/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m1:09\u001b[0m 923ms/step - accuracy: 0.4925 - binary_io_u_5: 0.4924 - loss: 0.9158Batch 158, Loss Value: 0.8901\n",
      "Batch 158, Gradient Norm: 0.3136\n",
      "\u001b[1m158/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m1:08\u001b[0m 922ms/step - accuracy: 0.4925 - binary_io_u_5: 0.4925 - loss: 0.9157Batch 159, Loss Value: 0.8837\n",
      "Batch 159, Gradient Norm: 0.2218\n",
      "\u001b[1m159/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m1:07\u001b[0m 920ms/step - accuracy: 0.4925 - binary_io_u_5: 0.4925 - loss: 0.9157Batch 160, Loss Value: 0.9183\n",
      "Batch 160, Gradient Norm: 1.2162\n",
      "\u001b[1m160/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m1:06\u001b[0m 919ms/step - accuracy: 0.4926 - binary_io_u_5: 0.4925 - loss: 0.9157Batch 161, Loss Value: 0.9023\n",
      "Batch 161, Gradient Norm: 0.9372\n",
      "\u001b[1m161/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m1:05\u001b[0m 917ms/step - accuracy: 0.4926 - binary_io_u_5: 0.4926 - loss: 0.9157Batch 162, Loss Value: 0.9171\n",
      "Batch 162, Gradient Norm: 0.4900\n",
      "\u001b[1m162/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m1:04\u001b[0m 916ms/step - accuracy: 0.4926 - binary_io_u_5: 0.4926 - loss: 0.9157Batch 163, Loss Value: 0.9059\n",
      "Batch 163, Gradient Norm: 0.2406\n",
      "\u001b[1m163/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m1:03\u001b[0m 915ms/step - accuracy: 0.4927 - binary_io_u_5: 0.4926 - loss: 0.9156Batch 164, Loss Value: 0.8974\n",
      "Batch 164, Gradient Norm: 0.1567\n",
      "\u001b[1m164/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m1:02\u001b[0m 914ms/step - accuracy: 0.4927 - binary_io_u_5: 0.4927 - loss: 0.9156Batch 165, Loss Value: 0.9114\n",
      "Batch 165, Gradient Norm: 0.0012\n",
      "\u001b[1m165/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m1:01\u001b[0m 912ms/step - accuracy: 0.4927 - binary_io_u_5: 0.4927 - loss: 0.9156Batch 166, Loss Value: 0.8775\n",
      "Batch 166, Gradient Norm: 0.2167\n",
      "\u001b[1m166/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m1:00\u001b[0m 911ms/step - accuracy: 0.4928 - binary_io_u_5: 0.4927 - loss: 0.9156Batch 167, Loss Value: 0.9220\n",
      "Batch 167, Gradient Norm: 0.4541\n",
      "\u001b[1m167/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m59s\u001b[0m 910ms/step - accuracy: 0.4928 - binary_io_u_5: 0.4928 - loss: 0.9156 Batch 168, Loss Value: 0.8781\n",
      "Batch 168, Gradient Norm: 0.5498\n",
      "\u001b[1m168/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m58s\u001b[0m 908ms/step - accuracy: 0.4929 - binary_io_u_5: 0.4928 - loss: 0.9155Batch 169, Loss Value: 0.8827\n",
      "Batch 169, Gradient Norm: 0.2964\n",
      "\u001b[1m169/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m57s\u001b[0m 907ms/step - accuracy: 0.4929 - binary_io_u_5: 0.4929 - loss: 0.9155Batch 170, Loss Value: 0.8795\n",
      "Batch 170, Gradient Norm: 0.0366\n",
      "\u001b[1m170/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m56s\u001b[0m 905ms/step - accuracy: 0.4929 - binary_io_u_5: 0.4929 - loss: 0.9155Batch 171, Loss Value: 0.8804\n",
      "Batch 171, Gradient Norm: 0.3103\n",
      "\u001b[1m171/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m55s\u001b[0m 904ms/step - accuracy: 0.4930 - binary_io_u_5: 0.4929 - loss: 0.9155Batch 172, Loss Value: 0.8932\n",
      "Batch 172, Gradient Norm: 0.0848\n",
      "\u001b[1m172/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m54s\u001b[0m 903ms/step - accuracy: 0.4930 - binary_io_u_5: 0.4930 - loss: 0.9155Batch 173, Loss Value: 0.8925\n",
      "Batch 173, Gradient Norm: 0.1312\n",
      "\u001b[1m173/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m53s\u001b[0m 902ms/step - accuracy: 0.4930 - binary_io_u_5: 0.4930 - loss: 0.9155Batch 174, Loss Value: 0.8802\n",
      "Batch 174, Gradient Norm: 0.0575\n",
      "\u001b[1m174/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m52s\u001b[0m 900ms/step - accuracy: 0.4931 - binary_io_u_5: 0.4930 - loss: 0.9154Batch 175, Loss Value: 0.8776\n",
      "Batch 175, Gradient Norm: 0.0216\n",
      "\u001b[1m175/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m51s\u001b[0m 899ms/step - accuracy: 0.4931 - binary_io_u_5: 0.4931 - loss: 0.9154Batch 176, Loss Value: 0.8676\n",
      "Batch 176, Gradient Norm: 0.3222\n",
      "\u001b[1m176/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m50s\u001b[0m 898ms/step - accuracy: 0.4932 - binary_io_u_5: 0.4931 - loss: 0.9154Batch 177, Loss Value: 0.8911\n",
      "Batch 177, Gradient Norm: 0.2645\n",
      "\u001b[1m177/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m49s\u001b[0m 897ms/step - accuracy: 0.4932 - binary_io_u_5: 0.4931 - loss: 0.9154Batch 178, Loss Value: 0.8851\n",
      "Batch 178, Gradient Norm: 0.7803\n",
      "\u001b[1m178/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m48s\u001b[0m 896ms/step - accuracy: 0.4932 - binary_io_u_5: 0.4932 - loss: 0.9154Batch 179, Loss Value: 0.9052\n",
      "Batch 179, Gradient Norm: 0.0902\n",
      "\u001b[1m179/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m47s\u001b[0m 895ms/step - accuracy: 0.4933 - binary_io_u_5: 0.4932 - loss: 0.9153Batch 180, Loss Value: 0.8897\n",
      "Batch 180, Gradient Norm: 0.3373\n",
      "\u001b[1m180/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m46s\u001b[0m 894ms/step - accuracy: 0.4933 - binary_io_u_5: 0.4932 - loss: 0.9153Batch 181, Loss Value: 0.8694\n",
      "Batch 181, Gradient Norm: 0.4357\n",
      "\u001b[1m181/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m45s\u001b[0m 892ms/step - accuracy: 0.4933 - binary_io_u_5: 0.4933 - loss: 0.9153Batch 182, Loss Value: 0.8708\n",
      "Batch 182, Gradient Norm: 0.3652\n",
      "\u001b[1m182/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m44s\u001b[0m 891ms/step - accuracy: 0.4934 - binary_io_u_5: 0.4933 - loss: 0.9153Batch 183, Loss Value: 0.8859\n",
      "Batch 183, Gradient Norm: 0.3813\n",
      "\u001b[1m183/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m43s\u001b[0m 890ms/step - accuracy: 0.4934 - binary_io_u_5: 0.4933 - loss: 0.9153Batch 184, Loss Value: 0.9059\n",
      "Batch 184, Gradient Norm: 0.0158\n",
      "\u001b[1m184/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m42s\u001b[0m 889ms/step - accuracy: 0.4934 - binary_io_u_5: 0.4934 - loss: 0.9153Batch 185, Loss Value: 0.8561\n",
      "Batch 185, Gradient Norm: 0.1826\n",
      "\u001b[1m185/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m41s\u001b[0m 888ms/step - accuracy: 0.4935 - binary_io_u_5: 0.4934 - loss: 0.9152Batch 186, Loss Value: 0.8740\n",
      "Batch 186, Gradient Norm: 0.1600\n",
      "\u001b[1m186/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m40s\u001b[0m 887ms/step - accuracy: 0.4935 - binary_io_u_5: 0.4934 - loss: 0.9152Batch 187, Loss Value: 0.8721\n",
      "Batch 187, Gradient Norm: 0.0839\n",
      "\u001b[1m187/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m39s\u001b[0m 886ms/step - accuracy: 0.4935 - binary_io_u_5: 0.4934 - loss: 0.9152Batch 188, Loss Value: 0.8897\n",
      "Batch 188, Gradient Norm: 0.0532\n",
      "\u001b[1m188/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m38s\u001b[0m 886ms/step - accuracy: 0.4935 - binary_io_u_5: 0.4935 - loss: 0.9152Batch 189, Loss Value: 0.8750\n",
      "Batch 189, Gradient Norm: 0.1878\n",
      "\u001b[1m189/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m38s\u001b[0m 885ms/step - accuracy: 0.4936 - binary_io_u_5: 0.4935 - loss: 0.9152Batch 190, Loss Value: 0.8996\n",
      "Batch 190, Gradient Norm: 0.4313\n",
      "\u001b[1m190/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m37s\u001b[0m 883ms/step - accuracy: 0.4936 - binary_io_u_5: 0.4935 - loss: 0.9152Batch 191, Loss Value: 0.9324\n",
      "Batch 191, Gradient Norm: 1.0643\n",
      "\u001b[1m191/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m36s\u001b[0m 882ms/step - accuracy: 0.4936 - binary_io_u_5: 0.4935 - loss: 0.9152Batch 192, Loss Value: 0.8796\n",
      "Batch 192, Gradient Norm: 0.3953\n",
      "\u001b[1m192/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m35s\u001b[0m 881ms/step - accuracy: 0.4936 - binary_io_u_5: 0.4936 - loss: 0.9151Batch 193, Loss Value: 0.8935\n",
      "Batch 193, Gradient Norm: 0.1393\n",
      "\u001b[1m193/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m34s\u001b[0m 880ms/step - accuracy: 0.4937 - binary_io_u_5: 0.4936 - loss: 0.9151Batch 194, Loss Value: 0.8668\n",
      "Batch 194, Gradient Norm: 0.2604\n",
      "\u001b[1m194/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m33s\u001b[0m 879ms/step - accuracy: 0.4937 - binary_io_u_5: 0.4936 - loss: 0.9151Batch 195, Loss Value: 0.8987\n",
      "Batch 195, Gradient Norm: 0.0936\n",
      "\u001b[1m195/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m32s\u001b[0m 878ms/step - accuracy: 0.4937 - binary_io_u_5: 0.4936 - loss: 0.9151Batch 196, Loss Value: 0.8795\n",
      "Batch 196, Gradient Norm: 0.1539\n",
      "\u001b[1m196/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m31s\u001b[0m 877ms/step - accuracy: 0.4937 - binary_io_u_5: 0.4937 - loss: 0.9151Batch 197, Loss Value: 0.8890\n",
      "Batch 197, Gradient Norm: 0.5322\n",
      "\u001b[1m197/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m30s\u001b[0m 876ms/step - accuracy: 0.4938 - binary_io_u_5: 0.4937 - loss: 0.9151Batch 198, Loss Value: 0.9020\n",
      "Batch 198, Gradient Norm: 0.2772\n",
      "\u001b[1m198/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m29s\u001b[0m 875ms/step - accuracy: 0.4938 - binary_io_u_5: 0.4937 - loss: 0.9151Batch 199, Loss Value: 0.9151\n",
      "Batch 199, Gradient Norm: 0.2484\n",
      "\u001b[1m199/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m28s\u001b[0m 875ms/step - accuracy: 0.4938 - binary_io_u_5: 0.4937 - loss: 0.9151Batch 200, Loss Value: 0.8951\n",
      "Batch 200, Gradient Norm: 0.0360\n",
      "\u001b[1m200/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m27s\u001b[0m 874ms/step - accuracy: 0.4938 - binary_io_u_5: 0.4938 - loss: 0.9151Batch 201, Loss Value: 0.8969\n",
      "Batch 201, Gradient Norm: 0.1184\n",
      "\u001b[1m201/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m27s\u001b[0m 873ms/step - accuracy: 0.4938 - binary_io_u_5: 0.4938 - loss: 0.9150Batch 202, Loss Value: 0.8907\n",
      "Batch 202, Gradient Norm: 0.0756\n",
      "\u001b[1m202/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m26s\u001b[0m 871ms/step - accuracy: 0.4939 - binary_io_u_5: 0.4938 - loss: 0.9150Batch 203, Loss Value: 0.8930\n",
      "Batch 203, Gradient Norm: 0.0008\n",
      "\u001b[1m203/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m25s\u001b[0m 871ms/step - accuracy: 0.4939 - binary_io_u_5: 0.4938 - loss: 0.9150Batch 204, Loss Value: 0.9082\n",
      "Batch 204, Gradient Norm: 0.8563\n",
      "\u001b[1m204/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m24s\u001b[0m 870ms/step - accuracy: 0.4939 - binary_io_u_5: 0.4938 - loss: 0.9150Batch 205, Loss Value: 0.9087\n",
      "Batch 205, Gradient Norm: 0.1766\n",
      "\u001b[1m205/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m23s\u001b[0m 869ms/step - accuracy: 0.4939 - binary_io_u_5: 0.4938 - loss: 0.9150Batch 206, Loss Value: 0.8964\n",
      "Batch 206, Gradient Norm: 0.0344\n",
      "\u001b[1m206/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m22s\u001b[0m 868ms/step - accuracy: 0.4939 - binary_io_u_5: 0.4939 - loss: 0.9150Batch 207, Loss Value: 0.9047\n",
      "Batch 207, Gradient Norm: 0.4071\n",
      "\u001b[1m207/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m21s\u001b[0m 867ms/step - accuracy: 0.4939 - binary_io_u_5: 0.4939 - loss: 0.9150Batch 208, Loss Value: 0.9018\n",
      "Batch 208, Gradient Norm: 0.1799\n",
      "\u001b[1m208/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m20s\u001b[0m 866ms/step - accuracy: 0.4940 - binary_io_u_5: 0.4939 - loss: 0.9150Batch 209, Loss Value: 0.9098\n",
      "Batch 209, Gradient Norm: 0.1164\n",
      "\u001b[1m209/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m19s\u001b[0m 866ms/step - accuracy: 0.4940 - binary_io_u_5: 0.4939 - loss: 0.9150Batch 210, Loss Value: 0.8998\n",
      "Batch 210, Gradient Norm: 0.1623\n",
      "\u001b[1m210/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m19s\u001b[0m 865ms/step - accuracy: 0.4940 - binary_io_u_5: 0.4939 - loss: 0.9150Batch 211, Loss Value: 0.8967\n",
      "Batch 211, Gradient Norm: 0.1942\n",
      "\u001b[1m211/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m18s\u001b[0m 864ms/step - accuracy: 0.4940 - binary_io_u_5: 0.4939 - loss: 0.9150Batch 212, Loss Value: 0.8790\n",
      "Batch 212, Gradient Norm: 0.2253\n",
      "\u001b[1m212/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m17s\u001b[0m 863ms/step - accuracy: 0.4940 - binary_io_u_5: 0.4939 - loss: 0.9150Batch 213, Loss Value: 0.8773\n",
      "Batch 213, Gradient Norm: 0.0011\n",
      "\u001b[1m213/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m16s\u001b[0m 862ms/step - accuracy: 0.4940 - binary_io_u_5: 0.4940 - loss: 0.9149Batch 214, Loss Value: 0.9020\n",
      "Batch 214, Gradient Norm: 0.1058\n",
      "\u001b[1m214/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m15s\u001b[0m 861ms/step - accuracy: 0.4940 - binary_io_u_5: 0.4940 - loss: 0.9149Batch 215, Loss Value: 0.8638\n",
      "Batch 215, Gradient Norm: 0.2313\n",
      "\u001b[1m215/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m14s\u001b[0m 861ms/step - accuracy: 0.4941 - binary_io_u_5: 0.4940 - loss: 0.9149Batch 216, Loss Value: 0.9022\n",
      "Batch 216, Gradient Norm: 0.1718\n",
      "\u001b[1m216/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m13s\u001b[0m 860ms/step - accuracy: 0.4941 - binary_io_u_5: 0.4940 - loss: 0.9149Batch 217, Loss Value: 0.8945\n",
      "Batch 217, Gradient Norm: 0.1534\n",
      "\u001b[1m217/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m12s\u001b[0m 859ms/step - accuracy: 0.4941 - binary_io_u_5: 0.4940 - loss: 0.9149Batch 218, Loss Value: 0.8905\n",
      "Batch 218, Gradient Norm: 0.0577\n",
      "\u001b[1m218/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m12s\u001b[0m 859ms/step - accuracy: 0.4941 - binary_io_u_5: 0.4940 - loss: 0.9149Batch 219, Loss Value: 0.8755\n",
      "Batch 219, Gradient Norm: 0.3873\n",
      "\u001b[1m219/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m11s\u001b[0m 858ms/step - accuracy: 0.4941 - binary_io_u_5: 0.4940 - loss: 0.9149Batch 220, Loss Value: 0.9010\n",
      "Batch 220, Gradient Norm: 0.1891\n",
      "\u001b[1m220/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m10s\u001b[0m 857ms/step - accuracy: 0.4941 - binary_io_u_5: 0.4940 - loss: 0.9149Batch 221, Loss Value: 0.8989\n",
      "Batch 221, Gradient Norm: 0.2466\n",
      "\u001b[1m221/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m9s\u001b[0m 856ms/step - accuracy: 0.4941 - binary_io_u_5: 0.4940 - loss: 0.9149 Batch 222, Loss Value: 0.8979\n",
      "Batch 222, Gradient Norm: 0.6493\n",
      "\u001b[1m222/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m8s\u001b[0m 856ms/step - accuracy: 0.4941 - binary_io_u_5: 0.4940 - loss: 0.9149Batch 223, Loss Value: 0.9009\n",
      "Batch 223, Gradient Norm: 0.1441\n",
      "\u001b[1m223/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m7s\u001b[0m 855ms/step - accuracy: 0.4941 - binary_io_u_5: 0.4940 - loss: 0.9149Batch 224, Loss Value: 0.8702\n",
      "Batch 224, Gradient Norm: 0.1872\n",
      "\u001b[1m224/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m6s\u001b[0m 854ms/step - accuracy: 0.4941 - binary_io_u_5: 0.4940 - loss: 0.9149Batch 225, Loss Value: 0.8966\n",
      "Batch 225, Gradient Norm: 0.3768\n",
      "\u001b[1m225/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m5s\u001b[0m 854ms/step - accuracy: 0.4942 - binary_io_u_5: 0.4941 - loss: 0.9149Batch 226, Loss Value: 0.8858\n",
      "Batch 226, Gradient Norm: 0.1270\n",
      "\u001b[1m226/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m5s\u001b[0m 853ms/step - accuracy: 0.4942 - binary_io_u_5: 0.4941 - loss: 0.9149Batch 227, Loss Value: 0.9261\n",
      "Batch 227, Gradient Norm: 0.5495\n",
      "\u001b[1m227/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m4s\u001b[0m 853ms/step - accuracy: 0.4942 - binary_io_u_5: 0.4941 - loss: 0.9149Batch 228, Loss Value: 0.8942\n",
      "Batch 228, Gradient Norm: 0.3936\n",
      "\u001b[1m228/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m3s\u001b[0m 852ms/step - accuracy: 0.4942 - binary_io_u_5: 0.4941 - loss: 0.9149Batch 229, Loss Value: 0.8991\n",
      "Batch 229, Gradient Norm: 0.3263\n",
      "\u001b[1m229/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m2s\u001b[0m 852ms/step - accuracy: 0.4942 - binary_io_u_5: 0.4941 - loss: 0.9149Batch 230, Loss Value: 0.9168\n",
      "Batch 230, Gradient Norm: 0.0927\n",
      "\u001b[1m230/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m1s\u001b[0m 851ms/step - accuracy: 0.4942 - binary_io_u_5: 0.4941 - loss: 0.9149Batch 231, Loss Value: 0.8774\n",
      "Batch 231, Gradient Norm: 0.1145\n",
      "\u001b[1m231/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 851ms/step - accuracy: 0.4942 - binary_io_u_5: 0.4941 - loss: 0.9149Batch 232, Loss Value: 0.8835\n",
      "Batch 232, Gradient Norm: 0.4446\n",
      "\u001b[1m232/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m200s\u001b[0m 859ms/step - accuracy: 0.4942 - binary_io_u_5: 0.4941 - loss: 0.9148 - val_accuracy: 0.5270 - val_binary_io_u_5: 0.4949 - val_loss: 0.9400 - learning_rate: 0.0010\n",
      "Epoch 13/200\n",
      "Batch 1, Loss Value: 0.8805\n",
      "Batch 1, Gradient Norm: 0.0773\n",
      "\u001b[1m  1/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:11\u001b[0m 831ms/step - accuracy: 0.5000 - binary_io_u_5: 0.5000 - loss: 0.9152Batch 2, Loss Value: 0.8875\n",
      "Batch 2, Gradient Norm: 0.1091\n",
      "\u001b[1m  2/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:51\u001b[0m 745ms/step - accuracy: 0.5050 - binary_io_u_5: 0.5050 - loss: 0.9079Batch 3, Loss Value: 0.8952\n",
      "Batch 3, Gradient Norm: 0.1489\n",
      "\u001b[1m  3/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:52\u001b[0m 755ms/step - accuracy: 0.4944 - binary_io_u_5: 0.4944 - loss: 0.9108Batch 4, Loss Value: 0.8910\n",
      "Batch 4, Gradient Norm: 0.2165\n",
      "\u001b[1m  4/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:51\u001b[0m 754ms/step - accuracy: 0.4877 - binary_io_u_5: 0.4877 - loss: 0.9137Batch 5, Loss Value: 0.8740\n",
      "Batch 5, Gradient Norm: 0.2216\n",
      "\u001b[1m  5/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:50\u001b[0m 750ms/step - accuracy: 0.4830 - binary_io_u_5: 0.4830 - loss: 0.9165Batch 6, Loss Value: 0.9044\n",
      "Batch 6, Gradient Norm: 0.7094\n",
      "\u001b[1m  6/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:46\u001b[0m 738ms/step - accuracy: 0.4783 - binary_io_u_5: 0.4783 - loss: 0.9189Batch 7, Loss Value: 0.8822\n",
      "Batch 7, Gradient Norm: 0.0401\n",
      "\u001b[1m  7/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:43\u001b[0m 728ms/step - accuracy: 0.4747 - binary_io_u_5: 0.4747 - loss: 0.9204Batch 8, Loss Value: 0.8915\n",
      "Batch 8, Gradient Norm: 0.4284\n",
      "\u001b[1m  8/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:42\u001b[0m 724ms/step - accuracy: 0.4727 - binary_io_u_5: 0.4724 - loss: 0.9212Batch 9, Loss Value: 0.8902\n",
      "Batch 9, Gradient Norm: 0.1303\n",
      "\u001b[1m  9/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:41\u001b[0m 722ms/step - accuracy: 0.4725 - binary_io_u_5: 0.4720 - loss: 0.9215Batch 10, Loss Value: 0.8901\n",
      "Batch 10, Gradient Norm: 0.2343\n",
      "\u001b[1m 10/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:40\u001b[0m 721ms/step - accuracy: 0.4725 - binary_io_u_5: 0.4718 - loss: 0.9217Batch 11, Loss Value: 0.8947\n",
      "Batch 11, Gradient Norm: 0.1002\n",
      "\u001b[1m 11/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:38\u001b[0m 719ms/step - accuracy: 0.4721 - binary_io_u_5: 0.4714 - loss: 0.9221Batch 12, Loss Value: 0.8774\n",
      "Batch 12, Gradient Norm: 0.1030\n",
      "\u001b[1m 12/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:37\u001b[0m 716ms/step - accuracy: 0.4718 - binary_io_u_5: 0.4709 - loss: 0.9225Batch 13, Loss Value: 0.8848\n",
      "Batch 13, Gradient Norm: 0.1085\n",
      "\u001b[1m 13/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:36\u001b[0m 713ms/step - accuracy: 0.4717 - binary_io_u_5: 0.4708 - loss: 0.9227Batch 14, Loss Value: 0.8826\n",
      "Batch 14, Gradient Norm: 0.1872\n",
      "\u001b[1m 14/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:35\u001b[0m 712ms/step - accuracy: 0.4719 - binary_io_u_5: 0.4710 - loss: 0.9230Batch 15, Loss Value: 0.8736\n",
      "Batch 15, Gradient Norm: 0.1660\n",
      "\u001b[1m 15/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:34\u001b[0m 712ms/step - accuracy: 0.4726 - binary_io_u_5: 0.4716 - loss: 0.9229Batch 16, Loss Value: 0.8746\n",
      "Batch 16, Gradient Norm: 0.0566\n",
      "\u001b[1m 16/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:33\u001b[0m 709ms/step - accuracy: 0.4734 - binary_io_u_5: 0.4724 - loss: 0.9226Batch 17, Loss Value: 0.8748\n",
      "Batch 17, Gradient Norm: 0.3294\n",
      "\u001b[1m 17/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:31\u001b[0m 705ms/step - accuracy: 0.4741 - binary_io_u_5: 0.4731 - loss: 0.9224Batch 18, Loss Value: 0.8802\n",
      "Batch 18, Gradient Norm: 0.4222\n",
      "\u001b[1m 18/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:30\u001b[0m 705ms/step - accuracy: 0.4748 - binary_io_u_5: 0.4738 - loss: 0.9222Batch 19, Loss Value: 0.8966\n",
      "Batch 19, Gradient Norm: 0.2546\n",
      "\u001b[1m 19/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:30\u001b[0m 705ms/step - accuracy: 0.4754 - binary_io_u_5: 0.4744 - loss: 0.9220Batch 20, Loss Value: 0.8740\n",
      "Batch 20, Gradient Norm: 0.1132\n",
      "\u001b[1m 20/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:29\u001b[0m 705ms/step - accuracy: 0.4760 - binary_io_u_5: 0.4750 - loss: 0.9218Batch 21, Loss Value: 0.8672\n",
      "Batch 21, Gradient Norm: 0.2972\n",
      "\u001b[1m 21/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:28\u001b[0m 705ms/step - accuracy: 0.4766 - binary_io_u_5: 0.4756 - loss: 0.9216Batch 22, Loss Value: 0.8687\n",
      "Batch 22, Gradient Norm: 0.0667\n",
      "\u001b[1m 22/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:27\u001b[0m 704ms/step - accuracy: 0.4770 - binary_io_u_5: 0.4760 - loss: 0.9215Batch 23, Loss Value: 0.8909\n",
      "Batch 23, Gradient Norm: 0.1430\n",
      "\u001b[1m 23/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:26\u001b[0m 703ms/step - accuracy: 0.4774 - binary_io_u_5: 0.4764 - loss: 0.9214Batch 24, Loss Value: 0.8814\n",
      "Batch 24, Gradient Norm: 0.0873\n",
      "\u001b[1m 24/232\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:26\u001b[0m 703ms/step - accuracy: 0.4779 - binary_io_u_5: 0.4769 - loss: 0.9212Batch 25, Loss Value: 0.8930\n",
      "Batch 25, Gradient Norm: 0.0045\n",
      "\u001b[1m 25/232\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:25\u001b[0m 703ms/step - accuracy: 0.4784 - binary_io_u_5: 0.4774 - loss: 0.9210Batch 26, Loss Value: 0.8939\n",
      "Batch 26, Gradient Norm: 0.1262\n",
      "\u001b[1m 26/232\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:24\u001b[0m 703ms/step - accuracy: 0.4789 - binary_io_u_5: 0.4779 - loss: 0.9208Batch 27, Loss Value: 0.8859\n",
      "Batch 27, Gradient Norm: 0.1260\n",
      "\u001b[1m 27/232\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:23\u001b[0m 702ms/step - accuracy: 0.4793 - binary_io_u_5: 0.4784 - loss: 0.9206Batch 28, Loss Value: 0.8862\n",
      "Batch 28, Gradient Norm: 0.2659\n",
      "\u001b[1m 28/232\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:22\u001b[0m 700ms/step - accuracy: 0.4797 - binary_io_u_5: 0.4788 - loss: 0.9204Batch 29, Loss Value: 0.8769\n",
      "Batch 29, Gradient Norm: 0.1371\n",
      "\u001b[1m 29/232\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:21\u001b[0m 699ms/step - accuracy: 0.4800 - binary_io_u_5: 0.4791 - loss: 0.9203Batch 30, Loss Value: 0.8919\n",
      "Batch 30, Gradient Norm: 0.3664\n",
      "\u001b[1m 30/232\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:21\u001b[0m 699ms/step - accuracy: 0.4803 - binary_io_u_5: 0.4794 - loss: 0.9201Batch 31, Loss Value: 0.8929\n",
      "Batch 31, Gradient Norm: 0.1001\n",
      "\u001b[1m 31/232\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:20\u001b[0m 699ms/step - accuracy: 0.4806 - binary_io_u_5: 0.4797 - loss: 0.9200Batch 32, Loss Value: 0.8928\n",
      "Batch 32, Gradient Norm: 0.0036\n",
      "\u001b[1m 32/232\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:19\u001b[0m 700ms/step - accuracy: 0.4810 - binary_io_u_5: 0.4801 - loss: 0.9198Batch 33, Loss Value: 0.8960\n",
      "Batch 33, Gradient Norm: 0.5864\n",
      "\u001b[1m 33/232\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:19\u001b[0m 700ms/step - accuracy: 0.4813 - binary_io_u_5: 0.4804 - loss: 0.9196Batch 34, Loss Value: 0.8830\n",
      "Batch 34, Gradient Norm: 0.2587\n",
      "\u001b[1m 34/232\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:18\u001b[0m 700ms/step - accuracy: 0.4817 - binary_io_u_5: 0.4808 - loss: 0.9195Batch 35, Loss Value: 0.8961\n",
      "Batch 35, Gradient Norm: 0.4313\n",
      "\u001b[1m 35/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:18\u001b[0m 701ms/step - accuracy: 0.4820 - binary_io_u_5: 0.4811 - loss: 0.9193Batch 36, Loss Value: 0.8873\n",
      "Batch 36, Gradient Norm: 0.2642\n",
      "\u001b[1m 36/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:17\u001b[0m 700ms/step - accuracy: 0.4823 - binary_io_u_5: 0.4815 - loss: 0.9192Batch 37, Loss Value: 0.8854\n",
      "Batch 37, Gradient Norm: 0.2925\n",
      "\u001b[1m 37/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:16\u001b[0m 700ms/step - accuracy: 0.4826 - binary_io_u_5: 0.4818 - loss: 0.9191Batch 38, Loss Value: 0.8720\n",
      "Batch 38, Gradient Norm: 0.1508\n",
      "\u001b[1m 38/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:15\u001b[0m 699ms/step - accuracy: 0.4830 - binary_io_u_5: 0.4821 - loss: 0.9189Batch 39, Loss Value: 0.8869\n",
      "Batch 39, Gradient Norm: 0.3008\n",
      "\u001b[1m 39/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:14\u001b[0m 698ms/step - accuracy: 0.4833 - binary_io_u_5: 0.4824 - loss: 0.9187Batch 40, Loss Value: 0.8839\n",
      "Batch 40, Gradient Norm: 0.0725\n",
      "\u001b[1m 40/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:14\u001b[0m 699ms/step - accuracy: 0.4836 - binary_io_u_5: 0.4828 - loss: 0.9186Batch 41, Loss Value: 0.8979\n",
      "Batch 41, Gradient Norm: 0.2716\n",
      "\u001b[1m 41/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:13\u001b[0m 699ms/step - accuracy: 0.4839 - binary_io_u_5: 0.4831 - loss: 0.9184Batch 42, Loss Value: 0.8871\n",
      "Batch 42, Gradient Norm: 0.2856\n",
      "\u001b[1m 42/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:12\u001b[0m 699ms/step - accuracy: 0.4842 - binary_io_u_5: 0.4834 - loss: 0.9183Batch 43, Loss Value: 0.8909\n",
      "Batch 43, Gradient Norm: 0.1720\n",
      "\u001b[1m 43/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:12\u001b[0m 700ms/step - accuracy: 0.4845 - binary_io_u_5: 0.4837 - loss: 0.9182Batch 44, Loss Value: 0.8925\n",
      "Batch 44, Gradient Norm: 0.0251\n",
      "\u001b[1m 44/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:11\u001b[0m 699ms/step - accuracy: 0.4848 - binary_io_u_5: 0.4840 - loss: 0.9181Batch 45, Loss Value: 0.8730\n",
      "Batch 45, Gradient Norm: 0.0515\n",
      "\u001b[1m 45/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:10\u001b[0m 699ms/step - accuracy: 0.4852 - binary_io_u_5: 0.4843 - loss: 0.9179Batch 46, Loss Value: 0.8934\n",
      "Batch 46, Gradient Norm: 0.0355\n",
      "\u001b[1m 46/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:10\u001b[0m 699ms/step - accuracy: 0.4855 - binary_io_u_5: 0.4847 - loss: 0.9178Batch 47, Loss Value: 0.8782\n",
      "Batch 47, Gradient Norm: 0.1485\n",
      "\u001b[1m 47/232\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:09\u001b[0m 699ms/step - accuracy: 0.4858 - binary_io_u_5: 0.4850 - loss: 0.9177Batch 48, Loss Value: 0.8851\n",
      "Batch 48, Gradient Norm: 0.0952\n",
      "\u001b[1m 48/232\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:08\u001b[0m 699ms/step - accuracy: 0.4860 - binary_io_u_5: 0.4852 - loss: 0.9176Batch 49, Loss Value: 0.8995\n",
      "Batch 49, Gradient Norm: 0.3757\n",
      "\u001b[1m 49/232\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:07\u001b[0m 698ms/step - accuracy: 0.4863 - binary_io_u_5: 0.4855 - loss: 0.9175Batch 50, Loss Value: 0.8947\n",
      "Batch 50, Gradient Norm: 36.7094\n",
      "\u001b[1m 50/232\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:07\u001b[0m 699ms/step - accuracy: 0.4865 - binary_io_u_5: 0.4858 - loss: 0.9174Batch 51, Loss Value: 0.8680\n",
      "Batch 51, Gradient Norm: 0.6637\n",
      "\u001b[1m 51/232\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:06\u001b[0m 699ms/step - accuracy: 0.4868 - binary_io_u_5: 0.4861 - loss: 0.9172Batch 52, Loss Value: 0.8929\n",
      "Batch 52, Gradient Norm: 0.0075\n",
      "\u001b[1m 52/232\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:05\u001b[0m 699ms/step - accuracy: 0.4871 - binary_io_u_5: 0.4864 - loss: 0.9171Batch 53, Loss Value: 0.8849\n",
      "Batch 53, Gradient Norm: 0.3091\n",
      "\u001b[1m 53/232\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:05\u001b[0m 699ms/step - accuracy: 0.4874 - binary_io_u_5: 0.4867 - loss: 0.9170Batch 54, Loss Value: 0.8905\n",
      "Batch 54, Gradient Norm: 0.1434\n",
      "\u001b[1m 54/232\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:04\u001b[0m 699ms/step - accuracy: 0.4876 - binary_io_u_5: 0.4869 - loss: 0.9169Batch 55, Loss Value: 0.8853\n",
      "Batch 55, Gradient Norm: 1.4300\n",
      "\u001b[1m 55/232\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:03\u001b[0m 699ms/step - accuracy: 0.4879 - binary_io_u_5: 0.4871 - loss: 0.9168Batch 56, Loss Value: 0.8723\n",
      "Batch 56, Gradient Norm: 0.2166\n",
      "\u001b[1m 56/232\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:03\u001b[0m 699ms/step - accuracy: 0.4881 - binary_io_u_5: 0.4874 - loss: 0.9167Batch 57, Loss Value: 0.8804\n",
      "Batch 57, Gradient Norm: 2.4204\n",
      "\u001b[1m 57/232\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:02\u001b[0m 699ms/step - accuracy: 0.4883 - binary_io_u_5: 0.4876 - loss: 0.9166Batch 58, Loss Value: 0.8984\n",
      "Batch 58, Gradient Norm: 0.3914\n",
      "\u001b[1m 58/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:01\u001b[0m 699ms/step - accuracy: 0.4885 - binary_io_u_5: 0.4878 - loss: 0.9165Batch 59, Loss Value: 0.8891\n",
      "Batch 59, Gradient Norm: 0.1552\n",
      "\u001b[1m 59/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:00\u001b[0m 699ms/step - accuracy: 0.4887 - binary_io_u_5: 0.4880 - loss: 0.9164Batch 60, Loss Value: 0.8658\n",
      "Batch 60, Gradient Norm: 0.1440\n",
      "\u001b[1m 60/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:00\u001b[0m 698ms/step - accuracy: 0.4889 - binary_io_u_5: 0.4882 - loss: 0.9164Batch 61, Loss Value: 0.9016\n",
      "Batch 61, Gradient Norm: 0.5794\n",
      "\u001b[1m 61/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:59\u001b[0m 698ms/step - accuracy: 0.4891 - binary_io_u_5: 0.4884 - loss: 0.9163Batch 62, Loss Value: 0.8821\n",
      "Batch 62, Gradient Norm: 0.1299\n",
      "\u001b[1m 62/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:58\u001b[0m 698ms/step - accuracy: 0.4892 - binary_io_u_5: 0.4885 - loss: 0.9163Batch 63, Loss Value: 0.8922\n",
      "Batch 63, Gradient Norm: 0.1455\n",
      "\u001b[1m 63/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:57\u001b[0m 698ms/step - accuracy: 0.4893 - binary_io_u_5: 0.4887 - loss: 0.9162Batch 64, Loss Value: 0.8938\n",
      "Batch 64, Gradient Norm: 0.2137\n",
      "\u001b[1m 64/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:57\u001b[0m 698ms/step - accuracy: 0.4895 - binary_io_u_5: 0.4888 - loss: 0.9161Batch 65, Loss Value: 0.8842\n",
      "Batch 65, Gradient Norm: 0.1773\n",
      "\u001b[1m 65/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:56\u001b[0m 699ms/step - accuracy: 0.4896 - binary_io_u_5: 0.4889 - loss: 0.9161Batch 66, Loss Value: 0.8780\n",
      "Batch 66, Gradient Norm: 0.5276\n",
      "\u001b[1m 66/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:55\u001b[0m 698ms/step - accuracy: 0.4897 - binary_io_u_5: 0.4890 - loss: 0.9160Batch 67, Loss Value: 0.8794\n",
      "Batch 67, Gradient Norm: 0.2272\n",
      "\u001b[1m 67/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:55\u001b[0m 698ms/step - accuracy: 0.4898 - binary_io_u_5: 0.4891 - loss: 0.9160Batch 68, Loss Value: 0.8949\n",
      "Batch 68, Gradient Norm: 0.0711\n",
      "\u001b[1m 68/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:54\u001b[0m 698ms/step - accuracy: 0.4899 - binary_io_u_5: 0.4893 - loss: 0.9160Batch 69, Loss Value: 0.8922\n",
      "Batch 69, Gradient Norm: 0.0719\n",
      "\u001b[1m 69/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:53\u001b[0m 698ms/step - accuracy: 0.4900 - binary_io_u_5: 0.4894 - loss: 0.9159Batch 70, Loss Value: 0.8922\n",
      "Batch 70, Gradient Norm: 0.0426\n",
      "\u001b[1m 70/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:52\u001b[0m 697ms/step - accuracy: 0.4902 - binary_io_u_5: 0.4895 - loss: 0.9158Batch 71, Loss Value: 0.8613\n",
      "Batch 71, Gradient Norm: 0.1657\n",
      "\u001b[1m 71/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:52\u001b[0m 698ms/step - accuracy: 0.4903 - binary_io_u_5: 0.4897 - loss: 0.9158Batch 72, Loss Value: 0.8833\n",
      "Batch 72, Gradient Norm: 1.4791\n",
      "\u001b[1m 72/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:51\u001b[0m 699ms/step - accuracy: 0.4904 - binary_io_u_5: 0.4898 - loss: 0.9157Batch 73, Loss Value: 0.8742\n",
      "Batch 73, Gradient Norm: 0.0532\n",
      "\u001b[1m 73/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:51\u001b[0m 700ms/step - accuracy: 0.4905 - binary_io_u_5: 0.4899 - loss: 0.9157Batch 74, Loss Value: 0.8834\n",
      "Batch 74, Gradient Norm: 0.1289\n",
      "\u001b[1m 74/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:50\u001b[0m 700ms/step - accuracy: 0.4906 - binary_io_u_5: 0.4900 - loss: 0.9156Batch 75, Loss Value: 0.8796\n",
      "Batch 75, Gradient Norm: 0.1463\n",
      "\u001b[1m 75/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:50\u001b[0m 701ms/step - accuracy: 0.4907 - binary_io_u_5: 0.4901 - loss: 0.9155Batch 76, Loss Value: 0.8914\n",
      "Batch 76, Gradient Norm: 0.0806\n",
      "\u001b[1m 76/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:49\u001b[0m 701ms/step - accuracy: 0.4909 - binary_io_u_5: 0.4902 - loss: 0.9155Batch 77, Loss Value: 0.8809\n",
      "Batch 77, Gradient Norm: 0.6581\n",
      "\u001b[1m 77/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:48\u001b[0m 702ms/step - accuracy: 0.4910 - binary_io_u_5: 0.4904 - loss: 0.9154Batch 78, Loss Value: 0.8773\n",
      "Batch 78, Gradient Norm: 0.2396\n",
      "\u001b[1m 78/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:48\u001b[0m 702ms/step - accuracy: 0.4911 - binary_io_u_5: 0.4905 - loss: 0.9154Batch 79, Loss Value: 0.8742\n",
      "Batch 79, Gradient Norm: 0.0007\n",
      "\u001b[1m 79/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:47\u001b[0m 702ms/step - accuracy: 0.4912 - binary_io_u_5: 0.4906 - loss: 0.9153Batch 80, Loss Value: 0.8898\n",
      "Batch 80, Gradient Norm: 0.1389\n",
      "\u001b[1m 80/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:46\u001b[0m 702ms/step - accuracy: 0.4913 - binary_io_u_5: 0.4907 - loss: 0.9153Batch 81, Loss Value: 0.8807\n",
      "Batch 81, Gradient Norm: 0.4459\n",
      "\u001b[1m 81/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:46\u001b[0m 702ms/step - accuracy: 0.4914 - binary_io_u_5: 0.4908 - loss: 0.9152Batch 82, Loss Value: 0.8939\n",
      "Batch 82, Gradient Norm: 0.1337\n",
      "\u001b[1m 82/232\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m1:45\u001b[0m 702ms/step - accuracy: 0.4914 - binary_io_u_5: 0.4908 - loss: 0.9152Batch 83, Loss Value: 0.8938\n",
      "Batch 83, Gradient Norm: 0.0723\n",
      "\u001b[1m 83/232\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m1:44\u001b[0m 703ms/step - accuracy: 0.4915 - binary_io_u_5: 0.4909 - loss: 0.9151Batch 84, Loss Value: 0.8928\n",
      "Batch 84, Gradient Norm: 0.0111\n",
      "\u001b[1m 84/232\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m1:43\u001b[0m 703ms/step - accuracy: 0.4916 - binary_io_u_5: 0.4910 - loss: 0.9151Batch 85, Loss Value: 0.8926\n",
      "Batch 85, Gradient Norm: 0.0451\n",
      "\u001b[1m 85/232\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m1:43\u001b[0m 703ms/step - accuracy: 0.4916 - binary_io_u_5: 0.4910 - loss: 0.9151Batch 86, Loss Value: 0.8928\n",
      "Batch 86, Gradient Norm: 0.0061\n",
      "\u001b[1m 86/232\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m1:42\u001b[0m 703ms/step - accuracy: 0.4917 - binary_io_u_5: 0.4911 - loss: 0.9150Batch 87, Loss Value: 0.8849\n",
      "Batch 87, Gradient Norm: 0.3362\n",
      "\u001b[1m 87/232\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m1:41\u001b[0m 703ms/step - accuracy: 0.4917 - binary_io_u_5: 0.4912 - loss: 0.9150Batch 88, Loss Value: 0.8928\n",
      "Batch 88, Gradient Norm: 0.0060\n",
      "\u001b[1m 88/232\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m1:41\u001b[0m 703ms/step - accuracy: 0.4918 - binary_io_u_5: 0.4912 - loss: 0.9150Batch 89, Loss Value: 0.8978\n",
      "Batch 89, Gradient Norm: 0.3179\n",
      "\u001b[1m 89/232\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m1:40\u001b[0m 702ms/step - accuracy: 0.4918 - binary_io_u_5: 0.4913 - loss: 0.9149Batch 90, Loss Value: 0.8886\n",
      "Batch 90, Gradient Norm: 0.1935\n",
      "\u001b[1m 90/232\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m1:39\u001b[0m 701ms/step - accuracy: 0.4919 - binary_io_u_5: 0.4913 - loss: 0.9149Batch 91, Loss Value: 0.8740\n",
      "Batch 91, Gradient Norm: 0.1262\n",
      "\u001b[1m 91/232\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m1:38\u001b[0m 698ms/step - accuracy: 0.4919 - binary_io_u_5: 0.4913 - loss: 0.9149Batch 92, Loss Value: 0.8905\n",
      "Batch 92, Gradient Norm: 0.1146\n",
      "\u001b[1m 92/232\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m1:37\u001b[0m 695ms/step - accuracy: 0.4919 - binary_io_u_5: 0.4914 - loss: 0.9148Batch 93, Loss Value: 0.8928\n",
      "Batch 93, Gradient Norm: 0.0166\n",
      "\u001b[1m 93/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m1:36\u001b[0m 693ms/step - accuracy: 0.4920 - binary_io_u_5: 0.4914 - loss: 0.9148Batch 94, Loss Value: 0.8989\n",
      "Batch 94, Gradient Norm: 0.0602\n",
      "\u001b[1m 94/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m1:35\u001b[0m 691ms/step - accuracy: 0.4920 - binary_io_u_5: 0.4915 - loss: 0.9148Batch 95, Loss Value: 0.8928\n",
      "Batch 95, Gradient Norm: 0.0100\n",
      "\u001b[1m 95/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m1:34\u001b[0m 688ms/step - accuracy: 0.4920 - binary_io_u_5: 0.4915 - loss: 0.9148Batch 96, Loss Value: 0.8928\n",
      "Batch 96, Gradient Norm: 0.0096\n",
      "\u001b[1m 96/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m1:33\u001b[0m 686ms/step - accuracy: 0.4921 - binary_io_u_5: 0.4915 - loss: 0.9148Batch 97, Loss Value: 0.8934\n",
      "Batch 97, Gradient Norm: 0.0317\n",
      "\u001b[1m 97/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m1:32\u001b[0m 684ms/step - accuracy: 0.4921 - binary_io_u_5: 0.4915 - loss: 0.9147Batch 98, Loss Value: 0.8929\n",
      "Batch 98, Gradient Norm: 0.0034\n",
      "\u001b[1m 98/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m1:31\u001b[0m 681ms/step - accuracy: 0.4921 - binary_io_u_5: 0.4916 - loss: 0.9147Batch 99, Loss Value: 0.8842\n",
      "Batch 99, Gradient Norm: 0.2619\n",
      "\u001b[1m 99/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m1:30\u001b[0m 679ms/step - accuracy: 0.4921 - binary_io_u_5: 0.4916 - loss: 0.9147Batch 100, Loss Value: 0.8927\n",
      "Batch 100, Gradient Norm: 0.0100\n",
      "\u001b[1m100/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m1:29\u001b[0m 677ms/step - accuracy: 0.4921 - binary_io_u_5: 0.4916 - loss: 0.9147Batch 101, Loss Value: 0.8930\n",
      "Batch 101, Gradient Norm: 0.0142\n",
      "\u001b[1m101/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m1:28\u001b[0m 675ms/step - accuracy: 0.4922 - binary_io_u_5: 0.4916 - loss: 0.9147Batch 102, Loss Value: 0.8749\n",
      "Batch 102, Gradient Norm: 0.0377\n",
      "\u001b[1m102/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m1:27\u001b[0m 673ms/step - accuracy: 0.4922 - binary_io_u_5: 0.4917 - loss: 0.9146Batch 103, Loss Value: 0.8929\n",
      "Batch 103, Gradient Norm: 0.0018\n",
      "\u001b[1m103/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m1:26\u001b[0m 671ms/step - accuracy: 0.4922 - binary_io_u_5: 0.4917 - loss: 0.9146Batch 104, Loss Value: 0.8901\n",
      "Batch 104, Gradient Norm: 0.1845\n",
      "\u001b[1m104/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m1:25\u001b[0m 669ms/step - accuracy: 0.4922 - binary_io_u_5: 0.4917 - loss: 0.9146Batch 105, Loss Value: 0.8706\n",
      "Batch 105, Gradient Norm: 0.0817\n",
      "\u001b[1m105/232\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m1:24\u001b[0m 667ms/step - accuracy: 0.4922 - binary_io_u_5: 0.4917 - loss: 0.9146Batch 106, Loss Value: 0.8921\n",
      "Batch 106, Gradient Norm: 0.1387\n",
      "\u001b[1m106/232\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m1:23\u001b[0m 666ms/step - accuracy: 0.4923 - binary_io_u_5: 0.4918 - loss: 0.9146Batch 107, Loss Value: 0.8854\n",
      "Batch 107, Gradient Norm: 0.1991\n",
      "\u001b[1m107/232\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m1:22\u001b[0m 664ms/step - accuracy: 0.4923 - binary_io_u_5: 0.4918 - loss: 0.9145Batch 108, Loss Value: 0.8633\n",
      "Batch 108, Gradient Norm: 2.6405\n",
      "\u001b[1m108/232\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m1:22\u001b[0m 662ms/step - accuracy: 0.4923 - binary_io_u_5: 0.4918 - loss: 0.9145Batch 109, Loss Value: 0.8843\n",
      "Batch 109, Gradient Norm: 0.4551\n",
      "\u001b[1m109/232\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m1:21\u001b[0m 660ms/step - accuracy: 0.4924 - binary_io_u_5: 0.4919 - loss: 0.9145Batch 110, Loss Value: 0.8917\n",
      "Batch 110, Gradient Norm: 0.0162\n",
      "\u001b[1m110/232\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m1:20\u001b[0m 658ms/step - accuracy: 0.4924 - binary_io_u_5: 0.4919 - loss: 0.9145Batch 111, Loss Value: 0.8682\n",
      "Batch 111, Gradient Norm: 0.0745\n",
      "\u001b[1m111/232\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m1:19\u001b[0m 656ms/step - accuracy: 0.4924 - binary_io_u_5: 0.4919 - loss: 0.9144Batch 112, Loss Value: 0.8873\n",
      "Batch 112, Gradient Norm: 0.3702\n",
      "\u001b[1m112/232\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m1:18\u001b[0m 655ms/step - accuracy: 0.4925 - binary_io_u_5: 0.4920 - loss: 0.9144Batch 113, Loss Value: 0.8835\n",
      "Batch 113, Gradient Norm: 0.1557\n",
      "\u001b[1m113/232\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m1:17\u001b[0m 653ms/step - accuracy: 0.4925 - binary_io_u_5: 0.4920 - loss: 0.9144Batch 114, Loss Value: 0.8911\n",
      "Batch 114, Gradient Norm: 0.1134\n",
      "\u001b[1m114/232\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m1:16\u001b[0m 651ms/step - accuracy: 0.4925 - binary_io_u_5: 0.4920 - loss: 0.9144Batch 115, Loss Value: 0.8928\n",
      "Batch 115, Gradient Norm: 0.0069\n",
      "\u001b[1m115/232\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m1:15\u001b[0m 649ms/step - accuracy: 0.4926 - binary_io_u_5: 0.4921 - loss: 0.9143Batch 116, Loss Value: 0.8891\n",
      "Batch 116, Gradient Norm: 0.1742\n",
      "\u001b[1m116/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m1:15\u001b[0m 648ms/step - accuracy: 0.4926 - binary_io_u_5: 0.4921 - loss: 0.9143Batch 117, Loss Value: 0.8928\n",
      "Batch 117, Gradient Norm: 0.0068\n",
      "\u001b[1m117/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m1:14\u001b[0m 646ms/step - accuracy: 0.4926 - binary_io_u_5: 0.4921 - loss: 0.9143Batch 118, Loss Value: 0.8934\n",
      "Batch 118, Gradient Norm: 0.0338\n",
      "\u001b[1m118/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m1:13\u001b[0m 644ms/step - accuracy: 0.4927 - binary_io_u_5: 0.4922 - loss: 0.9143Batch 119, Loss Value: 0.8925\n",
      "Batch 119, Gradient Norm: 0.0260\n",
      "\u001b[1m119/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m1:12\u001b[0m 643ms/step - accuracy: 0.4927 - binary_io_u_5: 0.4922 - loss: 0.9143Batch 120, Loss Value: 0.8930\n",
      "Batch 120, Gradient Norm: 0.0128\n",
      "\u001b[1m120/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m1:11\u001b[0m 642ms/step - accuracy: 0.4927 - binary_io_u_5: 0.4923 - loss: 0.9142Batch 121, Loss Value: 0.8748\n",
      "Batch 121, Gradient Norm: 0.3081\n",
      "\u001b[1m121/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m1:11\u001b[0m 640ms/step - accuracy: 0.4928 - binary_io_u_5: 0.4923 - loss: 0.9142Batch 122, Loss Value: 0.8919\n",
      "Batch 122, Gradient Norm: 0.0307\n",
      "\u001b[1m122/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m1:10\u001b[0m 639ms/step - accuracy: 0.4928 - binary_io_u_5: 0.4923 - loss: 0.9142Batch 123, Loss Value: 0.8905\n",
      "Batch 123, Gradient Norm: 0.2614\n",
      "\u001b[1m123/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m1:09\u001b[0m 637ms/step - accuracy: 0.4928 - binary_io_u_5: 0.4924 - loss: 0.9142Batch 124, Loss Value: 0.8810\n",
      "Batch 124, Gradient Norm: 0.3350\n",
      "\u001b[1m124/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m1:08\u001b[0m 636ms/step - accuracy: 0.4929 - binary_io_u_5: 0.4924 - loss: 0.9141Batch 125, Loss Value: 0.8929\n",
      "Batch 125, Gradient Norm: 0.0003\n",
      "\u001b[1m125/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m1:07\u001b[0m 634ms/step - accuracy: 0.4929 - binary_io_u_5: 0.4924 - loss: 0.9141Batch 126, Loss Value: 0.8922\n",
      "Batch 126, Gradient Norm: 0.0396\n",
      "\u001b[1m126/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m1:07\u001b[0m 633ms/step - accuracy: 0.4929 - binary_io_u_5: 0.4925 - loss: 0.9141Batch 127, Loss Value: 0.8949\n",
      "Batch 127, Gradient Norm: 0.1372\n",
      "\u001b[1m127/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m1:06\u001b[0m 632ms/step - accuracy: 0.4930 - binary_io_u_5: 0.4925 - loss: 0.9141Batch 128, Loss Value: 0.8928\n",
      "Batch 128, Gradient Norm: 0.0065\n",
      "\u001b[1m128/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m1:05\u001b[0m 630ms/step - accuracy: 0.4930 - binary_io_u_5: 0.4926 - loss: 0.9140Batch 129, Loss Value: 0.8929\n",
      "Batch 129, Gradient Norm: 0.0001\n",
      "\u001b[1m129/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m1:04\u001b[0m 629ms/step - accuracy: 0.4930 - binary_io_u_5: 0.4926 - loss: 0.9140Batch 130, Loss Value: 0.8929\n",
      "Batch 130, Gradient Norm: 0.0050\n",
      "\u001b[1m130/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m1:04\u001b[0m 628ms/step - accuracy: 0.4931 - binary_io_u_5: 0.4926 - loss: 0.9140Batch 131, Loss Value: 0.8939\n",
      "Batch 131, Gradient Norm: 0.3529\n",
      "\u001b[1m131/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m1:03\u001b[0m 626ms/step - accuracy: 0.4931 - binary_io_u_5: 0.4927 - loss: 0.9140Batch 132, Loss Value: 0.8929\n",
      "Batch 132, Gradient Norm: 0.0009\n",
      "\u001b[1m132/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m1:02\u001b[0m 625ms/step - accuracy: 0.4931 - binary_io_u_5: 0.4927 - loss: 0.9140Batch 133, Loss Value: 0.8929\n",
      "Batch 133, Gradient Norm: 0.0004\n",
      "\u001b[1m133/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m1:01\u001b[0m 624ms/step - accuracy: 0.4931 - binary_io_u_5: 0.4927 - loss: 0.9140Batch 134, Loss Value: 0.8929\n",
      "Batch 134, Gradient Norm: 0.0005\n",
      "\u001b[1m134/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m1:01\u001b[0m 623ms/step - accuracy: 0.4932 - binary_io_u_5: 0.4927 - loss: 0.9139Batch 135, Loss Value: 0.8929\n",
      "Batch 135, Gradient Norm: 0.0011\n",
      "\u001b[1m135/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m1:00\u001b[0m 622ms/step - accuracy: 0.4932 - binary_io_u_5: 0.4928 - loss: 0.9139Batch 136, Loss Value: 0.8930\n",
      "Batch 136, Gradient Norm: 0.0739\n",
      "\u001b[1m136/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m59s\u001b[0m 620ms/step - accuracy: 0.4932 - binary_io_u_5: 0.4928 - loss: 0.9139 Batch 137, Loss Value: 0.8929\n",
      "Batch 137, Gradient Norm: 0.0035\n",
      "\u001b[1m137/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m58s\u001b[0m 619ms/step - accuracy: 0.4933 - binary_io_u_5: 0.4928 - loss: 0.9139Batch 138, Loss Value: 0.8731\n",
      "Batch 138, Gradient Norm: 0.2911\n",
      "\u001b[1m138/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m58s\u001b[0m 618ms/step - accuracy: 0.4933 - binary_io_u_5: 0.4929 - loss: 0.9139Batch 139, Loss Value: 0.8929\n",
      "Batch 139, Gradient Norm: 0.0007\n",
      "\u001b[1m139/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m57s\u001b[0m 617ms/step - accuracy: 0.4933 - binary_io_u_5: 0.4929 - loss: 0.9139Batch 140, Loss Value: 0.8766\n",
      "Batch 140, Gradient Norm: 0.5066\n",
      "\u001b[1m140/232\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m56s\u001b[0m 616ms/step - accuracy: 0.4933 - binary_io_u_5: 0.4929 - loss: 0.9139Batch 141, Loss Value: 0.8929\n",
      "Batch 141, Gradient Norm: 0.0001\n",
      "\u001b[1m141/232\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m55s\u001b[0m 615ms/step - accuracy: 0.4933 - binary_io_u_5: 0.4929 - loss: 0.9138Batch 142, Loss Value: 0.8929\n",
      "Batch 142, Gradient Norm: 0.0008\n",
      "\u001b[1m142/232\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m55s\u001b[0m 614ms/step - accuracy: 0.4934 - binary_io_u_5: 0.4929 - loss: 0.9138Batch 143, Loss Value: 0.8929\n",
      "Batch 143, Gradient Norm: 0.0000\n",
      "\u001b[1m143/232\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m54s\u001b[0m 613ms/step - accuracy: 0.4934 - binary_io_u_5: 0.4930 - loss: 0.9138Batch 144, Loss Value: 0.8929\n",
      "Batch 144, Gradient Norm: 0.0015\n",
      "\u001b[1m144/232\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m53s\u001b[0m 612ms/step - accuracy: 0.4934 - binary_io_u_5: 0.4930 - loss: 0.9138Batch 145, Loss Value: 0.8929\n",
      "Batch 145, Gradient Norm: 0.0007\n",
      "\u001b[1m145/232\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m53s\u001b[0m 611ms/step - accuracy: 0.4934 - binary_io_u_5: 0.4930 - loss: 0.9138Batch 146, Loss Value: 0.8922\n",
      "Batch 146, Gradient Norm: 0.0909\n",
      "\u001b[1m146/232\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m52s\u001b[0m 610ms/step - accuracy: 0.4935 - binary_io_u_5: 0.4930 - loss: 0.9138Batch 147, Loss Value: 0.8928\n",
      "Batch 147, Gradient Norm: 0.0069\n",
      "\u001b[1m147/232\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m51s\u001b[0m 609ms/step - accuracy: 0.4935 - binary_io_u_5: 0.4931 - loss: 0.9138Batch 148, Loss Value: 0.8926\n",
      "Batch 148, Gradient Norm: 0.0176\n",
      "\u001b[1m148/232\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m51s\u001b[0m 608ms/step - accuracy: 0.4935 - binary_io_u_5: 0.4931 - loss: 0.9137Batch 149, Loss Value: 0.8929\n",
      "Batch 149, Gradient Norm: 0.0023\n",
      "\u001b[1m149/232\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m50s\u001b[0m 607ms/step - accuracy: 0.4935 - binary_io_u_5: 0.4931 - loss: 0.9137Batch 150, Loss Value: 0.8929\n",
      "Batch 150, Gradient Norm: 0.0083\n",
      "\u001b[1m150/232\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m49s\u001b[0m 606ms/step - accuracy: 0.4935 - binary_io_u_5: 0.4931 - loss: 0.9137Batch 151, Loss Value: 0.8929\n",
      "Batch 151, Gradient Norm: 0.0030\n",
      "\u001b[1m151/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m48s\u001b[0m 605ms/step - accuracy: 0.4936 - binary_io_u_5: 0.4932 - loss: 0.9137Batch 152, Loss Value: 0.8929\n",
      "Batch 152, Gradient Norm: 0.0001\n",
      "\u001b[1m152/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m48s\u001b[0m 604ms/step - accuracy: 0.4936 - binary_io_u_5: 0.4932 - loss: 0.9137Batch 153, Loss Value: 0.8894\n",
      "Batch 153, Gradient Norm: 0.0958\n",
      "\u001b[1m153/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m47s\u001b[0m 603ms/step - accuracy: 0.4936 - binary_io_u_5: 0.4932 - loss: 0.9137Batch 154, Loss Value: 0.8929\n",
      "Batch 154, Gradient Norm: 0.0002\n",
      "\u001b[1m154/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m46s\u001b[0m 602ms/step - accuracy: 0.4936 - binary_io_u_5: 0.4932 - loss: 0.9137Batch 155, Loss Value: 0.8929\n",
      "Batch 155, Gradient Norm: 0.0416\n",
      "\u001b[1m155/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m46s\u001b[0m 601ms/step - accuracy: 0.4936 - binary_io_u_5: 0.4933 - loss: 0.9137Batch 156, Loss Value: 0.8935\n",
      "Batch 156, Gradient Norm: 0.0681\n",
      "\u001b[1m156/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m45s\u001b[0m 601ms/step - accuracy: 0.4937 - binary_io_u_5: 0.4933 - loss: 0.9137Batch 157, Loss Value: 0.8921\n",
      "Batch 157, Gradient Norm: 0.0542\n",
      "\u001b[1m157/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m44s\u001b[0m 600ms/step - accuracy: 0.4937 - binary_io_u_5: 0.4933 - loss: 0.9136Batch 158, Loss Value: 0.8930\n",
      "Batch 158, Gradient Norm: 0.0051\n",
      "\u001b[1m158/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m44s\u001b[0m 599ms/step - accuracy: 0.4937 - binary_io_u_5: 0.4933 - loss: 0.9136Batch 159, Loss Value: 0.8902\n",
      "Batch 159, Gradient Norm: 0.1736\n",
      "\u001b[1m159/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m43s\u001b[0m 598ms/step - accuracy: 0.4937 - binary_io_u_5: 0.4933 - loss: 0.9136Batch 160, Loss Value: 0.8929\n",
      "Batch 160, Gradient Norm: 0.0029\n",
      "\u001b[1m160/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m42s\u001b[0m 597ms/step - accuracy: 0.4937 - binary_io_u_5: 0.4933 - loss: 0.9136Batch 161, Loss Value: 0.8929\n",
      "Batch 161, Gradient Norm: 0.0026\n",
      "\u001b[1m161/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m42s\u001b[0m 596ms/step - accuracy: 0.4937 - binary_io_u_5: 0.4934 - loss: 0.9136Batch 162, Loss Value: 0.8924\n",
      "Batch 162, Gradient Norm: 0.0397\n",
      "\u001b[1m162/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m41s\u001b[0m 596ms/step - accuracy: 0.4938 - binary_io_u_5: 0.4934 - loss: 0.9136Batch 163, Loss Value: 0.8890\n",
      "Batch 163, Gradient Norm: 0.1130\n",
      "\u001b[1m163/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m41s\u001b[0m 595ms/step - accuracy: 0.4938 - binary_io_u_5: 0.4934 - loss: 0.9136Batch 164, Loss Value: 0.8874\n",
      "Batch 164, Gradient Norm: 0.1477\n",
      "\u001b[1m164/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m40s\u001b[0m 594ms/step - accuracy: 0.4938 - binary_io_u_5: 0.4934 - loss: 0.9136Batch 165, Loss Value: 0.8847\n",
      "Batch 165, Gradient Norm: 0.2794\n",
      "\u001b[1m165/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m39s\u001b[0m 593ms/step - accuracy: 0.4938 - binary_io_u_5: 0.4934 - loss: 0.9136Batch 166, Loss Value: 0.8933\n",
      "Batch 166, Gradient Norm: 0.0270\n",
      "\u001b[1m166/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m39s\u001b[0m 592ms/step - accuracy: 0.4938 - binary_io_u_5: 0.4935 - loss: 0.9136Batch 167, Loss Value: 0.8805\n",
      "Batch 167, Gradient Norm: 0.2767\n",
      "\u001b[1m167/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m38s\u001b[0m 592ms/step - accuracy: 0.4938 - binary_io_u_5: 0.4935 - loss: 0.9136Batch 168, Loss Value: 0.8894\n",
      "Batch 168, Gradient Norm: 0.1446\n",
      "\u001b[1m168/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m37s\u001b[0m 591ms/step - accuracy: 0.4938 - binary_io_u_5: 0.4935 - loss: 0.9135Batch 169, Loss Value: 0.8971\n",
      "Batch 169, Gradient Norm: 0.1150\n",
      "\u001b[1m169/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m37s\u001b[0m 590ms/step - accuracy: 0.4939 - binary_io_u_5: 0.4935 - loss: 0.9135Batch 170, Loss Value: 0.8749\n",
      "Batch 170, Gradient Norm: 0.1538\n",
      "\u001b[1m170/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m36s\u001b[0m 590ms/step - accuracy: 0.4939 - binary_io_u_5: 0.4935 - loss: 0.9135Batch 171, Loss Value: 0.9071\n",
      "Batch 171, Gradient Norm: 0.2246\n",
      "\u001b[1m171/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m35s\u001b[0m 589ms/step - accuracy: 0.4939 - binary_io_u_5: 0.4935 - loss: 0.9135Batch 172, Loss Value: 0.8893\n",
      "Batch 172, Gradient Norm: 0.1088\n",
      "\u001b[1m172/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m35s\u001b[0m 588ms/step - accuracy: 0.4939 - binary_io_u_5: 0.4935 - loss: 0.9135Batch 173, Loss Value: 0.8920\n",
      "Batch 173, Gradient Norm: 0.1300\n",
      "\u001b[1m173/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m34s\u001b[0m 588ms/step - accuracy: 0.4939 - binary_io_u_5: 0.4935 - loss: 0.9135Batch 174, Loss Value: 0.8982\n",
      "Batch 174, Gradient Norm: 1.2706\n",
      "\u001b[1m174/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m34s\u001b[0m 587ms/step - accuracy: 0.4939 - binary_io_u_5: 0.4935 - loss: 0.9135Batch 175, Loss Value: 0.8935\n",
      "Batch 175, Gradient Norm: 0.0545\n",
      "\u001b[1m175/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m33s\u001b[0m 586ms/step - accuracy: 0.4939 - binary_io_u_5: 0.4935 - loss: 0.9135Batch 176, Loss Value: 0.8870\n",
      "Batch 176, Gradient Norm: 0.1635\n",
      "\u001b[1m176/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m32s\u001b[0m 585ms/step - accuracy: 0.4939 - binary_io_u_5: 0.4935 - loss: 0.9135Batch 177, Loss Value: 0.8983\n",
      "Batch 177, Gradient Norm: 0.0993\n",
      "\u001b[1m177/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m32s\u001b[0m 585ms/step - accuracy: 0.4939 - binary_io_u_5: 0.4935 - loss: 0.9135Batch 178, Loss Value: 0.9025\n",
      "Batch 178, Gradient Norm: 0.1943\n",
      "\u001b[1m178/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m31s\u001b[0m 584ms/step - accuracy: 0.4939 - binary_io_u_5: 0.4935 - loss: 0.9135Batch 179, Loss Value: 0.8732\n",
      "Batch 179, Gradient Norm: 0.2676\n",
      "\u001b[1m179/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m30s\u001b[0m 584ms/step - accuracy: 0.4939 - binary_io_u_5: 0.4935 - loss: 0.9135Batch 180, Loss Value: 0.9089\n",
      "Batch 180, Gradient Norm: 0.2709\n",
      "\u001b[1m180/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m30s\u001b[0m 583ms/step - accuracy: 0.4939 - binary_io_u_5: 0.4935 - loss: 0.9135Batch 181, Loss Value: 0.9415\n",
      "Batch 181, Gradient Norm: 0.0114\n",
      "\u001b[1m181/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m29s\u001b[0m 582ms/step - accuracy: 0.4939 - binary_io_u_5: 0.4935 - loss: 0.9135Batch 182, Loss Value: 0.8691\n",
      "Batch 182, Gradient Norm: 0.2258\n",
      "\u001b[1m182/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m29s\u001b[0m 582ms/step - accuracy: 0.4939 - binary_io_u_5: 0.4935 - loss: 0.9135Batch 183, Loss Value: 0.8831\n",
      "Batch 183, Gradient Norm: 0.0190\n",
      "\u001b[1m183/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m28s\u001b[0m 581ms/step - accuracy: 0.4939 - binary_io_u_5: 0.4935 - loss: 0.9135Batch 184, Loss Value: 0.8927\n",
      "Batch 184, Gradient Norm: 0.0142\n",
      "\u001b[1m184/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m27s\u001b[0m 580ms/step - accuracy: 0.4939 - binary_io_u_5: 0.4935 - loss: 0.9135Batch 185, Loss Value: 0.8781\n",
      "Batch 185, Gradient Norm: 0.1603\n",
      "\u001b[1m185/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m27s\u001b[0m 580ms/step - accuracy: 0.4939 - binary_io_u_5: 0.4935 - loss: 0.9135Batch 186, Loss Value: 0.8741\n",
      "Batch 186, Gradient Norm: 0.2504\n",
      "\u001b[1m186/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m26s\u001b[0m 579ms/step - accuracy: 0.4939 - binary_io_u_5: 0.4935 - loss: 0.9135Batch 187, Loss Value: 0.8935\n",
      "Batch 187, Gradient Norm: 0.3270\n",
      "\u001b[1m187/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m26s\u001b[0m 579ms/step - accuracy: 0.4939 - binary_io_u_5: 0.4935 - loss: 0.9135Batch 188, Loss Value: 0.8847\n",
      "Batch 188, Gradient Norm: 0.2649\n",
      "\u001b[1m188/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m25s\u001b[0m 578ms/step - accuracy: 0.4939 - binary_io_u_5: 0.4935 - loss: 0.9135Batch 189, Loss Value: 0.8909\n",
      "Batch 189, Gradient Norm: 0.1081\n",
      "\u001b[1m189/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m24s\u001b[0m 577ms/step - accuracy: 0.4939 - binary_io_u_5: 0.4935 - loss: 0.9135Batch 190, Loss Value: 0.8781\n",
      "Batch 190, Gradient Norm: 0.1474\n",
      "\u001b[1m190/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m24s\u001b[0m 577ms/step - accuracy: 0.4939 - binary_io_u_5: 0.4936 - loss: 0.9135Batch 191, Loss Value: 0.8855\n",
      "Batch 191, Gradient Norm: 0.2648\n",
      "\u001b[1m191/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m23s\u001b[0m 576ms/step - accuracy: 0.4939 - binary_io_u_5: 0.4936 - loss: 0.9135Batch 192, Loss Value: 0.8860\n",
      "Batch 192, Gradient Norm: 0.2830\n",
      "\u001b[1m192/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m23s\u001b[0m 576ms/step - accuracy: 0.4939 - binary_io_u_5: 0.4936 - loss: 0.9135Batch 193, Loss Value: 0.8928\n",
      "Batch 193, Gradient Norm: 0.0048\n",
      "\u001b[1m193/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m22s\u001b[0m 575ms/step - accuracy: 0.4939 - binary_io_u_5: 0.4936 - loss: 0.9135Batch 194, Loss Value: 0.8844\n",
      "Batch 194, Gradient Norm: 0.1604\n",
      "\u001b[1m194/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m21s\u001b[0m 575ms/step - accuracy: 0.4939 - binary_io_u_5: 0.4936 - loss: 0.9135Batch 195, Loss Value: 0.8920\n",
      "Batch 195, Gradient Norm: 0.0827\n",
      "\u001b[1m195/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m21s\u001b[0m 574ms/step - accuracy: 0.4939 - binary_io_u_5: 0.4936 - loss: 0.9135Batch 196, Loss Value: 0.8931\n",
      "Batch 196, Gradient Norm: 0.0135\n",
      "\u001b[1m196/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m20s\u001b[0m 573ms/step - accuracy: 0.4939 - binary_io_u_5: 0.4936 - loss: 0.9135Batch 197, Loss Value: 0.8822\n",
      "Batch 197, Gradient Norm: 0.3353\n",
      "\u001b[1m197/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m20s\u001b[0m 573ms/step - accuracy: 0.4939 - binary_io_u_5: 0.4936 - loss: 0.9135Batch 198, Loss Value: 0.8929\n",
      "Batch 198, Gradient Norm: 0.0025\n",
      "\u001b[1m198/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m19s\u001b[0m 572ms/step - accuracy: 0.4939 - binary_io_u_5: 0.4936 - loss: 0.9135Batch 199, Loss Value: 0.8936\n",
      "Batch 199, Gradient Norm: 0.6142\n",
      "\u001b[1m199/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m18s\u001b[0m 572ms/step - accuracy: 0.4939 - binary_io_u_5: 0.4936 - loss: 0.9135Batch 200, Loss Value: 0.8929\n",
      "Batch 200, Gradient Norm: 0.0016\n",
      "\u001b[1m200/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m18s\u001b[0m 571ms/step - accuracy: 0.4939 - binary_io_u_5: 0.4936 - loss: 0.9135Batch 201, Loss Value: 0.8922\n",
      "Batch 201, Gradient Norm: 0.0753\n",
      "\u001b[1m201/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m17s\u001b[0m 571ms/step - accuracy: 0.4939 - binary_io_u_5: 0.4936 - loss: 0.9135Batch 202, Loss Value: 0.8946\n",
      "Batch 202, Gradient Norm: 0.1142\n",
      "\u001b[1m202/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m17s\u001b[0m 570ms/step - accuracy: 0.4939 - binary_io_u_5: 0.4936 - loss: 0.9135Batch 203, Loss Value: 0.8912\n",
      "Batch 203, Gradient Norm: 0.8896\n",
      "\u001b[1m203/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m16s\u001b[0m 570ms/step - accuracy: 0.4939 - binary_io_u_5: 0.4936 - loss: 0.9135Batch 204, Loss Value: 0.8970\n",
      "Batch 204, Gradient Norm: 0.2393\n",
      "\u001b[1m204/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m15s\u001b[0m 569ms/step - accuracy: 0.4939 - binary_io_u_5: 0.4936 - loss: 0.9135Batch 205, Loss Value: 0.8916\n",
      "Batch 205, Gradient Norm: 0.2519\n",
      "\u001b[1m205/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m15s\u001b[0m 569ms/step - accuracy: 0.4939 - binary_io_u_5: 0.4935 - loss: 0.9135Batch 206, Loss Value: 0.8846\n",
      "Batch 206, Gradient Norm: 0.3569\n",
      "\u001b[1m206/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m14s\u001b[0m 568ms/step - accuracy: 0.4939 - binary_io_u_5: 0.4935 - loss: 0.9135Batch 207, Loss Value: 0.8930\n",
      "Batch 207, Gradient Norm: 0.0067\n",
      "\u001b[1m207/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m14s\u001b[0m 568ms/step - accuracy: 0.4939 - binary_io_u_5: 0.4935 - loss: 0.9135Batch 208, Loss Value: 0.8800\n",
      "Batch 208, Gradient Norm: 0.3270\n",
      "\u001b[1m208/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m13s\u001b[0m 567ms/step - accuracy: 0.4939 - binary_io_u_5: 0.4935 - loss: 0.9135Batch 209, Loss Value: 0.8641\n",
      "Batch 209, Gradient Norm: 0.3732\n",
      "\u001b[1m209/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m13s\u001b[0m 567ms/step - accuracy: 0.4939 - binary_io_u_5: 0.4935 - loss: 0.9135Batch 210, Loss Value: 0.8878\n",
      "Batch 210, Gradient Norm: 0.6668\n",
      "\u001b[1m210/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m12s\u001b[0m 567ms/step - accuracy: 0.4939 - binary_io_u_5: 0.4935 - loss: 0.9135Batch 211, Loss Value: 0.8636\n",
      "Batch 211, Gradient Norm: 0.2188\n",
      "\u001b[1m211/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m11s\u001b[0m 566ms/step - accuracy: 0.4939 - binary_io_u_5: 0.4935 - loss: 0.9135Batch 212, Loss Value: 0.8958\n",
      "Batch 212, Gradient Norm: 0.4481\n",
      "\u001b[1m212/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m11s\u001b[0m 566ms/step - accuracy: 0.4938 - binary_io_u_5: 0.4935 - loss: 0.9135Batch 213, Loss Value: 0.8813\n",
      "Batch 213, Gradient Norm: 0.4438\n",
      "\u001b[1m213/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m10s\u001b[0m 565ms/step - accuracy: 0.4938 - binary_io_u_5: 0.4935 - loss: 0.9135Batch 214, Loss Value: 0.8837\n",
      "Batch 214, Gradient Norm: 0.2441\n",
      "\u001b[1m214/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m10s\u001b[0m 565ms/step - accuracy: 0.4938 - binary_io_u_5: 0.4935 - loss: 0.9135Batch 215, Loss Value: 0.8901\n",
      "Batch 215, Gradient Norm: 0.3454\n",
      "\u001b[1m215/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m9s\u001b[0m 564ms/step - accuracy: 0.4938 - binary_io_u_5: 0.4935 - loss: 0.9135 Batch 216, Loss Value: 0.8999\n",
      "Batch 216, Gradient Norm: 0.3241\n",
      "\u001b[1m216/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m9s\u001b[0m 564ms/step - accuracy: 0.4938 - binary_io_u_5: 0.4935 - loss: 0.9135Batch 217, Loss Value: 0.8868\n",
      "Batch 217, Gradient Norm: 0.3949\n",
      "\u001b[1m217/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m8s\u001b[0m 563ms/step - accuracy: 0.4938 - binary_io_u_5: 0.4935 - loss: 0.9135Batch 218, Loss Value: 0.8980\n",
      "Batch 218, Gradient Norm: 0.0649\n",
      "\u001b[1m218/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m7s\u001b[0m 563ms/step - accuracy: 0.4938 - binary_io_u_5: 0.4935 - loss: 0.9135Batch 219, Loss Value: 0.8749\n",
      "Batch 219, Gradient Norm: 0.2460\n",
      "\u001b[1m219/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m7s\u001b[0m 562ms/step - accuracy: 0.4938 - binary_io_u_5: 0.4935 - loss: 0.9135Batch 220, Loss Value: 0.8863\n",
      "Batch 220, Gradient Norm: 0.0727\n",
      "\u001b[1m220/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m6s\u001b[0m 562ms/step - accuracy: 0.4938 - binary_io_u_5: 0.4935 - loss: 0.9135Batch 221, Loss Value: 0.8832\n",
      "Batch 221, Gradient Norm: 0.1465\n",
      "\u001b[1m221/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m6s\u001b[0m 562ms/step - accuracy: 0.4938 - binary_io_u_5: 0.4935 - loss: 0.9135Batch 222, Loss Value: 0.8981\n",
      "Batch 222, Gradient Norm: 0.1720\n",
      "\u001b[1m222/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m5s\u001b[0m 561ms/step - accuracy: 0.4939 - binary_io_u_5: 0.4935 - loss: 0.9135Batch 223, Loss Value: 0.8956\n",
      "Batch 223, Gradient Norm: 9.1991\n",
      "\u001b[1m223/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m5s\u001b[0m 561ms/step - accuracy: 0.4939 - binary_io_u_5: 0.4936 - loss: 0.9135Batch 224, Loss Value: 0.8935\n",
      "Batch 224, Gradient Norm: 0.0815\n",
      "\u001b[1m224/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m4s\u001b[0m 560ms/step - accuracy: 0.4939 - binary_io_u_5: 0.4936 - loss: 0.9135Batch 225, Loss Value: 0.9033\n",
      "Batch 225, Gradient Norm: 0.2520\n",
      "\u001b[1m225/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m3s\u001b[0m 560ms/step - accuracy: 0.4939 - binary_io_u_5: 0.4936 - loss: 0.9135Batch 226, Loss Value: 0.8902\n",
      "Batch 226, Gradient Norm: 0.1462\n",
      "\u001b[1m226/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m3s\u001b[0m 560ms/step - accuracy: 0.4939 - binary_io_u_5: 0.4936 - loss: 0.9135Batch 227, Loss Value: 0.8931\n",
      "Batch 227, Gradient Norm: 0.0093\n",
      "\u001b[1m227/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m2s\u001b[0m 559ms/step - accuracy: 0.4939 - binary_io_u_5: 0.4936 - loss: 0.9135Batch 228, Loss Value: 0.8930\n",
      "Batch 228, Gradient Norm: 0.0082\n",
      "\u001b[1m228/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m2s\u001b[0m 559ms/step - accuracy: 0.4939 - binary_io_u_5: 0.4936 - loss: 0.9135Batch 229, Loss Value: 0.8922\n",
      "Batch 229, Gradient Norm: 0.0581\n",
      "\u001b[1m229/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m1s\u001b[0m 558ms/step - accuracy: 0.4939 - binary_io_u_5: 0.4936 - loss: 0.9135Batch 230, Loss Value: 0.8930\n",
      "Batch 230, Gradient Norm: 0.0145\n",
      "\u001b[1m230/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m1s\u001b[0m 558ms/step - accuracy: 0.4939 - binary_io_u_5: 0.4936 - loss: 0.9135Batch 231, Loss Value: 0.8929\n",
      "Batch 231, Gradient Norm: 0.0019\n",
      "\u001b[1m231/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 557ms/step - accuracy: 0.4939 - binary_io_u_5: 0.4936 - loss: 0.9135Batch 232, Loss Value: 0.8919\n",
      "Batch 232, Gradient Norm: 0.0687\n",
      "\u001b[1m232/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m131s\u001b[0m 563ms/step - accuracy: 0.4939 - binary_io_u_5: 0.4936 - loss: 0.9135 - val_accuracy: 0.4949 - val_binary_io_u_5: 0.4949 - val_loss: 0.9127 - learning_rate: 0.0010\n",
      "Epoch 14/200\n",
      "Batch 1, Loss Value: 0.9010\n",
      "Batch 1, Gradient Norm: 0.0301\n",
      "\u001b[1m  1/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:52\u001b[0m 486ms/step - accuracy: 0.4200 - binary_io_u_5: 0.4200 - loss: 0.9469Batch 2, Loss Value: 0.9046\n",
      "Batch 2, Gradient Norm: 0.0931\n",
      "\u001b[1m  2/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:47\u001b[0m 467ms/step - accuracy: 0.4450 - binary_io_u_5: 0.4425 - loss: 0.9356Batch 3, Loss Value: 0.8979\n",
      "Batch 3, Gradient Norm: 0.1324\n",
      "\u001b[1m  3/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:46\u001b[0m 465ms/step - accuracy: 0.4533 - binary_io_u_5: 0.4506 - loss: 0.9330Batch 4, Loss Value: 0.9056\n",
      "Batch 4, Gradient Norm: 0.0625\n",
      "\u001b[1m  4/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:46\u001b[0m 468ms/step - accuracy: 0.4637 - binary_io_u_5: 0.4610 - loss: 0.9278Batch 5, Loss Value: 0.9122\n",
      "Batch 5, Gradient Norm: 0.3880\n",
      "\u001b[1m  5/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:46\u001b[0m 469ms/step - accuracy: 0.4682 - binary_io_u_5: 0.4656 - loss: 0.9259Batch 6, Loss Value: 0.9138\n",
      "Batch 6, Gradient Norm: 0.5460\n",
      "\u001b[1m  6/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:45\u001b[0m 469ms/step - accuracy: 0.4718 - binary_io_u_5: 0.4691 - loss: 0.9243Batch 7, Loss Value: 0.9108\n",
      "Batch 7, Gradient Norm: 0.2628\n",
      "\u001b[1m  7/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:45\u001b[0m 469ms/step - accuracy: 0.4742 - binary_io_u_5: 0.4715 - loss: 0.9234Batch 8, Loss Value: 0.9277\n",
      "Batch 8, Gradient Norm: 0.2604\n",
      "\u001b[1m  8/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:44\u001b[0m 469ms/step - accuracy: 0.4776 - binary_io_u_5: 0.4748 - loss: 0.9216Batch 9, Loss Value: 0.9090\n",
      "Batch 9, Gradient Norm: 7.6315\n",
      "\u001b[1m  9/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:44\u001b[0m 470ms/step - accuracy: 0.4796 - binary_io_u_5: 0.4766 - loss: 0.9207Batch 10, Loss Value: 0.9213\n",
      "Batch 10, Gradient Norm: 0.0256\n",
      "\u001b[1m 10/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:44\u001b[0m 470ms/step - accuracy: 0.4808 - binary_io_u_5: 0.4777 - loss: 0.9203Batch 11, Loss Value: 0.9058\n",
      "Batch 11, Gradient Norm: 0.7696\n",
      "\u001b[1m 11/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:43\u001b[0m 470ms/step - accuracy: 0.4818 - binary_io_u_5: 0.4787 - loss: 0.9199Batch 12, Loss Value: 0.8964\n",
      "Batch 12, Gradient Norm: 0.0935\n",
      "\u001b[1m 12/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:43\u001b[0m 469ms/step - accuracy: 0.4827 - binary_io_u_5: 0.4798 - loss: 0.9196Batch 13, Loss Value: 0.9121\n",
      "Batch 13, Gradient Norm: 0.1245\n",
      "\u001b[1m 13/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:42\u001b[0m 469ms/step - accuracy: 0.4838 - binary_io_u_5: 0.4810 - loss: 0.9191Batch 14, Loss Value: 0.9065\n",
      "Batch 14, Gradient Norm: 0.0477\n",
      "\u001b[1m 14/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:42\u001b[0m 468ms/step - accuracy: 0.4846 - binary_io_u_5: 0.4820 - loss: 0.9188Batch 15, Loss Value: 0.9185\n",
      "Batch 15, Gradient Norm: 0.7083\n",
      "\u001b[1m 15/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:41\u001b[0m 468ms/step - accuracy: 0.4855 - binary_io_u_5: 0.4830 - loss: 0.9183Batch 16, Loss Value: 0.9083\n",
      "Batch 16, Gradient Norm: 0.2031\n",
      "\u001b[1m 16/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:41\u001b[0m 468ms/step - accuracy: 0.4864 - binary_io_u_5: 0.4840 - loss: 0.9179Batch 17, Loss Value: 0.9132\n",
      "Batch 17, Gradient Norm: 0.0997\n",
      "\u001b[1m 17/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:40\u001b[0m 468ms/step - accuracy: 0.4875 - binary_io_u_5: 0.4852 - loss: 0.9173Batch 18, Loss Value: 0.9038\n",
      "Batch 18, Gradient Norm: 0.2132\n",
      "\u001b[1m 18/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:40\u001b[0m 467ms/step - accuracy: 0.4885 - binary_io_u_5: 0.4864 - loss: 0.9168Batch 19, Loss Value: 0.8993\n",
      "Batch 19, Gradient Norm: 0.7250\n",
      "\u001b[1m 19/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:39\u001b[0m 467ms/step - accuracy: 0.4895 - binary_io_u_5: 0.4874 - loss: 0.9162Batch 20, Loss Value: 0.8918\n",
      "Batch 20, Gradient Norm: 0.5806\n",
      "\u001b[1m 20/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:39\u001b[0m 467ms/step - accuracy: 0.4906 - binary_io_u_5: 0.4886 - loss: 0.9158Batch 21, Loss Value: 0.9032\n",
      "Batch 21, Gradient Norm: 0.1646\n",
      "\u001b[1m 21/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:38\u001b[0m 468ms/step - accuracy: 0.4917 - binary_io_u_5: 0.4897 - loss: 0.9153Batch 22, Loss Value: 0.9066\n",
      "Batch 22, Gradient Norm: 0.0005\n",
      "\u001b[1m 22/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:38\u001b[0m 468ms/step - accuracy: 0.4926 - binary_io_u_5: 0.4907 - loss: 0.9149Batch 23, Loss Value: 0.9070\n",
      "Batch 23, Gradient Norm: 0.0277\n",
      "\u001b[1m 23/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:37\u001b[0m 468ms/step - accuracy: 0.4934 - binary_io_u_5: 0.4915 - loss: 0.9147Batch 24, Loss Value: 0.9071\n",
      "Batch 24, Gradient Norm: 0.0769\n",
      "\u001b[1m 24/232\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:37\u001b[0m 469ms/step - accuracy: 0.4941 - binary_io_u_5: 0.4923 - loss: 0.9144Batch 25, Loss Value: 0.9091\n",
      "Batch 25, Gradient Norm: 0.0921\n",
      "\u001b[1m 25/232\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:37\u001b[0m 469ms/step - accuracy: 0.4947 - binary_io_u_5: 0.4929 - loss: 0.9142Batch 26, Loss Value: 0.9066\n",
      "Batch 26, Gradient Norm: 0.0072\n",
      "\u001b[1m 26/232\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:36\u001b[0m 470ms/step - accuracy: 0.4952 - binary_io_u_5: 0.4934 - loss: 0.9141Batch 27, Loss Value: 0.9066\n",
      "Batch 27, Gradient Norm: 0.0001\n",
      "\u001b[1m 27/232\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:36\u001b[0m 470ms/step - accuracy: 0.4958 - binary_io_u_5: 0.4941 - loss: 0.9138Batch 28, Loss Value: 0.9099\n",
      "Batch 28, Gradient Norm: 0.1647\n",
      "\u001b[1m 28/232\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:35\u001b[0m 470ms/step - accuracy: 0.4965 - binary_io_u_5: 0.4948 - loss: 0.9135Batch 29, Loss Value: 0.9067\n",
      "Batch 29, Gradient Norm: 0.0031\n",
      "\u001b[1m 29/232\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:35\u001b[0m 470ms/step - accuracy: 0.4971 - binary_io_u_5: 0.4954 - loss: 0.9133Batch 30, Loss Value: 0.9104\n",
      "Batch 30, Gradient Norm: 0.1128\n",
      "\u001b[1m 30/232\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:34\u001b[0m 470ms/step - accuracy: 0.4977 - binary_io_u_5: 0.4961 - loss: 0.9130Batch 31, Loss Value: 0.9044\n",
      "Batch 31, Gradient Norm: 0.1198\n",
      "\u001b[1m 31/232\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:34\u001b[0m 470ms/step - accuracy: 0.4983 - binary_io_u_5: 0.4967 - loss: 0.9127Batch 32, Loss Value: 0.9230\n",
      "Batch 32, Gradient Norm: 0.2378\n",
      "\u001b[1m 32/232\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:34\u001b[0m 470ms/step - accuracy: 0.4989 - binary_io_u_5: 0.4973 - loss: 0.9124Batch 33, Loss Value: 0.9014\n",
      "Batch 33, Gradient Norm: 0.3453\n",
      "\u001b[1m 33/232\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:33\u001b[0m 471ms/step - accuracy: 0.4994 - binary_io_u_5: 0.4979 - loss: 0.9121Batch 34, Loss Value: 0.9066\n",
      "Batch 34, Gradient Norm: 0.0005\n",
      "\u001b[1m 34/232\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:33\u001b[0m 471ms/step - accuracy: 0.4999 - binary_io_u_5: 0.4984 - loss: 0.9119Batch 35, Loss Value: 0.9066\n",
      "Batch 35, Gradient Norm: 0.0017\n",
      "\u001b[1m 35/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:32\u001b[0m 471ms/step - accuracy: 0.5003 - binary_io_u_5: 0.4988 - loss: 0.9117Batch 36, Loss Value: 0.9066\n",
      "Batch 36, Gradient Norm: 0.0071\n",
      "\u001b[1m 36/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:32\u001b[0m 471ms/step - accuracy: 0.5007 - binary_io_u_5: 0.4992 - loss: 0.9115Batch 37, Loss Value: 0.9145\n",
      "Batch 37, Gradient Norm: 0.2795\n",
      "\u001b[1m 37/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:32\u001b[0m 472ms/step - accuracy: 0.5010 - binary_io_u_5: 0.4995 - loss: 0.9114Batch 38, Loss Value: 0.9127\n",
      "Batch 38, Gradient Norm: 0.5349\n",
      "\u001b[1m 38/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:31\u001b[0m 472ms/step - accuracy: 0.5012 - binary_io_u_5: 0.4998 - loss: 0.9113Batch 39, Loss Value: 0.9065\n",
      "Batch 39, Gradient Norm: 0.0071\n",
      "\u001b[1m 39/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:31\u001b[0m 472ms/step - accuracy: 0.5014 - binary_io_u_5: 0.5000 - loss: 0.9112Batch 40, Loss Value: 0.9077\n",
      "Batch 40, Gradient Norm: 0.1454\n",
      "\u001b[1m 40/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:30\u001b[0m 471ms/step - accuracy: 0.5016 - binary_io_u_5: 0.5002 - loss: 0.9111Batch 41, Loss Value: 0.9019\n",
      "Batch 41, Gradient Norm: 0.1892\n",
      "\u001b[1m 41/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:30\u001b[0m 471ms/step - accuracy: 0.5017 - binary_io_u_5: 0.5004 - loss: 0.9110Batch 42, Loss Value: 0.8989\n",
      "Batch 42, Gradient Norm: 0.2240\n",
      "\u001b[1m 42/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:29\u001b[0m 471ms/step - accuracy: 0.5018 - binary_io_u_5: 0.5005 - loss: 0.9110Batch 43, Loss Value: 0.9066\n",
      "Batch 43, Gradient Norm: 0.0053\n",
      "\u001b[1m 43/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:29\u001b[0m 471ms/step - accuracy: 0.5020 - binary_io_u_5: 0.5007 - loss: 0.9109Batch 44, Loss Value: 0.9065\n",
      "Batch 44, Gradient Norm: 0.0102\n",
      "\u001b[1m 44/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:28\u001b[0m 471ms/step - accuracy: 0.5021 - binary_io_u_5: 0.5009 - loss: 0.9108Batch 45, Loss Value: 0.9061\n",
      "Batch 45, Gradient Norm: 0.0421\n",
      "\u001b[1m 45/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:28\u001b[0m 471ms/step - accuracy: 0.5023 - binary_io_u_5: 0.5010 - loss: 0.9108Batch 46, Loss Value: 0.9066\n",
      "Batch 46, Gradient Norm: 0.0000\n",
      "\u001b[1m 46/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:27\u001b[0m 471ms/step - accuracy: 0.5024 - binary_io_u_5: 0.5011 - loss: 0.9107Batch 47, Loss Value: 0.9066\n",
      "Batch 47, Gradient Norm: 0.0014\n",
      "\u001b[1m 47/232\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:27\u001b[0m 471ms/step - accuracy: 0.5025 - binary_io_u_5: 0.5012 - loss: 0.9107Batch 48, Loss Value: 0.9255\n",
      "Batch 48, Gradient Norm: 0.3424\n",
      "\u001b[1m 48/232\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:26\u001b[0m 471ms/step - accuracy: 0.5025 - binary_io_u_5: 0.5013 - loss: 0.9107Batch 49, Loss Value: 0.8922\n",
      "Batch 49, Gradient Norm: 0.1127\n",
      "\u001b[1m 49/232\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:26\u001b[0m 470ms/step - accuracy: 0.5026 - binary_io_u_5: 0.5014 - loss: 0.9106Batch 50, Loss Value: 0.9063\n",
      "Batch 50, Gradient Norm: 0.0546\n",
      "\u001b[1m 50/232\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:25\u001b[0m 470ms/step - accuracy: 0.5027 - binary_io_u_5: 0.5016 - loss: 0.9105Batch 51, Loss Value: 0.9101\n",
      "Batch 51, Gradient Norm: 0.1615\n",
      "\u001b[1m 51/232\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:25\u001b[0m 471ms/step - accuracy: 0.5028 - binary_io_u_5: 0.5016 - loss: 0.9105Batch 52, Loss Value: 0.9165\n",
      "Batch 52, Gradient Norm: 0.2818\n",
      "\u001b[1m 52/232\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:24\u001b[0m 470ms/step - accuracy: 0.5029 - binary_io_u_5: 0.5017 - loss: 0.9105Batch 53, Loss Value: 0.9099\n",
      "Batch 53, Gradient Norm: 0.2899\n",
      "\u001b[1m 53/232\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:24\u001b[0m 470ms/step - accuracy: 0.5029 - binary_io_u_5: 0.5018 - loss: 0.9104Batch 54, Loss Value: 0.9055\n",
      "Batch 54, Gradient Norm: 0.0470\n",
      "\u001b[1m 54/232\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:23\u001b[0m 470ms/step - accuracy: 0.5029 - binary_io_u_5: 0.5018 - loss: 0.9104Batch 55, Loss Value: 0.9066\n",
      "Batch 55, Gradient Norm: 0.0064\n",
      "\u001b[1m 55/232\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:23\u001b[0m 470ms/step - accuracy: 0.5030 - binary_io_u_5: 0.5019 - loss: 0.9104Batch 56, Loss Value: 0.9053\n",
      "Batch 56, Gradient Norm: 0.5796\n",
      "\u001b[1m 56/232\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:22\u001b[0m 470ms/step - accuracy: 0.5030 - binary_io_u_5: 0.5019 - loss: 0.9104Batch 57, Loss Value: 0.9065\n",
      "Batch 57, Gradient Norm: 0.0176\n",
      "\u001b[1m 57/232\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:22\u001b[0m 470ms/step - accuracy: 0.5030 - binary_io_u_5: 0.5019 - loss: 0.9104Batch 58, Loss Value: 0.9073\n",
      "Batch 58, Gradient Norm: 0.0615\n",
      "\u001b[1m 58/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:21\u001b[0m 471ms/step - accuracy: 0.5030 - binary_io_u_5: 0.5019 - loss: 0.9104Batch 59, Loss Value: 0.8957\n",
      "Batch 59, Gradient Norm: 0.1879\n",
      "\u001b[1m 59/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:21\u001b[0m 471ms/step - accuracy: 0.5030 - binary_io_u_5: 0.5019 - loss: 0.9104Batch 60, Loss Value: 0.9027\n",
      "Batch 60, Gradient Norm: 0.0509\n",
      "\u001b[1m 60/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:21\u001b[0m 471ms/step - accuracy: 0.5030 - binary_io_u_5: 0.5020 - loss: 0.9104Batch 61, Loss Value: 0.9111\n",
      "Batch 61, Gradient Norm: 0.1382\n",
      "\u001b[1m 61/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:20\u001b[0m 472ms/step - accuracy: 0.5030 - binary_io_u_5: 0.5020 - loss: 0.9104Batch 62, Loss Value: 0.9172\n",
      "Batch 62, Gradient Norm: 0.3150\n",
      "\u001b[1m 62/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:20\u001b[0m 472ms/step - accuracy: 0.5029 - binary_io_u_5: 0.5019 - loss: 0.9104Batch 63, Loss Value: 0.9071\n",
      "Batch 63, Gradient Norm: 0.0336\n",
      "\u001b[1m 63/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:19\u001b[0m 472ms/step - accuracy: 0.5029 - binary_io_u_5: 0.5019 - loss: 0.9104Batch 64, Loss Value: 0.9249\n",
      "Batch 64, Gradient Norm: 0.3558\n",
      "\u001b[1m 64/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:19\u001b[0m 472ms/step - accuracy: 0.5029 - binary_io_u_5: 0.5019 - loss: 0.9104Batch 65, Loss Value: 0.8993\n",
      "Batch 65, Gradient Norm: 0.0751\n",
      "\u001b[1m 65/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:18\u001b[0m 472ms/step - accuracy: 0.5028 - binary_io_u_5: 0.5019 - loss: 0.9105Batch 66, Loss Value: 0.8980\n",
      "Batch 66, Gradient Norm: 0.1220\n",
      "\u001b[1m 66/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:18\u001b[0m 472ms/step - accuracy: 0.5028 - binary_io_u_5: 0.5019 - loss: 0.9105Batch 67, Loss Value: 0.9048\n",
      "Batch 67, Gradient Norm: 0.1886\n",
      "\u001b[1m 67/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:17\u001b[0m 472ms/step - accuracy: 0.5028 - binary_io_u_5: 0.5019 - loss: 0.9105Batch 68, Loss Value: 0.8741\n",
      "Batch 68, Gradient Norm: 0.3299\n",
      "\u001b[1m 68/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:17\u001b[0m 472ms/step - accuracy: 0.5028 - binary_io_u_5: 0.5018 - loss: 0.9105Batch 69, Loss Value: 0.9254\n",
      "Batch 69, Gradient Norm: 0.2520\n",
      "\u001b[1m 69/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:16\u001b[0m 472ms/step - accuracy: 0.5027 - binary_io_u_5: 0.5018 - loss: 0.9105Batch 70, Loss Value: 0.9254\n",
      "Batch 70, Gradient Norm: 0.1355\n",
      "\u001b[1m 70/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:16\u001b[0m 472ms/step - accuracy: 0.5027 - binary_io_u_5: 0.5018 - loss: 0.9105Batch 71, Loss Value: 0.9155\n",
      "Batch 71, Gradient Norm: 0.4040\n",
      "\u001b[1m 71/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:15\u001b[0m 472ms/step - accuracy: 0.5026 - binary_io_u_5: 0.5017 - loss: 0.9106Batch 72, Loss Value: 0.8956\n",
      "Batch 72, Gradient Norm: 0.1970\n",
      "\u001b[1m 72/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:15\u001b[0m 472ms/step - accuracy: 0.5026 - binary_io_u_5: 0.5017 - loss: 0.9106Batch 73, Loss Value: 0.9160\n",
      "Batch 73, Gradient Norm: 0.1300\n",
      "\u001b[1m 73/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:15\u001b[0m 472ms/step - accuracy: 0.5026 - binary_io_u_5: 0.5017 - loss: 0.9106Batch 74, Loss Value: 0.9148\n",
      "Batch 74, Gradient Norm: 0.6721\n",
      "\u001b[1m 74/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:14\u001b[0m 472ms/step - accuracy: 0.5025 - binary_io_u_5: 0.5017 - loss: 0.9106Batch 75, Loss Value: 0.9064\n",
      "Batch 75, Gradient Norm: 0.0950\n",
      "\u001b[1m 75/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:14\u001b[0m 472ms/step - accuracy: 0.5025 - binary_io_u_5: 0.5017 - loss: 0.9106Batch 76, Loss Value: 0.9069\n",
      "Batch 76, Gradient Norm: 0.0462\n",
      "\u001b[1m 76/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:13\u001b[0m 472ms/step - accuracy: 0.5025 - binary_io_u_5: 0.5016 - loss: 0.9106Batch 77, Loss Value: 0.9080\n",
      "Batch 77, Gradient Norm: 0.0938\n",
      "\u001b[1m 77/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:13\u001b[0m 472ms/step - accuracy: 0.5025 - binary_io_u_5: 0.5016 - loss: 0.9106Batch 78, Loss Value: 0.9010\n",
      "Batch 78, Gradient Norm: 0.3393\n",
      "\u001b[1m 78/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:12\u001b[0m 472ms/step - accuracy: 0.5024 - binary_io_u_5: 0.5016 - loss: 0.9107Batch 79, Loss Value: 0.9246\n",
      "Batch 79, Gradient Norm: 0.3232\n",
      "\u001b[1m 79/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:12\u001b[0m 472ms/step - accuracy: 0.5024 - binary_io_u_5: 0.5016 - loss: 0.9107Batch 80, Loss Value: 0.9062\n",
      "Batch 80, Gradient Norm: 0.0296\n",
      "\u001b[1m 80/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:11\u001b[0m 472ms/step - accuracy: 0.5024 - binary_io_u_5: 0.5015 - loss: 0.9107Batch 81, Loss Value: 0.9143\n",
      "Batch 81, Gradient Norm: 0.1853\n",
      "\u001b[1m 81/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:11\u001b[0m 472ms/step - accuracy: 0.5023 - binary_io_u_5: 0.5015 - loss: 0.9107Batch 82, Loss Value: 0.9151\n",
      "Batch 82, Gradient Norm: 0.2743\n",
      "\u001b[1m 82/232\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m1:10\u001b[0m 472ms/step - accuracy: 0.5022 - binary_io_u_5: 0.5014 - loss: 0.9107Batch 83, Loss Value: 0.9199\n",
      "Batch 83, Gradient Norm: 0.1811\n",
      "\u001b[1m 83/232\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m1:10\u001b[0m 472ms/step - accuracy: 0.5022 - binary_io_u_5: 0.5014 - loss: 0.9107Batch 84, Loss Value: 0.9046\n",
      "Batch 84, Gradient Norm: 0.4658\n",
      "\u001b[1m 84/232\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m1:09\u001b[0m 472ms/step - accuracy: 0.5021 - binary_io_u_5: 0.5013 - loss: 0.9108Batch 85, Loss Value: 0.9130\n",
      "Batch 85, Gradient Norm: 0.1813\n",
      "\u001b[1m 85/232\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m1:09\u001b[0m 472ms/step - accuracy: 0.5021 - binary_io_u_5: 0.5013 - loss: 0.9108Batch 86, Loss Value: 0.9268\n",
      "Batch 86, Gradient Norm: 0.4229\n",
      "\u001b[1m 86/232\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m1:08\u001b[0m 472ms/step - accuracy: 0.5020 - binary_io_u_5: 0.5012 - loss: 0.9108Batch 87, Loss Value: 0.9316\n",
      "Batch 87, Gradient Norm: 0.1935\n",
      "\u001b[1m 87/232\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m1:08\u001b[0m 472ms/step - accuracy: 0.5019 - binary_io_u_5: 0.5012 - loss: 0.9108Batch 88, Loss Value: 0.9111\n",
      "Batch 88, Gradient Norm: 0.2266\n",
      "\u001b[1m 88/232\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m1:07\u001b[0m 472ms/step - accuracy: 0.5019 - binary_io_u_5: 0.5011 - loss: 0.9109Batch 89, Loss Value: 0.9009\n",
      "Batch 89, Gradient Norm: 0.1724\n",
      "\u001b[1m 89/232\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m1:07\u001b[0m 472ms/step - accuracy: 0.5018 - binary_io_u_5: 0.5010 - loss: 0.9109Batch 90, Loss Value: 0.9040\n",
      "Batch 90, Gradient Norm: 0.3533\n",
      "\u001b[1m 90/232\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m1:07\u001b[0m 472ms/step - accuracy: 0.5017 - binary_io_u_5: 0.5009 - loss: 0.9109Batch 91, Loss Value: 0.9119\n",
      "Batch 91, Gradient Norm: 2.0412\n",
      "\u001b[1m 91/232\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m1:06\u001b[0m 472ms/step - accuracy: 0.5016 - binary_io_u_5: 0.5009 - loss: 0.9110Batch 92, Loss Value: 0.9434\n",
      "Batch 92, Gradient Norm: 0.0296\n",
      "\u001b[1m 92/232\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m1:06\u001b[0m 472ms/step - accuracy: 0.5015 - binary_io_u_5: 0.5008 - loss: 0.9110Batch 93, Loss Value: 0.9419\n",
      "Batch 93, Gradient Norm: 0.0331\n",
      "\u001b[1m 93/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m1:05\u001b[0m 472ms/step - accuracy: 0.5014 - binary_io_u_5: 0.5007 - loss: 0.9110Batch 94, Loss Value: 0.9184\n",
      "Batch 94, Gradient Norm: 0.3413\n",
      "\u001b[1m 94/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m1:05\u001b[0m 473ms/step - accuracy: 0.5014 - binary_io_u_5: 0.5006 - loss: 0.9111Batch 95, Loss Value: 0.9088\n",
      "Batch 95, Gradient Norm: 0.0725\n",
      "\u001b[1m 95/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m1:04\u001b[0m 472ms/step - accuracy: 0.5013 - binary_io_u_5: 0.5006 - loss: 0.9111Batch 96, Loss Value: 0.9242\n",
      "Batch 96, Gradient Norm: 0.4570\n",
      "\u001b[1m 96/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m1:04\u001b[0m 472ms/step - accuracy: 0.5012 - binary_io_u_5: 0.5005 - loss: 0.9111Batch 97, Loss Value: 0.9173\n",
      "Batch 97, Gradient Norm: 0.2085\n",
      "\u001b[1m 97/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m1:03\u001b[0m 473ms/step - accuracy: 0.5012 - binary_io_u_5: 0.5005 - loss: 0.9112Batch 98, Loss Value: 0.8957\n",
      "Batch 98, Gradient Norm: 0.1972\n",
      "\u001b[1m 98/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m1:03\u001b[0m 473ms/step - accuracy: 0.5011 - binary_io_u_5: 0.5004 - loss: 0.9112Batch 99, Loss Value: 0.9038\n",
      "Batch 99, Gradient Norm: 0.1775\n",
      "\u001b[1m 99/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m1:02\u001b[0m 473ms/step - accuracy: 0.5011 - binary_io_u_5: 0.5003 - loss: 0.9112Batch 100, Loss Value: 0.9010\n",
      "Batch 100, Gradient Norm: 0.4342\n",
      "\u001b[1m100/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m1:02\u001b[0m 473ms/step - accuracy: 0.5010 - binary_io_u_5: 0.5003 - loss: 0.9112Batch 101, Loss Value: 0.9332\n",
      "Batch 101, Gradient Norm: 0.7406\n",
      "\u001b[1m101/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m1:01\u001b[0m 473ms/step - accuracy: 0.5010 - binary_io_u_5: 0.5003 - loss: 0.9113Batch 102, Loss Value: 0.9190\n",
      "Batch 102, Gradient Norm: 0.2554\n",
      "\u001b[1m102/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m1:01\u001b[0m 473ms/step - accuracy: 0.5009 - binary_io_u_5: 0.5002 - loss: 0.9113Batch 103, Loss Value: 0.9427\n",
      "Batch 103, Gradient Norm: 0.0186\n",
      "\u001b[1m103/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m1:00\u001b[0m 473ms/step - accuracy: 0.5008 - binary_io_u_5: 0.5002 - loss: 0.9113Batch 104, Loss Value: 0.9104\n",
      "Batch 104, Gradient Norm: 0.1953\n",
      "\u001b[1m104/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m1:00\u001b[0m 473ms/step - accuracy: 0.5008 - binary_io_u_5: 0.5001 - loss: 0.9113Batch 105, Loss Value: 0.8957\n",
      "Batch 105, Gradient Norm: 0.1560\n",
      "\u001b[1m105/232\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m1:00\u001b[0m 473ms/step - accuracy: 0.5007 - binary_io_u_5: 0.5001 - loss: 0.9113Batch 106, Loss Value: 0.9395\n",
      "Batch 106, Gradient Norm: 0.3814\n",
      "\u001b[1m106/232\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m59s\u001b[0m 472ms/step - accuracy: 0.5007 - binary_io_u_5: 0.5000 - loss: 0.9114 Batch 107, Loss Value: 0.9248\n",
      "Batch 107, Gradient Norm: 0.4749\n",
      "\u001b[1m107/232\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m59s\u001b[0m 472ms/step - accuracy: 0.5006 - binary_io_u_5: 0.5000 - loss: 0.9114Batch 108, Loss Value: 0.8746\n",
      "Batch 108, Gradient Norm: 0.5756\n",
      "\u001b[1m108/232\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m58s\u001b[0m 472ms/step - accuracy: 0.5006 - binary_io_u_5: 0.4999 - loss: 0.9114Batch 109, Loss Value: 0.9160\n",
      "Batch 109, Gradient Norm: 0.2097\n",
      "\u001b[1m109/232\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m58s\u001b[0m 472ms/step - accuracy: 0.5005 - binary_io_u_5: 0.4999 - loss: 0.9114Batch 110, Loss Value: 0.9053\n",
      "Batch 110, Gradient Norm: 0.5581\n",
      "\u001b[1m110/232\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m57s\u001b[0m 472ms/step - accuracy: 0.5005 - binary_io_u_5: 0.4998 - loss: 0.9115Batch 111, Loss Value: 0.9321\n",
      "Batch 111, Gradient Norm: 0.5887\n",
      "\u001b[1m111/232\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m57s\u001b[0m 472ms/step - accuracy: 0.5005 - binary_io_u_5: 0.4998 - loss: 0.9115Batch 112, Loss Value: 0.9210\n",
      "Batch 112, Gradient Norm: 0.1938\n",
      "\u001b[1m112/232\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m56s\u001b[0m 472ms/step - accuracy: 0.5004 - binary_io_u_5: 0.4998 - loss: 0.9115Batch 113, Loss Value: 0.9240\n",
      "Batch 113, Gradient Norm: 0.2868\n",
      "\u001b[1m113/232\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m56s\u001b[0m 472ms/step - accuracy: 0.5004 - binary_io_u_5: 0.4997 - loss: 0.9115Batch 114, Loss Value: 0.8957\n",
      "Batch 114, Gradient Norm: 0.3441\n",
      "\u001b[1m114/232\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m55s\u001b[0m 472ms/step - accuracy: 0.5004 - binary_io_u_5: 0.4997 - loss: 0.9116Batch 115, Loss Value: 0.8955\n",
      "Batch 115, Gradient Norm: 0.1682\n",
      "\u001b[1m115/232\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m55s\u001b[0m 472ms/step - accuracy: 0.5003 - binary_io_u_5: 0.4997 - loss: 0.9116Batch 116, Loss Value: 0.8865\n",
      "Batch 116, Gradient Norm: 0.1185\n",
      "\u001b[1m116/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m54s\u001b[0m 472ms/step - accuracy: 0.5003 - binary_io_u_5: 0.4996 - loss: 0.9116Batch 117, Loss Value: 0.9420\n",
      "Batch 117, Gradient Norm: 0.0061\n",
      "\u001b[1m117/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m54s\u001b[0m 472ms/step - accuracy: 0.5003 - binary_io_u_5: 0.4996 - loss: 0.9116Batch 118, Loss Value: 0.9171\n",
      "Batch 118, Gradient Norm: 0.2718\n",
      "\u001b[1m118/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m53s\u001b[0m 472ms/step - accuracy: 0.5002 - binary_io_u_5: 0.4996 - loss: 0.9117Batch 119, Loss Value: 0.9043\n",
      "Batch 119, Gradient Norm: 0.1599\n",
      "\u001b[1m119/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m53s\u001b[0m 471ms/step - accuracy: 0.5002 - binary_io_u_5: 0.4996 - loss: 0.9117Batch 120, Loss Value: 0.8968\n",
      "Batch 120, Gradient Norm: 0.1496\n",
      "\u001b[1m120/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m52s\u001b[0m 471ms/step - accuracy: 0.5002 - binary_io_u_5: 0.4995 - loss: 0.9117Batch 121, Loss Value: 0.9411\n",
      "Batch 121, Gradient Norm: 0.2654\n",
      "\u001b[1m121/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m52s\u001b[0m 472ms/step - accuracy: 0.5001 - binary_io_u_5: 0.4995 - loss: 0.9117Batch 122, Loss Value: 0.8980\n",
      "Batch 122, Gradient Norm: 0.1749\n",
      "\u001b[1m122/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m51s\u001b[0m 471ms/step - accuracy: 0.5001 - binary_io_u_5: 0.4995 - loss: 0.9118Batch 123, Loss Value: 0.9086\n",
      "Batch 123, Gradient Norm: 0.1291\n",
      "\u001b[1m123/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m51s\u001b[0m 471ms/step - accuracy: 0.5001 - binary_io_u_5: 0.4994 - loss: 0.9118Batch 124, Loss Value: 0.9109\n",
      "Batch 124, Gradient Norm: 0.2259\n",
      "\u001b[1m124/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m50s\u001b[0m 471ms/step - accuracy: 0.5000 - binary_io_u_5: 0.4994 - loss: 0.9118Batch 125, Loss Value: 0.9110\n",
      "Batch 125, Gradient Norm: 0.2403\n",
      "\u001b[1m125/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m50s\u001b[0m 471ms/step - accuracy: 0.5000 - binary_io_u_5: 0.4994 - loss: 0.9118Batch 126, Loss Value: 0.9046\n",
      "Batch 126, Gradient Norm: 0.1050\n",
      "\u001b[1m126/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m49s\u001b[0m 471ms/step - accuracy: 0.5000 - binary_io_u_5: 0.4994 - loss: 0.9119Batch 127, Loss Value: 0.9260\n",
      "Batch 127, Gradient Norm: 0.0795\n",
      "\u001b[1m127/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m49s\u001b[0m 471ms/step - accuracy: 0.4999 - binary_io_u_5: 0.4993 - loss: 0.9119Batch 128, Loss Value: 0.9179\n",
      "Batch 128, Gradient Norm: 0.3719\n",
      "\u001b[1m128/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m49s\u001b[0m 471ms/step - accuracy: 0.4999 - binary_io_u_5: 0.4993 - loss: 0.9119Batch 129, Loss Value: 0.9111\n",
      "Batch 129, Gradient Norm: 0.1458\n",
      "\u001b[1m129/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m48s\u001b[0m 471ms/step - accuracy: 0.4999 - binary_io_u_5: 0.4993 - loss: 0.9119Batch 130, Loss Value: 0.9047\n",
      "Batch 130, Gradient Norm: 0.1429\n",
      "\u001b[1m130/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m48s\u001b[0m 471ms/step - accuracy: 0.4999 - binary_io_u_5: 0.4993 - loss: 0.9119Batch 131, Loss Value: 0.9438\n",
      "Batch 131, Gradient Norm: 0.0440\n",
      "\u001b[1m131/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m47s\u001b[0m 471ms/step - accuracy: 0.4998 - binary_io_u_5: 0.4992 - loss: 0.9120Batch 132, Loss Value: 0.9122\n",
      "Batch 132, Gradient Norm: 0.1175\n",
      "\u001b[1m132/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m47s\u001b[0m 471ms/step - accuracy: 0.4998 - binary_io_u_5: 0.4992 - loss: 0.9120Batch 133, Loss Value: 0.9293\n",
      "Batch 133, Gradient Norm: 0.8407\n",
      "\u001b[1m133/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m46s\u001b[0m 471ms/step - accuracy: 0.4998 - binary_io_u_5: 0.4992 - loss: 0.9120Batch 134, Loss Value: 0.8973\n",
      "Batch 134, Gradient Norm: 0.0945\n",
      "\u001b[1m134/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m46s\u001b[0m 471ms/step - accuracy: 0.4998 - binary_io_u_5: 0.4992 - loss: 0.9120Batch 135, Loss Value: 0.9074\n",
      "Batch 135, Gradient Norm: 0.2055\n",
      "\u001b[1m135/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m45s\u001b[0m 471ms/step - accuracy: 0.4997 - binary_io_u_5: 0.4992 - loss: 0.9120Batch 136, Loss Value: 0.8965\n",
      "Batch 136, Gradient Norm: 0.3248\n",
      "\u001b[1m136/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m45s\u001b[0m 471ms/step - accuracy: 0.4997 - binary_io_u_5: 0.4991 - loss: 0.9121Batch 137, Loss Value: 0.9098\n",
      "Batch 137, Gradient Norm: 0.7761\n",
      "\u001b[1m137/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m44s\u001b[0m 471ms/step - accuracy: 0.4997 - binary_io_u_5: 0.4991 - loss: 0.9121Batch 138, Loss Value: 0.9116\n",
      "Batch 138, Gradient Norm: 0.6391\n",
      "\u001b[1m138/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m44s\u001b[0m 471ms/step - accuracy: 0.4997 - binary_io_u_5: 0.4991 - loss: 0.9121Batch 139, Loss Value: 0.9188\n",
      "Batch 139, Gradient Norm: 0.4034\n",
      "\u001b[1m139/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m43s\u001b[0m 471ms/step - accuracy: 0.4996 - binary_io_u_5: 0.4991 - loss: 0.9121Batch 140, Loss Value: 0.9221\n",
      "Batch 140, Gradient Norm: 0.4978\n",
      "\u001b[1m140/232\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m43s\u001b[0m 471ms/step - accuracy: 0.4996 - binary_io_u_5: 0.4991 - loss: 0.9121Batch 141, Loss Value: 0.8946\n",
      "Batch 141, Gradient Norm: 0.3212\n",
      "\u001b[1m141/232\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m42s\u001b[0m 471ms/step - accuracy: 0.4996 - binary_io_u_5: 0.4990 - loss: 0.9121Batch 142, Loss Value: 0.9429\n",
      "Batch 142, Gradient Norm: 0.0654\n",
      "\u001b[1m142/232\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m42s\u001b[0m 471ms/step - accuracy: 0.4996 - binary_io_u_5: 0.4990 - loss: 0.9122Batch 143, Loss Value: 0.9439\n",
      "Batch 143, Gradient Norm: 0.0387\n",
      "\u001b[1m143/232\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m41s\u001b[0m 471ms/step - accuracy: 0.4995 - binary_io_u_5: 0.4990 - loss: 0.9122Batch 144, Loss Value: 0.9352\n",
      "Batch 144, Gradient Norm: 0.6716\n",
      "\u001b[1m144/232\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m41s\u001b[0m 471ms/step - accuracy: 0.4995 - binary_io_u_5: 0.4989 - loss: 0.9122Batch 145, Loss Value: 0.9229\n",
      "Batch 145, Gradient Norm: 0.2498\n",
      "\u001b[1m145/232\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m40s\u001b[0m 471ms/step - accuracy: 0.4995 - binary_io_u_5: 0.4989 - loss: 0.9122Batch 146, Loss Value: 0.9349\n",
      "Batch 146, Gradient Norm: 0.9953\n",
      "\u001b[1m146/232\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m40s\u001b[0m 471ms/step - accuracy: 0.4994 - binary_io_u_5: 0.4989 - loss: 0.9122Batch 147, Loss Value: 0.9194\n",
      "Batch 147, Gradient Norm: 0.2185\n",
      "\u001b[1m147/232\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m39s\u001b[0m 471ms/step - accuracy: 0.4994 - binary_io_u_5: 0.4989 - loss: 0.9123Batch 148, Loss Value: 0.9151\n",
      "Batch 148, Gradient Norm: 0.4128\n",
      "\u001b[1m148/232\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m39s\u001b[0m 471ms/step - accuracy: 0.4994 - binary_io_u_5: 0.4988 - loss: 0.9123Batch 149, Loss Value: 0.9434\n",
      "Batch 149, Gradient Norm: 0.0450\n",
      "\u001b[1m149/232\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m39s\u001b[0m 471ms/step - accuracy: 0.4994 - binary_io_u_5: 0.4988 - loss: 0.9123Batch 150, Loss Value: 0.9425\n",
      "Batch 150, Gradient Norm: 0.0313\n",
      "\u001b[1m150/232\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m38s\u001b[0m 471ms/step - accuracy: 0.4993 - binary_io_u_5: 0.4988 - loss: 0.9123Batch 151, Loss Value: 0.9377\n",
      "Batch 151, Gradient Norm: 0.2502\n",
      "\u001b[1m151/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m38s\u001b[0m 471ms/step - accuracy: 0.4993 - binary_io_u_5: 0.4988 - loss: 0.9123Batch 152, Loss Value: 0.9248\n",
      "Batch 152, Gradient Norm: 1.0103\n",
      "\u001b[1m152/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m37s\u001b[0m 470ms/step - accuracy: 0.4993 - binary_io_u_5: 0.4987 - loss: 0.9123Batch 153, Loss Value: 0.9128\n",
      "Batch 153, Gradient Norm: 0.9167\n",
      "\u001b[1m153/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m37s\u001b[0m 470ms/step - accuracy: 0.4992 - binary_io_u_5: 0.4987 - loss: 0.9124Batch 154, Loss Value: 0.9007\n",
      "Batch 154, Gradient Norm: 0.0733\n",
      "\u001b[1m154/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m36s\u001b[0m 470ms/step - accuracy: 0.4992 - binary_io_u_5: 0.4987 - loss: 0.9124Batch 155, Loss Value: 0.9131\n",
      "Batch 155, Gradient Norm: 0.2537\n",
      "\u001b[1m155/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m36s\u001b[0m 470ms/step - accuracy: 0.4992 - binary_io_u_5: 0.4986 - loss: 0.9124Batch 156, Loss Value: 0.8922\n",
      "Batch 156, Gradient Norm: 0.2659\n",
      "\u001b[1m156/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m35s\u001b[0m 470ms/step - accuracy: 0.4991 - binary_io_u_5: 0.4986 - loss: 0.9124Batch 157, Loss Value: 0.9250\n",
      "Batch 157, Gradient Norm: 0.3905\n",
      "\u001b[1m157/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m35s\u001b[0m 470ms/step - accuracy: 0.4991 - binary_io_u_5: 0.4986 - loss: 0.9124Batch 158, Loss Value: 0.9067\n",
      "Batch 158, Gradient Norm: 0.3515\n",
      "\u001b[1m158/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m34s\u001b[0m 470ms/step - accuracy: 0.4991 - binary_io_u_5: 0.4986 - loss: 0.9124Batch 159, Loss Value: 0.9103\n",
      "Batch 159, Gradient Norm: 0.0717\n",
      "\u001b[1m159/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m34s\u001b[0m 470ms/step - accuracy: 0.4990 - binary_io_u_5: 0.4985 - loss: 0.9125Batch 160, Loss Value: 0.9306\n",
      "Batch 160, Gradient Norm: 1.7131\n",
      "\u001b[1m160/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m33s\u001b[0m 470ms/step - accuracy: 0.4990 - binary_io_u_5: 0.4985 - loss: 0.9125Batch 161, Loss Value: 0.8978\n",
      "Batch 161, Gradient Norm: 0.1363\n",
      "\u001b[1m161/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m33s\u001b[0m 470ms/step - accuracy: 0.4990 - binary_io_u_5: 0.4985 - loss: 0.9125Batch 162, Loss Value: 0.8931\n",
      "Batch 162, Gradient Norm: 0.2213\n",
      "\u001b[1m162/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m32s\u001b[0m 470ms/step - accuracy: 0.4990 - binary_io_u_5: 0.4985 - loss: 0.9125Batch 163, Loss Value: 0.9038\n",
      "Batch 163, Gradient Norm: 0.0559\n",
      "\u001b[1m163/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m32s\u001b[0m 470ms/step - accuracy: 0.4990 - binary_io_u_5: 0.4985 - loss: 0.9125Batch 164, Loss Value: 0.9075\n",
      "Batch 164, Gradient Norm: 0.0607\n",
      "\u001b[1m164/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m31s\u001b[0m 470ms/step - accuracy: 0.4989 - binary_io_u_5: 0.4984 - loss: 0.9125Batch 165, Loss Value: 0.8943\n",
      "Batch 165, Gradient Norm: 0.5932\n",
      "\u001b[1m165/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m31s\u001b[0m 470ms/step - accuracy: 0.4989 - binary_io_u_5: 0.4984 - loss: 0.9125Batch 166, Loss Value: 0.9096\n",
      "Batch 166, Gradient Norm: 0.0917\n",
      "\u001b[1m166/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m31s\u001b[0m 470ms/step - accuracy: 0.4989 - binary_io_u_5: 0.4984 - loss: 0.9125Batch 167, Loss Value: 0.9058\n",
      "Batch 167, Gradient Norm: 0.0545\n",
      "\u001b[1m167/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m30s\u001b[0m 470ms/step - accuracy: 0.4989 - binary_io_u_5: 0.4984 - loss: 0.9125Batch 168, Loss Value: 0.9070\n",
      "Batch 168, Gradient Norm: 0.0327\n",
      "\u001b[1m168/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m30s\u001b[0m 470ms/step - accuracy: 0.4988 - binary_io_u_5: 0.4984 - loss: 0.9125Batch 169, Loss Value: 0.8986\n",
      "Batch 169, Gradient Norm: 0.5275\n",
      "\u001b[1m169/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m29s\u001b[0m 470ms/step - accuracy: 0.4988 - binary_io_u_5: 0.4983 - loss: 0.9126Batch 170, Loss Value: 0.9006\n",
      "Batch 170, Gradient Norm: 0.0669\n",
      "\u001b[1m170/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m29s\u001b[0m 470ms/step - accuracy: 0.4988 - binary_io_u_5: 0.4983 - loss: 0.9126Batch 171, Loss Value: 0.9063\n",
      "Batch 171, Gradient Norm: 0.0198\n",
      "\u001b[1m171/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m28s\u001b[0m 470ms/step - accuracy: 0.4988 - binary_io_u_5: 0.4983 - loss: 0.9126Batch 172, Loss Value: 0.9064\n",
      "Batch 172, Gradient Norm: 0.0608\n",
      "\u001b[1m172/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m28s\u001b[0m 470ms/step - accuracy: 0.4988 - binary_io_u_5: 0.4983 - loss: 0.9126Batch 173, Loss Value: 0.9034\n",
      "Batch 173, Gradient Norm: 0.1872\n",
      "\u001b[1m173/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m27s\u001b[0m 470ms/step - accuracy: 0.4987 - binary_io_u_5: 0.4983 - loss: 0.9126Batch 174, Loss Value: 0.8979\n",
      "Batch 174, Gradient Norm: 0.2112\n",
      "\u001b[1m174/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m27s\u001b[0m 470ms/step - accuracy: 0.4987 - binary_io_u_5: 0.4982 - loss: 0.9126Batch 175, Loss Value: 0.9061\n",
      "Batch 175, Gradient Norm: 0.0326\n",
      "\u001b[1m175/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m26s\u001b[0m 470ms/step - accuracy: 0.4987 - binary_io_u_5: 0.4982 - loss: 0.9126Batch 176, Loss Value: 0.9066\n",
      "Batch 176, Gradient Norm: 0.0039\n",
      "\u001b[1m176/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m26s\u001b[0m 470ms/step - accuracy: 0.4987 - binary_io_u_5: 0.4982 - loss: 0.9126Batch 177, Loss Value: 0.9059\n",
      "Batch 177, Gradient Norm: 0.0552\n",
      "\u001b[1m177/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m25s\u001b[0m 470ms/step - accuracy: 0.4987 - binary_io_u_5: 0.4982 - loss: 0.9126Batch 178, Loss Value: 0.9067\n",
      "Batch 178, Gradient Norm: 0.0018\n",
      "\u001b[1m178/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m25s\u001b[0m 470ms/step - accuracy: 0.4987 - binary_io_u_5: 0.4982 - loss: 0.9126Batch 179, Loss Value: 0.9066\n",
      "Batch 179, Gradient Norm: 0.0007\n",
      "\u001b[1m179/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m24s\u001b[0m 470ms/step - accuracy: 0.4987 - binary_io_u_5: 0.4982 - loss: 0.9126Batch 180, Loss Value: 0.9066\n",
      "Batch 180, Gradient Norm: 0.0002\n",
      "\u001b[1m180/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m24s\u001b[0m 470ms/step - accuracy: 0.4986 - binary_io_u_5: 0.4982 - loss: 0.9126Batch 181, Loss Value: 0.9066\n",
      "Batch 181, Gradient Norm: 0.0006\n",
      "\u001b[1m181/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m23s\u001b[0m 470ms/step - accuracy: 0.4986 - binary_io_u_5: 0.4982 - loss: 0.9126Batch 182, Loss Value: 0.9066\n",
      "Batch 182, Gradient Norm: 0.0014\n",
      "\u001b[1m182/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m23s\u001b[0m 470ms/step - accuracy: 0.4986 - binary_io_u_5: 0.4982 - loss: 0.9126Batch 183, Loss Value: 0.9066\n",
      "Batch 183, Gradient Norm: 0.0022\n",
      "\u001b[1m183/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m23s\u001b[0m 470ms/step - accuracy: 0.4986 - binary_io_u_5: 0.4981 - loss: 0.9126Batch 184, Loss Value: 0.9066\n",
      "Batch 184, Gradient Norm: 0.0001\n",
      "\u001b[1m184/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m22s\u001b[0m 470ms/step - accuracy: 0.4986 - binary_io_u_5: 0.4981 - loss: 0.9126Batch 185, Loss Value: 0.9066\n",
      "Batch 185, Gradient Norm: 0.0000\n",
      "\u001b[1m185/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m22s\u001b[0m 470ms/step - accuracy: 0.4986 - binary_io_u_5: 0.4981 - loss: 0.9126Batch 186, Loss Value: 0.9066\n",
      "Batch 186, Gradient Norm: 0.0008\n",
      "\u001b[1m186/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m21s\u001b[0m 470ms/step - accuracy: 0.4986 - binary_io_u_5: 0.4981 - loss: 0.9127Batch 187, Loss Value: 0.9066\n",
      "Batch 187, Gradient Norm: 0.0009\n",
      "\u001b[1m187/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m21s\u001b[0m 470ms/step - accuracy: 0.4986 - binary_io_u_5: 0.4981 - loss: 0.9127Batch 188, Loss Value: 0.9066\n",
      "Batch 188, Gradient Norm: 0.0000\n",
      "\u001b[1m188/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m20s\u001b[0m 470ms/step - accuracy: 0.4985 - binary_io_u_5: 0.4981 - loss: 0.9127Batch 189, Loss Value: 0.9066\n",
      "Batch 189, Gradient Norm: 0.0005\n",
      "\u001b[1m189/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m20s\u001b[0m 470ms/step - accuracy: 0.4985 - binary_io_u_5: 0.4981 - loss: 0.9127Batch 190, Loss Value: 0.9066\n",
      "Batch 190, Gradient Norm: 0.0002\n",
      "\u001b[1m190/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m19s\u001b[0m 470ms/step - accuracy: 0.4985 - binary_io_u_5: 0.4981 - loss: 0.9127Batch 191, Loss Value: 0.9066\n",
      "Batch 191, Gradient Norm: 0.0001\n",
      "\u001b[1m191/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m19s\u001b[0m 470ms/step - accuracy: 0.4985 - binary_io_u_5: 0.4980 - loss: 0.9127Batch 192, Loss Value: 0.9066\n",
      "Batch 192, Gradient Norm: 0.0000\n",
      "\u001b[1m192/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m18s\u001b[0m 470ms/step - accuracy: 0.4985 - binary_io_u_5: 0.4980 - loss: 0.9127Batch 193, Loss Value: 0.9066\n",
      "Batch 193, Gradient Norm: 0.0000\n",
      "\u001b[1m193/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m18s\u001b[0m 470ms/step - accuracy: 0.4985 - binary_io_u_5: 0.4980 - loss: 0.9127Batch 194, Loss Value: 0.9067\n",
      "Batch 194, Gradient Norm: 0.0016\n",
      "\u001b[1m194/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m17s\u001b[0m 470ms/step - accuracy: 0.4985 - binary_io_u_5: 0.4980 - loss: 0.9127Batch 195, Loss Value: 0.9066\n",
      "Batch 195, Gradient Norm: 0.0000\n",
      "\u001b[1m195/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m17s\u001b[0m 470ms/step - accuracy: 0.4985 - binary_io_u_5: 0.4980 - loss: 0.9127Batch 196, Loss Value: 0.9066\n",
      "Batch 196, Gradient Norm: 0.0001\n",
      "\u001b[1m196/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m16s\u001b[0m 470ms/step - accuracy: 0.4984 - binary_io_u_5: 0.4980 - loss: 0.9127Batch 197, Loss Value: 0.9066\n",
      "Batch 197, Gradient Norm: 0.0000\n",
      "\u001b[1m197/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m16s\u001b[0m 470ms/step - accuracy: 0.4984 - binary_io_u_5: 0.4980 - loss: 0.9127Batch 198, Loss Value: 0.9066\n",
      "Batch 198, Gradient Norm: 0.0000\n",
      "\u001b[1m198/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m15s\u001b[0m 470ms/step - accuracy: 0.4984 - binary_io_u_5: 0.4980 - loss: 0.9127Batch 199, Loss Value: 0.9066\n",
      "Batch 199, Gradient Norm: 0.0000\n",
      "\u001b[1m199/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m15s\u001b[0m 470ms/step - accuracy: 0.4984 - binary_io_u_5: 0.4980 - loss: 0.9127Batch 200, Loss Value: 0.9066\n",
      "Batch 200, Gradient Norm: 0.0000\n",
      "\u001b[1m200/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m15s\u001b[0m 470ms/step - accuracy: 0.4984 - binary_io_u_5: 0.4979 - loss: 0.9127Batch 201, Loss Value: 0.9066\n",
      "Batch 201, Gradient Norm: 0.0000\n",
      "\u001b[1m201/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m14s\u001b[0m 470ms/step - accuracy: 0.4984 - binary_io_u_5: 0.4979 - loss: 0.9127Batch 202, Loss Value: 0.9066\n",
      "Batch 202, Gradient Norm: 0.0001\n",
      "\u001b[1m202/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m14s\u001b[0m 470ms/step - accuracy: 0.4984 - binary_io_u_5: 0.4979 - loss: 0.9127Batch 203, Loss Value: 0.9066\n",
      "Batch 203, Gradient Norm: 0.0000\n",
      "\u001b[1m203/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m13s\u001b[0m 470ms/step - accuracy: 0.4983 - binary_io_u_5: 0.4979 - loss: 0.9127Batch 204, Loss Value: 0.9066\n",
      "Batch 204, Gradient Norm: 0.0003\n",
      "\u001b[1m204/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m13s\u001b[0m 470ms/step - accuracy: 0.4983 - binary_io_u_5: 0.4979 - loss: 0.9127Batch 205, Loss Value: 0.9066\n",
      "Batch 205, Gradient Norm: 0.0001\n",
      "\u001b[1m205/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m12s\u001b[0m 470ms/step - accuracy: 0.4983 - binary_io_u_5: 0.4979 - loss: 0.9127Batch 206, Loss Value: 0.9066\n",
      "Batch 206, Gradient Norm: 0.0001\n",
      "\u001b[1m206/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m12s\u001b[0m 470ms/step - accuracy: 0.4983 - binary_io_u_5: 0.4979 - loss: 0.9127Batch 207, Loss Value: 0.9066\n",
      "Batch 207, Gradient Norm: 0.0000\n",
      "\u001b[1m207/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m11s\u001b[0m 469ms/step - accuracy: 0.4983 - binary_io_u_5: 0.4979 - loss: 0.9127Batch 208, Loss Value: 0.9066\n",
      "Batch 208, Gradient Norm: 0.0000\n",
      "\u001b[1m208/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m11s\u001b[0m 469ms/step - accuracy: 0.4983 - binary_io_u_5: 0.4979 - loss: 0.9127Batch 209, Loss Value: 0.9066\n",
      "Batch 209, Gradient Norm: 0.0001\n",
      "\u001b[1m209/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m10s\u001b[0m 469ms/step - accuracy: 0.4983 - binary_io_u_5: 0.4978 - loss: 0.9127Batch 210, Loss Value: 0.9066\n",
      "Batch 210, Gradient Norm: 0.0002\n",
      "\u001b[1m210/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m10s\u001b[0m 469ms/step - accuracy: 0.4982 - binary_io_u_5: 0.4978 - loss: 0.9127Batch 211, Loss Value: 0.9066\n",
      "Batch 211, Gradient Norm: 0.0042\n",
      "\u001b[1m211/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m9s\u001b[0m 469ms/step - accuracy: 0.4982 - binary_io_u_5: 0.4978 - loss: 0.9127 Batch 212, Loss Value: 0.9066\n",
      "Batch 212, Gradient Norm: 0.0000\n",
      "\u001b[1m212/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m9s\u001b[0m 469ms/step - accuracy: 0.4982 - binary_io_u_5: 0.4978 - loss: 0.9127Batch 213, Loss Value: 0.9066\n",
      "Batch 213, Gradient Norm: 0.0002\n",
      "\u001b[1m213/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m8s\u001b[0m 469ms/step - accuracy: 0.4982 - binary_io_u_5: 0.4978 - loss: 0.9127Batch 214, Loss Value: 0.9066\n",
      "Batch 214, Gradient Norm: 0.0000\n",
      "\u001b[1m214/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m8s\u001b[0m 469ms/step - accuracy: 0.4982 - binary_io_u_5: 0.4978 - loss: 0.9127Batch 215, Loss Value: 0.9066\n",
      "Batch 215, Gradient Norm: 0.0000\n",
      "\u001b[1m215/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m7s\u001b[0m 469ms/step - accuracy: 0.4982 - binary_io_u_5: 0.4978 - loss: 0.9127Batch 216, Loss Value: 0.9066\n",
      "Batch 216, Gradient Norm: 0.0001\n",
      "\u001b[1m216/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m7s\u001b[0m 469ms/step - accuracy: 0.4982 - binary_io_u_5: 0.4978 - loss: 0.9127Batch 217, Loss Value: 0.9066\n",
      "Batch 217, Gradient Norm: 0.0000\n",
      "\u001b[1m217/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m7s\u001b[0m 469ms/step - accuracy: 0.4982 - binary_io_u_5: 0.4978 - loss: 0.9127Batch 218, Loss Value: 0.9066\n",
      "Batch 218, Gradient Norm: 0.0000\n",
      "\u001b[1m218/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m6s\u001b[0m 469ms/step - accuracy: 0.4982 - binary_io_u_5: 0.4978 - loss: 0.9127Batch 219, Loss Value: 0.9066\n",
      "Batch 219, Gradient Norm: 0.0000\n",
      "\u001b[1m219/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m6s\u001b[0m 469ms/step - accuracy: 0.4982 - binary_io_u_5: 0.4978 - loss: 0.9127Batch 220, Loss Value: 0.9067\n",
      "Batch 220, Gradient Norm: 0.0018\n",
      "\u001b[1m220/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m5s\u001b[0m 469ms/step - accuracy: 0.4982 - binary_io_u_5: 0.4978 - loss: 0.9127Batch 221, Loss Value: 0.9066\n",
      "Batch 221, Gradient Norm: 0.0000\n",
      "\u001b[1m221/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m5s\u001b[0m 469ms/step - accuracy: 0.4982 - binary_io_u_5: 0.4978 - loss: 0.9127Batch 222, Loss Value: 0.9066\n",
      "Batch 222, Gradient Norm: 0.0000\n",
      "\u001b[1m222/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m4s\u001b[0m 469ms/step - accuracy: 0.4981 - binary_io_u_5: 0.4977 - loss: 0.9127Batch 223, Loss Value: 0.9066\n",
      "Batch 223, Gradient Norm: 0.0004\n",
      "\u001b[1m223/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m4s\u001b[0m 469ms/step - accuracy: 0.4981 - binary_io_u_5: 0.4977 - loss: 0.9127Batch 224, Loss Value: 0.9066\n",
      "Batch 224, Gradient Norm: 0.0000\n",
      "\u001b[1m224/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m3s\u001b[0m 469ms/step - accuracy: 0.4981 - binary_io_u_5: 0.4977 - loss: 0.9127Batch 225, Loss Value: 0.9066\n",
      "Batch 225, Gradient Norm: 0.0001\n",
      "\u001b[1m225/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m3s\u001b[0m 469ms/step - accuracy: 0.4981 - binary_io_u_5: 0.4977 - loss: 0.9127Batch 226, Loss Value: 0.9066\n",
      "Batch 226, Gradient Norm: 0.0000\n",
      "\u001b[1m226/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m2s\u001b[0m 469ms/step - accuracy: 0.4981 - binary_io_u_5: 0.4977 - loss: 0.9127Batch 227, Loss Value: 0.9066\n",
      "Batch 227, Gradient Norm: 0.0003\n",
      "\u001b[1m227/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m2s\u001b[0m 469ms/step - accuracy: 0.4981 - binary_io_u_5: 0.4977 - loss: 0.9127Batch 228, Loss Value: 0.9066\n",
      "Batch 228, Gradient Norm: 0.0000\n",
      "\u001b[1m228/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m1s\u001b[0m 469ms/step - accuracy: 0.4981 - binary_io_u_5: 0.4977 - loss: 0.9127Batch 229, Loss Value: 0.9066\n",
      "Batch 229, Gradient Norm: 0.0000\n",
      "\u001b[1m229/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m1s\u001b[0m 469ms/step - accuracy: 0.4981 - binary_io_u_5: 0.4977 - loss: 0.9127Batch 230, Loss Value: 0.9066\n",
      "Batch 230, Gradient Norm: 0.0000\n",
      "\u001b[1m230/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 469ms/step - accuracy: 0.4981 - binary_io_u_5: 0.4977 - loss: 0.9127Batch 231, Loss Value: 0.9066\n",
      "Batch 231, Gradient Norm: 0.0000\n",
      "\u001b[1m231/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 469ms/step - accuracy: 0.4981 - binary_io_u_5: 0.4977 - loss: 0.9127Batch 232, Loss Value: 0.9066\n",
      "Batch 232, Gradient Norm: 0.0000\n",
      "\u001b[1m232/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m110s\u001b[0m 475ms/step - accuracy: 0.4980 - binary_io_u_5: 0.4977 - loss: 0.9128 - val_accuracy: 0.4949 - val_binary_io_u_5: 0.4949 - val_loss: 0.9126 - learning_rate: 0.0010\n",
      "Epoch 15/200\n",
      "Batch 1, Loss Value: 0.8929\n",
      "Batch 1, Gradient Norm: 0.0000\n",
      "\u001b[1m  1/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:51\u001b[0m 482ms/step - accuracy: 0.5300 - binary_io_u_5: 0.5300 - loss: 0.8929Batch 2, Loss Value: 0.8929\n",
      "Batch 2, Gradient Norm: 0.0005\n",
      "\u001b[1m  2/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:46\u001b[0m 463ms/step - accuracy: 0.5125 - binary_io_u_5: 0.5125 - loss: 0.9050Batch 3, Loss Value: 0.8929\n",
      "Batch 3, Gradient Norm: 0.0000\n",
      "\u001b[1m  3/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:46\u001b[0m 464ms/step - accuracy: 0.5061 - binary_io_u_5: 0.5061 - loss: 0.9094Batch 4, Loss Value: 0.8929\n",
      "Batch 4, Gradient Norm: 0.0000\n",
      "\u001b[1m  4/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:45\u001b[0m 462ms/step - accuracy: 0.5002 - binary_io_u_5: 0.5002 - loss: 0.9132Batch 5, Loss Value: 0.8929\n",
      "Batch 5, Gradient Norm: 0.0000\n",
      "\u001b[1m  5/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:44\u001b[0m 462ms/step - accuracy: 0.4938 - binary_io_u_5: 0.4938 - loss: 0.9164Batch 6, Loss Value: 0.8929\n",
      "Batch 6, Gradient Norm: 0.0000\n",
      "\u001b[1m  6/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:44\u001b[0m 461ms/step - accuracy: 0.4904 - binary_io_u_5: 0.4904 - loss: 0.9181Batch 7, Loss Value: 0.8929\n",
      "Batch 7, Gradient Norm: 0.0000\n",
      "\u001b[1m  7/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:44\u001b[0m 464ms/step - accuracy: 0.4899 - binary_io_u_5: 0.4899 - loss: 0.9181Batch 8, Loss Value: 0.8929\n",
      "Batch 8, Gradient Norm: 0.0002\n",
      "\u001b[1m  8/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:43\u001b[0m 464ms/step - accuracy: 0.4893 - binary_io_u_5: 0.4893 - loss: 0.9183Batch 9, Loss Value: 0.8929\n",
      "Batch 9, Gradient Norm: 0.0000\n",
      "\u001b[1m  9/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:43\u001b[0m 464ms/step - accuracy: 0.4892 - binary_io_u_5: 0.4892 - loss: 0.9182Batch 10, Loss Value: 0.8929\n",
      "Batch 10, Gradient Norm: 0.0000\n",
      "\u001b[1m 10/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:43\u001b[0m 465ms/step - accuracy: 0.4891 - binary_io_u_5: 0.4891 - loss: 0.9183Batch 11, Loss Value: 0.8929\n",
      "Batch 11, Gradient Norm: 0.0001\n",
      "\u001b[1m 11/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:42\u001b[0m 464ms/step - accuracy: 0.4899 - binary_io_u_5: 0.4899 - loss: 0.9178Batch 12, Loss Value: 0.8929\n",
      "Batch 12, Gradient Norm: 0.0000\n",
      "\u001b[1m 12/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:42\u001b[0m 464ms/step - accuracy: 0.4904 - binary_io_u_5: 0.4904 - loss: 0.9174Batch 13, Loss Value: 0.8929\n",
      "Batch 13, Gradient Norm: 0.0000\n",
      "\u001b[1m 13/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:41\u001b[0m 464ms/step - accuracy: 0.4912 - binary_io_u_5: 0.4912 - loss: 0.9169Batch 14, Loss Value: 0.8929\n",
      "Batch 14, Gradient Norm: 0.0000\n",
      "\u001b[1m 14/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:40\u001b[0m 463ms/step - accuracy: 0.4917 - binary_io_u_5: 0.4917 - loss: 0.9166Batch 15, Loss Value: 0.8929\n",
      "Batch 15, Gradient Norm: 0.0000\n",
      "\u001b[1m 15/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:40\u001b[0m 463ms/step - accuracy: 0.4922 - binary_io_u_5: 0.4922 - loss: 0.9163Batch 16, Loss Value: 0.8929\n",
      "Batch 16, Gradient Norm: 0.0000\n",
      "\u001b[1m 16/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:40\u001b[0m 464ms/step - accuracy: 0.4927 - binary_io_u_5: 0.4927 - loss: 0.9160Batch 17, Loss Value: 0.8929\n",
      "Batch 17, Gradient Norm: 0.0001\n",
      "\u001b[1m 17/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:39\u001b[0m 464ms/step - accuracy: 0.4931 - binary_io_u_5: 0.4931 - loss: 0.9158Batch 18, Loss Value: 0.8929\n",
      "Batch 18, Gradient Norm: 0.0000\n",
      "\u001b[1m 18/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:39\u001b[0m 464ms/step - accuracy: 0.4933 - binary_io_u_5: 0.4933 - loss: 0.9157Batch 19, Loss Value: 0.8929\n",
      "Batch 19, Gradient Norm: 0.0001\n",
      "\u001b[1m 19/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:38\u001b[0m 463ms/step - accuracy: 0.4933 - binary_io_u_5: 0.4933 - loss: 0.9157Batch 20, Loss Value: 0.8929\n",
      "Batch 20, Gradient Norm: 0.0000\n",
      "\u001b[1m 20/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:38\u001b[0m 463ms/step - accuracy: 0.4935 - binary_io_u_5: 0.4935 - loss: 0.9155Batch 21, Loss Value: 0.8929\n",
      "Batch 21, Gradient Norm: 0.0000\n",
      "\u001b[1m 21/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:37\u001b[0m 463ms/step - accuracy: 0.4939 - binary_io_u_5: 0.4939 - loss: 0.9152Batch 22, Loss Value: 0.8929\n",
      "Batch 22, Gradient Norm: 0.0000\n",
      "\u001b[1m 22/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:37\u001b[0m 463ms/step - accuracy: 0.4942 - binary_io_u_5: 0.4942 - loss: 0.9150Batch 23, Loss Value: 0.8929\n",
      "Batch 23, Gradient Norm: 0.0000\n",
      "\u001b[1m 23/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:36\u001b[0m 463ms/step - accuracy: 0.4942 - binary_io_u_5: 0.4942 - loss: 0.9149Batch 24, Loss Value: 0.8929\n",
      "Batch 24, Gradient Norm: 0.0000\n",
      "\u001b[1m 24/232\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:36\u001b[0m 463ms/step - accuracy: 0.4943 - binary_io_u_5: 0.4943 - loss: 0.9147Batch 25, Loss Value: 0.8932\n",
      "Batch 25, Gradient Norm: 0.0256\n",
      "\u001b[1m 25/232\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:35\u001b[0m 464ms/step - accuracy: 0.4945 - binary_io_u_5: 0.4945 - loss: 0.9145Batch 26, Loss Value: 0.8929\n",
      "Batch 26, Gradient Norm: 0.0000\n",
      "\u001b[1m 26/232\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:35\u001b[0m 464ms/step - accuracy: 0.4946 - binary_io_u_5: 0.4946 - loss: 0.9144Batch 27, Loss Value: 0.8929\n",
      "Batch 27, Gradient Norm: 0.0000\n",
      "\u001b[1m 27/232\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:35\u001b[0m 464ms/step - accuracy: 0.4947 - binary_io_u_5: 0.4947 - loss: 0.9143Batch 28, Loss Value: 0.8929\n",
      "Batch 28, Gradient Norm: 0.0000\n",
      "\u001b[1m 28/232\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:34\u001b[0m 464ms/step - accuracy: 0.4947 - binary_io_u_5: 0.4947 - loss: 0.9142Batch 29, Loss Value: 0.8929\n",
      "Batch 29, Gradient Norm: 0.0002\n",
      "\u001b[1m 29/232\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:34\u001b[0m 465ms/step - accuracy: 0.4947 - binary_io_u_5: 0.4947 - loss: 0.9141Batch 30, Loss Value: 0.8929\n",
      "Batch 30, Gradient Norm: 0.0000\n",
      "\u001b[1m 30/232\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:33\u001b[0m 465ms/step - accuracy: 0.4948 - binary_io_u_5: 0.4948 - loss: 0.9141Batch 31, Loss Value: 0.8929\n",
      "Batch 31, Gradient Norm: 0.0000\n",
      "\u001b[1m 31/232\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:33\u001b[0m 465ms/step - accuracy: 0.4948 - binary_io_u_5: 0.4948 - loss: 0.9140Batch 32, Loss Value: 0.8929\n",
      "Batch 32, Gradient Norm: 0.0011\n",
      "\u001b[1m 32/232\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:33\u001b[0m 466ms/step - accuracy: 0.4948 - binary_io_u_5: 0.4948 - loss: 0.9140Batch 33, Loss Value: 0.8929\n",
      "Batch 33, Gradient Norm: 0.0010\n",
      "\u001b[1m 33/232\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:32\u001b[0m 466ms/step - accuracy: 0.4949 - binary_io_u_5: 0.4949 - loss: 0.9139Batch 34, Loss Value: 0.8929\n",
      "Batch 34, Gradient Norm: 0.0001\n",
      "\u001b[1m 34/232\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:32\u001b[0m 466ms/step - accuracy: 0.4949 - binary_io_u_5: 0.4949 - loss: 0.9139Batch 35, Loss Value: 0.8929\n",
      "Batch 35, Gradient Norm: 0.0000\n",
      "\u001b[1m 35/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:31\u001b[0m 466ms/step - accuracy: 0.4949 - binary_io_u_5: 0.4949 - loss: 0.9139Batch 36, Loss Value: 0.8929\n",
      "Batch 36, Gradient Norm: 0.0000\n",
      "\u001b[1m 36/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:31\u001b[0m 466ms/step - accuracy: 0.4948 - binary_io_u_5: 0.4948 - loss: 0.9139Batch 37, Loss Value: 0.8929\n",
      "Batch 37, Gradient Norm: 0.0007\n",
      "\u001b[1m 37/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:30\u001b[0m 466ms/step - accuracy: 0.4948 - binary_io_u_5: 0.4948 - loss: 0.9139Batch 38, Loss Value: 0.8929\n",
      "Batch 38, Gradient Norm: 0.0001\n",
      "\u001b[1m 38/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:30\u001b[0m 466ms/step - accuracy: 0.4946 - binary_io_u_5: 0.4946 - loss: 0.9139Batch 39, Loss Value: 0.8929\n",
      "Batch 39, Gradient Norm: 0.0000\n",
      "\u001b[1m 39/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:29\u001b[0m 465ms/step - accuracy: 0.4944 - binary_io_u_5: 0.4944 - loss: 0.9140Batch 40, Loss Value: 0.8929\n",
      "Batch 40, Gradient Norm: 0.0000\n",
      "\u001b[1m 40/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:29\u001b[0m 465ms/step - accuracy: 0.4942 - binary_io_u_5: 0.4942 - loss: 0.9141Batch 41, Loss Value: 0.8929\n",
      "Batch 41, Gradient Norm: 0.0000\n",
      "\u001b[1m 41/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:28\u001b[0m 465ms/step - accuracy: 0.4940 - binary_io_u_5: 0.4940 - loss: 0.9142Batch 42, Loss Value: 0.8929\n",
      "Batch 42, Gradient Norm: 0.0001\n",
      "\u001b[1m 42/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:28\u001b[0m 465ms/step - accuracy: 0.4939 - binary_io_u_5: 0.4939 - loss: 0.9142Batch 43, Loss Value: 0.8929\n",
      "Batch 43, Gradient Norm: 0.0000\n",
      "\u001b[1m 43/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:27\u001b[0m 465ms/step - accuracy: 0.4937 - binary_io_u_5: 0.4937 - loss: 0.9143Batch 44, Loss Value: 0.8929\n",
      "Batch 44, Gradient Norm: 0.0000\n",
      "\u001b[1m 44/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:27\u001b[0m 465ms/step - accuracy: 0.4936 - binary_io_u_5: 0.4936 - loss: 0.9143Batch 45, Loss Value: 0.8929\n",
      "Batch 45, Gradient Norm: 0.0021\n",
      "\u001b[1m 45/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:26\u001b[0m 465ms/step - accuracy: 0.4935 - binary_io_u_5: 0.4935 - loss: 0.9144Batch 46, Loss Value: 0.8929\n",
      "Batch 46, Gradient Norm: 0.0002\n",
      "\u001b[1m 46/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:26\u001b[0m 465ms/step - accuracy: 0.4934 - binary_io_u_5: 0.4934 - loss: 0.9144Batch 47, Loss Value: 0.8929\n",
      "Batch 47, Gradient Norm: 0.0000\n",
      "\u001b[1m 47/232\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:25\u001b[0m 465ms/step - accuracy: 0.4932 - binary_io_u_5: 0.4932 - loss: 0.9145Batch 48, Loss Value: 0.8929\n",
      "Batch 48, Gradient Norm: 0.0000\n",
      "\u001b[1m 48/232\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:25\u001b[0m 465ms/step - accuracy: 0.4931 - binary_io_u_5: 0.4931 - loss: 0.9145Batch 49, Loss Value: 0.8929\n",
      "Batch 49, Gradient Norm: 0.0000\n",
      "\u001b[1m 49/232\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:25\u001b[0m 465ms/step - accuracy: 0.4929 - binary_io_u_5: 0.4929 - loss: 0.9146Batch 50, Loss Value: 0.8929\n",
      "Batch 50, Gradient Norm: 0.0000\n",
      "\u001b[1m 50/232\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:24\u001b[0m 465ms/step - accuracy: 0.4927 - binary_io_u_5: 0.4927 - loss: 0.9147Batch 51, Loss Value: 0.8930\n",
      "Batch 51, Gradient Norm: 0.0100\n",
      "\u001b[1m 51/232\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:24\u001b[0m 465ms/step - accuracy: 0.4926 - binary_io_u_5: 0.4926 - loss: 0.9148Batch 52, Loss Value: 0.8929\n",
      "Batch 52, Gradient Norm: 0.0000\n",
      "\u001b[1m 52/232\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:23\u001b[0m 465ms/step - accuracy: 0.4925 - binary_io_u_5: 0.4925 - loss: 0.9148Batch 53, Loss Value: 0.8929\n",
      "Batch 53, Gradient Norm: 0.0001\n",
      "\u001b[1m 53/232\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:23\u001b[0m 465ms/step - accuracy: 0.4923 - binary_io_u_5: 0.4923 - loss: 0.9149Batch 54, Loss Value: 0.8929\n",
      "Batch 54, Gradient Norm: 0.0001\n",
      "\u001b[1m 54/232\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:22\u001b[0m 465ms/step - accuracy: 0.4922 - binary_io_u_5: 0.4922 - loss: 0.9149Batch 55, Loss Value: 0.8929\n",
      "Batch 55, Gradient Norm: 0.0000\n",
      "\u001b[1m 55/232\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:22\u001b[0m 465ms/step - accuracy: 0.4921 - binary_io_u_5: 0.4921 - loss: 0.9150Batch 56, Loss Value: 0.8929\n",
      "Batch 56, Gradient Norm: 0.0005\n",
      "\u001b[1m 56/232\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:21\u001b[0m 465ms/step - accuracy: 0.4919 - binary_io_u_5: 0.4919 - loss: 0.9151Batch 57, Loss Value: 0.8929\n",
      "Batch 57, Gradient Norm: 0.0001\n",
      "\u001b[1m 57/232\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:21\u001b[0m 465ms/step - accuracy: 0.4918 - binary_io_u_5: 0.4918 - loss: 0.9151Batch 58, Loss Value: 0.8929\n",
      "Batch 58, Gradient Norm: 0.0005\n",
      "\u001b[1m 58/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:20\u001b[0m 465ms/step - accuracy: 0.4917 - binary_io_u_5: 0.4917 - loss: 0.9152Batch 59, Loss Value: 0.8929\n",
      "Batch 59, Gradient Norm: 0.0001\n",
      "\u001b[1m 59/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:20\u001b[0m 465ms/step - accuracy: 0.4916 - binary_io_u_5: 0.4916 - loss: 0.9152Batch 60, Loss Value: 0.8929\n",
      "Batch 60, Gradient Norm: 0.0000\n",
      "\u001b[1m 60/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:20\u001b[0m 465ms/step - accuracy: 0.4914 - binary_io_u_5: 0.4914 - loss: 0.9153Batch 61, Loss Value: 0.8929\n",
      "Batch 61, Gradient Norm: 0.0002\n",
      "\u001b[1m 61/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:19\u001b[0m 466ms/step - accuracy: 0.4913 - binary_io_u_5: 0.4913 - loss: 0.9154Batch 62, Loss Value: 0.8929\n",
      "Batch 62, Gradient Norm: 0.0001\n",
      "\u001b[1m 62/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:19\u001b[0m 466ms/step - accuracy: 0.4912 - binary_io_u_5: 0.4912 - loss: 0.9154Batch 63, Loss Value: 0.8929\n",
      "Batch 63, Gradient Norm: 0.0012\n",
      "\u001b[1m 63/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:18\u001b[0m 466ms/step - accuracy: 0.4911 - binary_io_u_5: 0.4911 - loss: 0.9155Batch 64, Loss Value: 0.8929\n",
      "Batch 64, Gradient Norm: 0.0001\n",
      "\u001b[1m 64/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:18\u001b[0m 466ms/step - accuracy: 0.4910 - binary_io_u_5: 0.4910 - loss: 0.9155Batch 65, Loss Value: 0.8929\n",
      "Batch 65, Gradient Norm: 0.0000\n",
      "\u001b[1m 65/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:17\u001b[0m 466ms/step - accuracy: 0.4910 - binary_io_u_5: 0.4910 - loss: 0.9156Batch 66, Loss Value: 0.8929\n",
      "Batch 66, Gradient Norm: 0.0000\n",
      "\u001b[1m 66/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:17\u001b[0m 466ms/step - accuracy: 0.4909 - binary_io_u_5: 0.4909 - loss: 0.9156Batch 67, Loss Value: 0.8929\n",
      "Batch 67, Gradient Norm: 0.0000\n",
      "\u001b[1m 67/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:16\u001b[0m 466ms/step - accuracy: 0.4908 - binary_io_u_5: 0.4908 - loss: 0.9156Batch 68, Loss Value: 0.8929\n",
      "Batch 68, Gradient Norm: 0.0002\n",
      "\u001b[1m 68/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:16\u001b[0m 466ms/step - accuracy: 0.4908 - binary_io_u_5: 0.4908 - loss: 0.9157Batch 69, Loss Value: 0.8929\n",
      "Batch 69, Gradient Norm: 0.0001\n",
      "\u001b[1m 69/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:15\u001b[0m 466ms/step - accuracy: 0.4907 - binary_io_u_5: 0.4907 - loss: 0.9157Batch 70, Loss Value: 0.8929\n",
      "Batch 70, Gradient Norm: 0.0002\n",
      "\u001b[1m 70/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:15\u001b[0m 466ms/step - accuracy: 0.4907 - binary_io_u_5: 0.4907 - loss: 0.9157Batch 71, Loss Value: 0.8929\n",
      "Batch 71, Gradient Norm: 0.0002\n",
      "\u001b[1m 71/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:15\u001b[0m 466ms/step - accuracy: 0.4907 - binary_io_u_5: 0.4907 - loss: 0.9157Batch 72, Loss Value: 0.8929\n",
      "Batch 72, Gradient Norm: 0.0001\n",
      "\u001b[1m 72/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:14\u001b[0m 466ms/step - accuracy: 0.4907 - binary_io_u_5: 0.4907 - loss: 0.9157Batch 73, Loss Value: 0.8929\n",
      "Batch 73, Gradient Norm: 0.0000\n",
      "\u001b[1m 73/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:14\u001b[0m 466ms/step - accuracy: 0.4906 - binary_io_u_5: 0.4906 - loss: 0.9158Batch 74, Loss Value: 0.8929\n",
      "Batch 74, Gradient Norm: 0.0000\n",
      "\u001b[1m 74/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:13\u001b[0m 466ms/step - accuracy: 0.4906 - binary_io_u_5: 0.4906 - loss: 0.9158Batch 75, Loss Value: 0.8929\n",
      "Batch 75, Gradient Norm: 0.0004\n",
      "\u001b[1m 75/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:13\u001b[0m 466ms/step - accuracy: 0.4906 - binary_io_u_5: 0.4906 - loss: 0.9158Batch 76, Loss Value: 0.8929\n",
      "Batch 76, Gradient Norm: 0.0000\n",
      "\u001b[1m 76/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:12\u001b[0m 466ms/step - accuracy: 0.4906 - binary_io_u_5: 0.4906 - loss: 0.9158Batch 77, Loss Value: 0.8929\n",
      "Batch 77, Gradient Norm: 0.0004\n",
      "\u001b[1m 77/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:12\u001b[0m 466ms/step - accuracy: 0.4906 - binary_io_u_5: 0.4906 - loss: 0.9157Batch 78, Loss Value: 0.8929\n",
      "Batch 78, Gradient Norm: 0.0000\n",
      "\u001b[1m 78/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:11\u001b[0m 466ms/step - accuracy: 0.4906 - binary_io_u_5: 0.4906 - loss: 0.9157Batch 79, Loss Value: 0.8929\n",
      "Batch 79, Gradient Norm: 0.0004\n",
      "\u001b[1m 79/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:11\u001b[0m 466ms/step - accuracy: 0.4906 - binary_io_u_5: 0.4906 - loss: 0.9157Batch 80, Loss Value: 0.8929\n",
      "Batch 80, Gradient Norm: 0.0000\n",
      "\u001b[1m 80/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:10\u001b[0m 466ms/step - accuracy: 0.4906 - binary_io_u_5: 0.4906 - loss: 0.9157Batch 81, Loss Value: 0.8929\n",
      "Batch 81, Gradient Norm: 0.0000\n",
      "\u001b[1m 81/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:10\u001b[0m 467ms/step - accuracy: 0.4906 - binary_io_u_5: 0.4906 - loss: 0.9157Batch 82, Loss Value: 0.8929\n",
      "Batch 82, Gradient Norm: 0.0000\n",
      "\u001b[1m 82/232\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m1:09\u001b[0m 467ms/step - accuracy: 0.4906 - binary_io_u_5: 0.4906 - loss: 0.9157Batch 83, Loss Value: 0.8929\n",
      "Batch 83, Gradient Norm: 0.0001\n",
      "\u001b[1m 83/232\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m1:09\u001b[0m 467ms/step - accuracy: 0.4907 - binary_io_u_5: 0.4907 - loss: 0.9157Batch 84, Loss Value: 0.8929\n",
      "Batch 84, Gradient Norm: 0.0000\n",
      "\u001b[1m 84/232\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m1:09\u001b[0m 467ms/step - accuracy: 0.4907 - binary_io_u_5: 0.4907 - loss: 0.9157Batch 85, Loss Value: 0.8929\n",
      "Batch 85, Gradient Norm: 0.0068\n",
      "\u001b[1m 85/232\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m1:08\u001b[0m 467ms/step - accuracy: 0.4907 - binary_io_u_5: 0.4907 - loss: 0.9157Batch 86, Loss Value: 0.8929\n",
      "Batch 86, Gradient Norm: 0.0000\n",
      "\u001b[1m 86/232\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m1:08\u001b[0m 467ms/step - accuracy: 0.4907 - binary_io_u_5: 0.4907 - loss: 0.9157Batch 87, Loss Value: 0.8929\n",
      "Batch 87, Gradient Norm: 0.0008\n",
      "\u001b[1m 87/232\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m1:07\u001b[0m 467ms/step - accuracy: 0.4907 - binary_io_u_5: 0.4907 - loss: 0.9157Batch 88, Loss Value: 0.8929\n",
      "Batch 88, Gradient Norm: 0.0001\n",
      "\u001b[1m 88/232\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m1:07\u001b[0m 467ms/step - accuracy: 0.4907 - binary_io_u_5: 0.4907 - loss: 0.9157Batch 89, Loss Value: 0.8929\n",
      "Batch 89, Gradient Norm: 0.0000\n",
      "\u001b[1m 89/232\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m1:06\u001b[0m 467ms/step - accuracy: 0.4908 - binary_io_u_5: 0.4908 - loss: 0.9157Batch 90, Loss Value: 0.8929\n",
      "Batch 90, Gradient Norm: 0.0003\n",
      "\u001b[1m 90/232\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m1:06\u001b[0m 467ms/step - accuracy: 0.4908 - binary_io_u_5: 0.4908 - loss: 0.9157Batch 91, Loss Value: 0.8929\n",
      "Batch 91, Gradient Norm: 0.0003\n",
      "\u001b[1m 91/232\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m1:05\u001b[0m 467ms/step - accuracy: 0.4908 - binary_io_u_5: 0.4908 - loss: 0.9156Batch 92, Loss Value: 0.8929\n",
      "Batch 92, Gradient Norm: 0.0002\n",
      "\u001b[1m 92/232\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m1:05\u001b[0m 467ms/step - accuracy: 0.4908 - binary_io_u_5: 0.4908 - loss: 0.9156Batch 93, Loss Value: 0.8929\n",
      "Batch 93, Gradient Norm: 0.0014\n",
      "\u001b[1m 93/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m1:04\u001b[0m 468ms/step - accuracy: 0.4908 - binary_io_u_5: 0.4908 - loss: 0.9156Batch 94, Loss Value: 0.8929\n",
      "Batch 94, Gradient Norm: 0.0000\n",
      "\u001b[1m 94/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m1:04\u001b[0m 468ms/step - accuracy: 0.4908 - binary_io_u_5: 0.4908 - loss: 0.9156Batch 95, Loss Value: 0.8929\n",
      "Batch 95, Gradient Norm: 0.0000\n",
      "\u001b[1m 95/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m1:04\u001b[0m 468ms/step - accuracy: 0.4909 - binary_io_u_5: 0.4909 - loss: 0.9156Batch 96, Loss Value: 0.8929\n",
      "Batch 96, Gradient Norm: 0.0001\n",
      "\u001b[1m 96/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m1:03\u001b[0m 468ms/step - accuracy: 0.4909 - binary_io_u_5: 0.4909 - loss: 0.9156Batch 97, Loss Value: 0.8930\n",
      "Batch 97, Gradient Norm: 0.0062\n",
      "\u001b[1m 97/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m1:03\u001b[0m 468ms/step - accuracy: 0.4909 - binary_io_u_5: 0.4909 - loss: 0.9156Batch 98, Loss Value: 0.8929\n",
      "Batch 98, Gradient Norm: 0.0002\n",
      "\u001b[1m 98/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m1:02\u001b[0m 468ms/step - accuracy: 0.4909 - binary_io_u_5: 0.4909 - loss: 0.9155Batch 99, Loss Value: 0.8929\n",
      "Batch 99, Gradient Norm: 0.0002\n",
      "\u001b[1m 99/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m1:02\u001b[0m 468ms/step - accuracy: 0.4910 - binary_io_u_5: 0.4910 - loss: 0.9155Batch 100, Loss Value: 0.8929\n",
      "Batch 100, Gradient Norm: 0.0000\n",
      "\u001b[1m100/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m1:01\u001b[0m 468ms/step - accuracy: 0.4910 - binary_io_u_5: 0.4910 - loss: 0.9155Batch 101, Loss Value: 0.8929\n",
      "Batch 101, Gradient Norm: 0.0000\n",
      "\u001b[1m101/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m1:01\u001b[0m 468ms/step - accuracy: 0.4911 - binary_io_u_5: 0.4911 - loss: 0.9155Batch 102, Loss Value: 0.8929\n",
      "Batch 102, Gradient Norm: 0.0002\n",
      "\u001b[1m102/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m1:00\u001b[0m 468ms/step - accuracy: 0.4911 - binary_io_u_5: 0.4911 - loss: 0.9154Batch 103, Loss Value: 0.8929\n",
      "Batch 103, Gradient Norm: 0.0001\n",
      "\u001b[1m103/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m1:00\u001b[0m 468ms/step - accuracy: 0.4911 - binary_io_u_5: 0.4911 - loss: 0.9154Batch 104, Loss Value: 0.8929\n",
      "Batch 104, Gradient Norm: 0.0000\n",
      "\u001b[1m104/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m59s\u001b[0m 468ms/step - accuracy: 0.4912 - binary_io_u_5: 0.4912 - loss: 0.9154 Batch 105, Loss Value: 0.8929\n",
      "Batch 105, Gradient Norm: 0.0000\n",
      "\u001b[1m105/232\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m59s\u001b[0m 468ms/step - accuracy: 0.4912 - binary_io_u_5: 0.4912 - loss: 0.9153Batch 106, Loss Value: 0.8929\n",
      "Batch 106, Gradient Norm: 0.0000\n",
      "\u001b[1m106/232\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m58s\u001b[0m 468ms/step - accuracy: 0.4913 - binary_io_u_5: 0.4913 - loss: 0.9153Batch 107, Loss Value: 0.8928\n",
      "Batch 107, Gradient Norm: 0.0140\n",
      "\u001b[1m107/232\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m58s\u001b[0m 468ms/step - accuracy: 0.4913 - binary_io_u_5: 0.4913 - loss: 0.9153Batch 108, Loss Value: 0.8929\n",
      "Batch 108, Gradient Norm: 0.0000\n",
      "\u001b[1m108/232\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m58s\u001b[0m 468ms/step - accuracy: 0.4913 - binary_io_u_5: 0.4913 - loss: 0.9153Batch 109, Loss Value: 0.8929\n",
      "Batch 109, Gradient Norm: 0.0001\n",
      "\u001b[1m109/232\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m57s\u001b[0m 468ms/step - accuracy: 0.4914 - binary_io_u_5: 0.4914 - loss: 0.9152Batch 110, Loss Value: 0.8929\n",
      "Batch 110, Gradient Norm: 0.0004\n",
      "\u001b[1m110/232\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m57s\u001b[0m 468ms/step - accuracy: 0.4914 - binary_io_u_5: 0.4914 - loss: 0.9152Batch 111, Loss Value: 0.8929\n",
      "Batch 111, Gradient Norm: 0.0000\n",
      "\u001b[1m111/232\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m56s\u001b[0m 468ms/step - accuracy: 0.4914 - binary_io_u_5: 0.4914 - loss: 0.9152Batch 112, Loss Value: 0.8929\n",
      "Batch 112, Gradient Norm: 0.0000\n",
      "\u001b[1m112/232\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m56s\u001b[0m 469ms/step - accuracy: 0.4915 - binary_io_u_5: 0.4915 - loss: 0.9152Batch 113, Loss Value: 0.8929\n",
      "Batch 113, Gradient Norm: 0.0001\n",
      "\u001b[1m113/232\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m55s\u001b[0m 469ms/step - accuracy: 0.4915 - binary_io_u_5: 0.4915 - loss: 0.9152Batch 114, Loss Value: 0.8929\n",
      "Batch 114, Gradient Norm: 0.0000\n",
      "\u001b[1m114/232\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m55s\u001b[0m 469ms/step - accuracy: 0.4915 - binary_io_u_5: 0.4915 - loss: 0.9151Batch 115, Loss Value: 0.8929\n",
      "Batch 115, Gradient Norm: 0.0000\n",
      "\u001b[1m115/232\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m54s\u001b[0m 469ms/step - accuracy: 0.4916 - binary_io_u_5: 0.4916 - loss: 0.9151Batch 116, Loss Value: 0.8929\n",
      "Batch 116, Gradient Norm: 0.0005\n",
      "\u001b[1m116/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m54s\u001b[0m 469ms/step - accuracy: 0.4916 - binary_io_u_5: 0.4916 - loss: 0.9151Batch 117, Loss Value: 0.8929\n",
      "Batch 117, Gradient Norm: 0.0003\n",
      "\u001b[1m117/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m53s\u001b[0m 469ms/step - accuracy: 0.4916 - binary_io_u_5: 0.4916 - loss: 0.9151Batch 118, Loss Value: 0.8929\n",
      "Batch 118, Gradient Norm: 0.0000\n",
      "\u001b[1m118/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m53s\u001b[0m 469ms/step - accuracy: 0.4917 - binary_io_u_5: 0.4917 - loss: 0.9151Batch 119, Loss Value: 0.8929\n",
      "Batch 119, Gradient Norm: 0.0002\n",
      "\u001b[1m119/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m52s\u001b[0m 469ms/step - accuracy: 0.4917 - binary_io_u_5: 0.4917 - loss: 0.9150Batch 120, Loss Value: 0.8929\n",
      "Batch 120, Gradient Norm: 0.0013\n",
      "\u001b[1m120/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m52s\u001b[0m 469ms/step - accuracy: 0.4918 - binary_io_u_5: 0.4918 - loss: 0.9150Batch 121, Loss Value: 0.8929\n",
      "Batch 121, Gradient Norm: 0.0001\n",
      "\u001b[1m121/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m52s\u001b[0m 469ms/step - accuracy: 0.4918 - binary_io_u_5: 0.4918 - loss: 0.9150Batch 122, Loss Value: 0.8929\n",
      "Batch 122, Gradient Norm: 0.0001\n",
      "\u001b[1m122/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m51s\u001b[0m 469ms/step - accuracy: 0.4918 - binary_io_u_5: 0.4918 - loss: 0.9150Batch 123, Loss Value: 0.8929\n",
      "Batch 123, Gradient Norm: 0.0000\n",
      "\u001b[1m123/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m51s\u001b[0m 469ms/step - accuracy: 0.4918 - binary_io_u_5: 0.4918 - loss: 0.9150Batch 124, Loss Value: 0.8929\n",
      "Batch 124, Gradient Norm: 0.0003\n",
      "\u001b[1m124/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m50s\u001b[0m 469ms/step - accuracy: 0.4919 - binary_io_u_5: 0.4919 - loss: 0.9149Batch 125, Loss Value: 0.8929\n",
      "Batch 125, Gradient Norm: 0.0005\n",
      "\u001b[1m125/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m50s\u001b[0m 469ms/step - accuracy: 0.4919 - binary_io_u_5: 0.4919 - loss: 0.9149Batch 126, Loss Value: 0.8929\n",
      "Batch 126, Gradient Norm: 0.0000\n",
      "\u001b[1m126/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m49s\u001b[0m 470ms/step - accuracy: 0.4919 - binary_io_u_5: 0.4919 - loss: 0.9149Batch 127, Loss Value: 0.8929\n",
      "Batch 127, Gradient Norm: 0.0000\n",
      "\u001b[1m127/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m49s\u001b[0m 470ms/step - accuracy: 0.4920 - binary_io_u_5: 0.4920 - loss: 0.9149Batch 128, Loss Value: 0.8929\n",
      "Batch 128, Gradient Norm: 0.0001\n",
      "\u001b[1m128/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m48s\u001b[0m 470ms/step - accuracy: 0.4920 - binary_io_u_5: 0.4920 - loss: 0.9149Batch 129, Loss Value: 0.8929\n",
      "Batch 129, Gradient Norm: 0.0001\n",
      "\u001b[1m129/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m48s\u001b[0m 470ms/step - accuracy: 0.4920 - binary_io_u_5: 0.4920 - loss: 0.9149Batch 130, Loss Value: 0.8929\n",
      "Batch 130, Gradient Norm: 0.0000\n",
      "\u001b[1m130/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m47s\u001b[0m 470ms/step - accuracy: 0.4920 - binary_io_u_5: 0.4920 - loss: 0.9149Batch 131, Loss Value: 0.8929\n",
      "Batch 131, Gradient Norm: 0.0030\n",
      "\u001b[1m131/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m47s\u001b[0m 470ms/step - accuracy: 0.4921 - binary_io_u_5: 0.4921 - loss: 0.9148Batch 132, Loss Value: 0.8894\n",
      "Batch 132, Gradient Norm: 0.1657\n",
      "\u001b[1m132/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m46s\u001b[0m 470ms/step - accuracy: 0.4921 - binary_io_u_5: 0.4921 - loss: 0.9148Batch 133, Loss Value: 0.8929\n",
      "Batch 133, Gradient Norm: 0.0001\n",
      "\u001b[1m133/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m46s\u001b[0m 470ms/step - accuracy: 0.4921 - binary_io_u_5: 0.4921 - loss: 0.9148Batch 134, Loss Value: 0.8929\n",
      "Batch 134, Gradient Norm: 0.0010\n",
      "\u001b[1m134/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m46s\u001b[0m 470ms/step - accuracy: 0.4921 - binary_io_u_5: 0.4921 - loss: 0.9148Batch 135, Loss Value: 0.8929\n",
      "Batch 135, Gradient Norm: 0.0000\n",
      "\u001b[1m135/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m45s\u001b[0m 470ms/step - accuracy: 0.4922 - binary_io_u_5: 0.4922 - loss: 0.9148Batch 136, Loss Value: 0.8929\n",
      "Batch 136, Gradient Norm: 0.0002\n",
      "\u001b[1m136/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m45s\u001b[0m 470ms/step - accuracy: 0.4922 - binary_io_u_5: 0.4922 - loss: 0.9148Batch 137, Loss Value: 0.8929\n",
      "Batch 137, Gradient Norm: 0.0000\n",
      "\u001b[1m137/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m44s\u001b[0m 470ms/step - accuracy: 0.4922 - binary_io_u_5: 0.4922 - loss: 0.9148Batch 138, Loss Value: 0.8929\n",
      "Batch 138, Gradient Norm: 0.0001\n",
      "\u001b[1m138/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m44s\u001b[0m 470ms/step - accuracy: 0.4922 - binary_io_u_5: 0.4922 - loss: 0.9148Batch 139, Loss Value: 0.8929\n",
      "Batch 139, Gradient Norm: 0.0000\n",
      "\u001b[1m139/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m43s\u001b[0m 470ms/step - accuracy: 0.4923 - binary_io_u_5: 0.4923 - loss: 0.9147Batch 140, Loss Value: 0.8929\n",
      "Batch 140, Gradient Norm: 0.0015\n",
      "\u001b[1m140/232\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m43s\u001b[0m 470ms/step - accuracy: 0.4923 - binary_io_u_5: 0.4923 - loss: 0.9147Batch 141, Loss Value: 0.8929\n",
      "Batch 141, Gradient Norm: 0.0011\n",
      "\u001b[1m141/232\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m42s\u001b[0m 470ms/step - accuracy: 0.4923 - binary_io_u_5: 0.4923 - loss: 0.9147Batch 142, Loss Value: 0.8929\n",
      "Batch 142, Gradient Norm: 0.0012\n",
      "\u001b[1m142/232\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m42s\u001b[0m 470ms/step - accuracy: 0.4923 - binary_io_u_5: 0.4923 - loss: 0.9147Batch 143, Loss Value: 0.8929\n",
      "Batch 143, Gradient Norm: 0.0009\n",
      "\u001b[1m143/232\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m41s\u001b[0m 470ms/step - accuracy: 0.4923 - binary_io_u_5: 0.4923 - loss: 0.9147Batch 144, Loss Value: 0.8932\n",
      "Batch 144, Gradient Norm: 0.0283\n",
      "\u001b[1m144/232\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m41s\u001b[0m 470ms/step - accuracy: 0.4924 - binary_io_u_5: 0.4924 - loss: 0.9147Batch 145, Loss Value: 0.8929\n",
      "Batch 145, Gradient Norm: 0.0000\n",
      "\u001b[1m145/232\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m40s\u001b[0m 470ms/step - accuracy: 0.4924 - binary_io_u_5: 0.4924 - loss: 0.9147Batch 146, Loss Value: 0.8929\n",
      "Batch 146, Gradient Norm: 0.0000\n",
      "\u001b[1m146/232\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m40s\u001b[0m 470ms/step - accuracy: 0.4924 - binary_io_u_5: 0.4924 - loss: 0.9147Batch 147, Loss Value: 0.8929\n",
      "Batch 147, Gradient Norm: 0.0000\n",
      "\u001b[1m147/232\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m39s\u001b[0m 470ms/step - accuracy: 0.4925 - binary_io_u_5: 0.4924 - loss: 0.9146Batch 148, Loss Value: 0.8929\n",
      "Batch 148, Gradient Norm: 0.0000\n",
      "\u001b[1m148/232\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m39s\u001b[0m 470ms/step - accuracy: 0.4925 - binary_io_u_5: 0.4925 - loss: 0.9146Batch 149, Loss Value: 0.8929\n",
      "Batch 149, Gradient Norm: 0.0000\n",
      "\u001b[1m149/232\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m39s\u001b[0m 470ms/step - accuracy: 0.4925 - binary_io_u_5: 0.4925 - loss: 0.9146Batch 150, Loss Value: 0.8929\n",
      "Batch 150, Gradient Norm: 0.0001\n",
      "\u001b[1m150/232\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m38s\u001b[0m 470ms/step - accuracy: 0.4925 - binary_io_u_5: 0.4925 - loss: 0.9146Batch 151, Loss Value: 0.8929\n",
      "Batch 151, Gradient Norm: 0.0002\n",
      "\u001b[1m151/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m38s\u001b[0m 470ms/step - accuracy: 0.4925 - binary_io_u_5: 0.4925 - loss: 0.9146Batch 152, Loss Value: 0.8929\n",
      "Batch 152, Gradient Norm: 0.0001\n",
      "\u001b[1m152/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m37s\u001b[0m 470ms/step - accuracy: 0.4926 - binary_io_u_5: 0.4926 - loss: 0.9146Batch 153, Loss Value: 0.8929\n",
      "Batch 153, Gradient Norm: 0.0001\n",
      "\u001b[1m153/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m37s\u001b[0m 470ms/step - accuracy: 0.4926 - binary_io_u_5: 0.4926 - loss: 0.9146Batch 154, Loss Value: 0.8929\n",
      "Batch 154, Gradient Norm: 0.0001\n",
      "\u001b[1m154/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m36s\u001b[0m 470ms/step - accuracy: 0.4926 - binary_io_u_5: 0.4926 - loss: 0.9146Batch 155, Loss Value: 0.8929\n",
      "Batch 155, Gradient Norm: 0.0001\n",
      "\u001b[1m155/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m36s\u001b[0m 470ms/step - accuracy: 0.4926 - binary_io_u_5: 0.4926 - loss: 0.9145Batch 156, Loss Value: 0.8929\n",
      "Batch 156, Gradient Norm: 0.0000\n",
      "\u001b[1m156/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m35s\u001b[0m 470ms/step - accuracy: 0.4926 - binary_io_u_5: 0.4926 - loss: 0.9145Batch 157, Loss Value: 0.8929\n",
      "Batch 157, Gradient Norm: 0.0001\n",
      "\u001b[1m157/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m35s\u001b[0m 470ms/step - accuracy: 0.4927 - binary_io_u_5: 0.4927 - loss: 0.9145Batch 158, Loss Value: 0.8929\n",
      "Batch 158, Gradient Norm: 0.0001\n",
      "\u001b[1m158/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m34s\u001b[0m 470ms/step - accuracy: 0.4927 - binary_io_u_5: 0.4927 - loss: 0.9145Batch 159, Loss Value: 0.8929\n",
      "Batch 159, Gradient Norm: 0.0002\n",
      "\u001b[1m159/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m34s\u001b[0m 470ms/step - accuracy: 0.4927 - binary_io_u_5: 0.4927 - loss: 0.9145Batch 160, Loss Value: 0.8929\n",
      "Batch 160, Gradient Norm: 0.0002\n",
      "\u001b[1m160/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m33s\u001b[0m 470ms/step - accuracy: 0.4927 - binary_io_u_5: 0.4927 - loss: 0.9145Batch 161, Loss Value: 0.8930\n",
      "Batch 161, Gradient Norm: 0.0049\n",
      "\u001b[1m161/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m33s\u001b[0m 471ms/step - accuracy: 0.4927 - binary_io_u_5: 0.4927 - loss: 0.9145Batch 162, Loss Value: 0.8929\n",
      "Batch 162, Gradient Norm: 0.0000\n",
      "\u001b[1m162/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m32s\u001b[0m 471ms/step - accuracy: 0.4927 - binary_io_u_5: 0.4927 - loss: 0.9145Batch 163, Loss Value: 0.8929\n",
      "Batch 163, Gradient Norm: 0.0004\n",
      "\u001b[1m163/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m32s\u001b[0m 471ms/step - accuracy: 0.4928 - binary_io_u_5: 0.4928 - loss: 0.9145Batch 164, Loss Value: 0.8929\n",
      "Batch 164, Gradient Norm: 0.0005\n",
      "\u001b[1m164/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m32s\u001b[0m 471ms/step - accuracy: 0.4928 - binary_io_u_5: 0.4928 - loss: 0.9145Batch 165, Loss Value: 0.8929\n",
      "Batch 165, Gradient Norm: 0.0000\n",
      "\u001b[1m165/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m31s\u001b[0m 471ms/step - accuracy: 0.4928 - binary_io_u_5: 0.4928 - loss: 0.9145Batch 166, Loss Value: 0.8929\n",
      "Batch 166, Gradient Norm: 0.0005\n",
      "\u001b[1m166/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m31s\u001b[0m 471ms/step - accuracy: 0.4928 - binary_io_u_5: 0.4928 - loss: 0.9145Batch 167, Loss Value: 0.8930\n",
      "Batch 167, Gradient Norm: 0.0054\n",
      "\u001b[1m167/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m30s\u001b[0m 471ms/step - accuracy: 0.4928 - binary_io_u_5: 0.4928 - loss: 0.9144Batch 168, Loss Value: 0.8929\n",
      "Batch 168, Gradient Norm: 0.0014\n",
      "\u001b[1m168/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m30s\u001b[0m 471ms/step - accuracy: 0.4928 - binary_io_u_5: 0.4928 - loss: 0.9144Batch 169, Loss Value: 0.8929\n",
      "Batch 169, Gradient Norm: 0.0001\n",
      "\u001b[1m169/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m29s\u001b[0m 471ms/step - accuracy: 0.4929 - binary_io_u_5: 0.4929 - loss: 0.9144Batch 170, Loss Value: 0.8929\n",
      "Batch 170, Gradient Norm: 0.0007\n",
      "\u001b[1m170/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m29s\u001b[0m 471ms/step - accuracy: 0.4929 - binary_io_u_5: 0.4929 - loss: 0.9144Batch 171, Loss Value: 0.8929\n",
      "Batch 171, Gradient Norm: 0.0000\n",
      "\u001b[1m171/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m28s\u001b[0m 471ms/step - accuracy: 0.4929 - binary_io_u_5: 0.4929 - loss: 0.9144Batch 172, Loss Value: 0.8929\n",
      "Batch 172, Gradient Norm: 0.0000\n",
      "\u001b[1m172/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m28s\u001b[0m 471ms/step - accuracy: 0.4929 - binary_io_u_5: 0.4929 - loss: 0.9144Batch 173, Loss Value: 0.8929\n",
      "Batch 173, Gradient Norm: 0.0000\n",
      "\u001b[1m173/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m27s\u001b[0m 471ms/step - accuracy: 0.4929 - binary_io_u_5: 0.4929 - loss: 0.9144Batch 174, Loss Value: 0.8929\n",
      "Batch 174, Gradient Norm: 0.0003\n",
      "\u001b[1m174/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m27s\u001b[0m 471ms/step - accuracy: 0.4929 - binary_io_u_5: 0.4929 - loss: 0.9144Batch 175, Loss Value: 0.8929\n",
      "Batch 175, Gradient Norm: 0.0002\n",
      "\u001b[1m175/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m26s\u001b[0m 471ms/step - accuracy: 0.4930 - binary_io_u_5: 0.4930 - loss: 0.9144Batch 176, Loss Value: 0.8929\n",
      "Batch 176, Gradient Norm: 0.0005\n",
      "\u001b[1m176/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m26s\u001b[0m 472ms/step - accuracy: 0.4930 - binary_io_u_5: 0.4930 - loss: 0.9144Batch 177, Loss Value: 0.8929\n",
      "Batch 177, Gradient Norm: 0.0000\n",
      "\u001b[1m177/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m25s\u001b[0m 472ms/step - accuracy: 0.4930 - binary_io_u_5: 0.4930 - loss: 0.9144Batch 178, Loss Value: 0.8929\n",
      "Batch 178, Gradient Norm: 0.0000\n",
      "\u001b[1m178/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m25s\u001b[0m 472ms/step - accuracy: 0.4930 - binary_io_u_5: 0.4930 - loss: 0.9144Batch 179, Loss Value: 0.8929\n",
      "Batch 179, Gradient Norm: 0.0001\n",
      "\u001b[1m179/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m24s\u001b[0m 472ms/step - accuracy: 0.4930 - binary_io_u_5: 0.4930 - loss: 0.9144Batch 180, Loss Value: 0.8929\n",
      "Batch 180, Gradient Norm: 0.0004\n",
      "\u001b[1m180/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m24s\u001b[0m 472ms/step - accuracy: 0.4930 - binary_io_u_5: 0.4930 - loss: 0.9144Batch 181, Loss Value: 0.8929\n",
      "Batch 181, Gradient Norm: 0.0000\n",
      "\u001b[1m181/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m24s\u001b[0m 472ms/step - accuracy: 0.4930 - binary_io_u_5: 0.4930 - loss: 0.9144Batch 182, Loss Value: 0.8929\n",
      "Batch 182, Gradient Norm: 0.0004\n",
      "\u001b[1m182/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m23s\u001b[0m 472ms/step - accuracy: 0.4931 - binary_io_u_5: 0.4931 - loss: 0.9143Batch 183, Loss Value: 0.8929\n",
      "Batch 183, Gradient Norm: 0.0000\n",
      "\u001b[1m183/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m23s\u001b[0m 472ms/step - accuracy: 0.4931 - binary_io_u_5: 0.4931 - loss: 0.9143Batch 184, Loss Value: 0.8929\n",
      "Batch 184, Gradient Norm: 0.0000\n",
      "\u001b[1m184/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m22s\u001b[0m 472ms/step - accuracy: 0.4931 - binary_io_u_5: 0.4931 - loss: 0.9143Batch 185, Loss Value: 0.8929\n",
      "Batch 185, Gradient Norm: 0.0001\n",
      "\u001b[1m185/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m22s\u001b[0m 472ms/step - accuracy: 0.4931 - binary_io_u_5: 0.4931 - loss: 0.9143Batch 186, Loss Value: 0.8929\n",
      "Batch 186, Gradient Norm: 0.0002\n",
      "\u001b[1m186/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m21s\u001b[0m 472ms/step - accuracy: 0.4931 - binary_io_u_5: 0.4931 - loss: 0.9143Batch 187, Loss Value: 0.8930\n",
      "Batch 187, Gradient Norm: 0.0110\n",
      "\u001b[1m187/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m21s\u001b[0m 472ms/step - accuracy: 0.4931 - binary_io_u_5: 0.4931 - loss: 0.9143Batch 188, Loss Value: 0.8929\n",
      "Batch 188, Gradient Norm: 0.0001\n",
      "\u001b[1m188/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m20s\u001b[0m 472ms/step - accuracy: 0.4931 - binary_io_u_5: 0.4931 - loss: 0.9143Batch 189, Loss Value: 0.8929\n",
      "Batch 189, Gradient Norm: 0.0000\n",
      "\u001b[1m189/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m20s\u001b[0m 472ms/step - accuracy: 0.4931 - binary_io_u_5: 0.4931 - loss: 0.9143Batch 190, Loss Value: 0.8929\n",
      "Batch 190, Gradient Norm: 0.0001\n",
      "\u001b[1m190/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m19s\u001b[0m 472ms/step - accuracy: 0.4931 - binary_io_u_5: 0.4931 - loss: 0.9143Batch 191, Loss Value: 0.8929\n",
      "Batch 191, Gradient Norm: 0.0000\n",
      "\u001b[1m191/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m19s\u001b[0m 472ms/step - accuracy: 0.4931 - binary_io_u_5: 0.4931 - loss: 0.9143Batch 192, Loss Value: 0.8929\n",
      "Batch 192, Gradient Norm: 0.0000\n",
      "\u001b[1m192/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m18s\u001b[0m 472ms/step - accuracy: 0.4932 - binary_io_u_5: 0.4932 - loss: 0.9143Batch 193, Loss Value: 0.8929\n",
      "Batch 193, Gradient Norm: 0.0002\n",
      "\u001b[1m193/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m18s\u001b[0m 472ms/step - accuracy: 0.4932 - binary_io_u_5: 0.4932 - loss: 0.9143Batch 194, Loss Value: 0.8929\n",
      "Batch 194, Gradient Norm: 0.0000\n",
      "\u001b[1m194/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m17s\u001b[0m 472ms/step - accuracy: 0.4932 - binary_io_u_5: 0.4932 - loss: 0.9143Batch 195, Loss Value: 0.8929\n",
      "Batch 195, Gradient Norm: 0.0003\n",
      "\u001b[1m195/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m17s\u001b[0m 472ms/step - accuracy: 0.4932 - binary_io_u_5: 0.4932 - loss: 0.9143Batch 196, Loss Value: 0.8929\n",
      "Batch 196, Gradient Norm: 0.0008\n",
      "\u001b[1m196/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m16s\u001b[0m 472ms/step - accuracy: 0.4932 - binary_io_u_5: 0.4932 - loss: 0.9143Batch 197, Loss Value: 0.8929\n",
      "Batch 197, Gradient Norm: 0.0003\n",
      "\u001b[1m197/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m16s\u001b[0m 472ms/step - accuracy: 0.4932 - binary_io_u_5: 0.4932 - loss: 0.9143Batch 198, Loss Value: 0.8929\n",
      "Batch 198, Gradient Norm: 0.0001\n",
      "\u001b[1m198/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m16s\u001b[0m 472ms/step - accuracy: 0.4932 - binary_io_u_5: 0.4932 - loss: 0.9143Batch 199, Loss Value: 0.8929\n",
      "Batch 199, Gradient Norm: 0.0001\n",
      "\u001b[1m199/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m15s\u001b[0m 472ms/step - accuracy: 0.4932 - binary_io_u_5: 0.4932 - loss: 0.9143Batch 200, Loss Value: 0.8929\n",
      "Batch 200, Gradient Norm: 0.0004\n",
      "\u001b[1m200/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m15s\u001b[0m 472ms/step - accuracy: 0.4932 - binary_io_u_5: 0.4932 - loss: 0.9143Batch 201, Loss Value: 0.8929\n",
      "Batch 201, Gradient Norm: 0.0001\n",
      "\u001b[1m201/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m14s\u001b[0m 472ms/step - accuracy: 0.4932 - binary_io_u_5: 0.4932 - loss: 0.9143Batch 202, Loss Value: 0.8929\n",
      "Batch 202, Gradient Norm: 0.0001\n",
      "\u001b[1m202/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m14s\u001b[0m 472ms/step - accuracy: 0.4932 - binary_io_u_5: 0.4932 - loss: 0.9143Batch 203, Loss Value: 0.8929\n",
      "Batch 203, Gradient Norm: 0.0008\n",
      "\u001b[1m203/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m13s\u001b[0m 472ms/step - accuracy: 0.4933 - binary_io_u_5: 0.4933 - loss: 0.9143Batch 204, Loss Value: 0.8929\n",
      "Batch 204, Gradient Norm: 0.0005\n",
      "\u001b[1m204/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m13s\u001b[0m 472ms/step - accuracy: 0.4933 - binary_io_u_5: 0.4933 - loss: 0.9143Batch 205, Loss Value: 0.8929\n",
      "Batch 205, Gradient Norm: 0.0001\n",
      "\u001b[1m205/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m12s\u001b[0m 472ms/step - accuracy: 0.4933 - binary_io_u_5: 0.4933 - loss: 0.9143Batch 206, Loss Value: 0.8929\n",
      "Batch 206, Gradient Norm: 0.0001\n",
      "\u001b[1m206/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m12s\u001b[0m 472ms/step - accuracy: 0.4933 - binary_io_u_5: 0.4933 - loss: 0.9143Batch 207, Loss Value: 0.8929\n",
      "Batch 207, Gradient Norm: 0.0000\n",
      "\u001b[1m207/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m11s\u001b[0m 472ms/step - accuracy: 0.4933 - binary_io_u_5: 0.4933 - loss: 0.9143Batch 208, Loss Value: 0.8929\n",
      "Batch 208, Gradient Norm: 0.0002\n",
      "\u001b[1m208/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m11s\u001b[0m 472ms/step - accuracy: 0.4933 - binary_io_u_5: 0.4933 - loss: 0.9142Batch 209, Loss Value: 0.8929\n",
      "Batch 209, Gradient Norm: 0.0002\n",
      "\u001b[1m209/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m10s\u001b[0m 472ms/step - accuracy: 0.4933 - binary_io_u_5: 0.4933 - loss: 0.9142Batch 210, Loss Value: 0.8929\n",
      "Batch 210, Gradient Norm: 0.0000\n",
      "\u001b[1m210/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m10s\u001b[0m 472ms/step - accuracy: 0.4933 - binary_io_u_5: 0.4933 - loss: 0.9142Batch 211, Loss Value: 0.8929\n",
      "Batch 211, Gradient Norm: 0.0000\n",
      "\u001b[1m211/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m9s\u001b[0m 472ms/step - accuracy: 0.4933 - binary_io_u_5: 0.4933 - loss: 0.9142 Batch 212, Loss Value: 0.8929\n",
      "Batch 212, Gradient Norm: 0.0001\n",
      "\u001b[1m212/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m9s\u001b[0m 472ms/step - accuracy: 0.4933 - binary_io_u_5: 0.4933 - loss: 0.9142Batch 213, Loss Value: 0.8929\n",
      "Batch 213, Gradient Norm: 0.0010\n",
      "\u001b[1m213/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m8s\u001b[0m 472ms/step - accuracy: 0.4933 - binary_io_u_5: 0.4933 - loss: 0.9142Batch 214, Loss Value: 0.8929\n",
      "Batch 214, Gradient Norm: 0.0000\n",
      "\u001b[1m214/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m8s\u001b[0m 472ms/step - accuracy: 0.4933 - binary_io_u_5: 0.4933 - loss: 0.9142Batch 215, Loss Value: 0.8929\n",
      "Batch 215, Gradient Norm: 0.0001\n",
      "\u001b[1m215/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m8s\u001b[0m 472ms/step - accuracy: 0.4934 - binary_io_u_5: 0.4934 - loss: 0.9142Batch 216, Loss Value: 0.8937\n",
      "Batch 216, Gradient Norm: 0.0806\n",
      "\u001b[1m216/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m7s\u001b[0m 472ms/step - accuracy: 0.4934 - binary_io_u_5: 0.4934 - loss: 0.9142Batch 217, Loss Value: 0.8929\n",
      "Batch 217, Gradient Norm: 0.0000\n",
      "\u001b[1m217/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m7s\u001b[0m 472ms/step - accuracy: 0.4934 - binary_io_u_5: 0.4934 - loss: 0.9142Batch 218, Loss Value: 0.8929\n",
      "Batch 218, Gradient Norm: 0.0000\n",
      "\u001b[1m218/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m6s\u001b[0m 472ms/step - accuracy: 0.4934 - binary_io_u_5: 0.4934 - loss: 0.9142Batch 219, Loss Value: 0.8929\n",
      "Batch 219, Gradient Norm: 0.0000\n",
      "\u001b[1m219/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m6s\u001b[0m 472ms/step - accuracy: 0.4934 - binary_io_u_5: 0.4934 - loss: 0.9142Batch 220, Loss Value: 0.8929\n",
      "Batch 220, Gradient Norm: 0.0000\n",
      "\u001b[1m220/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m5s\u001b[0m 472ms/step - accuracy: 0.4934 - binary_io_u_5: 0.4934 - loss: 0.9142Batch 221, Loss Value: 0.8929\n",
      "Batch 221, Gradient Norm: 0.0000\n",
      "\u001b[1m221/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m5s\u001b[0m 472ms/step - accuracy: 0.4934 - binary_io_u_5: 0.4934 - loss: 0.9142Batch 222, Loss Value: 0.8929\n",
      "Batch 222, Gradient Norm: 0.0001\n",
      "\u001b[1m222/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m4s\u001b[0m 472ms/step - accuracy: 0.4934 - binary_io_u_5: 0.4934 - loss: 0.9142Batch 223, Loss Value: 0.8929\n",
      "Batch 223, Gradient Norm: 0.0000\n",
      "\u001b[1m223/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m4s\u001b[0m 472ms/step - accuracy: 0.4934 - binary_io_u_5: 0.4934 - loss: 0.9142Batch 224, Loss Value: 0.8929\n",
      "Batch 224, Gradient Norm: 0.0013\n",
      "\u001b[1m224/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m3s\u001b[0m 472ms/step - accuracy: 0.4934 - binary_io_u_5: 0.4934 - loss: 0.9142Batch 225, Loss Value: 0.8929\n",
      "Batch 225, Gradient Norm: 0.0001\n",
      "\u001b[1m225/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m3s\u001b[0m 472ms/step - accuracy: 0.4934 - binary_io_u_5: 0.4934 - loss: 0.9142Batch 226, Loss Value: 0.8929\n",
      "Batch 226, Gradient Norm: 0.0002\n",
      "\u001b[1m226/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m2s\u001b[0m 472ms/step - accuracy: 0.4934 - binary_io_u_5: 0.4934 - loss: 0.9142Batch 227, Loss Value: 0.8929\n",
      "Batch 227, Gradient Norm: 0.0005\n",
      "\u001b[1m227/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m2s\u001b[0m 472ms/step - accuracy: 0.4934 - binary_io_u_5: 0.4934 - loss: 0.9142Batch 228, Loss Value: 0.8929\n",
      "Batch 228, Gradient Norm: 0.0002\n",
      "\u001b[1m228/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m1s\u001b[0m 472ms/step - accuracy: 0.4935 - binary_io_u_5: 0.4935 - loss: 0.9142Batch 229, Loss Value: 0.8929\n",
      "Batch 229, Gradient Norm: 0.0000\n",
      "\u001b[1m229/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m1s\u001b[0m 472ms/step - accuracy: 0.4935 - binary_io_u_5: 0.4935 - loss: 0.9142Batch 230, Loss Value: 0.8929\n",
      "Batch 230, Gradient Norm: 0.0000\n",
      "\u001b[1m230/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 472ms/step - accuracy: 0.4935 - binary_io_u_5: 0.4935 - loss: 0.9142Batch 231, Loss Value: 0.8929\n",
      "Batch 231, Gradient Norm: 0.0000\n",
      "\u001b[1m231/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 472ms/step - accuracy: 0.4935 - binary_io_u_5: 0.4935 - loss: 0.9142Batch 232, Loss Value: 0.8929\n",
      "Batch 232, Gradient Norm: 0.0002\n",
      "\u001b[1m232/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m111s\u001b[0m 478ms/step - accuracy: 0.4935 - binary_io_u_5: 0.4935 - loss: 0.9142 - val_accuracy: 0.4949 - val_binary_io_u_5: 0.4949 - val_loss: 0.9122 - learning_rate: 0.0010\n",
      "Epoch 16/200\n",
      "Batch 1, Loss Value: 0.9204\n",
      "Batch 1, Gradient Norm: 0.0010\n",
      "\u001b[1m  1/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:54\u001b[0m 497ms/step - accuracy: 0.4500 - binary_io_u_5: 0.4500 - loss: 0.9443Batch 2, Loss Value: 0.9204\n",
      "Batch 2, Gradient Norm: 0.0000\n",
      "\u001b[1m  2/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:48\u001b[0m 471ms/step - accuracy: 0.4425 - binary_io_u_5: 0.4425 - loss: 0.9449Batch 3, Loss Value: 0.9204\n",
      "Batch 3, Gradient Norm: 0.0000\n",
      "\u001b[1m  3/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:47\u001b[0m 470ms/step - accuracy: 0.4428 - binary_io_u_5: 0.4428 - loss: 0.9442Batch 4, Loss Value: 0.9204\n",
      "Batch 4, Gradient Norm: 0.0003\n",
      "\u001b[1m  4/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:47\u001b[0m 472ms/step - accuracy: 0.4452 - binary_io_u_5: 0.4452 - loss: 0.9429Batch 5, Loss Value: 0.9204\n",
      "Batch 5, Gradient Norm: 0.0004\n",
      "\u001b[1m  5/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:46\u001b[0m 471ms/step - accuracy: 0.4486 - binary_io_u_5: 0.4486 - loss: 0.9411Batch 6, Loss Value: 0.9204\n",
      "Batch 6, Gradient Norm: 0.0001\n",
      "\u001b[1m  6/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:46\u001b[0m 472ms/step - accuracy: 0.4502 - binary_io_u_5: 0.4502 - loss: 0.9402Batch 7, Loss Value: 0.9204\n",
      "Batch 7, Gradient Norm: 0.0002\n",
      "\u001b[1m  7/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:46\u001b[0m 474ms/step - accuracy: 0.4542 - binary_io_u_5: 0.4542 - loss: 0.9377Batch 8, Loss Value: 0.9204\n",
      "Batch 8, Gradient Norm: 0.0001\n",
      "\u001b[1m  8/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:46\u001b[0m 475ms/step - accuracy: 0.4586 - binary_io_u_5: 0.4586 - loss: 0.9351Batch 9, Loss Value: 0.9204\n",
      "Batch 9, Gradient Norm: 0.0008\n",
      "\u001b[1m  9/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:46\u001b[0m 476ms/step - accuracy: 0.4616 - binary_io_u_5: 0.4616 - loss: 0.9333Batch 10, Loss Value: 0.9204\n",
      "Batch 10, Gradient Norm: 0.0002\n",
      "\u001b[1m 10/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:45\u001b[0m 475ms/step - accuracy: 0.4639 - binary_io_u_5: 0.4639 - loss: 0.9320Batch 11, Loss Value: 0.9204\n",
      "Batch 11, Gradient Norm: 0.0000\n",
      "\u001b[1m 11/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:44\u001b[0m 475ms/step - accuracy: 0.4656 - binary_io_u_5: 0.4656 - loss: 0.9311Batch 12, Loss Value: 0.9204\n",
      "Batch 12, Gradient Norm: 0.0006\n",
      "\u001b[1m 12/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:44\u001b[0m 474ms/step - accuracy: 0.4676 - binary_io_u_5: 0.4676 - loss: 0.9299Batch 13, Loss Value: 0.9204\n",
      "Batch 13, Gradient Norm: 0.0006\n",
      "\u001b[1m 13/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:43\u001b[0m 474ms/step - accuracy: 0.4694 - binary_io_u_5: 0.4694 - loss: 0.9290Batch 14, Loss Value: 0.9203\n",
      "Batch 14, Gradient Norm: 0.0058\n",
      "\u001b[1m 14/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:43\u001b[0m 474ms/step - accuracy: 0.4707 - binary_io_u_5: 0.4707 - loss: 0.9282Batch 15, Loss Value: 0.9204\n",
      "Batch 15, Gradient Norm: 0.0016\n",
      "\u001b[1m 15/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:42\u001b[0m 475ms/step - accuracy: 0.4719 - binary_io_u_5: 0.4719 - loss: 0.9276Batch 16, Loss Value: 0.9204\n",
      "Batch 16, Gradient Norm: 0.0016\n",
      "\u001b[1m 16/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:42\u001b[0m 475ms/step - accuracy: 0.4732 - binary_io_u_5: 0.4732 - loss: 0.9269Batch 17, Loss Value: 0.9204\n",
      "Batch 17, Gradient Norm: 0.0008\n",
      "\u001b[1m 17/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:42\u001b[0m 475ms/step - accuracy: 0.4745 - binary_io_u_5: 0.4745 - loss: 0.9262Batch 18, Loss Value: 0.9204\n",
      "Batch 18, Gradient Norm: 0.0001\n",
      "\u001b[1m 18/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:41\u001b[0m 476ms/step - accuracy: 0.4754 - binary_io_u_5: 0.4754 - loss: 0.9256Batch 19, Loss Value: 0.9204\n",
      "Batch 19, Gradient Norm: 0.0008\n",
      "\u001b[1m 19/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:41\u001b[0m 475ms/step - accuracy: 0.4763 - binary_io_u_5: 0.4763 - loss: 0.9251Batch 20, Loss Value: 0.9204\n",
      "Batch 20, Gradient Norm: 0.0005\n",
      "\u001b[1m 20/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:40\u001b[0m 474ms/step - accuracy: 0.4772 - binary_io_u_5: 0.4772 - loss: 0.9246Batch 21, Loss Value: 0.9204\n",
      "Batch 21, Gradient Norm: 0.0010\n",
      "\u001b[1m 21/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:40\u001b[0m 475ms/step - accuracy: 0.4780 - binary_io_u_5: 0.4780 - loss: 0.9241Batch 22, Loss Value: 0.9204\n",
      "Batch 22, Gradient Norm: 0.0001\n",
      "\u001b[1m 22/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:39\u001b[0m 475ms/step - accuracy: 0.4787 - binary_io_u_5: 0.4787 - loss: 0.9237Batch 23, Loss Value: 0.9204\n",
      "Batch 23, Gradient Norm: 0.0005\n",
      "\u001b[1m 23/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:39\u001b[0m 475ms/step - accuracy: 0.4793 - binary_io_u_5: 0.4793 - loss: 0.9233Batch 24, Loss Value: 0.9204\n",
      "Batch 24, Gradient Norm: 0.0033\n",
      "\u001b[1m 24/232\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:38\u001b[0m 475ms/step - accuracy: 0.4799 - binary_io_u_5: 0.4799 - loss: 0.9230Batch 25, Loss Value: 0.9204\n",
      "Batch 25, Gradient Norm: 0.0000\n",
      "\u001b[1m 25/232\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:38\u001b[0m 475ms/step - accuracy: 0.4805 - binary_io_u_5: 0.4805 - loss: 0.9227Batch 26, Loss Value: 0.9204\n",
      "Batch 26, Gradient Norm: 0.0010\n",
      "\u001b[1m 26/232\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:37\u001b[0m 475ms/step - accuracy: 0.4812 - binary_io_u_5: 0.4812 - loss: 0.9223Batch 27, Loss Value: 0.9204\n",
      "Batch 27, Gradient Norm: 0.0004\n",
      "\u001b[1m 27/232\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:37\u001b[0m 474ms/step - accuracy: 0.4819 - binary_io_u_5: 0.4819 - loss: 0.9218Batch 28, Loss Value: 0.9203\n",
      "Batch 28, Gradient Norm: 0.0099\n",
      "\u001b[1m 28/232\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:36\u001b[0m 474ms/step - accuracy: 0.4825 - binary_io_u_5: 0.4825 - loss: 0.9215Batch 29, Loss Value: 0.9204\n",
      "Batch 29, Gradient Norm: 0.0018\n",
      "\u001b[1m 29/232\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:36\u001b[0m 475ms/step - accuracy: 0.4831 - binary_io_u_5: 0.4831 - loss: 0.9212Batch 30, Loss Value: 0.9204\n",
      "Batch 30, Gradient Norm: 0.0007\n",
      "\u001b[1m 30/232\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:35\u001b[0m 475ms/step - accuracy: 0.4836 - binary_io_u_5: 0.4836 - loss: 0.9208Batch 31, Loss Value: 0.9205\n",
      "Batch 31, Gradient Norm: 0.0056\n",
      "\u001b[1m 31/232\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:35\u001b[0m 474ms/step - accuracy: 0.4842 - binary_io_u_5: 0.4842 - loss: 0.9205Batch 32, Loss Value: 0.9204\n",
      "Batch 32, Gradient Norm: 0.0007\n",
      "\u001b[1m 32/232\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:34\u001b[0m 474ms/step - accuracy: 0.4848 - binary_io_u_5: 0.4848 - loss: 0.9201Batch 33, Loss Value: 0.9204\n",
      "Batch 33, Gradient Norm: 0.0003\n",
      "\u001b[1m 33/232\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:34\u001b[0m 474ms/step - accuracy: 0.4852 - binary_io_u_5: 0.4852 - loss: 0.9198Batch 34, Loss Value: 0.9204\n",
      "Batch 34, Gradient Norm: 0.0006\n",
      "\u001b[1m 34/232\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:33\u001b[0m 474ms/step - accuracy: 0.4857 - binary_io_u_5: 0.4857 - loss: 0.9195Batch 35, Loss Value: 0.9204\n",
      "Batch 35, Gradient Norm: 0.0008\n",
      "\u001b[1m 35/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:33\u001b[0m 474ms/step - accuracy: 0.4861 - binary_io_u_5: 0.4861 - loss: 0.9193Batch 36, Loss Value: 0.9204\n",
      "Batch 36, Gradient Norm: 0.0026\n",
      "\u001b[1m 36/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:32\u001b[0m 474ms/step - accuracy: 0.4865 - binary_io_u_5: 0.4865 - loss: 0.9190Batch 37, Loss Value: 0.9204\n",
      "Batch 37, Gradient Norm: 0.0028\n",
      "\u001b[1m 37/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:32\u001b[0m 474ms/step - accuracy: 0.4869 - binary_io_u_5: 0.4869 - loss: 0.9187Batch 38, Loss Value: 0.9204\n",
      "Batch 38, Gradient Norm: 0.0011\n",
      "\u001b[1m 38/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:31\u001b[0m 474ms/step - accuracy: 0.4873 - binary_io_u_5: 0.4873 - loss: 0.9185Batch 39, Loss Value: 0.9204\n",
      "Batch 39, Gradient Norm: 0.0036\n",
      "\u001b[1m 39/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:31\u001b[0m 474ms/step - accuracy: 0.4877 - binary_io_u_5: 0.4877 - loss: 0.9182Batch 40, Loss Value: 0.9202\n",
      "Batch 40, Gradient Norm: 0.0141\n",
      "\u001b[1m 40/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:31\u001b[0m 474ms/step - accuracy: 0.4880 - binary_io_u_5: 0.4880 - loss: 0.9180Batch 41, Loss Value: 0.9204\n",
      "Batch 41, Gradient Norm: 0.0019\n",
      "\u001b[1m 41/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:30\u001b[0m 474ms/step - accuracy: 0.4884 - binary_io_u_5: 0.4884 - loss: 0.9177Batch 42, Loss Value: 0.9203\n",
      "Batch 42, Gradient Norm: 0.0149\n",
      "\u001b[1m 42/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:30\u001b[0m 474ms/step - accuracy: 0.4887 - binary_io_u_5: 0.4887 - loss: 0.9175Batch 43, Loss Value: 0.9204\n",
      "Batch 43, Gradient Norm: 0.0024\n",
      "\u001b[1m 43/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:29\u001b[0m 474ms/step - accuracy: 0.4890 - binary_io_u_5: 0.4890 - loss: 0.9174Batch 44, Loss Value: 0.9204\n",
      "Batch 44, Gradient Norm: 0.0022\n",
      "\u001b[1m 44/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:29\u001b[0m 474ms/step - accuracy: 0.4892 - binary_io_u_5: 0.4892 - loss: 0.9172Batch 45, Loss Value: 0.9195\n",
      "Batch 45, Gradient Norm: 0.0633\n",
      "\u001b[1m 45/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:28\u001b[0m 473ms/step - accuracy: 0.4893 - binary_io_u_5: 0.4893 - loss: 0.9171Batch 46, Loss Value: 0.9203\n",
      "Batch 46, Gradient Norm: 0.0054\n",
      "\u001b[1m 46/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:28\u001b[0m 473ms/step - accuracy: 0.4895 - binary_io_u_5: 0.4895 - loss: 0.9170Batch 47, Loss Value: 0.9200\n",
      "Batch 47, Gradient Norm: 0.0279\n",
      "\u001b[1m 47/232\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:27\u001b[0m 473ms/step - accuracy: 0.4897 - binary_io_u_5: 0.4897 - loss: 0.9168Batch 48, Loss Value: 0.9204\n",
      "Batch 48, Gradient Norm: 0.0021\n",
      "\u001b[1m 48/232\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:27\u001b[0m 473ms/step - accuracy: 0.4899 - binary_io_u_5: 0.4899 - loss: 0.9167Batch 49, Loss Value: 0.9204\n",
      "Batch 49, Gradient Norm: 0.0021\n",
      "\u001b[1m 49/232\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:26\u001b[0m 473ms/step - accuracy: 0.4901 - binary_io_u_5: 0.4901 - loss: 0.9166Batch 50, Loss Value: 0.9204\n",
      "Batch 50, Gradient Norm: 0.0013\n",
      "\u001b[1m 50/232\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:26\u001b[0m 473ms/step - accuracy: 0.4902 - binary_io_u_5: 0.4902 - loss: 0.9165Batch 51, Loss Value: 0.9204\n",
      "Batch 51, Gradient Norm: 0.0017\n",
      "\u001b[1m 51/232\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:25\u001b[0m 473ms/step - accuracy: 0.4903 - binary_io_u_5: 0.4903 - loss: 0.9164Batch 52, Loss Value: 0.9208\n",
      "Batch 52, Gradient Norm: 0.0385\n",
      "\u001b[1m 52/232\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:25\u001b[0m 473ms/step - accuracy: 0.4905 - binary_io_u_5: 0.4905 - loss: 0.9163Batch 53, Loss Value: 0.9205\n",
      "Batch 53, Gradient Norm: 0.0054\n",
      "\u001b[1m 53/232\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:24\u001b[0m 473ms/step - accuracy: 0.4906 - binary_io_u_5: 0.4906 - loss: 0.9162Batch 54, Loss Value: 0.9204\n",
      "Batch 54, Gradient Norm: 0.0002\n",
      "\u001b[1m 54/232\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:24\u001b[0m 474ms/step - accuracy: 0.4907 - binary_io_u_5: 0.4907 - loss: 0.9161Batch 55, Loss Value: 0.9204\n",
      "Batch 55, Gradient Norm: 0.0019\n",
      "\u001b[1m 55/232\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:23\u001b[0m 474ms/step - accuracy: 0.4908 - binary_io_u_5: 0.4908 - loss: 0.9161Batch 56, Loss Value: 0.9204\n",
      "Batch 56, Gradient Norm: 0.0004\n",
      "\u001b[1m 56/232\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:23\u001b[0m 474ms/step - accuracy: 0.4908 - binary_io_u_5: 0.4908 - loss: 0.9160Batch 57, Loss Value: 0.9204\n",
      "Batch 57, Gradient Norm: 0.0003\n",
      "\u001b[1m 57/232\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:22\u001b[0m 474ms/step - accuracy: 0.4909 - binary_io_u_5: 0.4909 - loss: 0.9160Batch 58, Loss Value: 0.9204\n",
      "Batch 58, Gradient Norm: 0.0014\n",
      "\u001b[1m 58/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:22\u001b[0m 474ms/step - accuracy: 0.4909 - binary_io_u_5: 0.4909 - loss: 0.9159Batch 59, Loss Value: 0.9205\n",
      "Batch 59, Gradient Norm: 0.0030\n",
      "\u001b[1m 59/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:21\u001b[0m 474ms/step - accuracy: 0.4910 - binary_io_u_5: 0.4910 - loss: 0.9159Batch 60, Loss Value: 0.9204\n",
      "Batch 60, Gradient Norm: 0.0001\n",
      "\u001b[1m 60/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:21\u001b[0m 474ms/step - accuracy: 0.4910 - binary_io_u_5: 0.4910 - loss: 0.9158Batch 61, Loss Value: 0.9204\n",
      "Batch 61, Gradient Norm: 0.0004\n",
      "\u001b[1m 61/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:21\u001b[0m 474ms/step - accuracy: 0.4911 - binary_io_u_5: 0.4911 - loss: 0.9158Batch 62, Loss Value: 0.9204\n",
      "Batch 62, Gradient Norm: 0.0000\n",
      "\u001b[1m 62/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:20\u001b[0m 474ms/step - accuracy: 0.4912 - binary_io_u_5: 0.4912 - loss: 0.9157Batch 63, Loss Value: 0.9204\n",
      "Batch 63, Gradient Norm: 0.0004\n",
      "\u001b[1m 63/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:20\u001b[0m 474ms/step - accuracy: 0.4912 - binary_io_u_5: 0.4912 - loss: 0.9157Batch 64, Loss Value: 0.9203\n",
      "Batch 64, Gradient Norm: 0.0095\n",
      "\u001b[1m 64/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:19\u001b[0m 474ms/step - accuracy: 0.4912 - binary_io_u_5: 0.4912 - loss: 0.9157Batch 65, Loss Value: 0.9204\n",
      "Batch 65, Gradient Norm: 0.0001\n",
      "\u001b[1m 65/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:19\u001b[0m 474ms/step - accuracy: 0.4913 - binary_io_u_5: 0.4913 - loss: 0.9157Batch 66, Loss Value: 0.9204\n",
      "Batch 66, Gradient Norm: 0.0004\n",
      "\u001b[1m 66/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:18\u001b[0m 474ms/step - accuracy: 0.4913 - binary_io_u_5: 0.4913 - loss: 0.9156Batch 67, Loss Value: 0.9204\n",
      "Batch 67, Gradient Norm: 0.0000\n",
      "\u001b[1m 67/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:18\u001b[0m 474ms/step - accuracy: 0.4914 - binary_io_u_5: 0.4914 - loss: 0.9156Batch 68, Loss Value: 0.9204\n",
      "Batch 68, Gradient Norm: 0.0012\n",
      "\u001b[1m 68/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:17\u001b[0m 474ms/step - accuracy: 0.4915 - binary_io_u_5: 0.4915 - loss: 0.9155Batch 69, Loss Value: 0.9153\n",
      "Batch 69, Gradient Norm: 0.3080\n",
      "\u001b[1m 69/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:17\u001b[0m 474ms/step - accuracy: 0.4915 - binary_io_u_5: 0.4915 - loss: 0.9155Batch 70, Loss Value: 0.9203\n",
      "Batch 70, Gradient Norm: 0.0107\n",
      "\u001b[1m 70/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:16\u001b[0m 474ms/step - accuracy: 0.4916 - binary_io_u_5: 0.4916 - loss: 0.9154Batch 71, Loss Value: 0.9204\n",
      "Batch 71, Gradient Norm: 0.0006\n",
      "\u001b[1m 71/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:16\u001b[0m 474ms/step - accuracy: 0.4916 - binary_io_u_5: 0.4916 - loss: 0.9154Batch 72, Loss Value: 0.9204\n",
      "Batch 72, Gradient Norm: 0.0001\n",
      "\u001b[1m 72/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:15\u001b[0m 475ms/step - accuracy: 0.4917 - binary_io_u_5: 0.4917 - loss: 0.9153Batch 73, Loss Value: 0.9204\n",
      "Batch 73, Gradient Norm: 0.0001\n",
      "\u001b[1m 73/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:15\u001b[0m 475ms/step - accuracy: 0.4918 - binary_io_u_5: 0.4918 - loss: 0.9153Batch 74, Loss Value: 0.9204\n",
      "Batch 74, Gradient Norm: 0.0001\n",
      "\u001b[1m 74/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:14\u001b[0m 475ms/step - accuracy: 0.4919 - binary_io_u_5: 0.4919 - loss: 0.9152Batch 75, Loss Value: 0.9204\n",
      "Batch 75, Gradient Norm: 0.0002\n",
      "\u001b[1m 75/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:14\u001b[0m 475ms/step - accuracy: 0.4919 - binary_io_u_5: 0.4919 - loss: 0.9152Batch 76, Loss Value: 0.9204\n",
      "Batch 76, Gradient Norm: 0.0001\n",
      "\u001b[1m 76/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:14\u001b[0m 475ms/step - accuracy: 0.4920 - binary_io_u_5: 0.4920 - loss: 0.9151Batch 77, Loss Value: 0.9204\n",
      "Batch 77, Gradient Norm: 0.0039\n",
      "\u001b[1m 77/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:13\u001b[0m 475ms/step - accuracy: 0.4921 - binary_io_u_5: 0.4921 - loss: 0.9151Batch 78, Loss Value: 0.9204\n",
      "Batch 78, Gradient Norm: 0.0001\n",
      "\u001b[1m 78/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:13\u001b[0m 475ms/step - accuracy: 0.4922 - binary_io_u_5: 0.4922 - loss: 0.9150Batch 79, Loss Value: 0.9204\n",
      "Batch 79, Gradient Norm: 0.0002\n",
      "\u001b[1m 79/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:12\u001b[0m 475ms/step - accuracy: 0.4922 - binary_io_u_5: 0.4922 - loss: 0.9150Batch 80, Loss Value: 0.9204\n",
      "Batch 80, Gradient Norm: 0.0004\n",
      "\u001b[1m 80/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:12\u001b[0m 475ms/step - accuracy: 0.4923 - binary_io_u_5: 0.4923 - loss: 0.9149Batch 81, Loss Value: 0.9204\n",
      "Batch 81, Gradient Norm: 0.0002\n",
      "\u001b[1m 81/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:11\u001b[0m 475ms/step - accuracy: 0.4924 - binary_io_u_5: 0.4924 - loss: 0.9149Batch 82, Loss Value: 0.9204\n",
      "Batch 82, Gradient Norm: 0.0000\n",
      "\u001b[1m 82/232\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m1:11\u001b[0m 475ms/step - accuracy: 0.4924 - binary_io_u_5: 0.4924 - loss: 0.9148Batch 83, Loss Value: 0.9204\n",
      "Batch 83, Gradient Norm: 0.0008\n",
      "\u001b[1m 83/232\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m1:10\u001b[0m 475ms/step - accuracy: 0.4925 - binary_io_u_5: 0.4925 - loss: 0.9148Batch 84, Loss Value: 0.9204\n",
      "Batch 84, Gradient Norm: 0.0004\n",
      "\u001b[1m 84/232\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m1:10\u001b[0m 475ms/step - accuracy: 0.4926 - binary_io_u_5: 0.4926 - loss: 0.9147Batch 85, Loss Value: 0.9204\n",
      "Batch 85, Gradient Norm: 0.0008\n",
      "\u001b[1m 85/232\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m1:09\u001b[0m 475ms/step - accuracy: 0.4926 - binary_io_u_5: 0.4926 - loss: 0.9147Batch 86, Loss Value: 0.9204\n",
      "Batch 86, Gradient Norm: 0.0035\n",
      "\u001b[1m 86/232\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m1:09\u001b[0m 475ms/step - accuracy: 0.4927 - binary_io_u_5: 0.4927 - loss: 0.9146Batch 87, Loss Value: 0.9204\n",
      "Batch 87, Gradient Norm: 0.0001\n",
      "\u001b[1m 87/232\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m1:08\u001b[0m 475ms/step - accuracy: 0.4927 - binary_io_u_5: 0.4927 - loss: 0.9146Batch 88, Loss Value: 0.9204\n",
      "Batch 88, Gradient Norm: 0.0000\n",
      "\u001b[1m 88/232\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m1:08\u001b[0m 475ms/step - accuracy: 0.4928 - binary_io_u_5: 0.4928 - loss: 0.9146Batch 89, Loss Value: 0.9204\n",
      "Batch 89, Gradient Norm: 0.0001\n",
      "\u001b[1m 89/232\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m1:07\u001b[0m 475ms/step - accuracy: 0.4928 - binary_io_u_5: 0.4928 - loss: 0.9145Batch 90, Loss Value: 0.9204\n",
      "Batch 90, Gradient Norm: 0.0006\n",
      "\u001b[1m 90/232\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m1:07\u001b[0m 475ms/step - accuracy: 0.4929 - binary_io_u_5: 0.4929 - loss: 0.9145Batch 91, Loss Value: 0.9204\n",
      "Batch 91, Gradient Norm: 0.0005\n",
      "\u001b[1m 91/232\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m1:06\u001b[0m 475ms/step - accuracy: 0.4929 - binary_io_u_5: 0.4929 - loss: 0.9144Batch 92, Loss Value: 0.9204\n",
      "Batch 92, Gradient Norm: 0.0005\n",
      "\u001b[1m 92/232\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m1:06\u001b[0m 475ms/step - accuracy: 0.4930 - binary_io_u_5: 0.4930 - loss: 0.9144Batch 93, Loss Value: 0.9204\n",
      "Batch 93, Gradient Norm: 0.0002\n",
      "\u001b[1m 93/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m1:05\u001b[0m 475ms/step - accuracy: 0.4930 - binary_io_u_5: 0.4930 - loss: 0.9144Batch 94, Loss Value: 0.9204\n",
      "Batch 94, Gradient Norm: 0.0019\n",
      "\u001b[1m 94/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m1:05\u001b[0m 475ms/step - accuracy: 0.4931 - binary_io_u_5: 0.4931 - loss: 0.9143Batch 95, Loss Value: 0.9204\n",
      "Batch 95, Gradient Norm: 0.0002\n",
      "\u001b[1m 95/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m1:05\u001b[0m 475ms/step - accuracy: 0.4931 - binary_io_u_5: 0.4931 - loss: 0.9143Batch 96, Loss Value: 0.9204\n",
      "Batch 96, Gradient Norm: 0.0005\n",
      "\u001b[1m 96/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m1:04\u001b[0m 475ms/step - accuracy: 0.4932 - binary_io_u_5: 0.4932 - loss: 0.9143Batch 97, Loss Value: 0.9204\n",
      "Batch 97, Gradient Norm: 0.0009\n",
      "\u001b[1m 97/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m1:04\u001b[0m 475ms/step - accuracy: 0.4932 - binary_io_u_5: 0.4932 - loss: 0.9142Batch 98, Loss Value: 0.9204\n",
      "Batch 98, Gradient Norm: 0.0002\n",
      "\u001b[1m 98/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m1:03\u001b[0m 475ms/step - accuracy: 0.4933 - binary_io_u_5: 0.4933 - loss: 0.9142Batch 99, Loss Value: 0.9204\n",
      "Batch 99, Gradient Norm: 0.0001\n",
      "\u001b[1m 99/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m1:03\u001b[0m 475ms/step - accuracy: 0.4933 - binary_io_u_5: 0.4933 - loss: 0.9142Batch 100, Loss Value: 0.9204\n",
      "Batch 100, Gradient Norm: 0.0003\n",
      "\u001b[1m100/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m1:02\u001b[0m 475ms/step - accuracy: 0.4934 - binary_io_u_5: 0.4934 - loss: 0.9141Batch 101, Loss Value: 0.9204\n",
      "Batch 101, Gradient Norm: 0.0002\n",
      "\u001b[1m101/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m1:02\u001b[0m 475ms/step - accuracy: 0.4934 - binary_io_u_5: 0.4934 - loss: 0.9141Batch 102, Loss Value: 0.9204\n",
      "Batch 102, Gradient Norm: 0.0001\n",
      "\u001b[1m102/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m1:01\u001b[0m 475ms/step - accuracy: 0.4934 - binary_io_u_5: 0.4934 - loss: 0.9141Batch 103, Loss Value: 0.9204\n",
      "Batch 103, Gradient Norm: 0.0002\n",
      "\u001b[1m103/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m1:01\u001b[0m 475ms/step - accuracy: 0.4935 - binary_io_u_5: 0.4935 - loss: 0.9140Batch 104, Loss Value: 0.9204\n",
      "Batch 104, Gradient Norm: 0.0001\n",
      "\u001b[1m104/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m1:00\u001b[0m 475ms/step - accuracy: 0.4935 - binary_io_u_5: 0.4935 - loss: 0.9140Batch 105, Loss Value: 0.9204\n",
      "Batch 105, Gradient Norm: 0.0001\n",
      "\u001b[1m105/232\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m1:00\u001b[0m 475ms/step - accuracy: 0.4935 - binary_io_u_5: 0.4935 - loss: 0.9140Batch 106, Loss Value: 0.9204\n",
      "Batch 106, Gradient Norm: 0.0000\n",
      "\u001b[1m106/232\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m59s\u001b[0m 475ms/step - accuracy: 0.4936 - binary_io_u_5: 0.4936 - loss: 0.9139 Batch 107, Loss Value: 0.9204\n",
      "Batch 107, Gradient Norm: 0.0028\n",
      "\u001b[1m107/232\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m59s\u001b[0m 475ms/step - accuracy: 0.4936 - binary_io_u_5: 0.4936 - loss: 0.9139Batch 108, Loss Value: 0.9204\n",
      "Batch 108, Gradient Norm: 0.0010\n",
      "\u001b[1m108/232\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m58s\u001b[0m 475ms/step - accuracy: 0.4936 - binary_io_u_5: 0.4936 - loss: 0.9139Batch 109, Loss Value: 0.9204\n",
      "Batch 109, Gradient Norm: 0.0001\n",
      "\u001b[1m109/232\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m58s\u001b[0m 475ms/step - accuracy: 0.4937 - binary_io_u_5: 0.4937 - loss: 0.9138Batch 110, Loss Value: 0.9204\n",
      "Batch 110, Gradient Norm: 0.0011\n",
      "\u001b[1m110/232\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m57s\u001b[0m 475ms/step - accuracy: 0.4937 - binary_io_u_5: 0.4937 - loss: 0.9138Batch 111, Loss Value: 0.9205\n",
      "Batch 111, Gradient Norm: 0.0044\n",
      "\u001b[1m111/232\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m57s\u001b[0m 475ms/step - accuracy: 0.4937 - binary_io_u_5: 0.4937 - loss: 0.9138Batch 112, Loss Value: 0.9204\n",
      "Batch 112, Gradient Norm: 0.0015\n",
      "\u001b[1m112/232\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m56s\u001b[0m 475ms/step - accuracy: 0.4938 - binary_io_u_5: 0.4938 - loss: 0.9138Batch 113, Loss Value: 0.9204\n",
      "Batch 113, Gradient Norm: 0.0001\n",
      "\u001b[1m113/232\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m56s\u001b[0m 475ms/step - accuracy: 0.4938 - binary_io_u_5: 0.4938 - loss: 0.9137Batch 114, Loss Value: 0.9204\n",
      "Batch 114, Gradient Norm: 0.0003\n",
      "\u001b[1m114/232\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m55s\u001b[0m 474ms/step - accuracy: 0.4938 - binary_io_u_5: 0.4938 - loss: 0.9137Batch 115, Loss Value: 0.9204\n",
      "Batch 115, Gradient Norm: 0.0001\n",
      "\u001b[1m115/232\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m55s\u001b[0m 474ms/step - accuracy: 0.4938 - binary_io_u_5: 0.4938 - loss: 0.9137Batch 116, Loss Value: 0.9204\n",
      "Batch 116, Gradient Norm: 0.0023\n",
      "\u001b[1m116/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m55s\u001b[0m 474ms/step - accuracy: 0.4939 - binary_io_u_5: 0.4939 - loss: 0.9137Batch 117, Loss Value: 0.9204\n",
      "Batch 117, Gradient Norm: 0.0001\n",
      "\u001b[1m117/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m54s\u001b[0m 474ms/step - accuracy: 0.4939 - binary_io_u_5: 0.4939 - loss: 0.9136Batch 118, Loss Value: 0.9204\n",
      "Batch 118, Gradient Norm: 0.0000\n",
      "\u001b[1m118/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m54s\u001b[0m 474ms/step - accuracy: 0.4939 - binary_io_u_5: 0.4939 - loss: 0.9136Batch 119, Loss Value: 0.9204\n",
      "Batch 119, Gradient Norm: 0.0005\n",
      "\u001b[1m119/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m53s\u001b[0m 474ms/step - accuracy: 0.4940 - binary_io_u_5: 0.4940 - loss: 0.9136Batch 120, Loss Value: 0.9204\n",
      "Batch 120, Gradient Norm: 0.0001\n",
      "\u001b[1m120/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m53s\u001b[0m 474ms/step - accuracy: 0.4940 - binary_io_u_5: 0.4940 - loss: 0.9135Batch 121, Loss Value: 0.9204\n",
      "Batch 121, Gradient Norm: 0.0001\n",
      "\u001b[1m121/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m52s\u001b[0m 474ms/step - accuracy: 0.4940 - binary_io_u_5: 0.4940 - loss: 0.9135Batch 122, Loss Value: 0.9204\n",
      "Batch 122, Gradient Norm: 0.0001\n",
      "\u001b[1m122/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m52s\u001b[0m 474ms/step - accuracy: 0.4941 - binary_io_u_5: 0.4941 - loss: 0.9135Batch 123, Loss Value: 0.9204\n",
      "Batch 123, Gradient Norm: 0.0001\n",
      "\u001b[1m123/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m51s\u001b[0m 474ms/step - accuracy: 0.4941 - binary_io_u_5: 0.4941 - loss: 0.9135Batch 124, Loss Value: 0.9204\n",
      "Batch 124, Gradient Norm: 0.0003\n",
      "\u001b[1m124/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m51s\u001b[0m 475ms/step - accuracy: 0.4941 - binary_io_u_5: 0.4941 - loss: 0.9134Batch 125, Loss Value: 0.9204\n",
      "Batch 125, Gradient Norm: 0.0015\n",
      "\u001b[1m125/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m50s\u001b[0m 475ms/step - accuracy: 0.4941 - binary_io_u_5: 0.4941 - loss: 0.9134Batch 126, Loss Value: 0.9204\n",
      "Batch 126, Gradient Norm: 0.0010\n",
      "\u001b[1m126/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m50s\u001b[0m 475ms/step - accuracy: 0.4942 - binary_io_u_5: 0.4942 - loss: 0.9134Batch 127, Loss Value: 0.9204\n",
      "Batch 127, Gradient Norm: 0.0008\n",
      "\u001b[1m127/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m49s\u001b[0m 475ms/step - accuracy: 0.4942 - binary_io_u_5: 0.4942 - loss: 0.9134Batch 128, Loss Value: 0.9204\n",
      "Batch 128, Gradient Norm: 0.0001\n",
      "\u001b[1m128/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m49s\u001b[0m 475ms/step - accuracy: 0.4942 - binary_io_u_5: 0.4942 - loss: 0.9134Batch 129, Loss Value: 0.9204\n",
      "Batch 129, Gradient Norm: 0.0002\n",
      "\u001b[1m129/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m48s\u001b[0m 475ms/step - accuracy: 0.4942 - binary_io_u_5: 0.4942 - loss: 0.9134Batch 130, Loss Value: 0.9204\n",
      "Batch 130, Gradient Norm: 0.0001\n",
      "\u001b[1m130/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m48s\u001b[0m 475ms/step - accuracy: 0.4942 - binary_io_u_5: 0.4942 - loss: 0.9134Batch 131, Loss Value: 0.9204\n",
      "Batch 131, Gradient Norm: 0.0001\n",
      "\u001b[1m131/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m47s\u001b[0m 475ms/step - accuracy: 0.4942 - binary_io_u_5: 0.4942 - loss: 0.9133Batch 132, Loss Value: 0.9204\n",
      "Batch 132, Gradient Norm: 0.0013\n",
      "\u001b[1m132/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m47s\u001b[0m 475ms/step - accuracy: 0.4942 - binary_io_u_5: 0.4942 - loss: 0.9133Batch 133, Loss Value: 0.9204\n",
      "Batch 133, Gradient Norm: 0.0001\n",
      "\u001b[1m133/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m47s\u001b[0m 475ms/step - accuracy: 0.4943 - binary_io_u_5: 0.4943 - loss: 0.9133Batch 134, Loss Value: 0.9204\n",
      "Batch 134, Gradient Norm: 0.0001\n",
      "\u001b[1m134/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m46s\u001b[0m 475ms/step - accuracy: 0.4943 - binary_io_u_5: 0.4943 - loss: 0.9133Batch 135, Loss Value: 0.9204\n",
      "Batch 135, Gradient Norm: 0.0012\n",
      "\u001b[1m135/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m46s\u001b[0m 475ms/step - accuracy: 0.4943 - binary_io_u_5: 0.4943 - loss: 0.9133Batch 136, Loss Value: 0.9204\n",
      "Batch 136, Gradient Norm: 0.0007\n",
      "\u001b[1m136/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m45s\u001b[0m 475ms/step - accuracy: 0.4943 - binary_io_u_5: 0.4943 - loss: 0.9133Batch 137, Loss Value: 0.9204\n",
      "Batch 137, Gradient Norm: 0.0002\n",
      "\u001b[1m137/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m45s\u001b[0m 475ms/step - accuracy: 0.4943 - binary_io_u_5: 0.4943 - loss: 0.9133Batch 138, Loss Value: 0.9204\n",
      "Batch 138, Gradient Norm: 0.0008\n",
      "\u001b[1m138/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m44s\u001b[0m 475ms/step - accuracy: 0.4943 - binary_io_u_5: 0.4943 - loss: 0.9133Batch 139, Loss Value: 0.9204\n",
      "Batch 139, Gradient Norm: 0.0011\n",
      "\u001b[1m139/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m44s\u001b[0m 475ms/step - accuracy: 0.4943 - binary_io_u_5: 0.4943 - loss: 0.9132Batch 140, Loss Value: 0.9204\n",
      "Batch 140, Gradient Norm: 0.0003\n",
      "\u001b[1m140/232\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m43s\u001b[0m 475ms/step - accuracy: 0.4944 - binary_io_u_5: 0.4944 - loss: 0.9132Batch 141, Loss Value: 0.9204\n",
      "Batch 141, Gradient Norm: 0.0002\n",
      "\u001b[1m141/232\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m43s\u001b[0m 475ms/step - accuracy: 0.4944 - binary_io_u_5: 0.4944 - loss: 0.9132Batch 142, Loss Value: 0.9204\n",
      "Batch 142, Gradient Norm: 0.0019\n",
      "\u001b[1m142/232\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m42s\u001b[0m 475ms/step - accuracy: 0.4944 - binary_io_u_5: 0.4944 - loss: 0.9132Batch 143, Loss Value: 0.9204\n",
      "Batch 143, Gradient Norm: 0.0003\n",
      "\u001b[1m143/232\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m42s\u001b[0m 475ms/step - accuracy: 0.4944 - binary_io_u_5: 0.4944 - loss: 0.9132Batch 144, Loss Value: 0.9204\n",
      "Batch 144, Gradient Norm: 0.0002\n",
      "\u001b[1m144/232\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m41s\u001b[0m 475ms/step - accuracy: 0.4944 - binary_io_u_5: 0.4944 - loss: 0.9132Batch 145, Loss Value: 0.9204\n",
      "Batch 145, Gradient Norm: 0.0002\n",
      "\u001b[1m145/232\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m41s\u001b[0m 475ms/step - accuracy: 0.4944 - binary_io_u_5: 0.4944 - loss: 0.9132Batch 146, Loss Value: 0.9204\n",
      "Batch 146, Gradient Norm: 0.0022\n",
      "\u001b[1m146/232\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m40s\u001b[0m 475ms/step - accuracy: 0.4945 - binary_io_u_5: 0.4945 - loss: 0.9131Batch 147, Loss Value: 0.9204\n",
      "Batch 147, Gradient Norm: 0.0005\n",
      "\u001b[1m147/232\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m40s\u001b[0m 475ms/step - accuracy: 0.4945 - binary_io_u_5: 0.4945 - loss: 0.9131Batch 148, Loss Value: 0.9203\n",
      "Batch 148, Gradient Norm: 0.0110\n",
      "\u001b[1m148/232\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m39s\u001b[0m 475ms/step - accuracy: 0.4945 - binary_io_u_5: 0.4945 - loss: 0.9131Batch 149, Loss Value: 0.9204\n",
      "Batch 149, Gradient Norm: 0.0010\n",
      "\u001b[1m149/232\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m39s\u001b[0m 475ms/step - accuracy: 0.4945 - binary_io_u_5: 0.4945 - loss: 0.9131Batch 150, Loss Value: 0.9204\n",
      "Batch 150, Gradient Norm: 0.0001\n",
      "\u001b[1m150/232\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m38s\u001b[0m 475ms/step - accuracy: 0.4945 - binary_io_u_5: 0.4945 - loss: 0.9131Batch 151, Loss Value: 0.9204\n",
      "Batch 151, Gradient Norm: 0.0014\n",
      "\u001b[1m151/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m38s\u001b[0m 475ms/step - accuracy: 0.4946 - binary_io_u_5: 0.4946 - loss: 0.9131Batch 152, Loss Value: 0.9220\n",
      "Batch 152, Gradient Norm: 0.1333\n",
      "\u001b[1m152/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m37s\u001b[0m 475ms/step - accuracy: 0.4946 - binary_io_u_5: 0.4946 - loss: 0.9131Batch 153, Loss Value: 0.9204\n",
      "Batch 153, Gradient Norm: 0.0043\n",
      "\u001b[1m153/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m37s\u001b[0m 475ms/step - accuracy: 0.4946 - binary_io_u_5: 0.4946 - loss: 0.9130Batch 154, Loss Value: 0.9204\n",
      "Batch 154, Gradient Norm: 0.0009\n",
      "\u001b[1m154/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m37s\u001b[0m 475ms/step - accuracy: 0.4946 - binary_io_u_5: 0.4946 - loss: 0.9130Batch 155, Loss Value: 0.9203\n",
      "Batch 155, Gradient Norm: 0.0053\n",
      "\u001b[1m155/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m36s\u001b[0m 475ms/step - accuracy: 0.4946 - binary_io_u_5: 0.4946 - loss: 0.9130Batch 156, Loss Value: 0.9204\n",
      "Batch 156, Gradient Norm: 0.0005\n",
      "\u001b[1m156/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m36s\u001b[0m 475ms/step - accuracy: 0.4946 - binary_io_u_5: 0.4946 - loss: 0.9130Batch 157, Loss Value: 0.9203\n",
      "Batch 157, Gradient Norm: 0.0060\n",
      "\u001b[1m157/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m35s\u001b[0m 475ms/step - accuracy: 0.4947 - binary_io_u_5: 0.4947 - loss: 0.9130Batch 158, Loss Value: 0.9204\n",
      "Batch 158, Gradient Norm: 0.0007\n",
      "\u001b[1m158/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m35s\u001b[0m 474ms/step - accuracy: 0.4947 - binary_io_u_5: 0.4947 - loss: 0.9130Batch 159, Loss Value: 0.9204\n",
      "Batch 159, Gradient Norm: 0.0012\n",
      "\u001b[1m159/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m34s\u001b[0m 474ms/step - accuracy: 0.4947 - binary_io_u_5: 0.4947 - loss: 0.9130Batch 160, Loss Value: 0.9205\n",
      "Batch 160, Gradient Norm: 0.0115\n",
      "\u001b[1m160/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m34s\u001b[0m 474ms/step - accuracy: 0.4947 - binary_io_u_5: 0.4947 - loss: 0.9130Batch 161, Loss Value: 0.9201\n",
      "Batch 161, Gradient Norm: 0.0239\n",
      "\u001b[1m161/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m33s\u001b[0m 474ms/step - accuracy: 0.4947 - binary_io_u_5: 0.4947 - loss: 0.9129Batch 162, Loss Value: 0.9204\n",
      "Batch 162, Gradient Norm: 0.0016\n",
      "\u001b[1m162/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m33s\u001b[0m 474ms/step - accuracy: 0.4947 - binary_io_u_5: 0.4947 - loss: 0.9129Batch 163, Loss Value: 0.9205\n",
      "Batch 163, Gradient Norm: 0.0086\n",
      "\u001b[1m163/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m32s\u001b[0m 474ms/step - accuracy: 0.4948 - binary_io_u_5: 0.4948 - loss: 0.9129Batch 164, Loss Value: 0.9204\n",
      "Batch 164, Gradient Norm: 0.0001\n",
      "\u001b[1m164/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m32s\u001b[0m 474ms/step - accuracy: 0.4948 - binary_io_u_5: 0.4948 - loss: 0.9129Batch 165, Loss Value: 0.9204\n",
      "Batch 165, Gradient Norm: 0.0009\n",
      "\u001b[1m165/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m31s\u001b[0m 474ms/step - accuracy: 0.4948 - binary_io_u_5: 0.4948 - loss: 0.9129Batch 166, Loss Value: 0.9204\n",
      "Batch 166, Gradient Norm: 0.0001\n",
      "\u001b[1m166/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m31s\u001b[0m 474ms/step - accuracy: 0.4948 - binary_io_u_5: 0.4948 - loss: 0.9129Batch 167, Loss Value: 0.9204\n",
      "Batch 167, Gradient Norm: 0.0009\n",
      "\u001b[1m167/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m30s\u001b[0m 474ms/step - accuracy: 0.4948 - binary_io_u_5: 0.4948 - loss: 0.9129Batch 168, Loss Value: 0.9257\n",
      "Batch 168, Gradient Norm: 0.3306\n",
      "\u001b[1m168/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m30s\u001b[0m 474ms/step - accuracy: 0.4948 - binary_io_u_5: 0.4948 - loss: 0.9129Batch 169, Loss Value: 0.9204\n",
      "Batch 169, Gradient Norm: 0.0002\n",
      "\u001b[1m169/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m29s\u001b[0m 474ms/step - accuracy: 0.4948 - binary_io_u_5: 0.4948 - loss: 0.9129Batch 170, Loss Value: 0.9204\n",
      "Batch 170, Gradient Norm: 0.0000\n",
      "\u001b[1m170/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m29s\u001b[0m 474ms/step - accuracy: 0.4948 - binary_io_u_5: 0.4948 - loss: 0.9129Batch 171, Loss Value: 0.9204\n",
      "Batch 171, Gradient Norm: 0.0028\n",
      "\u001b[1m171/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m28s\u001b[0m 474ms/step - accuracy: 0.4949 - binary_io_u_5: 0.4949 - loss: 0.9129Batch 172, Loss Value: 0.9204\n",
      "Batch 172, Gradient Norm: 0.0006\n",
      "\u001b[1m172/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m28s\u001b[0m 474ms/step - accuracy: 0.4949 - binary_io_u_5: 0.4949 - loss: 0.9128Batch 173, Loss Value: 0.9204\n",
      "Batch 173, Gradient Norm: 0.0026\n",
      "\u001b[1m173/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m27s\u001b[0m 474ms/step - accuracy: 0.4949 - binary_io_u_5: 0.4949 - loss: 0.9128Batch 174, Loss Value: 0.9204\n",
      "Batch 174, Gradient Norm: 0.0014\n",
      "\u001b[1m174/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m27s\u001b[0m 474ms/step - accuracy: 0.4949 - binary_io_u_5: 0.4949 - loss: 0.9128Batch 175, Loss Value: 0.9203\n",
      "Batch 175, Gradient Norm: 0.0048\n",
      "\u001b[1m175/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m27s\u001b[0m 474ms/step - accuracy: 0.4949 - binary_io_u_5: 0.4949 - loss: 0.9128Batch 176, Loss Value: 0.9206\n",
      "Batch 176, Gradient Norm: 0.0168\n",
      "\u001b[1m176/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m26s\u001b[0m 474ms/step - accuracy: 0.4949 - binary_io_u_5: 0.4949 - loss: 0.9128Batch 177, Loss Value: 0.9204\n",
      "Batch 177, Gradient Norm: 0.0004\n",
      "\u001b[1m177/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m26s\u001b[0m 474ms/step - accuracy: 0.4949 - binary_io_u_5: 0.4949 - loss: 0.9128Batch 178, Loss Value: 0.9204\n",
      "Batch 178, Gradient Norm: 0.0002\n",
      "\u001b[1m178/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m25s\u001b[0m 474ms/step - accuracy: 0.4949 - binary_io_u_5: 0.4949 - loss: 0.9128Batch 179, Loss Value: 0.9204\n",
      "Batch 179, Gradient Norm: 0.0003\n",
      "\u001b[1m179/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m25s\u001b[0m 474ms/step - accuracy: 0.4949 - binary_io_u_5: 0.4949 - loss: 0.9128Batch 180, Loss Value: 0.9204\n",
      "Batch 180, Gradient Norm: 0.0035\n",
      "\u001b[1m180/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m24s\u001b[0m 474ms/step - accuracy: 0.4949 - binary_io_u_5: 0.4949 - loss: 0.9128Batch 181, Loss Value: 0.9204\n",
      "Batch 181, Gradient Norm: 0.0047\n",
      "\u001b[1m181/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m24s\u001b[0m 474ms/step - accuracy: 0.4949 - binary_io_u_5: 0.4949 - loss: 0.9128Batch 182, Loss Value: 0.9204\n",
      "Batch 182, Gradient Norm: 0.0038\n",
      "\u001b[1m182/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m23s\u001b[0m 474ms/step - accuracy: 0.4949 - binary_io_u_5: 0.4949 - loss: 0.9128Batch 183, Loss Value: 0.9206\n",
      "Batch 183, Gradient Norm: 0.0319\n",
      "\u001b[1m183/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m23s\u001b[0m 474ms/step - accuracy: 0.4949 - binary_io_u_5: 0.4949 - loss: 0.9128Batch 184, Loss Value: 0.9204\n",
      "Batch 184, Gradient Norm: 0.0007\n",
      "\u001b[1m184/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m22s\u001b[0m 474ms/step - accuracy: 0.4949 - binary_io_u_5: 0.4949 - loss: 0.9128Batch 185, Loss Value: 0.9204\n",
      "Batch 185, Gradient Norm: 0.0000\n",
      "\u001b[1m185/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m22s\u001b[0m 474ms/step - accuracy: 0.4949 - binary_io_u_5: 0.4949 - loss: 0.9128Batch 186, Loss Value: 0.9204\n",
      "Batch 186, Gradient Norm: 0.0004\n",
      "\u001b[1m186/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m21s\u001b[0m 474ms/step - accuracy: 0.4949 - binary_io_u_5: 0.4949 - loss: 0.9128Batch 187, Loss Value: 0.9199\n",
      "Batch 187, Gradient Norm: 0.0379\n",
      "\u001b[1m187/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m21s\u001b[0m 474ms/step - accuracy: 0.4949 - binary_io_u_5: 0.4949 - loss: 0.9128Batch 188, Loss Value: 0.9204\n",
      "Batch 188, Gradient Norm: 0.0010\n",
      "\u001b[1m188/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m20s\u001b[0m 474ms/step - accuracy: 0.4949 - binary_io_u_5: 0.4949 - loss: 0.9128Batch 189, Loss Value: 0.9204\n",
      "Batch 189, Gradient Norm: 0.0002\n",
      "\u001b[1m189/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m20s\u001b[0m 474ms/step - accuracy: 0.4949 - binary_io_u_5: 0.4949 - loss: 0.9128Batch 190, Loss Value: 0.9204\n",
      "Batch 190, Gradient Norm: 0.0037\n",
      "\u001b[1m190/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m19s\u001b[0m 474ms/step - accuracy: 0.4949 - binary_io_u_5: 0.4949 - loss: 0.9128Batch 191, Loss Value: 0.9204\n",
      "Batch 191, Gradient Norm: 0.0063\n",
      "\u001b[1m191/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m19s\u001b[0m 474ms/step - accuracy: 0.4949 - binary_io_u_5: 0.4949 - loss: 0.9128Batch 192, Loss Value: 0.9210\n",
      "Batch 192, Gradient Norm: 0.0521\n",
      "\u001b[1m192/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m18s\u001b[0m 474ms/step - accuracy: 0.4949 - binary_io_u_5: 0.4949 - loss: 0.9128Batch 193, Loss Value: 0.9204\n",
      "Batch 193, Gradient Norm: 0.0003\n",
      "\u001b[1m193/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m18s\u001b[0m 474ms/step - accuracy: 0.4949 - binary_io_u_5: 0.4949 - loss: 0.9128Batch 194, Loss Value: 0.9204\n",
      "Batch 194, Gradient Norm: 0.0007\n",
      "\u001b[1m194/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m17s\u001b[0m 474ms/step - accuracy: 0.4949 - binary_io_u_5: 0.4949 - loss: 0.9128Batch 195, Loss Value: 0.9217\n",
      "Batch 195, Gradient Norm: 0.1057\n",
      "\u001b[1m195/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m17s\u001b[0m 474ms/step - accuracy: 0.4949 - binary_io_u_5: 0.4949 - loss: 0.9128Batch 196, Loss Value: 0.9198\n",
      "Batch 196, Gradient Norm: 0.0526\n",
      "\u001b[1m196/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m17s\u001b[0m 474ms/step - accuracy: 0.4949 - binary_io_u_5: 0.4949 - loss: 0.9128Batch 197, Loss Value: 0.9206\n",
      "Batch 197, Gradient Norm: 0.0163\n",
      "\u001b[1m197/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m16s\u001b[0m 474ms/step - accuracy: 0.4949 - binary_io_u_5: 0.4949 - loss: 0.9128Batch 198, Loss Value: 0.9204\n",
      "Batch 198, Gradient Norm: 0.0001\n",
      "\u001b[1m198/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m16s\u001b[0m 474ms/step - accuracy: 0.4949 - binary_io_u_5: 0.4949 - loss: 0.9128Batch 199, Loss Value: 0.9204\n",
      "Batch 199, Gradient Norm: 0.0029\n",
      "\u001b[1m199/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m15s\u001b[0m 474ms/step - accuracy: 0.4949 - binary_io_u_5: 0.4949 - loss: 0.9128Batch 200, Loss Value: 0.9204\n",
      "Batch 200, Gradient Norm: 0.0008\n",
      "\u001b[1m200/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m15s\u001b[0m 474ms/step - accuracy: 0.4949 - binary_io_u_5: 0.4949 - loss: 0.9128Batch 201, Loss Value: 0.9204\n",
      "Batch 201, Gradient Norm: 0.0018\n",
      "\u001b[1m201/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m14s\u001b[0m 473ms/step - accuracy: 0.4949 - binary_io_u_5: 0.4949 - loss: 0.9128Batch 202, Loss Value: 0.9204\n",
      "Batch 202, Gradient Norm: 0.0003\n",
      "\u001b[1m202/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m14s\u001b[0m 473ms/step - accuracy: 0.4949 - binary_io_u_5: 0.4949 - loss: 0.9128Batch 203, Loss Value: 0.9201\n",
      "Batch 203, Gradient Norm: 0.0224\n",
      "\u001b[1m203/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m13s\u001b[0m 473ms/step - accuracy: 0.4949 - binary_io_u_5: 0.4949 - loss: 0.9128Batch 204, Loss Value: 0.9204\n",
      "Batch 204, Gradient Norm: 0.0003\n",
      "\u001b[1m204/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m13s\u001b[0m 473ms/step - accuracy: 0.4949 - binary_io_u_5: 0.4949 - loss: 0.9128Batch 205, Loss Value: 0.9204\n",
      "Batch 205, Gradient Norm: 0.0030\n",
      "\u001b[1m205/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m12s\u001b[0m 473ms/step - accuracy: 0.4949 - binary_io_u_5: 0.4949 - loss: 0.9128Batch 206, Loss Value: 0.9204\n",
      "Batch 206, Gradient Norm: 0.0013\n",
      "\u001b[1m206/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m12s\u001b[0m 473ms/step - accuracy: 0.4949 - binary_io_u_5: 0.4949 - loss: 0.9128Batch 207, Loss Value: 0.9209\n",
      "Batch 207, Gradient Norm: 0.0393\n",
      "\u001b[1m207/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m11s\u001b[0m 473ms/step - accuracy: 0.4949 - binary_io_u_5: 0.4949 - loss: 0.9128Batch 208, Loss Value: 0.9204\n",
      "Batch 208, Gradient Norm: 0.0023\n",
      "\u001b[1m208/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m11s\u001b[0m 473ms/step - accuracy: 0.4949 - binary_io_u_5: 0.4949 - loss: 0.9128Batch 209, Loss Value: 0.9205\n",
      "Batch 209, Gradient Norm: 0.0041\n",
      "\u001b[1m209/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m10s\u001b[0m 473ms/step - accuracy: 0.4949 - binary_io_u_5: 0.4949 - loss: 0.9128Batch 210, Loss Value: 0.9204\n",
      "Batch 210, Gradient Norm: 0.0039\n",
      "\u001b[1m210/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m10s\u001b[0m 473ms/step - accuracy: 0.4949 - binary_io_u_5: 0.4949 - loss: 0.9128Batch 211, Loss Value: 0.9203\n",
      "Batch 211, Gradient Norm: 0.0234\n",
      "\u001b[1m211/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m9s\u001b[0m 473ms/step - accuracy: 0.4949 - binary_io_u_5: 0.4949 - loss: 0.9128 Batch 212, Loss Value: 0.9204\n",
      "Batch 212, Gradient Norm: 0.0023\n",
      "\u001b[1m212/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m9s\u001b[0m 473ms/step - accuracy: 0.4949 - binary_io_u_5: 0.4949 - loss: 0.9128Batch 213, Loss Value: 0.9196\n",
      "Batch 213, Gradient Norm: 0.0523\n",
      "\u001b[1m213/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m8s\u001b[0m 473ms/step - accuracy: 0.4949 - binary_io_u_5: 0.4949 - loss: 0.9128Batch 214, Loss Value: 0.9205\n",
      "Batch 214, Gradient Norm: 0.0031\n",
      "\u001b[1m214/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m8s\u001b[0m 473ms/step - accuracy: 0.4949 - binary_io_u_5: 0.4949 - loss: 0.9128Batch 215, Loss Value: 0.9205\n",
      "Batch 215, Gradient Norm: 0.0105\n",
      "\u001b[1m215/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m8s\u001b[0m 473ms/step - accuracy: 0.4949 - binary_io_u_5: 0.4949 - loss: 0.9128Batch 216, Loss Value: 0.9204\n",
      "Batch 216, Gradient Norm: 0.0001\n",
      "\u001b[1m216/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m7s\u001b[0m 473ms/step - accuracy: 0.4949 - binary_io_u_5: 0.4949 - loss: 0.9128Batch 217, Loss Value: 0.9197\n",
      "Batch 217, Gradient Norm: 0.0495\n",
      "\u001b[1m217/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m7s\u001b[0m 473ms/step - accuracy: 0.4949 - binary_io_u_5: 0.4949 - loss: 0.9128Batch 218, Loss Value: 0.9204\n",
      "Batch 218, Gradient Norm: 0.0007\n",
      "\u001b[1m218/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m6s\u001b[0m 473ms/step - accuracy: 0.4949 - binary_io_u_5: 0.4949 - loss: 0.9128Batch 219, Loss Value: 0.9203\n",
      "Batch 219, Gradient Norm: 0.0046\n",
      "\u001b[1m219/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m6s\u001b[0m 473ms/step - accuracy: 0.4949 - binary_io_u_5: 0.4949 - loss: 0.9128Batch 220, Loss Value: 0.9204\n",
      "Batch 220, Gradient Norm: 0.0210\n",
      "\u001b[1m220/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m5s\u001b[0m 473ms/step - accuracy: 0.4949 - binary_io_u_5: 0.4949 - loss: 0.9128Batch 221, Loss Value: 0.9200\n",
      "Batch 221, Gradient Norm: 0.0349\n",
      "\u001b[1m221/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m5s\u001b[0m 473ms/step - accuracy: 0.4949 - binary_io_u_5: 0.4949 - loss: 0.9128Batch 222, Loss Value: 0.9204\n",
      "Batch 222, Gradient Norm: 0.0034\n",
      "\u001b[1m222/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m4s\u001b[0m 473ms/step - accuracy: 0.4949 - binary_io_u_5: 0.4949 - loss: 0.9128Batch 223, Loss Value: 0.9199\n",
      "Batch 223, Gradient Norm: 0.0411\n",
      "\u001b[1m223/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m4s\u001b[0m 473ms/step - accuracy: 0.4949 - binary_io_u_5: 0.4949 - loss: 0.9128Batch 224, Loss Value: 0.9204\n",
      "Batch 224, Gradient Norm: 0.0040\n",
      "\u001b[1m224/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m3s\u001b[0m 473ms/step - accuracy: 0.4949 - binary_io_u_5: 0.4949 - loss: 0.9128Batch 225, Loss Value: 0.9203\n",
      "Batch 225, Gradient Norm: 0.0091\n",
      "\u001b[1m225/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m3s\u001b[0m 473ms/step - accuracy: 0.4949 - binary_io_u_5: 0.4949 - loss: 0.9128Batch 226, Loss Value: 0.9204\n",
      "Batch 226, Gradient Norm: 0.0044\n",
      "\u001b[1m226/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m2s\u001b[0m 473ms/step - accuracy: 0.4949 - binary_io_u_5: 0.4949 - loss: 0.9128Batch 227, Loss Value: 0.9204\n",
      "Batch 227, Gradient Norm: 0.0005\n",
      "\u001b[1m227/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m2s\u001b[0m 473ms/step - accuracy: 0.4949 - binary_io_u_5: 0.4949 - loss: 0.9128Batch 228, Loss Value: 0.9204\n",
      "Batch 228, Gradient Norm: 0.0043\n",
      "\u001b[1m228/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m1s\u001b[0m 473ms/step - accuracy: 0.4949 - binary_io_u_5: 0.4949 - loss: 0.9128Batch 229, Loss Value: 0.9203\n",
      "Batch 229, Gradient Norm: 0.0060\n",
      "\u001b[1m229/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m1s\u001b[0m 473ms/step - accuracy: 0.4949 - binary_io_u_5: 0.4949 - loss: 0.9128Batch 230, Loss Value: 0.9205\n",
      "Batch 230, Gradient Norm: 0.0037\n",
      "\u001b[1m230/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 473ms/step - accuracy: 0.4949 - binary_io_u_5: 0.4949 - loss: 0.9128Batch 231, Loss Value: 0.9204\n",
      "Batch 231, Gradient Norm: 0.0037\n",
      "\u001b[1m231/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 473ms/step - accuracy: 0.4949 - binary_io_u_5: 0.4949 - loss: 0.9128Batch 232, Loss Value: 0.9203\n",
      "Batch 232, Gradient Norm: 0.0083\n",
      "\u001b[1m232/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m111s\u001b[0m 478ms/step - accuracy: 0.4949 - binary_io_u_5: 0.4949 - loss: 0.9128 - val_accuracy: 0.4949 - val_binary_io_u_5: 0.4949 - val_loss: 0.9122 - learning_rate: 0.0010\n",
      "Epoch 17/200\n",
      "Batch 1, Loss Value: 0.9409\n",
      "Batch 1, Gradient Norm: 0.0224\n",
      "\u001b[1m  1/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:55\u001b[0m 500ms/step - accuracy: 0.4500 - binary_io_u_5: 0.4500 - loss: 0.9444Batch 2, Loss Value: 0.9393\n",
      "Batch 2, Gradient Norm: 0.0701\n",
      "\u001b[1m  2/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:47\u001b[0m 469ms/step - accuracy: 0.4625 - binary_io_u_5: 0.4625 - loss: 0.9366Batch 3, Loss Value: 0.9399\n",
      "Batch 3, Gradient Norm: 0.0961\n",
      "\u001b[1m  3/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:47\u001b[0m 467ms/step - accuracy: 0.4683 - binary_io_u_5: 0.4683 - loss: 0.9331Batch 4, Loss Value: 0.9386\n",
      "Batch 4, Gradient Norm: 0.1814\n",
      "\u001b[1m  4/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:47\u001b[0m 471ms/step - accuracy: 0.4725 - binary_io_u_5: 0.4725 - loss: 0.9306Batch 5, Loss Value: 0.9404\n",
      "Batch 5, Gradient Norm: 0.1391\n",
      "\u001b[1m  5/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:46\u001b[0m 470ms/step - accuracy: 0.4712 - binary_io_u_5: 0.4712 - loss: 0.9301Batch 6, Loss Value: 0.9390\n",
      "Batch 6, Gradient Norm: 0.1603\n",
      "\u001b[1m  6/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:46\u001b[0m 470ms/step - accuracy: 0.4713 - binary_io_u_5: 0.4713 - loss: 0.9294Batch 7, Loss Value: 0.9404\n",
      "Batch 7, Gradient Norm: 0.0657\n",
      "\u001b[1m  7/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:45\u001b[0m 469ms/step - accuracy: 0.4705 - binary_io_u_5: 0.4705 - loss: 0.9293Batch 8, Loss Value: 0.9406\n",
      "Batch 8, Gradient Norm: 0.0464\n",
      "\u001b[1m  8/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:45\u001b[0m 469ms/step - accuracy: 0.4700 - binary_io_u_5: 0.4700 - loss: 0.9293Batch 9, Loss Value: 0.9362\n",
      "Batch 9, Gradient Norm: 0.2862\n",
      "\u001b[1m  9/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:44\u001b[0m 470ms/step - accuracy: 0.4698 - binary_io_u_5: 0.4698 - loss: 0.9292Batch 10, Loss Value: 0.9411\n",
      "Batch 10, Gradient Norm: 0.0060\n",
      "\u001b[1m 10/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:44\u001b[0m 471ms/step - accuracy: 0.4695 - binary_io_u_5: 0.4695 - loss: 0.9293Batch 11, Loss Value: 0.9403\n",
      "Batch 11, Gradient Norm: 0.0584\n",
      "\u001b[1m 11/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:44\u001b[0m 471ms/step - accuracy: 0.4693 - binary_io_u_5: 0.4693 - loss: 0.9293Batch 12, Loss Value: 0.9403\n",
      "Batch 12, Gradient Norm: 0.0620\n",
      "\u001b[1m 12/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:43\u001b[0m 471ms/step - accuracy: 0.4693 - binary_io_u_5: 0.4693 - loss: 0.9292Batch 13, Loss Value: 0.9406\n",
      "Batch 13, Gradient Norm: 0.0361\n",
      "\u001b[1m 13/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:43\u001b[0m 471ms/step - accuracy: 0.4696 - binary_io_u_5: 0.4696 - loss: 0.9290Batch 14, Loss Value: 0.9399\n",
      "Batch 14, Gradient Norm: 0.1250\n",
      "\u001b[1m 14/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:42\u001b[0m 471ms/step - accuracy: 0.4699 - binary_io_u_5: 0.4699 - loss: 0.9288Batch 15, Loss Value: 0.9369\n",
      "Batch 15, Gradient Norm: 0.2803\n",
      "\u001b[1m 15/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:42\u001b[0m 471ms/step - accuracy: 0.4703 - binary_io_u_5: 0.4703 - loss: 0.9286Batch 16, Loss Value: 0.9437\n",
      "Batch 16, Gradient Norm: 0.1744\n",
      "\u001b[1m 16/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:41\u001b[0m 470ms/step - accuracy: 0.4706 - binary_io_u_5: 0.4706 - loss: 0.9285Batch 17, Loss Value: 0.9411\n",
      "Batch 17, Gradient Norm: 0.0013\n",
      "\u001b[1m 17/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:41\u001b[0m 471ms/step - accuracy: 0.4709 - binary_io_u_5: 0.4709 - loss: 0.9284Batch 18, Loss Value: 0.9402\n",
      "Batch 18, Gradient Norm: 0.0667\n",
      "\u001b[1m 18/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:40\u001b[0m 471ms/step - accuracy: 0.4713 - binary_io_u_5: 0.4713 - loss: 0.9282Batch 19, Loss Value: 0.9411\n",
      "Batch 19, Gradient Norm: 0.0029\n",
      "\u001b[1m 19/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:40\u001b[0m 471ms/step - accuracy: 0.4716 - binary_io_u_5: 0.4716 - loss: 0.9280Batch 20, Loss Value: 0.9410\n",
      "Batch 20, Gradient Norm: 0.0137\n",
      "\u001b[1m 20/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:39\u001b[0m 471ms/step - accuracy: 0.4721 - binary_io_u_5: 0.4721 - loss: 0.9278Batch 21, Loss Value: 0.9412\n",
      "Batch 21, Gradient Norm: 0.0031\n",
      "\u001b[1m 21/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:39\u001b[0m 471ms/step - accuracy: 0.4726 - binary_io_u_5: 0.4726 - loss: 0.9275Batch 22, Loss Value: 0.9414\n",
      "Batch 22, Gradient Norm: 0.0265\n",
      "\u001b[1m 22/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:38\u001b[0m 471ms/step - accuracy: 0.4731 - binary_io_u_5: 0.4731 - loss: 0.9272Batch 23, Loss Value: 0.9398\n",
      "Batch 23, Gradient Norm: 0.1037\n",
      "\u001b[1m 23/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:38\u001b[0m 472ms/step - accuracy: 0.4737 - binary_io_u_5: 0.4737 - loss: 0.9269Batch 24, Loss Value: 0.9410\n",
      "Batch 24, Gradient Norm: 0.0140\n",
      "\u001b[1m 24/232\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:38\u001b[0m 472ms/step - accuracy: 0.4741 - binary_io_u_5: 0.4741 - loss: 0.9267Batch 25, Loss Value: 0.9328\n",
      "Batch 25, Gradient Norm: 0.4628\n",
      "\u001b[1m 25/232\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:37\u001b[0m 472ms/step - accuracy: 0.4745 - binary_io_u_5: 0.4745 - loss: 0.9265Batch 26, Loss Value: 0.9420\n",
      "Batch 26, Gradient Norm: 0.0996\n",
      "\u001b[1m 26/232\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:37\u001b[0m 471ms/step - accuracy: 0.4748 - binary_io_u_5: 0.4748 - loss: 0.9264Batch 27, Loss Value: 0.9398\n",
      "Batch 27, Gradient Norm: 0.0971\n",
      "\u001b[1m 27/232\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:36\u001b[0m 471ms/step - accuracy: 0.4751 - binary_io_u_5: 0.4751 - loss: 0.9262Batch 28, Loss Value: 0.9407\n",
      "Batch 28, Gradient Norm: 0.0350\n",
      "\u001b[1m 28/232\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:36\u001b[0m 471ms/step - accuracy: 0.4754 - binary_io_u_5: 0.4754 - loss: 0.9261Batch 29, Loss Value: 0.9422\n",
      "Batch 29, Gradient Norm: 0.1482\n",
      "\u001b[1m 29/232\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:35\u001b[0m 471ms/step - accuracy: 0.4756 - binary_io_u_5: 0.4756 - loss: 0.9260Batch 30, Loss Value: 0.9412\n",
      "Batch 30, Gradient Norm: 0.0135\n",
      "\u001b[1m 30/232\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:35\u001b[0m 472ms/step - accuracy: 0.4759 - binary_io_u_5: 0.4759 - loss: 0.9258Batch 31, Loss Value: 0.9409\n",
      "Batch 31, Gradient Norm: 0.0206\n",
      "\u001b[1m 31/232\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:34\u001b[0m 472ms/step - accuracy: 0.4761 - binary_io_u_5: 0.4761 - loss: 0.9257Batch 32, Loss Value: 0.9373\n",
      "Batch 32, Gradient Norm: 0.2245\n",
      "\u001b[1m 32/232\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:34\u001b[0m 473ms/step - accuracy: 0.4763 - binary_io_u_5: 0.4763 - loss: 0.9256Batch 33, Loss Value: 0.9401\n",
      "Batch 33, Gradient Norm: 0.0771\n",
      "\u001b[1m 33/232\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:34\u001b[0m 474ms/step - accuracy: 0.4766 - binary_io_u_5: 0.4766 - loss: 0.9254Batch 34, Loss Value: 0.9402\n",
      "Batch 34, Gradient Norm: 0.1981\n",
      "\u001b[1m 34/232\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:33\u001b[0m 474ms/step - accuracy: 0.4769 - binary_io_u_5: 0.4769 - loss: 0.9252Batch 35, Loss Value: 0.9229\n",
      "Batch 35, Gradient Norm: 0.3705\n",
      "\u001b[1m 35/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:33\u001b[0m 474ms/step - accuracy: 0.4772 - binary_io_u_5: 0.4772 - loss: 0.9251Batch 36, Loss Value: 0.9411\n",
      "Batch 36, Gradient Norm: 0.0086\n",
      "\u001b[1m 36/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:32\u001b[0m 474ms/step - accuracy: 0.4775 - binary_io_u_5: 0.4775 - loss: 0.9249Batch 37, Loss Value: 0.9406\n",
      "Batch 37, Gradient Norm: 0.0431\n",
      "\u001b[1m 37/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:32\u001b[0m 474ms/step - accuracy: 0.4778 - binary_io_u_5: 0.4778 - loss: 0.9247Batch 38, Loss Value: 0.9408\n",
      "Batch 38, Gradient Norm: 0.0554\n",
      "\u001b[1m 38/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:32\u001b[0m 474ms/step - accuracy: 0.4780 - binary_io_u_5: 0.4780 - loss: 0.9246Batch 39, Loss Value: 0.9141\n",
      "Batch 39, Gradient Norm: 0.4466\n",
      "\u001b[1m 39/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:31\u001b[0m 475ms/step - accuracy: 0.4782 - binary_io_u_5: 0.4782 - loss: 0.9244Batch 40, Loss Value: 0.9319\n",
      "Batch 40, Gradient Norm: 0.3438\n",
      "\u001b[1m 40/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:31\u001b[0m 474ms/step - accuracy: 0.4784 - binary_io_u_5: 0.4784 - loss: 0.9243Batch 41, Loss Value: 0.9399\n",
      "Batch 41, Gradient Norm: 0.0862\n",
      "\u001b[1m 41/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:30\u001b[0m 474ms/step - accuracy: 0.4785 - binary_io_u_5: 0.4785 - loss: 0.9242Batch 42, Loss Value: 0.9392\n",
      "Batch 42, Gradient Norm: 0.2111\n",
      "\u001b[1m 42/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:30\u001b[0m 474ms/step - accuracy: 0.4786 - binary_io_u_5: 0.4786 - loss: 0.9241Batch 43, Loss Value: 0.9406\n",
      "Batch 43, Gradient Norm: 0.0374\n",
      "\u001b[1m 43/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:29\u001b[0m 474ms/step - accuracy: 0.4787 - binary_io_u_5: 0.4787 - loss: 0.9240Batch 44, Loss Value: 0.9335\n",
      "Batch 44, Gradient Norm: 0.3824\n",
      "\u001b[1m 44/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:29\u001b[0m 474ms/step - accuracy: 0.4789 - binary_io_u_5: 0.4789 - loss: 0.9239Batch 45, Loss Value: 0.9270\n",
      "Batch 45, Gradient Norm: 0.3144\n",
      "\u001b[1m 45/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:28\u001b[0m 474ms/step - accuracy: 0.4790 - binary_io_u_5: 0.4790 - loss: 0.9238Batch 46, Loss Value: 0.9410\n",
      "Batch 46, Gradient Norm: 0.0114\n",
      "\u001b[1m 46/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:28\u001b[0m 474ms/step - accuracy: 0.4792 - binary_io_u_5: 0.4792 - loss: 0.9237Batch 47, Loss Value: 0.9219\n",
      "Batch 47, Gradient Norm: 0.0675\n",
      "\u001b[1m 47/232\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:27\u001b[0m 474ms/step - accuracy: 0.4793 - binary_io_u_5: 0.4793 - loss: 0.9236Batch 48, Loss Value: 0.9390\n",
      "Batch 48, Gradient Norm: 0.1429\n",
      "\u001b[1m 48/232\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:27\u001b[0m 474ms/step - accuracy: 0.4795 - binary_io_u_5: 0.4795 - loss: 0.9234Batch 49, Loss Value: 0.9421\n",
      "Batch 49, Gradient Norm: 0.0432\n",
      "\u001b[1m 49/232\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:26\u001b[0m 474ms/step - accuracy: 0.4797 - binary_io_u_5: 0.4797 - loss: 0.9233Batch 50, Loss Value: 0.9380\n",
      "Batch 50, Gradient Norm: 0.2620\n",
      "\u001b[1m 50/232\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:26\u001b[0m 474ms/step - accuracy: 0.4798 - binary_io_u_5: 0.4798 - loss: 0.9232Batch 51, Loss Value: 0.9396\n",
      "Batch 51, Gradient Norm: 0.1202\n",
      "\u001b[1m 51/232\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:25\u001b[0m 474ms/step - accuracy: 0.4800 - binary_io_u_5: 0.4800 - loss: 0.9230Batch 52, Loss Value: 0.9354\n",
      "Batch 52, Gradient Norm: 0.4321\n",
      "\u001b[1m 52/232\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:25\u001b[0m 474ms/step - accuracy: 0.4801 - binary_io_u_5: 0.4801 - loss: 0.9229Batch 53, Loss Value: 0.9351\n",
      "Batch 53, Gradient Norm: 0.1713\n",
      "\u001b[1m 53/232\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:24\u001b[0m 474ms/step - accuracy: 0.4802 - binary_io_u_5: 0.4802 - loss: 0.9229Batch 54, Loss Value: 0.9391\n",
      "Batch 54, Gradient Norm: 0.1480\n",
      "\u001b[1m 54/232\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:24\u001b[0m 474ms/step - accuracy: 0.4803 - binary_io_u_5: 0.4803 - loss: 0.9228Batch 55, Loss Value: 0.9332\n",
      "Batch 55, Gradient Norm: 0.1810\n",
      "\u001b[1m 55/232\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:23\u001b[0m 474ms/step - accuracy: 0.4804 - binary_io_u_5: 0.4804 - loss: 0.9227Batch 56, Loss Value: 0.9405\n",
      "Batch 56, Gradient Norm: 0.0444\n",
      "\u001b[1m 56/232\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:23\u001b[0m 474ms/step - accuracy: 0.4805 - binary_io_u_5: 0.4805 - loss: 0.9226Batch 57, Loss Value: 0.9329\n",
      "Batch 57, Gradient Norm: 0.2943\n",
      "\u001b[1m 57/232\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:22\u001b[0m 474ms/step - accuracy: 0.4806 - binary_io_u_5: 0.4806 - loss: 0.9226Batch 58, Loss Value: 0.9375\n",
      "Batch 58, Gradient Norm: 0.2530\n",
      "\u001b[1m 58/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:22\u001b[0m 474ms/step - accuracy: 0.4806 - binary_io_u_5: 0.4806 - loss: 0.9225Batch 59, Loss Value: 0.9354\n",
      "Batch 59, Gradient Norm: 0.1368\n",
      "\u001b[1m 59/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:22\u001b[0m 474ms/step - accuracy: 0.4807 - binary_io_u_5: 0.4807 - loss: 0.9224Batch 60, Loss Value: 0.9412\n",
      "Batch 60, Gradient Norm: 0.0055\n",
      "\u001b[1m 60/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:21\u001b[0m 474ms/step - accuracy: 0.4808 - binary_io_u_5: 0.4808 - loss: 0.9224Batch 61, Loss Value: 0.9400\n",
      "Batch 61, Gradient Norm: 0.0584\n",
      "\u001b[1m 61/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:21\u001b[0m 474ms/step - accuracy: 0.4808 - binary_io_u_5: 0.4808 - loss: 0.9224Batch 62, Loss Value: 0.9385\n",
      "Batch 62, Gradient Norm: 0.1759\n",
      "\u001b[1m 62/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:20\u001b[0m 474ms/step - accuracy: 0.4809 - binary_io_u_5: 0.4809 - loss: 0.9223Batch 63, Loss Value: 0.9410\n",
      "Batch 63, Gradient Norm: 0.0120\n",
      "\u001b[1m 63/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:20\u001b[0m 474ms/step - accuracy: 0.4810 - binary_io_u_5: 0.4810 - loss: 0.9223Batch 64, Loss Value: 0.9332\n",
      "Batch 64, Gradient Norm: 0.0853\n",
      "\u001b[1m 64/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:19\u001b[0m 474ms/step - accuracy: 0.4810 - binary_io_u_5: 0.4810 - loss: 0.9222Batch 65, Loss Value: 0.9415\n",
      "Batch 65, Gradient Norm: 0.0316\n",
      "\u001b[1m 65/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:19\u001b[0m 474ms/step - accuracy: 0.4811 - binary_io_u_5: 0.4811 - loss: 0.9222Batch 66, Loss Value: 0.9404\n",
      "Batch 66, Gradient Norm: 0.0537\n",
      "\u001b[1m 66/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:18\u001b[0m 474ms/step - accuracy: 0.4812 - binary_io_u_5: 0.4812 - loss: 0.9221Batch 67, Loss Value: 0.9409\n",
      "Batch 67, Gradient Norm: 0.0190\n",
      "\u001b[1m 67/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:18\u001b[0m 474ms/step - accuracy: 0.4812 - binary_io_u_5: 0.4812 - loss: 0.9221Batch 68, Loss Value: 0.9410\n",
      "Batch 68, Gradient Norm: 0.0168\n",
      "\u001b[1m 68/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:17\u001b[0m 474ms/step - accuracy: 0.4812 - binary_io_u_5: 0.4812 - loss: 0.9221Batch 69, Loss Value: 0.9411\n",
      "Batch 69, Gradient Norm: 0.0072\n",
      "\u001b[1m 69/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:17\u001b[0m 475ms/step - accuracy: 0.4813 - binary_io_u_5: 0.4813 - loss: 0.9220Batch 70, Loss Value: 0.9411\n",
      "Batch 70, Gradient Norm: 0.0044\n",
      "\u001b[1m 70/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:16\u001b[0m 474ms/step - accuracy: 0.4813 - binary_io_u_5: 0.4813 - loss: 0.9220Batch 71, Loss Value: 0.9398\n",
      "Batch 71, Gradient Norm: 0.0924\n",
      "\u001b[1m 71/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:16\u001b[0m 474ms/step - accuracy: 0.4814 - binary_io_u_5: 0.4814 - loss: 0.9219Batch 72, Loss Value: 0.9371\n",
      "Batch 72, Gradient Norm: 0.2521\n",
      "\u001b[1m 72/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:15\u001b[0m 474ms/step - accuracy: 0.4814 - binary_io_u_5: 0.4814 - loss: 0.9219Batch 73, Loss Value: 0.9301\n",
      "Batch 73, Gradient Norm: 0.5505\n",
      "\u001b[1m 73/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:15\u001b[0m 474ms/step - accuracy: 0.4815 - binary_io_u_5: 0.4815 - loss: 0.9219Batch 74, Loss Value: 0.9413\n",
      "Batch 74, Gradient Norm: 0.0142\n",
      "\u001b[1m 74/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:14\u001b[0m 474ms/step - accuracy: 0.4815 - binary_io_u_5: 0.4815 - loss: 0.9218Batch 75, Loss Value: 0.9441\n",
      "Batch 75, Gradient Norm: 0.0165\n",
      "\u001b[1m 75/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:14\u001b[0m 474ms/step - accuracy: 0.4816 - binary_io_u_5: 0.4816 - loss: 0.9218Batch 76, Loss Value: 0.9399\n",
      "Batch 76, Gradient Norm: 0.0887\n",
      "\u001b[1m 76/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:14\u001b[0m 474ms/step - accuracy: 0.4817 - binary_io_u_5: 0.4817 - loss: 0.9218Batch 77, Loss Value: 0.9408\n",
      "Batch 77, Gradient Norm: 0.0239\n",
      "\u001b[1m 77/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:13\u001b[0m 474ms/step - accuracy: 0.4817 - binary_io_u_5: 0.4817 - loss: 0.9217Batch 78, Loss Value: 0.9401\n",
      "Batch 78, Gradient Norm: 0.1291\n",
      "\u001b[1m 78/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:13\u001b[0m 474ms/step - accuracy: 0.4818 - binary_io_u_5: 0.4818 - loss: 0.9217Batch 79, Loss Value: 0.9424\n",
      "Batch 79, Gradient Norm: 0.1747\n",
      "\u001b[1m 79/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:12\u001b[0m 474ms/step - accuracy: 0.4818 - binary_io_u_5: 0.4818 - loss: 0.9216Batch 80, Loss Value: 0.9421\n",
      "Batch 80, Gradient Norm: 0.1056\n",
      "\u001b[1m 80/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:12\u001b[0m 474ms/step - accuracy: 0.4819 - binary_io_u_5: 0.4819 - loss: 0.9216Batch 81, Loss Value: 0.9413\n",
      "Batch 81, Gradient Norm: 0.0129\n",
      "\u001b[1m 81/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:11\u001b[0m 474ms/step - accuracy: 0.4820 - binary_io_u_5: 0.4820 - loss: 0.9216Batch 82, Loss Value: 0.9410\n",
      "Batch 82, Gradient Norm: 0.0078\n",
      "\u001b[1m 82/232\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m1:11\u001b[0m 474ms/step - accuracy: 0.4820 - binary_io_u_5: 0.4820 - loss: 0.9215Batch 83, Loss Value: 0.9321\n",
      "Batch 83, Gradient Norm: 0.3867\n",
      "\u001b[1m 83/232\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m1:10\u001b[0m 474ms/step - accuracy: 0.4820 - binary_io_u_5: 0.4820 - loss: 0.9215Batch 84, Loss Value: 0.9391\n",
      "Batch 84, Gradient Norm: 0.1256\n",
      "\u001b[1m 84/232\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m1:10\u001b[0m 474ms/step - accuracy: 0.4821 - binary_io_u_5: 0.4821 - loss: 0.9215Batch 85, Loss Value: 0.9364\n",
      "Batch 85, Gradient Norm: 0.3327\n",
      "\u001b[1m 85/232\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m1:09\u001b[0m 474ms/step - accuracy: 0.4821 - binary_io_u_5: 0.4821 - loss: 0.9214Batch 86, Loss Value: 0.9409\n",
      "Batch 86, Gradient Norm: 0.0186\n",
      "\u001b[1m 86/232\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m1:09\u001b[0m 474ms/step - accuracy: 0.4822 - binary_io_u_5: 0.4822 - loss: 0.9214Batch 87, Loss Value: 0.9403\n",
      "Batch 87, Gradient Norm: 0.0644\n",
      "\u001b[1m 87/232\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m1:08\u001b[0m 474ms/step - accuracy: 0.4822 - binary_io_u_5: 0.4822 - loss: 0.9214Batch 88, Loss Value: 0.9440\n",
      "Batch 88, Gradient Norm: 0.0115\n",
      "\u001b[1m 88/232\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m1:08\u001b[0m 474ms/step - accuracy: 0.4823 - binary_io_u_5: 0.4823 - loss: 0.9213Batch 89, Loss Value: 0.9411\n",
      "Batch 89, Gradient Norm: 0.0051\n",
      "\u001b[1m 89/232\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m1:07\u001b[0m 474ms/step - accuracy: 0.4823 - binary_io_u_5: 0.4823 - loss: 0.9213Batch 90, Loss Value: 0.9369\n",
      "Batch 90, Gradient Norm: 0.2780\n",
      "\u001b[1m 90/232\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m1:07\u001b[0m 474ms/step - accuracy: 0.4824 - binary_io_u_5: 0.4824 - loss: 0.9213Batch 91, Loss Value: 0.9444\n",
      "Batch 91, Gradient Norm: 0.0070\n",
      "\u001b[1m 91/232\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m1:06\u001b[0m 474ms/step - accuracy: 0.4825 - binary_io_u_5: 0.4825 - loss: 0.9212Batch 92, Loss Value: 0.9444\n",
      "Batch 92, Gradient Norm: 0.0147\n",
      "\u001b[1m 92/232\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m1:06\u001b[0m 474ms/step - accuracy: 0.4825 - binary_io_u_5: 0.4825 - loss: 0.9212Batch 93, Loss Value: 0.9409\n",
      "Batch 93, Gradient Norm: 0.0225\n",
      "\u001b[1m 93/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m1:05\u001b[0m 474ms/step - accuracy: 0.4826 - binary_io_u_5: 0.4826 - loss: 0.9211Batch 94, Loss Value: 0.9389\n",
      "Batch 94, Gradient Norm: 0.1669\n",
      "\u001b[1m 94/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m1:05\u001b[0m 475ms/step - accuracy: 0.4827 - binary_io_u_5: 0.4827 - loss: 0.9211Batch 95, Loss Value: 0.9376\n",
      "Batch 95, Gradient Norm: 0.2348\n",
      "\u001b[1m 95/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m1:05\u001b[0m 475ms/step - accuracy: 0.4827 - binary_io_u_5: 0.4827 - loss: 0.9210Batch 96, Loss Value: 0.9408\n",
      "Batch 96, Gradient Norm: 0.0236\n",
      "\u001b[1m 96/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m1:04\u001b[0m 475ms/step - accuracy: 0.4828 - binary_io_u_5: 0.4828 - loss: 0.9210Batch 97, Loss Value: 0.9338\n",
      "Batch 97, Gradient Norm: 0.3469\n",
      "\u001b[1m 97/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m1:04\u001b[0m 476ms/step - accuracy: 0.4829 - binary_io_u_5: 0.4829 - loss: 0.9209Batch 98, Loss Value: 0.9411\n",
      "Batch 98, Gradient Norm: 0.0065\n",
      "\u001b[1m 98/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m1:03\u001b[0m 476ms/step - accuracy: 0.4830 - binary_io_u_5: 0.4830 - loss: 0.9209Batch 99, Loss Value: 0.9400\n",
      "Batch 99, Gradient Norm: 0.0634\n",
      "\u001b[1m 99/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m1:03\u001b[0m 476ms/step - accuracy: 0.4830 - binary_io_u_5: 0.4830 - loss: 0.9208Batch 100, Loss Value: 0.9411\n",
      "Batch 100, Gradient Norm: 0.0019\n",
      "\u001b[1m100/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m1:02\u001b[0m 476ms/step - accuracy: 0.4831 - binary_io_u_5: 0.4831 - loss: 0.9208Batch 101, Loss Value: 0.9378\n",
      "Batch 101, Gradient Norm: 0.2361\n",
      "\u001b[1m101/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m1:02\u001b[0m 476ms/step - accuracy: 0.4832 - binary_io_u_5: 0.4832 - loss: 0.9207Batch 102, Loss Value: 0.9280\n",
      "Batch 102, Gradient Norm: 0.0555\n",
      "\u001b[1m102/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m1:01\u001b[0m 476ms/step - accuracy: 0.4833 - binary_io_u_5: 0.4833 - loss: 0.9207Batch 103, Loss Value: 0.9410\n",
      "Batch 103, Gradient Norm: 0.0126\n",
      "\u001b[1m103/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m1:01\u001b[0m 476ms/step - accuracy: 0.4833 - binary_io_u_5: 0.4833 - loss: 0.9207Batch 104, Loss Value: 0.9409\n",
      "Batch 104, Gradient Norm: 0.0170\n",
      "\u001b[1m104/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m1:00\u001b[0m 476ms/step - accuracy: 0.4834 - binary_io_u_5: 0.4834 - loss: 0.9206Batch 105, Loss Value: 0.9439\n",
      "Batch 105, Gradient Norm: 0.0125\n",
      "\u001b[1m105/232\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m1:00\u001b[0m 476ms/step - accuracy: 0.4834 - binary_io_u_5: 0.4834 - loss: 0.9206Batch 106, Loss Value: 0.9399\n",
      "Batch 106, Gradient Norm: 0.0951\n",
      "\u001b[1m106/232\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m59s\u001b[0m 476ms/step - accuracy: 0.4835 - binary_io_u_5: 0.4835 - loss: 0.9206 Batch 107, Loss Value: 0.9312\n",
      "Batch 107, Gradient Norm: 0.2782\n",
      "\u001b[1m107/232\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m59s\u001b[0m 476ms/step - accuracy: 0.4835 - binary_io_u_5: 0.4835 - loss: 0.9205Batch 108, Loss Value: 0.9410\n",
      "Batch 108, Gradient Norm: 0.0180\n",
      "\u001b[1m108/232\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m59s\u001b[0m 476ms/step - accuracy: 0.4836 - binary_io_u_5: 0.4836 - loss: 0.9205Batch 109, Loss Value: 0.9386\n",
      "Batch 109, Gradient Norm: 0.1410\n",
      "\u001b[1m109/232\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m58s\u001b[0m 476ms/step - accuracy: 0.4836 - binary_io_u_5: 0.4836 - loss: 0.9205Batch 110, Loss Value: 0.9420\n",
      "Batch 110, Gradient Norm: 0.0669\n",
      "\u001b[1m110/232\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m58s\u001b[0m 476ms/step - accuracy: 0.4837 - binary_io_u_5: 0.4837 - loss: 0.9204Batch 111, Loss Value: 0.9354\n",
      "Batch 111, Gradient Norm: 0.4011\n",
      "\u001b[1m111/232\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m57s\u001b[0m 476ms/step - accuracy: 0.4838 - binary_io_u_5: 0.4838 - loss: 0.9204Batch 112, Loss Value: 0.9353\n",
      "Batch 112, Gradient Norm: 0.2760\n",
      "\u001b[1m112/232\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m57s\u001b[0m 476ms/step - accuracy: 0.4838 - binary_io_u_5: 0.4838 - loss: 0.9204Batch 113, Loss Value: 0.9382\n",
      "Batch 113, Gradient Norm: 0.1802\n",
      "\u001b[1m113/232\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m56s\u001b[0m 476ms/step - accuracy: 0.4839 - binary_io_u_5: 0.4839 - loss: 0.9203Batch 114, Loss Value: 0.9434\n",
      "Batch 114, Gradient Norm: 0.1529\n",
      "\u001b[1m114/232\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m56s\u001b[0m 476ms/step - accuracy: 0.4839 - binary_io_u_5: 0.4839 - loss: 0.9203Batch 115, Loss Value: 0.9302\n",
      "Batch 115, Gradient Norm: 0.6215\n",
      "\u001b[1m115/232\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m55s\u001b[0m 476ms/step - accuracy: 0.4840 - binary_io_u_5: 0.4840 - loss: 0.9203Batch 116, Loss Value: 0.9412\n",
      "Batch 116, Gradient Norm: 0.1330\n",
      "\u001b[1m116/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m55s\u001b[0m 476ms/step - accuracy: 0.4840 - binary_io_u_5: 0.4840 - loss: 0.9202Batch 117, Loss Value: 0.9380\n",
      "Batch 117, Gradient Norm: 0.1759\n",
      "\u001b[1m117/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m54s\u001b[0m 475ms/step - accuracy: 0.4841 - binary_io_u_5: 0.4841 - loss: 0.9202Batch 118, Loss Value: 0.9291\n",
      "Batch 118, Gradient Norm: 0.3650\n",
      "\u001b[1m118/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m54s\u001b[0m 475ms/step - accuracy: 0.4841 - binary_io_u_5: 0.4841 - loss: 0.9202Batch 119, Loss Value: 0.9252\n",
      "Batch 119, Gradient Norm: 0.1373\n",
      "\u001b[1m119/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m53s\u001b[0m 475ms/step - accuracy: 0.4842 - binary_io_u_5: 0.4842 - loss: 0.9201Batch 120, Loss Value: 0.9359\n",
      "Batch 120, Gradient Norm: 0.0881\n",
      "\u001b[1m120/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m53s\u001b[0m 475ms/step - accuracy: 0.4842 - binary_io_u_5: 0.4842 - loss: 0.9201Batch 121, Loss Value: 0.9263\n",
      "Batch 121, Gradient Norm: 0.2119\n",
      "\u001b[1m121/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m52s\u001b[0m 475ms/step - accuracy: 0.4843 - binary_io_u_5: 0.4843 - loss: 0.9201Batch 122, Loss Value: 0.9289\n",
      "Batch 122, Gradient Norm: 0.4319\n",
      "\u001b[1m122/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m52s\u001b[0m 475ms/step - accuracy: 0.4843 - binary_io_u_5: 0.4843 - loss: 0.9200Batch 123, Loss Value: 0.9280\n",
      "Batch 123, Gradient Norm: 0.6647\n",
      "\u001b[1m123/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m51s\u001b[0m 475ms/step - accuracy: 0.4844 - binary_io_u_5: 0.4844 - loss: 0.9200Batch 124, Loss Value: 0.9320\n",
      "Batch 124, Gradient Norm: 0.1100\n",
      "\u001b[1m124/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m51s\u001b[0m 475ms/step - accuracy: 0.4844 - binary_io_u_5: 0.4844 - loss: 0.9200Batch 125, Loss Value: 0.9354\n",
      "Batch 125, Gradient Norm: 0.4067\n",
      "\u001b[1m125/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m50s\u001b[0m 475ms/step - accuracy: 0.4845 - binary_io_u_5: 0.4845 - loss: 0.9199Batch 126, Loss Value: 0.8954\n",
      "Batch 126, Gradient Norm: 0.1696\n",
      "\u001b[1m126/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m50s\u001b[0m 475ms/step - accuracy: 0.4845 - binary_io_u_5: 0.4845 - loss: 0.9199Batch 127, Loss Value: 0.9066\n",
      "Batch 127, Gradient Norm: 0.7779\n",
      "\u001b[1m127/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m49s\u001b[0m 475ms/step - accuracy: 0.4846 - binary_io_u_5: 0.4846 - loss: 0.9199Batch 128, Loss Value: 0.8983\n",
      "Batch 128, Gradient Norm: 0.4451\n",
      "\u001b[1m128/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m49s\u001b[0m 476ms/step - accuracy: 0.4846 - binary_io_u_5: 0.4846 - loss: 0.9199Batch 129, Loss Value: 0.9312\n",
      "Batch 129, Gradient Norm: 0.4298\n",
      "\u001b[1m129/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m49s\u001b[0m 476ms/step - accuracy: 0.4847 - binary_io_u_5: 0.4847 - loss: 0.9198Batch 130, Loss Value: 0.9288\n",
      "Batch 130, Gradient Norm: 0.1714\n",
      "\u001b[1m130/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m48s\u001b[0m 476ms/step - accuracy: 0.4847 - binary_io_u_5: 0.4847 - loss: 0.9198Batch 131, Loss Value: 0.9424\n",
      "Batch 131, Gradient Norm: 0.1172\n",
      "\u001b[1m131/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m48s\u001b[0m 476ms/step - accuracy: 0.4848 - binary_io_u_5: 0.4848 - loss: 0.9198Batch 132, Loss Value: 0.9133\n",
      "Batch 132, Gradient Norm: 0.0899\n",
      "\u001b[1m132/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m47s\u001b[0m 476ms/step - accuracy: 0.4848 - binary_io_u_5: 0.4848 - loss: 0.9197Batch 133, Loss Value: 0.9357\n",
      "Batch 133, Gradient Norm: 0.1715\n",
      "\u001b[1m133/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m47s\u001b[0m 477ms/step - accuracy: 0.4848 - binary_io_u_5: 0.4848 - loss: 0.9197Batch 134, Loss Value: 0.9363\n",
      "Batch 134, Gradient Norm: 0.4482\n",
      "\u001b[1m134/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m46s\u001b[0m 477ms/step - accuracy: 0.4849 - binary_io_u_5: 0.4849 - loss: 0.9197Batch 135, Loss Value: 0.9310\n",
      "Batch 135, Gradient Norm: 0.1357\n",
      "\u001b[1m135/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m46s\u001b[0m 477ms/step - accuracy: 0.4849 - binary_io_u_5: 0.4849 - loss: 0.9196Batch 136, Loss Value: 0.9236\n",
      "Batch 136, Gradient Norm: 0.8044\n",
      "\u001b[1m136/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m45s\u001b[0m 477ms/step - accuracy: 0.4850 - binary_io_u_5: 0.4850 - loss: 0.9196Batch 137, Loss Value: 0.9306\n",
      "Batch 137, Gradient Norm: 0.2053\n",
      "\u001b[1m137/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m45s\u001b[0m 476ms/step - accuracy: 0.4850 - binary_io_u_5: 0.4850 - loss: 0.9196Batch 138, Loss Value: 0.8949\n",
      "Batch 138, Gradient Norm: 0.0891\n",
      "\u001b[1m138/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m44s\u001b[0m 476ms/step - accuracy: 0.4851 - binary_io_u_5: 0.4851 - loss: 0.9195Batch 139, Loss Value: 0.9463\n",
      "Batch 139, Gradient Norm: 0.0534\n",
      "\u001b[1m139/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m44s\u001b[0m 476ms/step - accuracy: 0.4851 - binary_io_u_5: 0.4851 - loss: 0.9195Batch 140, Loss Value: 0.9095\n",
      "Batch 140, Gradient Norm: 0.4790\n",
      "\u001b[1m140/232\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m43s\u001b[0m 477ms/step - accuracy: 0.4851 - binary_io_u_5: 0.4851 - loss: 0.9195Batch 141, Loss Value: 0.8948\n",
      "Batch 141, Gradient Norm: 0.2489\n",
      "\u001b[1m141/232\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m43s\u001b[0m 477ms/step - accuracy: 0.4852 - binary_io_u_5: 0.4852 - loss: 0.9195Batch 142, Loss Value: 0.9232\n",
      "Batch 142, Gradient Norm: 0.0705\n",
      "\u001b[1m142/232\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m42s\u001b[0m 477ms/step - accuracy: 0.4852 - binary_io_u_5: 0.4852 - loss: 0.9194Batch 143, Loss Value: 0.9424\n",
      "Batch 143, Gradient Norm: 0.3180\n",
      "\u001b[1m143/232\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m42s\u001b[0m 477ms/step - accuracy: 0.4852 - binary_io_u_5: 0.4852 - loss: 0.9194Batch 144, Loss Value: 0.9234\n",
      "Batch 144, Gradient Norm: 0.3609\n",
      "\u001b[1m144/232\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m41s\u001b[0m 477ms/step - accuracy: 0.4853 - binary_io_u_5: 0.4853 - loss: 0.9194Batch 145, Loss Value: 0.9399\n",
      "Batch 145, Gradient Norm: 0.4638\n",
      "\u001b[1m145/232\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m41s\u001b[0m 477ms/step - accuracy: 0.4853 - binary_io_u_5: 0.4853 - loss: 0.9194Batch 146, Loss Value: 0.9274\n",
      "Batch 146, Gradient Norm: 0.1589\n",
      "\u001b[1m146/232\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m41s\u001b[0m 477ms/step - accuracy: 0.4854 - binary_io_u_5: 0.4854 - loss: 0.9193Batch 147, Loss Value: 0.9302\n",
      "Batch 147, Gradient Norm: 0.0670\n",
      "\u001b[1m147/232\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m40s\u001b[0m 477ms/step - accuracy: 0.4854 - binary_io_u_5: 0.4854 - loss: 0.9193Batch 148, Loss Value: 0.9200\n",
      "Batch 148, Gradient Norm: 0.4750\n",
      "\u001b[1m148/232\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m40s\u001b[0m 477ms/step - accuracy: 0.4854 - binary_io_u_5: 0.4855 - loss: 0.9193Batch 149, Loss Value: 0.9123\n",
      "Batch 149, Gradient Norm: 0.2253\n",
      "\u001b[1m149/232\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m39s\u001b[0m 477ms/step - accuracy: 0.4855 - binary_io_u_5: 0.4855 - loss: 0.9192Batch 150, Loss Value: 0.9448\n",
      "Batch 150, Gradient Norm: 0.0040\n",
      "\u001b[1m150/232\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m39s\u001b[0m 477ms/step - accuracy: 0.4855 - binary_io_u_5: 0.4856 - loss: 0.9192Batch 151, Loss Value: 0.8966\n",
      "Batch 151, Gradient Norm: 0.1997\n",
      "\u001b[1m151/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m38s\u001b[0m 477ms/step - accuracy: 0.4856 - binary_io_u_5: 0.4856 - loss: 0.9192Batch 152, Loss Value: 0.9239\n",
      "Batch 152, Gradient Norm: 0.3788\n",
      "\u001b[1m152/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m38s\u001b[0m 477ms/step - accuracy: 0.4856 - binary_io_u_5: 0.4857 - loss: 0.9192Batch 153, Loss Value: 0.9345\n",
      "Batch 153, Gradient Norm: 0.4104\n",
      "\u001b[1m153/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m37s\u001b[0m 477ms/step - accuracy: 0.4857 - binary_io_u_5: 0.4857 - loss: 0.9191Batch 154, Loss Value: 0.9218\n",
      "Batch 154, Gradient Norm: 0.1363\n",
      "\u001b[1m154/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m37s\u001b[0m 477ms/step - accuracy: 0.4857 - binary_io_u_5: 0.4857 - loss: 0.9191Batch 155, Loss Value: 0.9340\n",
      "Batch 155, Gradient Norm: 0.4017\n",
      "\u001b[1m155/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m36s\u001b[0m 478ms/step - accuracy: 0.4858 - binary_io_u_5: 0.4858 - loss: 0.9191Batch 156, Loss Value: 0.9405\n",
      "Batch 156, Gradient Norm: 0.0467\n",
      "\u001b[1m156/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m36s\u001b[0m 479ms/step - accuracy: 0.4858 - binary_io_u_5: 0.4858 - loss: 0.9191Batch 157, Loss Value: 0.9420\n",
      "Batch 157, Gradient Norm: 0.1640\n",
      "\u001b[1m157/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m35s\u001b[0m 479ms/step - accuracy: 0.4859 - binary_io_u_5: 0.4859 - loss: 0.9190Batch 158, Loss Value: 0.9408\n",
      "Batch 158, Gradient Norm: 0.3490\n",
      "\u001b[1m158/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m35s\u001b[0m 480ms/step - accuracy: 0.4859 - binary_io_u_5: 0.4859 - loss: 0.9190Batch 159, Loss Value: 0.9329\n",
      "Batch 159, Gradient Norm: 0.1162\n",
      "\u001b[1m159/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m35s\u001b[0m 480ms/step - accuracy: 0.4859 - binary_io_u_5: 0.4860 - loss: 0.9190Batch 160, Loss Value: 0.9413\n",
      "Batch 160, Gradient Norm: 0.0260\n",
      "\u001b[1m160/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m34s\u001b[0m 481ms/step - accuracy: 0.4860 - binary_io_u_5: 0.4860 - loss: 0.9189Batch 161, Loss Value: 0.9384\n",
      "Batch 161, Gradient Norm: 0.2581\n",
      "\u001b[1m161/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m34s\u001b[0m 481ms/step - accuracy: 0.4860 - binary_io_u_5: 0.4861 - loss: 0.9189Batch 162, Loss Value: 0.9409\n",
      "Batch 162, Gradient Norm: 0.0173\n",
      "\u001b[1m162/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m33s\u001b[0m 482ms/step - accuracy: 0.4861 - binary_io_u_5: 0.4861 - loss: 0.9189Batch 163, Loss Value: 0.9411\n",
      "Batch 163, Gradient Norm: 0.0031\n",
      "\u001b[1m163/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m33s\u001b[0m 482ms/step - accuracy: 0.4861 - binary_io_u_5: 0.4862 - loss: 0.9189Batch 164, Loss Value: 0.9411\n",
      "Batch 164, Gradient Norm: 0.0037\n",
      "\u001b[1m164/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m32s\u001b[0m 482ms/step - accuracy: 0.4862 - binary_io_u_5: 0.4862 - loss: 0.9188Batch 165, Loss Value: 0.9412\n",
      "Batch 165, Gradient Norm: 0.0014\n",
      "\u001b[1m165/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m32s\u001b[0m 482ms/step - accuracy: 0.4862 - binary_io_u_5: 0.4862 - loss: 0.9188Batch 166, Loss Value: 0.9412\n",
      "Batch 166, Gradient Norm: 0.0001\n",
      "\u001b[1m166/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m31s\u001b[0m 482ms/step - accuracy: 0.4862 - binary_io_u_5: 0.4863 - loss: 0.9188Batch 167, Loss Value: 0.9440\n",
      "Batch 167, Gradient Norm: 0.0129\n",
      "\u001b[1m167/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m31s\u001b[0m 482ms/step - accuracy: 0.4863 - binary_io_u_5: 0.4863 - loss: 0.9187Batch 168, Loss Value: 0.9410\n",
      "Batch 168, Gradient Norm: 0.0194\n",
      "\u001b[1m168/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m30s\u001b[0m 482ms/step - accuracy: 0.4863 - binary_io_u_5: 0.4864 - loss: 0.9187Batch 169, Loss Value: 0.9411\n",
      "Batch 169, Gradient Norm: 0.0010\n",
      "\u001b[1m169/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m30s\u001b[0m 482ms/step - accuracy: 0.4864 - binary_io_u_5: 0.4864 - loss: 0.9187Batch 170, Loss Value: 0.9412\n",
      "Batch 170, Gradient Norm: 0.0014\n",
      "\u001b[1m170/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m29s\u001b[0m 482ms/step - accuracy: 0.4864 - binary_io_u_5: 0.4864 - loss: 0.9187Batch 171, Loss Value: 0.9412\n",
      "Batch 171, Gradient Norm: 0.0004\n",
      "\u001b[1m171/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m29s\u001b[0m 482ms/step - accuracy: 0.4864 - binary_io_u_5: 0.4865 - loss: 0.9186Batch 172, Loss Value: 0.9411\n",
      "Batch 172, Gradient Norm: 0.0019\n",
      "\u001b[1m172/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m28s\u001b[0m 482ms/step - accuracy: 0.4865 - binary_io_u_5: 0.4865 - loss: 0.9186Batch 173, Loss Value: 0.9412\n",
      "Batch 173, Gradient Norm: 0.0002\n",
      "\u001b[1m173/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m28s\u001b[0m 482ms/step - accuracy: 0.4865 - binary_io_u_5: 0.4866 - loss: 0.9186Batch 174, Loss Value: 0.9410\n",
      "Batch 174, Gradient Norm: 0.0118\n",
      "\u001b[1m174/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m27s\u001b[0m 482ms/step - accuracy: 0.4866 - binary_io_u_5: 0.4866 - loss: 0.9186Batch 175, Loss Value: 0.9411\n",
      "Batch 175, Gradient Norm: 0.0036\n",
      "\u001b[1m175/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m27s\u001b[0m 482ms/step - accuracy: 0.4866 - binary_io_u_5: 0.4866 - loss: 0.9185Batch 176, Loss Value: 0.9410\n",
      "Batch 176, Gradient Norm: 0.0254\n",
      "\u001b[1m176/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m27s\u001b[0m 482ms/step - accuracy: 0.4866 - binary_io_u_5: 0.4867 - loss: 0.9185Batch 177, Loss Value: 0.9404\n",
      "Batch 177, Gradient Norm: 0.0596\n",
      "\u001b[1m177/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m26s\u001b[0m 482ms/step - accuracy: 0.4867 - binary_io_u_5: 0.4867 - loss: 0.9185Batch 178, Loss Value: 0.9193\n",
      "Batch 178, Gradient Norm: 0.3620\n",
      "\u001b[1m178/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m26s\u001b[0m 482ms/step - accuracy: 0.4867 - binary_io_u_5: 0.4868 - loss: 0.9185Batch 179, Loss Value: 0.9403\n",
      "Batch 179, Gradient Norm: 0.1321\n",
      "\u001b[1m179/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m25s\u001b[0m 482ms/step - accuracy: 0.4868 - binary_io_u_5: 0.4868 - loss: 0.9184Batch 180, Loss Value: 0.9412\n",
      "Batch 180, Gradient Norm: 0.0061\n",
      "\u001b[1m180/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m25s\u001b[0m 482ms/step - accuracy: 0.4868 - binary_io_u_5: 0.4869 - loss: 0.9184Batch 181, Loss Value: 0.9324\n",
      "Batch 181, Gradient Norm: 0.2210\n",
      "\u001b[1m181/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m24s\u001b[0m 482ms/step - accuracy: 0.4869 - binary_io_u_5: 0.4869 - loss: 0.9184Batch 182, Loss Value: 0.9356\n",
      "Batch 182, Gradient Norm: 0.0882\n",
      "\u001b[1m182/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m24s\u001b[0m 482ms/step - accuracy: 0.4869 - binary_io_u_5: 0.4870 - loss: 0.9183Batch 183, Loss Value: 0.9412\n",
      "Batch 183, Gradient Norm: 0.0003\n",
      "\u001b[1m183/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m23s\u001b[0m 482ms/step - accuracy: 0.4870 - binary_io_u_5: 0.4870 - loss: 0.9183Batch 184, Loss Value: 0.9412\n",
      "Batch 184, Gradient Norm: 0.0098\n",
      "\u001b[1m184/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m23s\u001b[0m 482ms/step - accuracy: 0.4870 - binary_io_u_5: 0.4870 - loss: 0.9183Batch 185, Loss Value: 0.9354\n",
      "Batch 185, Gradient Norm: 0.0064\n",
      "\u001b[1m185/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m22s\u001b[0m 482ms/step - accuracy: 0.4870 - binary_io_u_5: 0.4871 - loss: 0.9183Batch 186, Loss Value: 0.9365\n",
      "Batch 186, Gradient Norm: 0.2584\n",
      "\u001b[1m186/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m22s\u001b[0m 482ms/step - accuracy: 0.4871 - binary_io_u_5: 0.4871 - loss: 0.9182Batch 187, Loss Value: 0.9448\n",
      "Batch 187, Gradient Norm: 0.0073\n",
      "\u001b[1m187/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m21s\u001b[0m 482ms/step - accuracy: 0.4871 - binary_io_u_5: 0.4872 - loss: 0.9182Batch 188, Loss Value: 0.9299\n",
      "Batch 188, Gradient Norm: 0.1274\n",
      "\u001b[1m188/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m21s\u001b[0m 482ms/step - accuracy: 0.4872 - binary_io_u_5: 0.4872 - loss: 0.9182Batch 189, Loss Value: 0.9373\n",
      "Batch 189, Gradient Norm: 0.1673\n",
      "\u001b[1m189/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m20s\u001b[0m 482ms/step - accuracy: 0.4872 - binary_io_u_5: 0.4873 - loss: 0.9181Batch 190, Loss Value: 0.9411\n",
      "Batch 190, Gradient Norm: 0.0092\n",
      "\u001b[1m190/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m20s\u001b[0m 482ms/step - accuracy: 0.4872 - binary_io_u_5: 0.4873 - loss: 0.9181Batch 191, Loss Value: 0.9221\n",
      "Batch 191, Gradient Norm: 0.2427\n",
      "\u001b[1m191/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m19s\u001b[0m 482ms/step - accuracy: 0.4873 - binary_io_u_5: 0.4873 - loss: 0.9181Batch 192, Loss Value: 0.9063\n",
      "Batch 192, Gradient Norm: 0.2117\n",
      "\u001b[1m192/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m19s\u001b[0m 482ms/step - accuracy: 0.4873 - binary_io_u_5: 0.4874 - loss: 0.9181Batch 193, Loss Value: 0.9315\n",
      "Batch 193, Gradient Norm: 0.0843\n",
      "\u001b[1m193/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m18s\u001b[0m 482ms/step - accuracy: 0.4874 - binary_io_u_5: 0.4874 - loss: 0.9180Batch 194, Loss Value: 0.9271\n",
      "Batch 194, Gradient Norm: 0.1083\n",
      "\u001b[1m194/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m18s\u001b[0m 482ms/step - accuracy: 0.4874 - binary_io_u_5: 0.4875 - loss: 0.9180Batch 195, Loss Value: 0.9296\n",
      "Batch 195, Gradient Norm: 0.1651\n",
      "\u001b[1m195/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m17s\u001b[0m 482ms/step - accuracy: 0.4874 - binary_io_u_5: 0.4875 - loss: 0.9180Batch 196, Loss Value: 0.9415\n",
      "Batch 196, Gradient Norm: 0.3427\n",
      "\u001b[1m196/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m17s\u001b[0m 482ms/step - accuracy: 0.4875 - binary_io_u_5: 0.4875 - loss: 0.9180Batch 197, Loss Value: 0.9040\n",
      "Batch 197, Gradient Norm: 0.3171\n",
      "\u001b[1m197/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m16s\u001b[0m 482ms/step - accuracy: 0.4875 - binary_io_u_5: 0.4876 - loss: 0.9179Batch 198, Loss Value: 0.9152\n",
      "Batch 198, Gradient Norm: 0.2990\n",
      "\u001b[1m198/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m16s\u001b[0m 481ms/step - accuracy: 0.4876 - binary_io_u_5: 0.4876 - loss: 0.9179Batch 199, Loss Value: 0.9451\n",
      "Batch 199, Gradient Norm: 0.0176\n",
      "\u001b[1m199/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m15s\u001b[0m 481ms/step - accuracy: 0.4876 - binary_io_u_5: 0.4876 - loss: 0.9179Batch 200, Loss Value: 0.9179\n",
      "Batch 200, Gradient Norm: 0.0696\n",
      "\u001b[1m200/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m15s\u001b[0m 481ms/step - accuracy: 0.4876 - binary_io_u_5: 0.4877 - loss: 0.9179Batch 201, Loss Value: 0.9444\n",
      "Batch 201, Gradient Norm: 0.0098\n",
      "\u001b[1m201/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m14s\u001b[0m 481ms/step - accuracy: 0.4877 - binary_io_u_5: 0.4877 - loss: 0.9178Batch 202, Loss Value: 0.8909\n",
      "Batch 202, Gradient Norm: 0.8823\n",
      "\u001b[1m202/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m14s\u001b[0m 481ms/step - accuracy: 0.4877 - binary_io_u_5: 0.4877 - loss: 0.9178Batch 203, Loss Value: 0.9299\n",
      "Batch 203, Gradient Norm: 0.4013\n",
      "\u001b[1m203/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m13s\u001b[0m 481ms/step - accuracy: 0.4877 - binary_io_u_5: 0.4878 - loss: 0.9178Batch 204, Loss Value: 0.9218\n",
      "Batch 204, Gradient Norm: 0.1622\n",
      "\u001b[1m204/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m13s\u001b[0m 481ms/step - accuracy: 0.4878 - binary_io_u_5: 0.4878 - loss: 0.9178Batch 205, Loss Value: 0.9098\n",
      "Batch 205, Gradient Norm: 0.2157\n",
      "\u001b[1m205/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m12s\u001b[0m 481ms/step - accuracy: 0.4878 - binary_io_u_5: 0.4878 - loss: 0.9177Batch 206, Loss Value: 0.9175\n",
      "Batch 206, Gradient Norm: 0.4260\n",
      "\u001b[1m206/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m12s\u001b[0m 481ms/step - accuracy: 0.4878 - binary_io_u_5: 0.4879 - loss: 0.9177Batch 207, Loss Value: 0.9172\n",
      "Batch 207, Gradient Norm: 0.0834\n",
      "\u001b[1m207/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m12s\u001b[0m 481ms/step - accuracy: 0.4878 - binary_io_u_5: 0.4879 - loss: 0.9177Batch 208, Loss Value: 0.9223\n",
      "Batch 208, Gradient Norm: 0.2489\n",
      "\u001b[1m208/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m11s\u001b[0m 481ms/step - accuracy: 0.4879 - binary_io_u_5: 0.4879 - loss: 0.9177Batch 209, Loss Value: 0.8980\n",
      "Batch 209, Gradient Norm: 0.0865\n",
      "\u001b[1m209/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m11s\u001b[0m 481ms/step - accuracy: 0.4879 - binary_io_u_5: 0.4880 - loss: 0.9177Batch 210, Loss Value: 0.8935\n",
      "Batch 210, Gradient Norm: 0.3244\n",
      "\u001b[1m210/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m10s\u001b[0m 481ms/step - accuracy: 0.4879 - binary_io_u_5: 0.4880 - loss: 0.9176Batch 211, Loss Value: 0.9447\n",
      "Batch 211, Gradient Norm: 0.1147\n",
      "\u001b[1m211/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m10s\u001b[0m 481ms/step - accuracy: 0.4880 - binary_io_u_5: 0.4880 - loss: 0.9176Batch 212, Loss Value: 0.9285\n",
      "Batch 212, Gradient Norm: 0.1432\n",
      "\u001b[1m212/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m9s\u001b[0m 481ms/step - accuracy: 0.4880 - binary_io_u_5: 0.4881 - loss: 0.9176 Batch 213, Loss Value: 0.9260\n",
      "Batch 213, Gradient Norm: 0.1453\n",
      "\u001b[1m213/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m9s\u001b[0m 481ms/step - accuracy: 0.4880 - binary_io_u_5: 0.4881 - loss: 0.9176Batch 214, Loss Value: 0.9317\n",
      "Batch 214, Gradient Norm: 0.4755\n",
      "\u001b[1m214/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m8s\u001b[0m 481ms/step - accuracy: 0.4881 - binary_io_u_5: 0.4881 - loss: 0.9176Batch 215, Loss Value: 0.9279\n",
      "Batch 215, Gradient Norm: 0.4436\n",
      "\u001b[1m215/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m8s\u001b[0m 481ms/step - accuracy: 0.4881 - binary_io_u_5: 0.4882 - loss: 0.9175Batch 216, Loss Value: 0.9448\n",
      "Batch 216, Gradient Norm: 0.0185\n",
      "\u001b[1m216/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m7s\u001b[0m 481ms/step - accuracy: 0.4881 - binary_io_u_5: 0.4882 - loss: 0.9175Batch 217, Loss Value: 0.9209\n",
      "Batch 217, Gradient Norm: 0.3302\n",
      "\u001b[1m217/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m7s\u001b[0m 481ms/step - accuracy: 0.4882 - binary_io_u_5: 0.4882 - loss: 0.9175Batch 218, Loss Value: 0.9032\n",
      "Batch 218, Gradient Norm: 0.1646\n",
      "\u001b[1m218/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m6s\u001b[0m 481ms/step - accuracy: 0.4882 - binary_io_u_5: 0.4883 - loss: 0.9175Batch 219, Loss Value: 0.9449\n",
      "Batch 219, Gradient Norm: 0.0124\n",
      "\u001b[1m219/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m6s\u001b[0m 481ms/step - accuracy: 0.4882 - binary_io_u_5: 0.4883 - loss: 0.9174Batch 220, Loss Value: 0.9135\n",
      "Batch 220, Gradient Norm: 0.2100\n",
      "\u001b[1m220/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m5s\u001b[0m 481ms/step - accuracy: 0.4883 - binary_io_u_5: 0.4883 - loss: 0.9174Batch 221, Loss Value: 0.9450\n",
      "Batch 221, Gradient Norm: 0.0076\n",
      "\u001b[1m221/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m5s\u001b[0m 480ms/step - accuracy: 0.4883 - binary_io_u_5: 0.4884 - loss: 0.9174Batch 222, Loss Value: 0.9383\n",
      "Batch 222, Gradient Norm: 0.2145\n",
      "\u001b[1m222/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m4s\u001b[0m 480ms/step - accuracy: 0.4883 - binary_io_u_5: 0.4884 - loss: 0.9174Batch 223, Loss Value: 0.9299\n",
      "Batch 223, Gradient Norm: 0.1997\n",
      "\u001b[1m223/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m4s\u001b[0m 480ms/step - accuracy: 0.4884 - binary_io_u_5: 0.4884 - loss: 0.9173Batch 224, Loss Value: 0.9326\n",
      "Batch 224, Gradient Norm: 0.2701\n",
      "\u001b[1m224/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m3s\u001b[0m 480ms/step - accuracy: 0.4884 - binary_io_u_5: 0.4885 - loss: 0.9173Batch 225, Loss Value: 0.9390\n",
      "Batch 225, Gradient Norm: 0.5456\n",
      "\u001b[1m225/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m3s\u001b[0m 480ms/step - accuracy: 0.4884 - binary_io_u_5: 0.4885 - loss: 0.9173Batch 226, Loss Value: 0.9334\n",
      "Batch 226, Gradient Norm: 0.3278\n",
      "\u001b[1m226/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m2s\u001b[0m 480ms/step - accuracy: 0.4885 - binary_io_u_5: 0.4885 - loss: 0.9173Batch 227, Loss Value: 0.9126\n",
      "Batch 227, Gradient Norm: 0.1139\n",
      "\u001b[1m227/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m2s\u001b[0m 480ms/step - accuracy: 0.4885 - binary_io_u_5: 0.4886 - loss: 0.9173Batch 228, Loss Value: 0.9399\n",
      "Batch 228, Gradient Norm: 0.2081\n",
      "\u001b[1m228/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m1s\u001b[0m 480ms/step - accuracy: 0.4885 - binary_io_u_5: 0.4886 - loss: 0.9172Batch 229, Loss Value: 0.9296\n",
      "Batch 229, Gradient Norm: 0.2067\n",
      "\u001b[1m229/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m1s\u001b[0m 480ms/step - accuracy: 0.4886 - binary_io_u_5: 0.4886 - loss: 0.9172Batch 230, Loss Value: 0.9447\n",
      "Batch 230, Gradient Norm: 0.0057\n",
      "\u001b[1m230/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 480ms/step - accuracy: 0.4886 - binary_io_u_5: 0.4886 - loss: 0.9172Batch 231, Loss Value: 0.9319\n",
      "Batch 231, Gradient Norm: 0.1654\n",
      "\u001b[1m231/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 480ms/step - accuracy: 0.4886 - binary_io_u_5: 0.4887 - loss: 0.9172Batch 232, Loss Value: 0.9263\n",
      "Batch 232, Gradient Norm: 0.0964\n",
      "\u001b[1m232/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m113s\u001b[0m 486ms/step - accuracy: 0.4887 - binary_io_u_5: 0.4887 - loss: 0.9171 - val_accuracy: 0.4979 - val_binary_io_u_5: 0.4949 - val_loss: 0.9125 - learning_rate: 5.0000e-04\n",
      "Epoch 18/200\n",
      "Batch 1, Loss Value: 0.9490\n",
      "Batch 1, Gradient Norm: 0.0150\n",
      "\u001b[1m  1/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:56\u001b[0m 502ms/step - accuracy: 0.5000 - binary_io_u_5: 0.5000 - loss: 0.9399Batch 2, Loss Value: 0.9482\n",
      "Batch 2, Gradient Norm: 0.0054\n",
      "\u001b[1m  2/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:49\u001b[0m 474ms/step - accuracy: 0.4975 - binary_io_u_5: 0.4975 - loss: 0.9406Batch 3, Loss Value: 0.9484\n",
      "Batch 3, Gradient Norm: 0.0067\n",
      "\u001b[1m  3/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:48\u001b[0m 472ms/step - accuracy: 0.4783 - binary_io_u_5: 0.4783 - loss: 0.9424Batch 4, Loss Value: 0.9485\n",
      "Batch 4, Gradient Norm: 0.0088\n",
      "\u001b[1m  4/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:47\u001b[0m 472ms/step - accuracy: 0.4744 - binary_io_u_5: 0.4762 - loss: 0.9408Batch 5, Loss Value: 0.9482\n",
      "Batch 5, Gradient Norm: 0.0033\n",
      "\u001b[1m  5/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:47\u001b[0m 473ms/step - accuracy: 0.4719 - binary_io_u_5: 0.4746 - loss: 0.9402Batch 6, Loss Value: 0.9489\n",
      "Batch 6, Gradient Norm: 0.0034\n",
      "\u001b[1m  6/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:47\u001b[0m 474ms/step - accuracy: 0.4724 - binary_io_u_5: 0.4752 - loss: 0.9383Batch 7, Loss Value: 0.9490\n",
      "Batch 7, Gradient Norm: 0.0062\n",
      "\u001b[1m  7/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:46\u001b[0m 473ms/step - accuracy: 0.4731 - binary_io_u_5: 0.4759 - loss: 0.9365Batch 8, Loss Value: 0.9492\n",
      "Batch 8, Gradient Norm: 0.0014\n",
      "\u001b[1m  8/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:45\u001b[0m 472ms/step - accuracy: 0.4725 - binary_io_u_5: 0.4753 - loss: 0.9355Batch 9, Loss Value: 0.9479\n",
      "Batch 9, Gradient Norm: 0.0033\n",
      "\u001b[1m  9/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:44\u001b[0m 471ms/step - accuracy: 0.4729 - binary_io_u_5: 0.4756 - loss: 0.9346Batch 10, Loss Value: 0.9483\n",
      "Batch 10, Gradient Norm: 0.0015\n",
      "\u001b[1m 10/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:44\u001b[0m 472ms/step - accuracy: 0.4734 - binary_io_u_5: 0.4759 - loss: 0.9337Batch 11, Loss Value: 0.9485\n",
      "Batch 11, Gradient Norm: 0.0213\n",
      "\u001b[1m 11/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:44\u001b[0m 472ms/step - accuracy: 0.4746 - binary_io_u_5: 0.4770 - loss: 0.9325Batch 12, Loss Value: 0.9490\n",
      "Batch 12, Gradient Norm: 0.0023\n",
      "\u001b[1m 12/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:43\u001b[0m 472ms/step - accuracy: 0.4759 - binary_io_u_5: 0.4781 - loss: 0.9314Batch 13, Loss Value: 0.9493\n",
      "Batch 13, Gradient Norm: 0.0111\n",
      "\u001b[1m 13/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:43\u001b[0m 472ms/step - accuracy: 0.4770 - binary_io_u_5: 0.4789 - loss: 0.9306Batch 14, Loss Value: 0.9494\n",
      "Batch 14, Gradient Norm: 0.0185\n",
      "\u001b[1m 14/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:42\u001b[0m 472ms/step - accuracy: 0.4775 - binary_io_u_5: 0.4792 - loss: 0.9300Batch 15, Loss Value: 0.9480\n",
      "Batch 15, Gradient Norm: 0.0047\n",
      "\u001b[1m 15/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:42\u001b[0m 471ms/step - accuracy: 0.4784 - binary_io_u_5: 0.4800 - loss: 0.9295Batch 16, Loss Value: 0.9496\n",
      "Batch 16, Gradient Norm: 0.0336\n",
      "\u001b[1m 16/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:41\u001b[0m 471ms/step - accuracy: 0.4790 - binary_io_u_5: 0.4804 - loss: 0.9290Batch 17, Loss Value: 0.9484\n",
      "Batch 17, Gradient Norm: 0.0028\n",
      "\u001b[1m 17/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:41\u001b[0m 471ms/step - accuracy: 0.4798 - binary_io_u_5: 0.4811 - loss: 0.9286Batch 18, Loss Value: 0.9493\n",
      "Batch 18, Gradient Norm: 0.0048\n",
      "\u001b[1m 18/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:40\u001b[0m 471ms/step - accuracy: 0.4806 - binary_io_u_5: 0.4818 - loss: 0.9281Batch 19, Loss Value: 0.9368\n",
      "Batch 19, Gradient Norm: 0.1636\n",
      "\u001b[1m 19/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:40\u001b[0m 471ms/step - accuracy: 0.4813 - binary_io_u_5: 0.4824 - loss: 0.9277Batch 20, Loss Value: 0.9484\n",
      "Batch 20, Gradient Norm: 0.0058\n",
      "\u001b[1m 20/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:39\u001b[0m 471ms/step - accuracy: 0.4820 - binary_io_u_5: 0.4831 - loss: 0.9273Batch 21, Loss Value: 0.9484\n",
      "Batch 21, Gradient Norm: 0.0088\n",
      "\u001b[1m 21/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:39\u001b[0m 471ms/step - accuracy: 0.4827 - binary_io_u_5: 0.4838 - loss: 0.9269Batch 22, Loss Value: 0.9485\n",
      "Batch 22, Gradient Norm: 0.0079\n",
      "\u001b[1m 22/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:39\u001b[0m 472ms/step - accuracy: 0.4834 - binary_io_u_5: 0.4844 - loss: 0.9265Batch 23, Loss Value: 0.9485\n",
      "Batch 23, Gradient Norm: 0.0113\n",
      "\u001b[1m 23/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:38\u001b[0m 472ms/step - accuracy: 0.4842 - binary_io_u_5: 0.4851 - loss: 0.9260Batch 24, Loss Value: 0.9484\n",
      "Batch 24, Gradient Norm: 0.0050\n",
      "\u001b[1m 24/232\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:38\u001b[0m 472ms/step - accuracy: 0.4848 - binary_io_u_5: 0.4857 - loss: 0.9256Batch 25, Loss Value: 0.9483\n",
      "Batch 25, Gradient Norm: 0.0002\n",
      "\u001b[1m 25/232\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:37\u001b[0m 472ms/step - accuracy: 0.4853 - binary_io_u_5: 0.4862 - loss: 0.9253Batch 26, Loss Value: 0.9493\n",
      "Batch 26, Gradient Norm: 0.0316\n",
      "\u001b[1m 26/232\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:37\u001b[0m 471ms/step - accuracy: 0.4858 - binary_io_u_5: 0.4866 - loss: 0.9250Batch 27, Loss Value: 0.9483\n",
      "Batch 27, Gradient Norm: 0.0016\n",
      "\u001b[1m 27/232\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:36\u001b[0m 472ms/step - accuracy: 0.4863 - binary_io_u_5: 0.4871 - loss: 0.9247Batch 28, Loss Value: 0.9483\n",
      "Batch 28, Gradient Norm: 0.0004\n",
      "\u001b[1m 28/232\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:36\u001b[0m 473ms/step - accuracy: 0.4867 - binary_io_u_5: 0.4875 - loss: 0.9244Batch 29, Loss Value: 0.9483\n",
      "Batch 29, Gradient Norm: 0.0005\n",
      "\u001b[1m 29/232\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:36\u001b[0m 473ms/step - accuracy: 0.4872 - binary_io_u_5: 0.4880 - loss: 0.9241Batch 30, Loss Value: 0.9492\n",
      "Batch 30, Gradient Norm: 0.0022\n",
      "\u001b[1m 30/232\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:35\u001b[0m 474ms/step - accuracy: 0.4877 - binary_io_u_5: 0.4885 - loss: 0.9238Batch 31, Loss Value: 0.9483\n",
      "Batch 31, Gradient Norm: 0.0005\n",
      "\u001b[1m 31/232\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:35\u001b[0m 474ms/step - accuracy: 0.4883 - binary_io_u_5: 0.4890 - loss: 0.9234Batch 32, Loss Value: 0.9484\n",
      "Batch 32, Gradient Norm: 0.0086\n",
      "\u001b[1m 32/232\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:34\u001b[0m 474ms/step - accuracy: 0.4889 - binary_io_u_5: 0.4896 - loss: 0.9231Batch 33, Loss Value: 0.9483\n",
      "Batch 33, Gradient Norm: 0.0000\n",
      "\u001b[1m 33/232\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:34\u001b[0m 474ms/step - accuracy: 0.4894 - binary_io_u_5: 0.4901 - loss: 0.9227Batch 34, Loss Value: 0.9483\n",
      "Batch 34, Gradient Norm: 0.0000\n",
      "\u001b[1m 34/232\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:33\u001b[0m 474ms/step - accuracy: 0.4899 - binary_io_u_5: 0.4905 - loss: 0.9224Batch 35, Loss Value: 0.9483\n",
      "Batch 35, Gradient Norm: 0.0000\n",
      "\u001b[1m 35/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:33\u001b[0m 474ms/step - accuracy: 0.4902 - binary_io_u_5: 0.4909 - loss: 0.9221Batch 36, Loss Value: 0.9483\n",
      "Batch 36, Gradient Norm: 0.0002\n",
      "\u001b[1m 36/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:32\u001b[0m 474ms/step - accuracy: 0.4905 - binary_io_u_5: 0.4912 - loss: 0.9219Batch 37, Loss Value: 0.9483\n",
      "Batch 37, Gradient Norm: 0.0003\n",
      "\u001b[1m 37/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:32\u001b[0m 474ms/step - accuracy: 0.4908 - binary_io_u_5: 0.4914 - loss: 0.9217Batch 38, Loss Value: 0.9485\n",
      "Batch 38, Gradient Norm: 0.0055\n",
      "\u001b[1m 38/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:32\u001b[0m 475ms/step - accuracy: 0.4911 - binary_io_u_5: 0.4916 - loss: 0.9215Batch 39, Loss Value: 0.9483\n",
      "Batch 39, Gradient Norm: 0.0000\n",
      "\u001b[1m 39/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:31\u001b[0m 474ms/step - accuracy: 0.4913 - binary_io_u_5: 0.4918 - loss: 0.9214Batch 40, Loss Value: 0.9482\n",
      "Batch 40, Gradient Norm: 0.0042\n",
      "\u001b[1m 40/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:31\u001b[0m 474ms/step - accuracy: 0.4915 - binary_io_u_5: 0.4920 - loss: 0.9212Batch 41, Loss Value: 0.9483\n",
      "Batch 41, Gradient Norm: 0.0003\n",
      "\u001b[1m 41/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:30\u001b[0m 475ms/step - accuracy: 0.4917 - binary_io_u_5: 0.4922 - loss: 0.9211Batch 42, Loss Value: 0.9489\n",
      "Batch 42, Gradient Norm: 0.0055\n",
      "\u001b[1m 42/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:30\u001b[0m 475ms/step - accuracy: 0.4918 - binary_io_u_5: 0.4923 - loss: 0.9210Batch 43, Loss Value: 0.9483\n",
      "Batch 43, Gradient Norm: 0.0000\n",
      "\u001b[1m 43/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:29\u001b[0m 475ms/step - accuracy: 0.4919 - binary_io_u_5: 0.4924 - loss: 0.9209Batch 44, Loss Value: 0.9483\n",
      "Batch 44, Gradient Norm: 0.0010\n",
      "\u001b[1m 44/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:29\u001b[0m 474ms/step - accuracy: 0.4919 - binary_io_u_5: 0.4924 - loss: 0.9208Batch 45, Loss Value: 0.9483\n",
      "Batch 45, Gradient Norm: 0.0029\n",
      "\u001b[1m 45/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:28\u001b[0m 474ms/step - accuracy: 0.4920 - binary_io_u_5: 0.4925 - loss: 0.9207Batch 46, Loss Value: 0.9488\n",
      "Batch 46, Gradient Norm: 0.0090\n",
      "\u001b[1m 46/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:28\u001b[0m 474ms/step - accuracy: 0.4921 - binary_io_u_5: 0.4926 - loss: 0.9206Batch 47, Loss Value: 0.9478\n",
      "Batch 47, Gradient Norm: 0.0015\n",
      "\u001b[1m 47/232\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:27\u001b[0m 475ms/step - accuracy: 0.4922 - binary_io_u_5: 0.4926 - loss: 0.9205Batch 48, Loss Value: 0.9486\n",
      "Batch 48, Gradient Norm: 0.0046\n",
      "\u001b[1m 48/232\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:27\u001b[0m 475ms/step - accuracy: 0.4922 - binary_io_u_5: 0.4927 - loss: 0.9204Batch 49, Loss Value: 0.9483\n",
      "Batch 49, Gradient Norm: 0.0000\n",
      "\u001b[1m 49/232\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:26\u001b[0m 475ms/step - accuracy: 0.4923 - binary_io_u_5: 0.4927 - loss: 0.9203Batch 50, Loss Value: 0.9483\n",
      "Batch 50, Gradient Norm: 0.0001\n",
      "\u001b[1m 50/232\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:26\u001b[0m 475ms/step - accuracy: 0.4923 - binary_io_u_5: 0.4928 - loss: 0.9202Batch 51, Loss Value: 0.9483\n",
      "Batch 51, Gradient Norm: 0.0000\n",
      "\u001b[1m 51/232\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:26\u001b[0m 475ms/step - accuracy: 0.4924 - binary_io_u_5: 0.4928 - loss: 0.9202Batch 52, Loss Value: 0.9482\n",
      "Batch 52, Gradient Norm: 0.0011\n",
      "\u001b[1m 52/232\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:25\u001b[0m 475ms/step - accuracy: 0.4924 - binary_io_u_5: 0.4929 - loss: 0.9201Batch 53, Loss Value: 0.9485\n",
      "Batch 53, Gradient Norm: 0.0054\n",
      "\u001b[1m 53/232\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:25\u001b[0m 476ms/step - accuracy: 0.4925 - binary_io_u_5: 0.4929 - loss: 0.9200Batch 54, Loss Value: 0.9490\n",
      "Batch 54, Gradient Norm: 0.0043\n",
      "\u001b[1m 54/232\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:24\u001b[0m 475ms/step - accuracy: 0.4925 - binary_io_u_5: 0.4929 - loss: 0.9199Batch 55, Loss Value: 0.9483\n",
      "Batch 55, Gradient Norm: 0.0001\n",
      "\u001b[1m 55/232\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:24\u001b[0m 475ms/step - accuracy: 0.4926 - binary_io_u_5: 0.4930 - loss: 0.9198Batch 56, Loss Value: 0.9479\n",
      "Batch 56, Gradient Norm: 0.0021\n",
      "\u001b[1m 56/232\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:23\u001b[0m 475ms/step - accuracy: 0.4927 - binary_io_u_5: 0.4931 - loss: 0.9198Batch 57, Loss Value: 0.9483\n",
      "Batch 57, Gradient Norm: 0.0008\n",
      "\u001b[1m 57/232\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:23\u001b[0m 475ms/step - accuracy: 0.4927 - binary_io_u_5: 0.4931 - loss: 0.9197Batch 58, Loss Value: 0.9483\n",
      "Batch 58, Gradient Norm: 0.0000\n",
      "\u001b[1m 58/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:22\u001b[0m 475ms/step - accuracy: 0.4928 - binary_io_u_5: 0.4932 - loss: 0.9196Batch 59, Loss Value: 0.9483\n",
      "Batch 59, Gradient Norm: 0.0000\n",
      "\u001b[1m 59/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:22\u001b[0m 475ms/step - accuracy: 0.4928 - binary_io_u_5: 0.4932 - loss: 0.9196Batch 60, Loss Value: 0.9483\n",
      "Batch 60, Gradient Norm: 0.0000\n",
      "\u001b[1m 60/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:21\u001b[0m 476ms/step - accuracy: 0.4928 - binary_io_u_5: 0.4932 - loss: 0.9195Batch 61, Loss Value: 0.9483\n",
      "Batch 61, Gradient Norm: 0.0000\n",
      "\u001b[1m 61/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:21\u001b[0m 476ms/step - accuracy: 0.4929 - binary_io_u_5: 0.4932 - loss: 0.9195Batch 62, Loss Value: 0.9484\n",
      "Batch 62, Gradient Norm: 0.0046\n",
      "\u001b[1m 62/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:20\u001b[0m 476ms/step - accuracy: 0.4929 - binary_io_u_5: 0.4933 - loss: 0.9194Batch 63, Loss Value: 0.9483\n",
      "Batch 63, Gradient Norm: 0.0000\n",
      "\u001b[1m 63/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:20\u001b[0m 476ms/step - accuracy: 0.4929 - binary_io_u_5: 0.4933 - loss: 0.9194Batch 64, Loss Value: 0.9483\n",
      "Batch 64, Gradient Norm: 0.0015\n",
      "\u001b[1m 64/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:19\u001b[0m 476ms/step - accuracy: 0.4929 - binary_io_u_5: 0.4933 - loss: 0.9193Batch 65, Loss Value: 0.9483\n",
      "Batch 65, Gradient Norm: 0.0000\n",
      "\u001b[1m 65/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:19\u001b[0m 475ms/step - accuracy: 0.4929 - binary_io_u_5: 0.4933 - loss: 0.9193Batch 66, Loss Value: 0.9484\n",
      "Batch 66, Gradient Norm: 0.0080\n",
      "\u001b[1m 66/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:18\u001b[0m 475ms/step - accuracy: 0.4929 - binary_io_u_5: 0.4933 - loss: 0.9193Batch 67, Loss Value: 0.9483\n",
      "Batch 67, Gradient Norm: 0.0001\n",
      "\u001b[1m 67/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:18\u001b[0m 475ms/step - accuracy: 0.4930 - binary_io_u_5: 0.4933 - loss: 0.9192Batch 68, Loss Value: 0.9483\n",
      "Batch 68, Gradient Norm: 0.0006\n",
      "\u001b[1m 68/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:17\u001b[0m 476ms/step - accuracy: 0.4930 - binary_io_u_5: 0.4933 - loss: 0.9192Batch 69, Loss Value: 0.9483\n",
      "Batch 69, Gradient Norm: 0.0000\n",
      "\u001b[1m 69/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:17\u001b[0m 476ms/step - accuracy: 0.4930 - binary_io_u_5: 0.4933 - loss: 0.9192Batch 70, Loss Value: 0.9483\n",
      "Batch 70, Gradient Norm: 0.0005\n",
      "\u001b[1m 70/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:17\u001b[0m 476ms/step - accuracy: 0.4930 - binary_io_u_5: 0.4933 - loss: 0.9191Batch 71, Loss Value: 0.9483\n",
      "Batch 71, Gradient Norm: 0.0000\n",
      "\u001b[1m 71/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:16\u001b[0m 476ms/step - accuracy: 0.4930 - binary_io_u_5: 0.4933 - loss: 0.9191Batch 72, Loss Value: 0.9483\n",
      "Batch 72, Gradient Norm: 0.0000\n",
      "\u001b[1m 72/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:16\u001b[0m 476ms/step - accuracy: 0.4930 - binary_io_u_5: 0.4933 - loss: 0.9191Batch 73, Loss Value: 0.9483\n",
      "Batch 73, Gradient Norm: 0.0023\n",
      "\u001b[1m 73/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:15\u001b[0m 476ms/step - accuracy: 0.4930 - binary_io_u_5: 0.4933 - loss: 0.9190Batch 74, Loss Value: 0.9484\n",
      "Batch 74, Gradient Norm: 0.0068\n",
      "\u001b[1m 74/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:15\u001b[0m 476ms/step - accuracy: 0.4930 - binary_io_u_5: 0.4933 - loss: 0.9190Batch 75, Loss Value: 0.9483\n",
      "Batch 75, Gradient Norm: 0.0023\n",
      "\u001b[1m 75/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:14\u001b[0m 476ms/step - accuracy: 0.4931 - binary_io_u_5: 0.4934 - loss: 0.9190Batch 76, Loss Value: 0.9486\n",
      "Batch 76, Gradient Norm: 0.0051\n",
      "\u001b[1m 76/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:14\u001b[0m 476ms/step - accuracy: 0.4931 - binary_io_u_5: 0.4934 - loss: 0.9189Batch 77, Loss Value: 0.9483\n",
      "Batch 77, Gradient Norm: 0.0000\n",
      "\u001b[1m 77/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:13\u001b[0m 476ms/step - accuracy: 0.4931 - binary_io_u_5: 0.4934 - loss: 0.9189Batch 78, Loss Value: 0.9484\n",
      "Batch 78, Gradient Norm: 0.0042\n",
      "\u001b[1m 78/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:13\u001b[0m 476ms/step - accuracy: 0.4932 - binary_io_u_5: 0.4935 - loss: 0.9188Batch 79, Loss Value: 0.9487\n",
      "Batch 79, Gradient Norm: 0.0051\n",
      "\u001b[1m 79/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:12\u001b[0m 476ms/step - accuracy: 0.4933 - binary_io_u_5: 0.4935 - loss: 0.9187Batch 80, Loss Value: 0.9483\n",
      "Batch 80, Gradient Norm: 0.0000\n",
      "\u001b[1m 80/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:12\u001b[0m 476ms/step - accuracy: 0.4933 - binary_io_u_5: 0.4936 - loss: 0.9187Batch 81, Loss Value: 0.9487\n",
      "Batch 81, Gradient Norm: 0.0197\n",
      "\u001b[1m 81/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:11\u001b[0m 476ms/step - accuracy: 0.4934 - binary_io_u_5: 0.4936 - loss: 0.9186Batch 82, Loss Value: 0.9483\n",
      "Batch 82, Gradient Norm: 0.0000\n",
      "\u001b[1m 82/232\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m1:11\u001b[0m 476ms/step - accuracy: 0.4934 - binary_io_u_5: 0.4937 - loss: 0.9186Batch 83, Loss Value: 0.9483\n",
      "Batch 83, Gradient Norm: 0.0000\n",
      "\u001b[1m 83/232\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m1:10\u001b[0m 476ms/step - accuracy: 0.4935 - binary_io_u_5: 0.4937 - loss: 0.9185Batch 84, Loss Value: 0.9483\n",
      "Batch 84, Gradient Norm: 0.0001\n",
      "\u001b[1m 84/232\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m1:10\u001b[0m 476ms/step - accuracy: 0.4935 - binary_io_u_5: 0.4938 - loss: 0.9185Batch 85, Loss Value: 0.9483\n",
      "Batch 85, Gradient Norm: 0.0009\n",
      "\u001b[1m 85/232\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m1:10\u001b[0m 476ms/step - accuracy: 0.4936 - binary_io_u_5: 0.4939 - loss: 0.9184Batch 86, Loss Value: 0.9486\n",
      "Batch 86, Gradient Norm: 0.0111\n",
      "\u001b[1m 86/232\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m1:09\u001b[0m 476ms/step - accuracy: 0.4936 - binary_io_u_5: 0.4939 - loss: 0.9183Batch 87, Loss Value: 0.9483\n",
      "Batch 87, Gradient Norm: 0.0001\n",
      "\u001b[1m 87/232\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m1:09\u001b[0m 476ms/step - accuracy: 0.4937 - binary_io_u_5: 0.4939 - loss: 0.9183Batch 88, Loss Value: 0.9483\n",
      "Batch 88, Gradient Norm: 0.0000\n",
      "\u001b[1m 88/232\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m1:08\u001b[0m 476ms/step - accuracy: 0.4938 - binary_io_u_5: 0.4940 - loss: 0.9182Batch 89, Loss Value: 0.9483\n",
      "Batch 89, Gradient Norm: 0.0000\n",
      "\u001b[1m 89/232\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m1:08\u001b[0m 476ms/step - accuracy: 0.4938 - binary_io_u_5: 0.4940 - loss: 0.9182Batch 90, Loss Value: 0.9485\n",
      "Batch 90, Gradient Norm: 0.0024\n",
      "\u001b[1m 90/232\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m1:07\u001b[0m 476ms/step - accuracy: 0.4938 - binary_io_u_5: 0.4941 - loss: 0.9181Batch 91, Loss Value: 0.9483\n",
      "Batch 91, Gradient Norm: 0.0025\n",
      "\u001b[1m 91/232\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m1:07\u001b[0m 476ms/step - accuracy: 0.4939 - binary_io_u_5: 0.4941 - loss: 0.9181Batch 92, Loss Value: 0.9483\n",
      "Batch 92, Gradient Norm: 0.0047\n",
      "\u001b[1m 92/232\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m1:06\u001b[0m 477ms/step - accuracy: 0.4939 - binary_io_u_5: 0.4942 - loss: 0.9180Batch 93, Loss Value: 0.9483\n",
      "Batch 93, Gradient Norm: 0.0006\n",
      "\u001b[1m 93/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m1:06\u001b[0m 477ms/step - accuracy: 0.4940 - binary_io_u_5: 0.4942 - loss: 0.9180Batch 94, Loss Value: 0.9489\n",
      "Batch 94, Gradient Norm: 0.0034\n",
      "\u001b[1m 94/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m1:05\u001b[0m 477ms/step - accuracy: 0.4940 - binary_io_u_5: 0.4942 - loss: 0.9180Batch 95, Loss Value: 0.9498\n",
      "Batch 95, Gradient Norm: 0.0197\n",
      "\u001b[1m 95/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m1:05\u001b[0m 477ms/step - accuracy: 0.4940 - binary_io_u_5: 0.4942 - loss: 0.9179Batch 96, Loss Value: 0.9483\n",
      "Batch 96, Gradient Norm: 0.0000\n",
      "\u001b[1m 96/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m1:04\u001b[0m 477ms/step - accuracy: 0.4940 - binary_io_u_5: 0.4942 - loss: 0.9179Batch 97, Loss Value: 0.9483\n",
      "Batch 97, Gradient Norm: 0.0000\n",
      "\u001b[1m 97/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m1:04\u001b[0m 477ms/step - accuracy: 0.4940 - binary_io_u_5: 0.4943 - loss: 0.9179Batch 98, Loss Value: 0.9484\n",
      "Batch 98, Gradient Norm: 0.0024\n",
      "\u001b[1m 98/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m1:03\u001b[0m 477ms/step - accuracy: 0.4941 - binary_io_u_5: 0.4943 - loss: 0.9178Batch 99, Loss Value: 0.9485\n",
      "Batch 99, Gradient Norm: 0.0041\n",
      "\u001b[1m 99/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m1:03\u001b[0m 477ms/step - accuracy: 0.4941 - binary_io_u_5: 0.4943 - loss: 0.9178Batch 100, Loss Value: 0.9485\n",
      "Batch 100, Gradient Norm: 0.0068\n",
      "\u001b[1m100/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m1:02\u001b[0m 477ms/step - accuracy: 0.4941 - binary_io_u_5: 0.4943 - loss: 0.9178Batch 101, Loss Value: 0.9481\n",
      "Batch 101, Gradient Norm: 0.0050\n",
      "\u001b[1m101/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m1:02\u001b[0m 477ms/step - accuracy: 0.4942 - binary_io_u_5: 0.4944 - loss: 0.9177Batch 102, Loss Value: 0.9490\n",
      "Batch 102, Gradient Norm: 0.0049\n",
      "\u001b[1m102/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m1:01\u001b[0m 477ms/step - accuracy: 0.4942 - binary_io_u_5: 0.4944 - loss: 0.9177Batch 103, Loss Value: 0.9482\n",
      "Batch 103, Gradient Norm: 0.0029\n",
      "\u001b[1m103/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m1:01\u001b[0m 477ms/step - accuracy: 0.4942 - binary_io_u_5: 0.4944 - loss: 0.9176Batch 104, Loss Value: 0.9483\n",
      "Batch 104, Gradient Norm: 0.0001\n",
      "\u001b[1m104/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m1:01\u001b[0m 477ms/step - accuracy: 0.4943 - binary_io_u_5: 0.4945 - loss: 0.9176Batch 105, Loss Value: 0.9487\n",
      "Batch 105, Gradient Norm: 0.0054\n",
      "\u001b[1m105/232\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m1:00\u001b[0m 477ms/step - accuracy: 0.4943 - binary_io_u_5: 0.4945 - loss: 0.9175Batch 106, Loss Value: 0.9483\n",
      "Batch 106, Gradient Norm: 0.0011\n",
      "\u001b[1m106/232\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m1:00\u001b[0m 477ms/step - accuracy: 0.4943 - binary_io_u_5: 0.4945 - loss: 0.9175Batch 107, Loss Value: 0.9483\n",
      "Batch 107, Gradient Norm: 0.0011\n",
      "\u001b[1m107/232\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m59s\u001b[0m 477ms/step - accuracy: 0.4944 - binary_io_u_5: 0.4946 - loss: 0.9175 Batch 108, Loss Value: 0.9483\n",
      "Batch 108, Gradient Norm: 0.0014\n",
      "\u001b[1m108/232\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m59s\u001b[0m 477ms/step - accuracy: 0.4944 - binary_io_u_5: 0.4946 - loss: 0.9174Batch 109, Loss Value: 0.9484\n",
      "Batch 109, Gradient Norm: 0.0105\n",
      "\u001b[1m109/232\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m58s\u001b[0m 477ms/step - accuracy: 0.4944 - binary_io_u_5: 0.4946 - loss: 0.9174Batch 110, Loss Value: 0.9491\n",
      "Batch 110, Gradient Norm: 0.0111\n",
      "\u001b[1m110/232\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m58s\u001b[0m 477ms/step - accuracy: 0.4944 - binary_io_u_5: 0.4946 - loss: 0.9173Batch 111, Loss Value: 0.9492\n",
      "Batch 111, Gradient Norm: 0.0036\n",
      "\u001b[1m111/232\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m57s\u001b[0m 477ms/step - accuracy: 0.4945 - binary_io_u_5: 0.4947 - loss: 0.9173Batch 112, Loss Value: 0.9481\n",
      "Batch 112, Gradient Norm: 0.0023\n",
      "\u001b[1m112/232\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m57s\u001b[0m 477ms/step - accuracy: 0.4945 - binary_io_u_5: 0.4947 - loss: 0.9173Batch 113, Loss Value: 0.9485\n",
      "Batch 113, Gradient Norm: 0.0070\n",
      "\u001b[1m113/232\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m56s\u001b[0m 477ms/step - accuracy: 0.4945 - binary_io_u_5: 0.4947 - loss: 0.9172Batch 114, Loss Value: 0.9483\n",
      "Batch 114, Gradient Norm: 0.0020\n",
      "\u001b[1m114/232\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m56s\u001b[0m 477ms/step - accuracy: 0.4945 - binary_io_u_5: 0.4947 - loss: 0.9172Batch 115, Loss Value: 0.9484\n",
      "Batch 115, Gradient Norm: 0.0056\n",
      "\u001b[1m115/232\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m55s\u001b[0m 477ms/step - accuracy: 0.4945 - binary_io_u_5: 0.4947 - loss: 0.9172Batch 116, Loss Value: 0.9483\n",
      "Batch 116, Gradient Norm: 0.0004\n",
      "\u001b[1m116/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m55s\u001b[0m 477ms/step - accuracy: 0.4945 - binary_io_u_5: 0.4947 - loss: 0.9172Batch 117, Loss Value: 0.9483\n",
      "Batch 117, Gradient Norm: 0.0030\n",
      "\u001b[1m117/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m54s\u001b[0m 477ms/step - accuracy: 0.4945 - binary_io_u_5: 0.4947 - loss: 0.9171Batch 118, Loss Value: 0.9487\n",
      "Batch 118, Gradient Norm: 0.0095\n",
      "\u001b[1m118/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m54s\u001b[0m 477ms/step - accuracy: 0.4945 - binary_io_u_5: 0.4947 - loss: 0.9171Batch 119, Loss Value: 0.9484\n",
      "Batch 119, Gradient Norm: 0.0097\n",
      "\u001b[1m119/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m53s\u001b[0m 477ms/step - accuracy: 0.4945 - binary_io_u_5: 0.4947 - loss: 0.9171Batch 120, Loss Value: 0.9483\n",
      "Batch 120, Gradient Norm: 0.0032\n",
      "\u001b[1m120/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m53s\u001b[0m 477ms/step - accuracy: 0.4946 - binary_io_u_5: 0.4947 - loss: 0.9170Batch 121, Loss Value: 0.9483\n",
      "Batch 121, Gradient Norm: 0.0001\n",
      "\u001b[1m121/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m52s\u001b[0m 477ms/step - accuracy: 0.4946 - binary_io_u_5: 0.4947 - loss: 0.9170Batch 122, Loss Value: 0.9480\n",
      "Batch 122, Gradient Norm: 0.0034\n",
      "\u001b[1m122/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m52s\u001b[0m 477ms/step - accuracy: 0.4946 - binary_io_u_5: 0.4947 - loss: 0.9170Batch 123, Loss Value: 0.9489\n",
      "Batch 123, Gradient Norm: 0.0118\n",
      "\u001b[1m123/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m52s\u001b[0m 477ms/step - accuracy: 0.4946 - binary_io_u_5: 0.4947 - loss: 0.9170Batch 124, Loss Value: 0.9483\n",
      "Batch 124, Gradient Norm: 0.0012\n",
      "\u001b[1m124/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m51s\u001b[0m 477ms/step - accuracy: 0.4946 - binary_io_u_5: 0.4948 - loss: 0.9170Batch 125, Loss Value: 0.9486\n",
      "Batch 125, Gradient Norm: 0.0065\n",
      "\u001b[1m125/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m51s\u001b[0m 477ms/step - accuracy: 0.4946 - binary_io_u_5: 0.4948 - loss: 0.9169Batch 126, Loss Value: 0.9496\n",
      "Batch 126, Gradient Norm: 0.0039\n",
      "\u001b[1m126/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m50s\u001b[0m 477ms/step - accuracy: 0.4946 - binary_io_u_5: 0.4948 - loss: 0.9169Batch 127, Loss Value: 0.9483\n",
      "Batch 127, Gradient Norm: 0.0019\n",
      "\u001b[1m127/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m50s\u001b[0m 477ms/step - accuracy: 0.4946 - binary_io_u_5: 0.4948 - loss: 0.9169Batch 128, Loss Value: 0.9486\n",
      "Batch 128, Gradient Norm: 0.0084\n",
      "\u001b[1m128/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m49s\u001b[0m 477ms/step - accuracy: 0.4946 - binary_io_u_5: 0.4948 - loss: 0.9169Batch 129, Loss Value: 0.9494\n",
      "Batch 129, Gradient Norm: 0.0105\n",
      "\u001b[1m129/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m49s\u001b[0m 477ms/step - accuracy: 0.4946 - binary_io_u_5: 0.4948 - loss: 0.9168Batch 130, Loss Value: 0.9485\n",
      "Batch 130, Gradient Norm: 0.0039\n",
      "\u001b[1m130/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m48s\u001b[0m 477ms/step - accuracy: 0.4946 - binary_io_u_5: 0.4948 - loss: 0.9168Batch 131, Loss Value: 0.9501\n",
      "Batch 131, Gradient Norm: 0.0077\n",
      "\u001b[1m131/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m48s\u001b[0m 478ms/step - accuracy: 0.4946 - binary_io_u_5: 0.4948 - loss: 0.9168Batch 132, Loss Value: 0.9504\n",
      "Batch 132, Gradient Norm: 0.0223\n",
      "\u001b[1m132/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m47s\u001b[0m 477ms/step - accuracy: 0.4946 - binary_io_u_5: 0.4948 - loss: 0.9168Batch 133, Loss Value: 0.9489\n",
      "Batch 133, Gradient Norm: 0.0010\n",
      "\u001b[1m133/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m47s\u001b[0m 477ms/step - accuracy: 0.4947 - binary_io_u_5: 0.4948 - loss: 0.9167Batch 134, Loss Value: 0.9345\n",
      "Batch 134, Gradient Norm: 0.1473\n",
      "\u001b[1m134/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m46s\u001b[0m 477ms/step - accuracy: 0.4947 - binary_io_u_5: 0.4948 - loss: 0.9167Batch 135, Loss Value: 0.9497\n",
      "Batch 135, Gradient Norm: 0.0049\n",
      "\u001b[1m135/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m46s\u001b[0m 477ms/step - accuracy: 0.4947 - binary_io_u_5: 0.4948 - loss: 0.9167Batch 136, Loss Value: 0.9502\n",
      "Batch 136, Gradient Norm: 0.0100\n",
      "\u001b[1m136/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m45s\u001b[0m 477ms/step - accuracy: 0.4947 - binary_io_u_5: 0.4949 - loss: 0.9167Batch 137, Loss Value: 0.9484\n",
      "Batch 137, Gradient Norm: 0.0040\n",
      "\u001b[1m137/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m45s\u001b[0m 477ms/step - accuracy: 0.4947 - binary_io_u_5: 0.4949 - loss: 0.9166Batch 138, Loss Value: 0.9488\n",
      "Batch 138, Gradient Norm: 0.0066\n",
      "\u001b[1m138/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m44s\u001b[0m 478ms/step - accuracy: 0.4947 - binary_io_u_5: 0.4949 - loss: 0.9166Batch 139, Loss Value: 0.9394\n",
      "Batch 139, Gradient Norm: 0.5117\n",
      "\u001b[1m139/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m44s\u001b[0m 478ms/step - accuracy: 0.4947 - binary_io_u_5: 0.4949 - loss: 0.9166Batch 140, Loss Value: 0.9496\n",
      "Batch 140, Gradient Norm: 0.0160\n",
      "\u001b[1m140/232\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m43s\u001b[0m 478ms/step - accuracy: 0.4947 - binary_io_u_5: 0.4949 - loss: 0.9166Batch 141, Loss Value: 0.9485\n",
      "Batch 141, Gradient Norm: 0.0052\n",
      "\u001b[1m141/232\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m43s\u001b[0m 478ms/step - accuracy: 0.4948 - binary_io_u_5: 0.4949 - loss: 0.9166Batch 142, Loss Value: 0.9484\n",
      "Batch 142, Gradient Norm: 0.0023\n",
      "\u001b[1m142/232\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m42s\u001b[0m 478ms/step - accuracy: 0.4948 - binary_io_u_5: 0.4949 - loss: 0.9165Batch 143, Loss Value: 0.9489\n",
      "Batch 143, Gradient Norm: 0.0144\n",
      "\u001b[1m143/232\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m42s\u001b[0m 478ms/step - accuracy: 0.4948 - binary_io_u_5: 0.4949 - loss: 0.9165Batch 144, Loss Value: 0.9486\n",
      "Batch 144, Gradient Norm: 0.0013\n",
      "\u001b[1m144/232\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m42s\u001b[0m 478ms/step - accuracy: 0.4948 - binary_io_u_5: 0.4950 - loss: 0.9165Batch 145, Loss Value: 0.9487\n",
      "Batch 145, Gradient Norm: 0.0105\n",
      "\u001b[1m145/232\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m41s\u001b[0m 478ms/step - accuracy: 0.4948 - binary_io_u_5: 0.4950 - loss: 0.9165Batch 146, Loss Value: 0.9487\n",
      "Batch 146, Gradient Norm: 0.0073\n",
      "\u001b[1m146/232\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m41s\u001b[0m 478ms/step - accuracy: 0.4948 - binary_io_u_5: 0.4950 - loss: 0.9164Batch 147, Loss Value: 0.9492\n",
      "Batch 147, Gradient Norm: 0.0065\n",
      "\u001b[1m147/232\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m40s\u001b[0m 478ms/step - accuracy: 0.4949 - binary_io_u_5: 0.4950 - loss: 0.9164Batch 148, Loss Value: 0.9491\n",
      "Batch 148, Gradient Norm: 0.0085\n",
      "\u001b[1m148/232\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m40s\u001b[0m 478ms/step - accuracy: 0.4949 - binary_io_u_5: 0.4950 - loss: 0.9164Batch 149, Loss Value: 0.9234\n",
      "Batch 149, Gradient Norm: 0.0572\n",
      "\u001b[1m149/232\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m39s\u001b[0m 478ms/step - accuracy: 0.4949 - binary_io_u_5: 0.4950 - loss: 0.9164Batch 150, Loss Value: 0.9498\n",
      "Batch 150, Gradient Norm: 0.0133\n",
      "\u001b[1m150/232\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m39s\u001b[0m 478ms/step - accuracy: 0.4949 - binary_io_u_5: 0.4951 - loss: 0.9163Batch 151, Loss Value: 0.9501\n",
      "Batch 151, Gradient Norm: 0.0059\n",
      "\u001b[1m151/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m38s\u001b[0m 478ms/step - accuracy: 0.4949 - binary_io_u_5: 0.4951 - loss: 0.9163Batch 152, Loss Value: 0.9496\n",
      "Batch 152, Gradient Norm: 0.0066\n",
      "\u001b[1m152/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m38s\u001b[0m 478ms/step - accuracy: 0.4949 - binary_io_u_5: 0.4951 - loss: 0.9163Batch 153, Loss Value: 0.9492\n",
      "Batch 153, Gradient Norm: 0.0088\n",
      "\u001b[1m153/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m37s\u001b[0m 478ms/step - accuracy: 0.4950 - binary_io_u_5: 0.4951 - loss: 0.9163Batch 154, Loss Value: 0.9490\n",
      "Batch 154, Gradient Norm: 0.0146\n",
      "\u001b[1m154/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m37s\u001b[0m 478ms/step - accuracy: 0.4950 - binary_io_u_5: 0.4951 - loss: 0.9163Batch 155, Loss Value: 0.9492\n",
      "Batch 155, Gradient Norm: 0.0115\n",
      "\u001b[1m155/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m36s\u001b[0m 478ms/step - accuracy: 0.4950 - binary_io_u_5: 0.4951 - loss: 0.9162Batch 156, Loss Value: 0.9485\n",
      "Batch 156, Gradient Norm: 0.0054\n",
      "\u001b[1m156/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m36s\u001b[0m 478ms/step - accuracy: 0.4950 - binary_io_u_5: 0.4952 - loss: 0.9162Batch 157, Loss Value: 0.9496\n",
      "Batch 157, Gradient Norm: 0.0049\n",
      "\u001b[1m157/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m35s\u001b[0m 478ms/step - accuracy: 0.4951 - binary_io_u_5: 0.4952 - loss: 0.9162Batch 158, Loss Value: 0.9491\n",
      "Batch 158, Gradient Norm: 0.0151\n",
      "\u001b[1m158/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m35s\u001b[0m 478ms/step - accuracy: 0.4951 - binary_io_u_5: 0.4952 - loss: 0.9162Batch 159, Loss Value: 0.9503\n",
      "Batch 159, Gradient Norm: 0.0055\n",
      "\u001b[1m159/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m34s\u001b[0m 478ms/step - accuracy: 0.4951 - binary_io_u_5: 0.4952 - loss: 0.9162Batch 160, Loss Value: 0.9486\n",
      "Batch 160, Gradient Norm: 0.0118\n",
      "\u001b[1m160/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m34s\u001b[0m 478ms/step - accuracy: 0.4951 - binary_io_u_5: 0.4953 - loss: 0.9161Batch 161, Loss Value: 0.9429\n",
      "Batch 161, Gradient Norm: 0.2407\n",
      "\u001b[1m161/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m33s\u001b[0m 478ms/step - accuracy: 0.4951 - binary_io_u_5: 0.4953 - loss: 0.9161Batch 162, Loss Value: 0.9496\n",
      "Batch 162, Gradient Norm: 0.0158\n",
      "\u001b[1m162/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m33s\u001b[0m 478ms/step - accuracy: 0.4952 - binary_io_u_5: 0.4953 - loss: 0.9161Batch 163, Loss Value: 0.9498\n",
      "Batch 163, Gradient Norm: 0.0034\n",
      "\u001b[1m163/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m32s\u001b[0m 478ms/step - accuracy: 0.4952 - binary_io_u_5: 0.4953 - loss: 0.9161Batch 164, Loss Value: 0.9484\n",
      "Batch 164, Gradient Norm: 0.0120\n",
      "\u001b[1m164/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m32s\u001b[0m 478ms/step - accuracy: 0.4952 - binary_io_u_5: 0.4953 - loss: 0.9161Batch 165, Loss Value: 0.9486\n",
      "Batch 165, Gradient Norm: 0.0047\n",
      "\u001b[1m165/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m32s\u001b[0m 478ms/step - accuracy: 0.4952 - binary_io_u_5: 0.4953 - loss: 0.9160Batch 166, Loss Value: 0.9499\n",
      "Batch 166, Gradient Norm: 0.0052\n",
      "\u001b[1m166/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m31s\u001b[0m 478ms/step - accuracy: 0.4952 - binary_io_u_5: 0.4954 - loss: 0.9160Batch 167, Loss Value: 0.9494\n",
      "Batch 167, Gradient Norm: 0.0051\n",
      "\u001b[1m167/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m31s\u001b[0m 478ms/step - accuracy: 0.4953 - binary_io_u_5: 0.4954 - loss: 0.9160Batch 168, Loss Value: 0.9486\n",
      "Batch 168, Gradient Norm: 0.0067\n",
      "\u001b[1m168/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m30s\u001b[0m 478ms/step - accuracy: 0.4953 - binary_io_u_5: 0.4954 - loss: 0.9160Batch 169, Loss Value: 0.9489\n",
      "Batch 169, Gradient Norm: 0.0079\n",
      "\u001b[1m169/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m30s\u001b[0m 478ms/step - accuracy: 0.4953 - binary_io_u_5: 0.4954 - loss: 0.9160Batch 170, Loss Value: 0.9484\n",
      "Batch 170, Gradient Norm: 0.0094\n",
      "\u001b[1m170/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m29s\u001b[0m 478ms/step - accuracy: 0.4953 - binary_io_u_5: 0.4954 - loss: 0.9159Batch 171, Loss Value: 0.9485\n",
      "Batch 171, Gradient Norm: 0.0027\n",
      "\u001b[1m171/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m29s\u001b[0m 478ms/step - accuracy: 0.4953 - binary_io_u_5: 0.4955 - loss: 0.9159Batch 172, Loss Value: 0.9487\n",
      "Batch 172, Gradient Norm: 0.0068\n",
      "\u001b[1m172/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m28s\u001b[0m 478ms/step - accuracy: 0.4954 - binary_io_u_5: 0.4955 - loss: 0.9159Batch 173, Loss Value: 0.9492\n",
      "Batch 173, Gradient Norm: 0.0085\n",
      "\u001b[1m173/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m28s\u001b[0m 478ms/step - accuracy: 0.4954 - binary_io_u_5: 0.4955 - loss: 0.9159Batch 174, Loss Value: 0.9486\n",
      "Batch 174, Gradient Norm: 0.0067\n",
      "\u001b[1m174/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m27s\u001b[0m 478ms/step - accuracy: 0.4954 - binary_io_u_5: 0.4955 - loss: 0.9159Batch 175, Loss Value: 0.9500\n",
      "Batch 175, Gradient Norm: 0.0033\n",
      "\u001b[1m175/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m27s\u001b[0m 478ms/step - accuracy: 0.4954 - binary_io_u_5: 0.4955 - loss: 0.9158Batch 176, Loss Value: 0.9493\n",
      "Batch 176, Gradient Norm: 0.0143\n",
      "\u001b[1m176/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m26s\u001b[0m 478ms/step - accuracy: 0.4954 - binary_io_u_5: 0.4955 - loss: 0.9158Batch 177, Loss Value: 0.9487\n",
      "Batch 177, Gradient Norm: 0.0111\n",
      "\u001b[1m177/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m26s\u001b[0m 478ms/step - accuracy: 0.4955 - binary_io_u_5: 0.4955 - loss: 0.9158Batch 178, Loss Value: 0.9490\n",
      "Batch 178, Gradient Norm: 0.0029\n",
      "\u001b[1m178/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m25s\u001b[0m 478ms/step - accuracy: 0.4955 - binary_io_u_5: 0.4956 - loss: 0.9158Batch 179, Loss Value: 0.9485\n",
      "Batch 179, Gradient Norm: 0.0034\n",
      "\u001b[1m179/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m25s\u001b[0m 478ms/step - accuracy: 0.4955 - binary_io_u_5: 0.4956 - loss: 0.9158Batch 180, Loss Value: 0.9496\n",
      "Batch 180, Gradient Norm: 0.0128\n",
      "\u001b[1m180/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m24s\u001b[0m 478ms/step - accuracy: 0.4955 - binary_io_u_5: 0.4956 - loss: 0.9158Batch 181, Loss Value: 0.9485\n",
      "Batch 181, Gradient Norm: 0.0060\n",
      "\u001b[1m181/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m24s\u001b[0m 478ms/step - accuracy: 0.4955 - binary_io_u_5: 0.4956 - loss: 0.9157Batch 182, Loss Value: 0.9483\n",
      "Batch 182, Gradient Norm: 0.0041\n",
      "\u001b[1m182/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m23s\u001b[0m 478ms/step - accuracy: 0.4955 - binary_io_u_5: 0.4956 - loss: 0.9157Batch 183, Loss Value: 0.9491\n",
      "Batch 183, Gradient Norm: 0.0012\n",
      "\u001b[1m183/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m23s\u001b[0m 478ms/step - accuracy: 0.4955 - binary_io_u_5: 0.4956 - loss: 0.9157Batch 184, Loss Value: 0.9493\n",
      "Batch 184, Gradient Norm: 0.0061\n",
      "\u001b[1m184/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m22s\u001b[0m 478ms/step - accuracy: 0.4955 - binary_io_u_5: 0.4956 - loss: 0.9157Batch 185, Loss Value: 0.9491\n",
      "Batch 185, Gradient Norm: 0.0053\n",
      "\u001b[1m185/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m22s\u001b[0m 478ms/step - accuracy: 0.4956 - binary_io_u_5: 0.4956 - loss: 0.9157Batch 186, Loss Value: 0.9488\n",
      "Batch 186, Gradient Norm: 0.0071\n",
      "\u001b[1m186/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m21s\u001b[0m 478ms/step - accuracy: 0.4956 - binary_io_u_5: 0.4956 - loss: 0.9157Batch 187, Loss Value: 0.9513\n",
      "Batch 187, Gradient Norm: 0.0118\n",
      "\u001b[1m187/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m21s\u001b[0m 478ms/step - accuracy: 0.4956 - binary_io_u_5: 0.4956 - loss: 0.9157Batch 188, Loss Value: 0.9507\n",
      "Batch 188, Gradient Norm: 0.0180\n",
      "\u001b[1m188/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m21s\u001b[0m 478ms/step - accuracy: 0.4956 - binary_io_u_5: 0.4956 - loss: 0.9156Batch 189, Loss Value: 0.9504\n",
      "Batch 189, Gradient Norm: 0.0079\n",
      "\u001b[1m189/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m20s\u001b[0m 478ms/step - accuracy: 0.4956 - binary_io_u_5: 0.4956 - loss: 0.9156Batch 190, Loss Value: 0.9490\n",
      "Batch 190, Gradient Norm: 0.0080\n",
      "\u001b[1m190/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m20s\u001b[0m 478ms/step - accuracy: 0.4956 - binary_io_u_5: 0.4956 - loss: 0.9156Batch 191, Loss Value: 0.9497\n",
      "Batch 191, Gradient Norm: 0.0072\n",
      "\u001b[1m191/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m19s\u001b[0m 478ms/step - accuracy: 0.4956 - binary_io_u_5: 0.4956 - loss: 0.9156Batch 192, Loss Value: 0.9486\n",
      "Batch 192, Gradient Norm: 0.0098\n",
      "\u001b[1m192/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m19s\u001b[0m 478ms/step - accuracy: 0.4956 - binary_io_u_5: 0.4956 - loss: 0.9156Batch 193, Loss Value: 0.9502\n",
      "Batch 193, Gradient Norm: 0.0135\n",
      "\u001b[1m193/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m18s\u001b[0m 478ms/step - accuracy: 0.4956 - binary_io_u_5: 0.4957 - loss: 0.9156Batch 194, Loss Value: 0.9486\n",
      "Batch 194, Gradient Norm: 0.0066\n",
      "\u001b[1m194/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m18s\u001b[0m 478ms/step - accuracy: 0.4956 - binary_io_u_5: 0.4957 - loss: 0.9156Batch 195, Loss Value: 0.9486\n",
      "Batch 195, Gradient Norm: 0.0038\n",
      "\u001b[1m195/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m17s\u001b[0m 478ms/step - accuracy: 0.4956 - binary_io_u_5: 0.4957 - loss: 0.9156Batch 196, Loss Value: 0.9483\n",
      "Batch 196, Gradient Norm: 0.0057\n",
      "\u001b[1m196/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m17s\u001b[0m 478ms/step - accuracy: 0.4957 - binary_io_u_5: 0.4957 - loss: 0.9155Batch 197, Loss Value: 0.9489\n",
      "Batch 197, Gradient Norm: 0.0029\n",
      "\u001b[1m197/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m16s\u001b[0m 478ms/step - accuracy: 0.4957 - binary_io_u_5: 0.4957 - loss: 0.9155Batch 198, Loss Value: 0.9491\n",
      "Batch 198, Gradient Norm: 0.0078\n",
      "\u001b[1m198/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m16s\u001b[0m 478ms/step - accuracy: 0.4957 - binary_io_u_5: 0.4957 - loss: 0.9155Batch 199, Loss Value: 0.9490\n",
      "Batch 199, Gradient Norm: 0.0013\n",
      "\u001b[1m199/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m15s\u001b[0m 478ms/step - accuracy: 0.4957 - binary_io_u_5: 0.4957 - loss: 0.9155Batch 200, Loss Value: 0.9493\n",
      "Batch 200, Gradient Norm: 0.0063\n",
      "\u001b[1m200/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m15s\u001b[0m 479ms/step - accuracy: 0.4957 - binary_io_u_5: 0.4957 - loss: 0.9155Batch 201, Loss Value: 0.9493\n",
      "Batch 201, Gradient Norm: 0.0078\n",
      "\u001b[1m201/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m14s\u001b[0m 479ms/step - accuracy: 0.4957 - binary_io_u_5: 0.4957 - loss: 0.9155Batch 202, Loss Value: 0.9495\n",
      "Batch 202, Gradient Norm: 0.0043\n",
      "\u001b[1m202/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m14s\u001b[0m 479ms/step - accuracy: 0.4957 - binary_io_u_5: 0.4956 - loss: 0.9155Batch 203, Loss Value: 0.9487\n",
      "Batch 203, Gradient Norm: 0.0037\n",
      "\u001b[1m203/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m13s\u001b[0m 479ms/step - accuracy: 0.4957 - binary_io_u_5: 0.4956 - loss: 0.9155Batch 204, Loss Value: 0.9489\n",
      "Batch 204, Gradient Norm: 0.0029\n",
      "\u001b[1m204/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m13s\u001b[0m 479ms/step - accuracy: 0.4957 - binary_io_u_5: 0.4956 - loss: 0.9155Batch 205, Loss Value: 0.9489\n",
      "Batch 205, Gradient Norm: 0.0181\n",
      "\u001b[1m205/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m12s\u001b[0m 479ms/step - accuracy: 0.4957 - binary_io_u_5: 0.4956 - loss: 0.9155Batch 206, Loss Value: 0.9483\n",
      "Batch 206, Gradient Norm: 0.0039\n",
      "\u001b[1m206/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m12s\u001b[0m 479ms/step - accuracy: 0.4957 - binary_io_u_5: 0.4956 - loss: 0.9155Batch 207, Loss Value: 0.9483\n",
      "Batch 207, Gradient Norm: 0.0013\n",
      "\u001b[1m207/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m11s\u001b[0m 479ms/step - accuracy: 0.4957 - binary_io_u_5: 0.4956 - loss: 0.9155Batch 208, Loss Value: 0.9481\n",
      "Batch 208, Gradient Norm: 0.0067\n",
      "\u001b[1m208/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m11s\u001b[0m 479ms/step - accuracy: 0.4957 - binary_io_u_5: 0.4956 - loss: 0.9155Batch 209, Loss Value: 0.9489\n",
      "Batch 209, Gradient Norm: 0.0159\n",
      "\u001b[1m209/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m11s\u001b[0m 479ms/step - accuracy: 0.4957 - binary_io_u_5: 0.4956 - loss: 0.9155Batch 210, Loss Value: 0.9492\n",
      "Batch 210, Gradient Norm: 0.0074\n",
      "\u001b[1m210/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m10s\u001b[0m 479ms/step - accuracy: 0.4957 - binary_io_u_5: 0.4956 - loss: 0.9155Batch 211, Loss Value: 0.9490\n",
      "Batch 211, Gradient Norm: 0.0066\n",
      "\u001b[1m211/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m10s\u001b[0m 479ms/step - accuracy: 0.4957 - binary_io_u_5: 0.4956 - loss: 0.9155Batch 212, Loss Value: 0.9483\n",
      "Batch 212, Gradient Norm: 0.0012\n",
      "\u001b[1m212/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m9s\u001b[0m 479ms/step - accuracy: 0.4957 - binary_io_u_5: 0.4956 - loss: 0.9155 Batch 213, Loss Value: 0.9485\n",
      "Batch 213, Gradient Norm: 0.0043\n",
      "\u001b[1m213/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m9s\u001b[0m 479ms/step - accuracy: 0.4957 - binary_io_u_5: 0.4956 - loss: 0.9155Batch 214, Loss Value: 0.9484\n",
      "Batch 214, Gradient Norm: 0.0073\n",
      "\u001b[1m214/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m8s\u001b[0m 479ms/step - accuracy: 0.4957 - binary_io_u_5: 0.4956 - loss: 0.9155Batch 215, Loss Value: 0.9489\n",
      "Batch 215, Gradient Norm: 0.0033\n",
      "\u001b[1m215/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m8s\u001b[0m 479ms/step - accuracy: 0.4957 - binary_io_u_5: 0.4956 - loss: 0.9155Batch 216, Loss Value: 0.9493\n",
      "Batch 216, Gradient Norm: 0.0071\n",
      "\u001b[1m216/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m7s\u001b[0m 479ms/step - accuracy: 0.4957 - binary_io_u_5: 0.4956 - loss: 0.9154Batch 217, Loss Value: 0.9483\n",
      "Batch 217, Gradient Norm: 0.0004\n",
      "\u001b[1m217/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m7s\u001b[0m 479ms/step - accuracy: 0.4957 - binary_io_u_5: 0.4956 - loss: 0.9154Batch 218, Loss Value: 0.9484\n",
      "Batch 218, Gradient Norm: 0.0040\n",
      "\u001b[1m218/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m6s\u001b[0m 479ms/step - accuracy: 0.4957 - binary_io_u_5: 0.4956 - loss: 0.9154Batch 219, Loss Value: 0.9488\n",
      "Batch 219, Gradient Norm: 0.0092\n",
      "\u001b[1m219/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m6s\u001b[0m 479ms/step - accuracy: 0.4957 - binary_io_u_5: 0.4956 - loss: 0.9154Batch 220, Loss Value: 0.9484\n",
      "Batch 220, Gradient Norm: 0.0040\n",
      "\u001b[1m220/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m5s\u001b[0m 479ms/step - accuracy: 0.4957 - binary_io_u_5: 0.4956 - loss: 0.9154Batch 221, Loss Value: 0.9484\n",
      "Batch 221, Gradient Norm: 0.0116\n",
      "\u001b[1m221/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m5s\u001b[0m 479ms/step - accuracy: 0.4957 - binary_io_u_5: 0.4956 - loss: 0.9154Batch 222, Loss Value: 0.9484\n",
      "Batch 222, Gradient Norm: 0.0041\n",
      "\u001b[1m222/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m4s\u001b[0m 479ms/step - accuracy: 0.4957 - binary_io_u_5: 0.4956 - loss: 0.9154Batch 223, Loss Value: 0.9485\n",
      "Batch 223, Gradient Norm: 0.0038\n",
      "\u001b[1m223/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m4s\u001b[0m 479ms/step - accuracy: 0.4957 - binary_io_u_5: 0.4956 - loss: 0.9154Batch 224, Loss Value: 0.9486\n",
      "Batch 224, Gradient Norm: 0.0019\n",
      "\u001b[1m224/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m3s\u001b[0m 479ms/step - accuracy: 0.4957 - binary_io_u_5: 0.4956 - loss: 0.9154Batch 225, Loss Value: 0.9486\n",
      "Batch 225, Gradient Norm: 0.0096\n",
      "\u001b[1m225/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m3s\u001b[0m 479ms/step - accuracy: 0.4957 - binary_io_u_5: 0.4956 - loss: 0.9154Batch 226, Loss Value: 0.9485\n",
      "Batch 226, Gradient Norm: 0.0090\n",
      "\u001b[1m226/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m2s\u001b[0m 479ms/step - accuracy: 0.4957 - binary_io_u_5: 0.4956 - loss: 0.9154Batch 227, Loss Value: 0.9483\n",
      "Batch 227, Gradient Norm: 0.0026\n",
      "\u001b[1m227/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m2s\u001b[0m 479ms/step - accuracy: 0.4957 - binary_io_u_5: 0.4956 - loss: 0.9154Batch 228, Loss Value: 0.9487\n",
      "Batch 228, Gradient Norm: 0.0079\n",
      "\u001b[1m228/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m1s\u001b[0m 479ms/step - accuracy: 0.4957 - binary_io_u_5: 0.4956 - loss: 0.9154Batch 229, Loss Value: 0.9494\n",
      "Batch 229, Gradient Norm: 0.0036\n",
      "\u001b[1m229/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m1s\u001b[0m 479ms/step - accuracy: 0.4957 - binary_io_u_5: 0.4956 - loss: 0.9154Batch 230, Loss Value: 0.9486\n",
      "Batch 230, Gradient Norm: 0.0060\n",
      "\u001b[1m230/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 479ms/step - accuracy: 0.4957 - binary_io_u_5: 0.4956 - loss: 0.9154Batch 231, Loss Value: 0.9485\n",
      "Batch 231, Gradient Norm: 0.0083\n",
      "\u001b[1m231/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 479ms/step - accuracy: 0.4957 - binary_io_u_5: 0.4956 - loss: 0.9154Batch 232, Loss Value: 0.9487\n",
      "Batch 232, Gradient Norm: 0.0138\n",
      "\u001b[1m232/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m112s\u001b[0m 485ms/step - accuracy: 0.4957 - binary_io_u_5: 0.4956 - loss: 0.9154 - val_accuracy: 0.4951 - val_binary_io_u_5: 0.4949 - val_loss: 0.9079 - learning_rate: 5.0000e-04\n",
      "Epoch 19/200\n",
      "Batch 1, Loss Value: 0.9390\n",
      "Batch 1, Gradient Norm: 0.1530\n",
      "\u001b[1m  1/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:00\u001b[0m 520ms/step - accuracy: 0.5100 - binary_io_u_5: 0.5100 - loss: 0.9030Batch 2, Loss Value: 0.9236\n",
      "Batch 2, Gradient Norm: 0.2046\n",
      "\u001b[1m  2/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:51\u001b[0m 484ms/step - accuracy: 0.4850 - binary_io_u_5: 0.4850 - loss: 0.9142Batch 3, Loss Value: 0.9403\n",
      "Batch 3, Gradient Norm: 0.1173\n",
      "\u001b[1m  3/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:50\u001b[0m 482ms/step - accuracy: 0.4767 - binary_io_u_5: 0.4767 - loss: 0.9197Batch 4, Loss Value: 0.9153\n",
      "Batch 4, Gradient Norm: 0.1494\n",
      "\u001b[1m  4/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:50\u001b[0m 483ms/step - accuracy: 0.4762 - binary_io_u_5: 0.4762 - loss: 0.9214Batch 5, Loss Value: 0.9398\n",
      "Batch 5, Gradient Norm: 0.2904\n",
      "\u001b[1m  5/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:49\u001b[0m 482ms/step - accuracy: 0.4734 - binary_io_u_5: 0.4734 - loss: 0.9233Batch 6, Loss Value: 0.9451\n",
      "Batch 6, Gradient Norm: 0.0015\n",
      "\u001b[1m  6/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:48\u001b[0m 481ms/step - accuracy: 0.4714 - binary_io_u_5: 0.4717 - loss: 0.9249Batch 7, Loss Value: 0.9196\n",
      "Batch 7, Gradient Norm: 0.2341\n",
      "\u001b[1m  7/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:48\u001b[0m 481ms/step - accuracy: 0.4702 - binary_io_u_5: 0.4707 - loss: 0.9259Batch 8, Loss Value: 0.9442\n",
      "Batch 8, Gradient Norm: 0.0151\n",
      "\u001b[1m  8/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:47\u001b[0m 482ms/step - accuracy: 0.4711 - binary_io_u_5: 0.4717 - loss: 0.9253Batch 9, Loss Value: 0.9442\n",
      "Batch 9, Gradient Norm: 0.0146\n",
      "\u001b[1m  9/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:47\u001b[0m 483ms/step - accuracy: 0.4721 - binary_io_u_5: 0.4727 - loss: 0.9248Batch 10, Loss Value: 0.9381\n",
      "Batch 10, Gradient Norm: 0.1948\n",
      "\u001b[1m 10/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:47\u001b[0m 484ms/step - accuracy: 0.4734 - binary_io_u_5: 0.4740 - loss: 0.9240Batch 11, Loss Value: 0.9195\n",
      "Batch 11, Gradient Norm: 0.1474\n",
      "\u001b[1m 11/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:46\u001b[0m 483ms/step - accuracy: 0.4749 - binary_io_u_5: 0.4756 - loss: 0.9233Batch 12, Loss Value: 0.9295\n",
      "Batch 12, Gradient Norm: 0.0807\n",
      "\u001b[1m 12/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:46\u001b[0m 484ms/step - accuracy: 0.4761 - binary_io_u_5: 0.4768 - loss: 0.9226Batch 13, Loss Value: 0.9156\n",
      "Batch 13, Gradient Norm: 0.1794\n",
      "\u001b[1m 13/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:45\u001b[0m 484ms/step - accuracy: 0.4775 - binary_io_u_5: 0.4782 - loss: 0.9217Batch 14, Loss Value: 0.9054\n",
      "Batch 14, Gradient Norm: 0.1508\n",
      "\u001b[1m 14/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:45\u001b[0m 484ms/step - accuracy: 0.4788 - binary_io_u_5: 0.4795 - loss: 0.9209Batch 15, Loss Value: 0.9225\n",
      "Batch 15, Gradient Norm: 0.3171\n",
      "\u001b[1m 15/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:44\u001b[0m 483ms/step - accuracy: 0.4797 - binary_io_u_5: 0.4803 - loss: 0.9202Batch 16, Loss Value: 0.9120\n",
      "Batch 16, Gradient Norm: 0.4633\n",
      "\u001b[1m 16/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:44\u001b[0m 483ms/step - accuracy: 0.4803 - binary_io_u_5: 0.4809 - loss: 0.9198Batch 17, Loss Value: 0.9351\n",
      "Batch 17, Gradient Norm: 0.2532\n",
      "\u001b[1m 17/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:43\u001b[0m 482ms/step - accuracy: 0.4809 - binary_io_u_5: 0.4814 - loss: 0.9194Batch 18, Loss Value: 0.9377\n",
      "Batch 18, Gradient Norm: 0.1232\n",
      "\u001b[1m 18/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:43\u001b[0m 482ms/step - accuracy: 0.4812 - binary_io_u_5: 0.4817 - loss: 0.9191Batch 19, Loss Value: 0.9442\n",
      "Batch 19, Gradient Norm: 0.0038\n",
      "\u001b[1m 19/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:42\u001b[0m 482ms/step - accuracy: 0.4817 - binary_io_u_5: 0.4821 - loss: 0.9189Batch 20, Loss Value: 0.9217\n",
      "Batch 20, Gradient Norm: 0.1631\n",
      "\u001b[1m 20/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:42\u001b[0m 482ms/step - accuracy: 0.4822 - binary_io_u_5: 0.4825 - loss: 0.9186Batch 21, Loss Value: 0.9443\n",
      "Batch 21, Gradient Norm: 0.0132\n",
      "\u001b[1m 21/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:41\u001b[0m 482ms/step - accuracy: 0.4826 - binary_io_u_5: 0.4828 - loss: 0.9184Batch 22, Loss Value: 0.9254\n",
      "Batch 22, Gradient Norm: 0.2441\n",
      "\u001b[1m 22/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:41\u001b[0m 482ms/step - accuracy: 0.4830 - binary_io_u_5: 0.4831 - loss: 0.9181Batch 23, Loss Value: 0.9378\n",
      "Batch 23, Gradient Norm: 0.3137\n",
      "\u001b[1m 23/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:40\u001b[0m 482ms/step - accuracy: 0.4834 - binary_io_u_5: 0.4834 - loss: 0.9180Batch 24, Loss Value: 0.9269\n",
      "Batch 24, Gradient Norm: 0.3277\n",
      "\u001b[1m 24/232\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:40\u001b[0m 482ms/step - accuracy: 0.4836 - binary_io_u_5: 0.4836 - loss: 0.9179Batch 25, Loss Value: 0.9187\n",
      "Batch 25, Gradient Norm: 0.0592\n",
      "\u001b[1m 25/232\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:39\u001b[0m 482ms/step - accuracy: 0.4838 - binary_io_u_5: 0.4838 - loss: 0.9177Batch 26, Loss Value: 0.9445\n",
      "Batch 26, Gradient Norm: 0.0069\n",
      "\u001b[1m 26/232\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:39\u001b[0m 482ms/step - accuracy: 0.4841 - binary_io_u_5: 0.4840 - loss: 0.9176Batch 27, Loss Value: 0.9280\n",
      "Batch 27, Gradient Norm: 0.2447\n",
      "\u001b[1m 27/232\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:38\u001b[0m 482ms/step - accuracy: 0.4845 - binary_io_u_5: 0.4844 - loss: 0.9174Batch 28, Loss Value: 0.9261\n",
      "Batch 28, Gradient Norm: 0.0590\n",
      "\u001b[1m 28/232\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:38\u001b[0m 482ms/step - accuracy: 0.4848 - binary_io_u_5: 0.4847 - loss: 0.9173Batch 29, Loss Value: 0.9435\n",
      "Batch 29, Gradient Norm: 0.0239\n",
      "\u001b[1m 29/232\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:37\u001b[0m 482ms/step - accuracy: 0.4851 - binary_io_u_5: 0.4850 - loss: 0.9171Batch 30, Loss Value: 0.9217\n",
      "Batch 30, Gradient Norm: 0.2029\n",
      "\u001b[1m 30/232\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:37\u001b[0m 482ms/step - accuracy: 0.4854 - binary_io_u_5: 0.4853 - loss: 0.9169Batch 31, Loss Value: 0.9416\n",
      "Batch 31, Gradient Norm: 0.2721\n",
      "\u001b[1m 31/232\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:36\u001b[0m 481ms/step - accuracy: 0.4856 - binary_io_u_5: 0.4855 - loss: 0.9168Batch 32, Loss Value: 0.9407\n",
      "Batch 32, Gradient Norm: 0.0823\n",
      "\u001b[1m 32/232\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:36\u001b[0m 481ms/step - accuracy: 0.4858 - binary_io_u_5: 0.4856 - loss: 0.9167Batch 33, Loss Value: 0.9335\n",
      "Batch 33, Gradient Norm: 0.1006\n",
      "\u001b[1m 33/232\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:35\u001b[0m 481ms/step - accuracy: 0.4859 - binary_io_u_5: 0.4858 - loss: 0.9167Batch 34, Loss Value: 0.9254\n",
      "Batch 34, Gradient Norm: 0.2367\n",
      "\u001b[1m 34/232\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:35\u001b[0m 481ms/step - accuracy: 0.4861 - binary_io_u_5: 0.4859 - loss: 0.9166Batch 35, Loss Value: 0.9388\n",
      "Batch 35, Gradient Norm: 0.0987\n",
      "\u001b[1m 35/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:34\u001b[0m 481ms/step - accuracy: 0.4862 - binary_io_u_5: 0.4860 - loss: 0.9165Batch 36, Loss Value: 0.9106\n",
      "Batch 36, Gradient Norm: 0.1844\n",
      "\u001b[1m 36/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:34\u001b[0m 481ms/step - accuracy: 0.4864 - binary_io_u_5: 0.4862 - loss: 0.9164Batch 37, Loss Value: 0.9433\n",
      "Batch 37, Gradient Norm: 0.0137\n",
      "\u001b[1m 37/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:33\u001b[0m 481ms/step - accuracy: 0.4866 - binary_io_u_5: 0.4863 - loss: 0.9164Batch 38, Loss Value: 0.9241\n",
      "Batch 38, Gradient Norm: 0.0771\n",
      "\u001b[1m 38/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:33\u001b[0m 481ms/step - accuracy: 0.4867 - binary_io_u_5: 0.4865 - loss: 0.9163Batch 39, Loss Value: 0.9374\n",
      "Batch 39, Gradient Norm: 0.2265\n",
      "\u001b[1m 39/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:32\u001b[0m 481ms/step - accuracy: 0.4869 - binary_io_u_5: 0.4866 - loss: 0.9162Batch 40, Loss Value: 0.9292\n",
      "Batch 40, Gradient Norm: 0.1938\n",
      "\u001b[1m 40/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:32\u001b[0m 481ms/step - accuracy: 0.4871 - binary_io_u_5: 0.4868 - loss: 0.9161Batch 41, Loss Value: 0.9439\n",
      "Batch 41, Gradient Norm: 0.0079\n",
      "\u001b[1m 41/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:31\u001b[0m 481ms/step - accuracy: 0.4873 - binary_io_u_5: 0.4870 - loss: 0.9160Batch 42, Loss Value: 0.9315\n",
      "Batch 42, Gradient Norm: 0.1180\n",
      "\u001b[1m 42/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:31\u001b[0m 481ms/step - accuracy: 0.4875 - binary_io_u_5: 0.4872 - loss: 0.9158Batch 43, Loss Value: 0.9227\n",
      "Batch 43, Gradient Norm: 0.4680\n",
      "\u001b[1m 43/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:30\u001b[0m 481ms/step - accuracy: 0.4877 - binary_io_u_5: 0.4874 - loss: 0.9157Batch 44, Loss Value: 0.9152\n",
      "Batch 44, Gradient Norm: 0.2523\n",
      "\u001b[1m 44/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:30\u001b[0m 481ms/step - accuracy: 0.4879 - binary_io_u_5: 0.4876 - loss: 0.9156Batch 45, Loss Value: 0.9438\n",
      "Batch 45, Gradient Norm: 0.0082\n",
      "\u001b[1m 45/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:29\u001b[0m 481ms/step - accuracy: 0.4881 - binary_io_u_5: 0.4878 - loss: 0.9155Batch 46, Loss Value: 0.9328\n",
      "Batch 46, Gradient Norm: 0.2222\n",
      "\u001b[1m 46/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:29\u001b[0m 481ms/step - accuracy: 0.4883 - binary_io_u_5: 0.4879 - loss: 0.9154Batch 47, Loss Value: 0.9404\n",
      "Batch 47, Gradient Norm: 0.2167\n",
      "\u001b[1m 47/232\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:28\u001b[0m 481ms/step - accuracy: 0.4884 - binary_io_u_5: 0.4881 - loss: 0.9153Batch 48, Loss Value: 0.9158\n",
      "Batch 48, Gradient Norm: 0.1473\n",
      "\u001b[1m 48/232\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:28\u001b[0m 481ms/step - accuracy: 0.4886 - binary_io_u_5: 0.4882 - loss: 0.9153Batch 49, Loss Value: 0.9434\n",
      "Batch 49, Gradient Norm: 0.0121\n",
      "\u001b[1m 49/232\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:28\u001b[0m 481ms/step - accuracy: 0.4887 - binary_io_u_5: 0.4883 - loss: 0.9152Batch 50, Loss Value: 0.9206\n",
      "Batch 50, Gradient Norm: 0.1123\n",
      "\u001b[1m 50/232\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:27\u001b[0m 481ms/step - accuracy: 0.4888 - binary_io_u_5: 0.4885 - loss: 0.9151Batch 51, Loss Value: 0.9347\n",
      "Batch 51, Gradient Norm: 0.0999\n",
      "\u001b[1m 51/232\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:27\u001b[0m 481ms/step - accuracy: 0.4889 - binary_io_u_5: 0.4886 - loss: 0.9150Batch 52, Loss Value: 0.9322\n",
      "Batch 52, Gradient Norm: 0.0940\n",
      "\u001b[1m 52/232\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:26\u001b[0m 482ms/step - accuracy: 0.4891 - binary_io_u_5: 0.4887 - loss: 0.9150Batch 53, Loss Value: 0.9269\n",
      "Batch 53, Gradient Norm: 0.0709\n",
      "\u001b[1m 53/232\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:26\u001b[0m 482ms/step - accuracy: 0.4892 - binary_io_u_5: 0.4888 - loss: 0.9149Batch 54, Loss Value: 0.9087\n",
      "Batch 54, Gradient Norm: 0.2507\n",
      "\u001b[1m 54/232\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:25\u001b[0m 482ms/step - accuracy: 0.4892 - binary_io_u_5: 0.4889 - loss: 0.9148Batch 55, Loss Value: 0.9288\n",
      "Batch 55, Gradient Norm: 0.1199\n",
      "\u001b[1m 55/232\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:25\u001b[0m 482ms/step - accuracy: 0.4893 - binary_io_u_5: 0.4889 - loss: 0.9147Batch 56, Loss Value: 0.9327\n",
      "Batch 56, Gradient Norm: 0.1735\n",
      "\u001b[1m 56/232\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:24\u001b[0m 482ms/step - accuracy: 0.4894 - binary_io_u_5: 0.4890 - loss: 0.9147Batch 57, Loss Value: 0.9270\n",
      "Batch 57, Gradient Norm: 0.3274\n",
      "\u001b[1m 57/232\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:24\u001b[0m 481ms/step - accuracy: 0.4894 - binary_io_u_5: 0.4891 - loss: 0.9146Batch 58, Loss Value: 0.9162\n",
      "Batch 58, Gradient Norm: 0.3287\n",
      "\u001b[1m 58/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:23\u001b[0m 481ms/step - accuracy: 0.4895 - binary_io_u_5: 0.4891 - loss: 0.9146Batch 59, Loss Value: 0.9311\n",
      "Batch 59, Gradient Norm: 0.3648\n",
      "\u001b[1m 59/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:23\u001b[0m 481ms/step - accuracy: 0.4895 - binary_io_u_5: 0.4892 - loss: 0.9146Batch 60, Loss Value: 0.9152\n",
      "Batch 60, Gradient Norm: 0.1796\n",
      "\u001b[1m 60/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:22\u001b[0m 481ms/step - accuracy: 0.4896 - binary_io_u_5: 0.4892 - loss: 0.9145Batch 61, Loss Value: 0.9392\n",
      "Batch 61, Gradient Norm: 0.0860\n",
      "\u001b[1m 61/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:22\u001b[0m 481ms/step - accuracy: 0.4897 - binary_io_u_5: 0.4893 - loss: 0.9145Batch 62, Loss Value: 0.9380\n",
      "Batch 62, Gradient Norm: 0.1876\n",
      "\u001b[1m 62/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:21\u001b[0m 481ms/step - accuracy: 0.4897 - binary_io_u_5: 0.4893 - loss: 0.9145Batch 63, Loss Value: 0.9129\n",
      "Batch 63, Gradient Norm: 0.2423\n",
      "\u001b[1m 63/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:21\u001b[0m 482ms/step - accuracy: 0.4898 - binary_io_u_5: 0.4894 - loss: 0.9144Batch 64, Loss Value: 0.9233\n",
      "Batch 64, Gradient Norm: 0.2383\n",
      "\u001b[1m 64/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:20\u001b[0m 481ms/step - accuracy: 0.4898 - binary_io_u_5: 0.4894 - loss: 0.9144Batch 65, Loss Value: 0.9209\n",
      "Batch 65, Gradient Norm: 0.1599\n",
      "\u001b[1m 65/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:20\u001b[0m 482ms/step - accuracy: 0.4899 - binary_io_u_5: 0.4895 - loss: 0.9144Batch 66, Loss Value: 0.9366\n",
      "Batch 66, Gradient Norm: 0.1738\n",
      "\u001b[1m 66/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:19\u001b[0m 481ms/step - accuracy: 0.4900 - binary_io_u_5: 0.4895 - loss: 0.9143Batch 67, Loss Value: 0.9360\n",
      "Batch 67, Gradient Norm: 0.2084\n",
      "\u001b[1m 67/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:19\u001b[0m 482ms/step - accuracy: 0.4900 - binary_io_u_5: 0.4896 - loss: 0.9143Batch 68, Loss Value: 0.9444\n",
      "Batch 68, Gradient Norm: 0.0122\n",
      "\u001b[1m 68/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:18\u001b[0m 482ms/step - accuracy: 0.4901 - binary_io_u_5: 0.4896 - loss: 0.9143Batch 69, Loss Value: 0.9440\n",
      "Batch 69, Gradient Norm: 0.0039\n",
      "\u001b[1m 69/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:18\u001b[0m 482ms/step - accuracy: 0.4901 - binary_io_u_5: 0.4896 - loss: 0.9142Batch 70, Loss Value: 0.9225\n",
      "Batch 70, Gradient Norm: 0.2991\n",
      "\u001b[1m 70/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:18\u001b[0m 482ms/step - accuracy: 0.4902 - binary_io_u_5: 0.4897 - loss: 0.9142Batch 71, Loss Value: 0.9298\n",
      "Batch 71, Gradient Norm: 0.0992\n",
      "\u001b[1m 71/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:17\u001b[0m 482ms/step - accuracy: 0.4902 - binary_io_u_5: 0.4897 - loss: 0.9142Batch 72, Loss Value: 0.9444\n",
      "Batch 72, Gradient Norm: 0.0019\n",
      "\u001b[1m 72/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:17\u001b[0m 481ms/step - accuracy: 0.4903 - binary_io_u_5: 0.4898 - loss: 0.9141Batch 73, Loss Value: 0.9447\n",
      "Batch 73, Gradient Norm: 0.0037\n",
      "\u001b[1m 73/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:16\u001b[0m 481ms/step - accuracy: 0.4904 - binary_io_u_5: 0.4899 - loss: 0.9141Batch 74, Loss Value: 0.9446\n",
      "Batch 74, Gradient Norm: 0.0037\n",
      "\u001b[1m 74/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:16\u001b[0m 481ms/step - accuracy: 0.4904 - binary_io_u_5: 0.4900 - loss: 0.9140Batch 75, Loss Value: 0.9386\n",
      "Batch 75, Gradient Norm: 0.0132\n",
      "\u001b[1m 75/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:15\u001b[0m 481ms/step - accuracy: 0.4905 - binary_io_u_5: 0.4900 - loss: 0.9140Batch 76, Loss Value: 0.9111\n",
      "Batch 76, Gradient Norm: 0.1392\n",
      "\u001b[1m 76/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:15\u001b[0m 481ms/step - accuracy: 0.4906 - binary_io_u_5: 0.4901 - loss: 0.9140Batch 77, Loss Value: 0.9381\n",
      "Batch 77, Gradient Norm: 0.3599\n",
      "\u001b[1m 77/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:14\u001b[0m 481ms/step - accuracy: 0.4907 - binary_io_u_5: 0.4902 - loss: 0.9139Batch 78, Loss Value: 0.9446\n",
      "Batch 78, Gradient Norm: 0.0028\n",
      "\u001b[1m 78/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:14\u001b[0m 481ms/step - accuracy: 0.4908 - binary_io_u_5: 0.4903 - loss: 0.9139Batch 79, Loss Value: 0.9330\n",
      "Batch 79, Gradient Norm: 0.1455\n",
      "\u001b[1m 79/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:13\u001b[0m 481ms/step - accuracy: 0.4909 - binary_io_u_5: 0.4904 - loss: 0.9138Batch 80, Loss Value: 0.9287\n",
      "Batch 80, Gradient Norm: 0.2651\n",
      "\u001b[1m 80/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:13\u001b[0m 481ms/step - accuracy: 0.4910 - binary_io_u_5: 0.4905 - loss: 0.9138Batch 81, Loss Value: 0.9226\n",
      "Batch 81, Gradient Norm: 0.0948\n",
      "\u001b[1m 81/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:12\u001b[0m 481ms/step - accuracy: 0.4911 - binary_io_u_5: 0.4906 - loss: 0.9137Batch 82, Loss Value: 0.9099\n",
      "Batch 82, Gradient Norm: 0.3003\n",
      "\u001b[1m 82/232\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m1:12\u001b[0m 481ms/step - accuracy: 0.4912 - binary_io_u_5: 0.4907 - loss: 0.9137Batch 83, Loss Value: 0.9137\n",
      "Batch 83, Gradient Norm: 0.2427\n",
      "\u001b[1m 83/232\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m1:11\u001b[0m 481ms/step - accuracy: 0.4912 - binary_io_u_5: 0.4908 - loss: 0.9137Batch 84, Loss Value: 0.9447\n",
      "Batch 84, Gradient Norm: 0.0087\n",
      "\u001b[1m 84/232\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m1:11\u001b[0m 481ms/step - accuracy: 0.4913 - binary_io_u_5: 0.4909 - loss: 0.9136Batch 85, Loss Value: 0.9239\n",
      "Batch 85, Gradient Norm: 0.1749\n",
      "\u001b[1m 85/232\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m1:10\u001b[0m 481ms/step - accuracy: 0.4914 - binary_io_u_5: 0.4910 - loss: 0.9136Batch 86, Loss Value: 0.9333\n",
      "Batch 86, Gradient Norm: 0.1082\n",
      "\u001b[1m 86/232\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m1:10\u001b[0m 481ms/step - accuracy: 0.4915 - binary_io_u_5: 0.4911 - loss: 0.9135Batch 87, Loss Value: 0.9450\n",
      "Batch 87, Gradient Norm: 0.0328\n",
      "\u001b[1m 87/232\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m1:09\u001b[0m 481ms/step - accuracy: 0.4916 - binary_io_u_5: 0.4912 - loss: 0.9135Batch 88, Loss Value: 0.9308\n",
      "Batch 88, Gradient Norm: 0.4196\n",
      "\u001b[1m 88/232\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m1:09\u001b[0m 481ms/step - accuracy: 0.4917 - binary_io_u_5: 0.4913 - loss: 0.9134Batch 89, Loss Value: 0.9345\n",
      "Batch 89, Gradient Norm: 0.2412\n",
      "\u001b[1m 89/232\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m1:08\u001b[0m 481ms/step - accuracy: 0.4918 - binary_io_u_5: 0.4914 - loss: 0.9134Batch 90, Loss Value: 0.9077\n",
      "Batch 90, Gradient Norm: 0.3340\n",
      "\u001b[1m 90/232\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m1:08\u001b[0m 481ms/step - accuracy: 0.4919 - binary_io_u_5: 0.4915 - loss: 0.9134Batch 91, Loss Value: 0.9440\n",
      "Batch 91, Gradient Norm: 0.0084\n",
      "\u001b[1m 91/232\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m1:07\u001b[0m 481ms/step - accuracy: 0.4920 - binary_io_u_5: 0.4915 - loss: 0.9133Batch 92, Loss Value: 0.9354\n",
      "Batch 92, Gradient Norm: 0.3102\n",
      "\u001b[1m 92/232\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m1:07\u001b[0m 481ms/step - accuracy: 0.4921 - binary_io_u_5: 0.4916 - loss: 0.9133Batch 93, Loss Value: 0.9360\n",
      "Batch 93, Gradient Norm: 0.1762\n",
      "\u001b[1m 93/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m1:06\u001b[0m 481ms/step - accuracy: 0.4922 - binary_io_u_5: 0.4917 - loss: 0.9133Batch 94, Loss Value: 0.9385\n",
      "Batch 94, Gradient Norm: 0.1777\n",
      "\u001b[1m 94/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m1:06\u001b[0m 481ms/step - accuracy: 0.4922 - binary_io_u_5: 0.4918 - loss: 0.9132Batch 95, Loss Value: 0.9414\n",
      "Batch 95, Gradient Norm: 0.5368\n",
      "\u001b[1m 95/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m1:05\u001b[0m 481ms/step - accuracy: 0.4923 - binary_io_u_5: 0.4919 - loss: 0.9132Batch 96, Loss Value: 0.9449\n",
      "Batch 96, Gradient Norm: 0.0054\n",
      "\u001b[1m 96/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m1:05\u001b[0m 481ms/step - accuracy: 0.4924 - binary_io_u_5: 0.4920 - loss: 0.9132Batch 97, Loss Value: 0.9364\n",
      "Batch 97, Gradient Norm: 0.0954\n",
      "\u001b[1m 97/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m1:04\u001b[0m 481ms/step - accuracy: 0.4925 - binary_io_u_5: 0.4920 - loss: 0.9132Batch 98, Loss Value: 0.9443\n",
      "Batch 98, Gradient Norm: 0.0076\n",
      "\u001b[1m 98/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m1:04\u001b[0m 481ms/step - accuracy: 0.4926 - binary_io_u_5: 0.4921 - loss: 0.9131Batch 99, Loss Value: 0.9243\n",
      "Batch 99, Gradient Norm: 0.2551\n",
      "\u001b[1m 99/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m1:03\u001b[0m 481ms/step - accuracy: 0.4927 - binary_io_u_5: 0.4922 - loss: 0.9131Batch 100, Loss Value: 0.9443\n",
      "Batch 100, Gradient Norm: 0.0068\n",
      "\u001b[1m100/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m1:03\u001b[0m 481ms/step - accuracy: 0.4927 - binary_io_u_5: 0.4923 - loss: 0.9131Batch 101, Loss Value: 0.9444\n",
      "Batch 101, Gradient Norm: 0.0065\n",
      "\u001b[1m101/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m1:03\u001b[0m 481ms/step - accuracy: 0.4928 - binary_io_u_5: 0.4924 - loss: 0.9131Batch 102, Loss Value: 0.9437\n",
      "Batch 102, Gradient Norm: 0.0095\n",
      "\u001b[1m102/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m1:02\u001b[0m 481ms/step - accuracy: 0.4929 - binary_io_u_5: 0.4925 - loss: 0.9130Batch 103, Loss Value: 0.9175\n",
      "Batch 103, Gradient Norm: 0.4151\n",
      "\u001b[1m103/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m1:02\u001b[0m 481ms/step - accuracy: 0.4930 - binary_io_u_5: 0.4926 - loss: 0.9130Batch 104, Loss Value: 0.9340\n",
      "Batch 104, Gradient Norm: 0.1182\n",
      "\u001b[1m104/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m1:01\u001b[0m 481ms/step - accuracy: 0.4931 - binary_io_u_5: 0.4927 - loss: 0.9130Batch 105, Loss Value: 0.9090\n",
      "Batch 105, Gradient Norm: 0.3804\n",
      "\u001b[1m105/232\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m1:01\u001b[0m 481ms/step - accuracy: 0.4932 - binary_io_u_5: 0.4927 - loss: 0.9130Batch 106, Loss Value: 0.9284\n",
      "Batch 106, Gradient Norm: 0.1484\n",
      "\u001b[1m106/232\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m1:00\u001b[0m 481ms/step - accuracy: 0.4933 - binary_io_u_5: 0.4928 - loss: 0.9129Batch 107, Loss Value: 0.9333\n",
      "Batch 107, Gradient Norm: 0.1035\n",
      "\u001b[1m107/232\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m1:00\u001b[0m 481ms/step - accuracy: 0.4933 - binary_io_u_5: 0.4929 - loss: 0.9129Batch 108, Loss Value: 0.9290\n",
      "Batch 108, Gradient Norm: 0.1042\n",
      "\u001b[1m108/232\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m59s\u001b[0m 481ms/step - accuracy: 0.4934 - binary_io_u_5: 0.4930 - loss: 0.9129 Batch 109, Loss Value: 0.9432\n",
      "Batch 109, Gradient Norm: 0.2613\n",
      "\u001b[1m109/232\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m59s\u001b[0m 481ms/step - accuracy: 0.4935 - binary_io_u_5: 0.4931 - loss: 0.9129Batch 110, Loss Value: 0.9272\n",
      "Batch 110, Gradient Norm: 0.2898\n",
      "\u001b[1m110/232\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m58s\u001b[0m 481ms/step - accuracy: 0.4936 - binary_io_u_5: 0.4932 - loss: 0.9128Batch 111, Loss Value: 0.9254\n",
      "Batch 111, Gradient Norm: 0.1418\n",
      "\u001b[1m111/232\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m58s\u001b[0m 481ms/step - accuracy: 0.4937 - binary_io_u_5: 0.4932 - loss: 0.9128Batch 112, Loss Value: 0.9286\n",
      "Batch 112, Gradient Norm: 0.1774\n",
      "\u001b[1m112/232\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m57s\u001b[0m 481ms/step - accuracy: 0.4937 - binary_io_u_5: 0.4933 - loss: 0.9128Batch 113, Loss Value: 0.9284\n",
      "Batch 113, Gradient Norm: 0.0484\n",
      "\u001b[1m113/232\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m57s\u001b[0m 481ms/step - accuracy: 0.4938 - binary_io_u_5: 0.4934 - loss: 0.9128Batch 114, Loss Value: 0.9329\n",
      "Batch 114, Gradient Norm: 0.0554\n",
      "\u001b[1m114/232\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m56s\u001b[0m 481ms/step - accuracy: 0.4939 - binary_io_u_5: 0.4935 - loss: 0.9128Batch 115, Loss Value: 0.9343\n",
      "Batch 115, Gradient Norm: 0.0096\n",
      "\u001b[1m115/232\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m56s\u001b[0m 481ms/step - accuracy: 0.4939 - binary_io_u_5: 0.4935 - loss: 0.9128Batch 116, Loss Value: 0.9342\n",
      "Batch 116, Gradient Norm: 0.0004\n",
      "\u001b[1m116/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m55s\u001b[0m 481ms/step - accuracy: 0.4940 - binary_io_u_5: 0.4936 - loss: 0.9128Batch 117, Loss Value: 0.9334\n",
      "Batch 117, Gradient Norm: 0.0604\n",
      "\u001b[1m117/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m55s\u001b[0m 481ms/step - accuracy: 0.4940 - binary_io_u_5: 0.4936 - loss: 0.9127Batch 118, Loss Value: 0.9341\n",
      "Batch 118, Gradient Norm: 0.0103\n",
      "\u001b[1m118/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m54s\u001b[0m 481ms/step - accuracy: 0.4941 - binary_io_u_5: 0.4937 - loss: 0.9127Batch 119, Loss Value: 0.9252\n",
      "Batch 119, Gradient Norm: 0.1354\n",
      "\u001b[1m119/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m54s\u001b[0m 481ms/step - accuracy: 0.4942 - binary_io_u_5: 0.4938 - loss: 0.9127Batch 120, Loss Value: 0.9372\n",
      "Batch 120, Gradient Norm: 0.1613\n",
      "\u001b[1m120/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m53s\u001b[0m 481ms/step - accuracy: 0.4942 - binary_io_u_5: 0.4938 - loss: 0.9127Batch 121, Loss Value: 0.9338\n",
      "Batch 121, Gradient Norm: 0.0345\n",
      "\u001b[1m121/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m53s\u001b[0m 481ms/step - accuracy: 0.4943 - binary_io_u_5: 0.4939 - loss: 0.9127Batch 122, Loss Value: 0.9433\n",
      "Batch 122, Gradient Norm: 0.0131\n",
      "\u001b[1m122/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m52s\u001b[0m 481ms/step - accuracy: 0.4943 - binary_io_u_5: 0.4939 - loss: 0.9127Batch 123, Loss Value: 0.9344\n",
      "Batch 123, Gradient Norm: 0.0600\n",
      "\u001b[1m123/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m52s\u001b[0m 481ms/step - accuracy: 0.4944 - binary_io_u_5: 0.4940 - loss: 0.9127Batch 124, Loss Value: 0.9331\n",
      "Batch 124, Gradient Norm: 0.0982\n",
      "\u001b[1m124/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m51s\u001b[0m 481ms/step - accuracy: 0.4944 - binary_io_u_5: 0.4941 - loss: 0.9127Batch 125, Loss Value: 0.9350\n",
      "Batch 125, Gradient Norm: 0.1091\n",
      "\u001b[1m125/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m51s\u001b[0m 481ms/step - accuracy: 0.4945 - binary_io_u_5: 0.4941 - loss: 0.9127Batch 126, Loss Value: 0.9357\n",
      "Batch 126, Gradient Norm: 0.1116\n",
      "\u001b[1m126/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m50s\u001b[0m 481ms/step - accuracy: 0.4945 - binary_io_u_5: 0.4942 - loss: 0.9126Batch 127, Loss Value: 0.9342\n",
      "Batch 127, Gradient Norm: 0.0021\n",
      "\u001b[1m127/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m50s\u001b[0m 481ms/step - accuracy: 0.4946 - binary_io_u_5: 0.4942 - loss: 0.9126Batch 128, Loss Value: 0.9342\n",
      "Batch 128, Gradient Norm: 0.0001\n",
      "\u001b[1m128/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m50s\u001b[0m 481ms/step - accuracy: 0.4946 - binary_io_u_5: 0.4943 - loss: 0.9126Batch 129, Loss Value: 0.9342\n",
      "Batch 129, Gradient Norm: 0.0049\n",
      "\u001b[1m129/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m49s\u001b[0m 481ms/step - accuracy: 0.4947 - binary_io_u_5: 0.4943 - loss: 0.9126Batch 130, Loss Value: 0.9342\n",
      "Batch 130, Gradient Norm: 0.0008\n",
      "\u001b[1m130/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m49s\u001b[0m 481ms/step - accuracy: 0.4948 - binary_io_u_5: 0.4944 - loss: 0.9126Batch 131, Loss Value: 0.9342\n",
      "Batch 131, Gradient Norm: 0.0036\n",
      "\u001b[1m131/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m48s\u001b[0m 481ms/step - accuracy: 0.4948 - binary_io_u_5: 0.4945 - loss: 0.9126Batch 132, Loss Value: 0.9353\n",
      "Batch 132, Gradient Norm: 0.0538\n",
      "\u001b[1m132/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m48s\u001b[0m 481ms/step - accuracy: 0.4949 - binary_io_u_5: 0.4945 - loss: 0.9126Batch 133, Loss Value: 0.9340\n",
      "Batch 133, Gradient Norm: 0.0142\n",
      "\u001b[1m133/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m47s\u001b[0m 481ms/step - accuracy: 0.4949 - binary_io_u_5: 0.4946 - loss: 0.9126Batch 134, Loss Value: 0.9340\n",
      "Batch 134, Gradient Norm: 0.0157\n",
      "\u001b[1m134/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m47s\u001b[0m 481ms/step - accuracy: 0.4950 - binary_io_u_5: 0.4946 - loss: 0.9125Batch 135, Loss Value: 0.9342\n",
      "Batch 135, Gradient Norm: 0.0000\n",
      "\u001b[1m135/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m46s\u001b[0m 481ms/step - accuracy: 0.4950 - binary_io_u_5: 0.4947 - loss: 0.9125Batch 136, Loss Value: 0.9342\n",
      "Batch 136, Gradient Norm: 0.0000\n",
      "\u001b[1m136/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m46s\u001b[0m 481ms/step - accuracy: 0.4951 - binary_io_u_5: 0.4947 - loss: 0.9125Batch 137, Loss Value: 0.9342\n",
      "Batch 137, Gradient Norm: 0.0026\n",
      "\u001b[1m137/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m45s\u001b[0m 481ms/step - accuracy: 0.4951 - binary_io_u_5: 0.4948 - loss: 0.9125Batch 138, Loss Value: 0.9342\n",
      "Batch 138, Gradient Norm: 0.0002\n",
      "\u001b[1m138/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m45s\u001b[0m 481ms/step - accuracy: 0.4952 - binary_io_u_5: 0.4948 - loss: 0.9125Batch 139, Loss Value: 0.9342\n",
      "Batch 139, Gradient Norm: 0.0000\n",
      "\u001b[1m139/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m44s\u001b[0m 481ms/step - accuracy: 0.4952 - binary_io_u_5: 0.4949 - loss: 0.9125Batch 140, Loss Value: 0.9342\n",
      "Batch 140, Gradient Norm: 0.0001\n",
      "\u001b[1m140/232\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m44s\u001b[0m 481ms/step - accuracy: 0.4952 - binary_io_u_5: 0.4949 - loss: 0.9125Batch 141, Loss Value: 0.9342\n",
      "Batch 141, Gradient Norm: 0.0002\n",
      "\u001b[1m141/232\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m43s\u001b[0m 481ms/step - accuracy: 0.4953 - binary_io_u_5: 0.4949 - loss: 0.9125Batch 142, Loss Value: 0.9342\n",
      "Batch 142, Gradient Norm: 0.0000\n",
      "\u001b[1m142/232\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m43s\u001b[0m 481ms/step - accuracy: 0.4953 - binary_io_u_5: 0.4950 - loss: 0.9125Batch 143, Loss Value: 0.9342\n",
      "Batch 143, Gradient Norm: 0.0002\n",
      "\u001b[1m143/232\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m42s\u001b[0m 481ms/step - accuracy: 0.4954 - binary_io_u_5: 0.4950 - loss: 0.9125Batch 144, Loss Value: 0.9342\n",
      "Batch 144, Gradient Norm: 0.0000\n",
      "\u001b[1m144/232\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m42s\u001b[0m 481ms/step - accuracy: 0.4954 - binary_io_u_5: 0.4951 - loss: 0.9124Batch 145, Loss Value: 0.9357\n",
      "Batch 145, Gradient Norm: 0.0893\n",
      "\u001b[1m145/232\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m41s\u001b[0m 481ms/step - accuracy: 0.4954 - binary_io_u_5: 0.4951 - loss: 0.9124Batch 146, Loss Value: 0.9342\n",
      "Batch 146, Gradient Norm: 0.0004\n",
      "\u001b[1m146/232\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m41s\u001b[0m 481ms/step - accuracy: 0.4955 - binary_io_u_5: 0.4952 - loss: 0.9124Batch 147, Loss Value: 0.9340\n",
      "Batch 147, Gradient Norm: 0.0149\n",
      "\u001b[1m147/232\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m40s\u001b[0m 481ms/step - accuracy: 0.4955 - binary_io_u_5: 0.4952 - loss: 0.9124Batch 148, Loss Value: 0.9239\n",
      "Batch 148, Gradient Norm: 0.0448\n",
      "\u001b[1m148/232\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m40s\u001b[0m 481ms/step - accuracy: 0.4956 - binary_io_u_5: 0.4952 - loss: 0.9124Batch 149, Loss Value: 0.9342\n",
      "Batch 149, Gradient Norm: 0.0014\n",
      "\u001b[1m149/232\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m39s\u001b[0m 481ms/step - accuracy: 0.4956 - binary_io_u_5: 0.4953 - loss: 0.9124Batch 150, Loss Value: 0.9342\n",
      "Batch 150, Gradient Norm: 0.0063\n",
      "\u001b[1m150/232\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m39s\u001b[0m 481ms/step - accuracy: 0.4956 - binary_io_u_5: 0.4953 - loss: 0.9124Batch 151, Loss Value: 0.9342\n",
      "Batch 151, Gradient Norm: 0.0003\n",
      "\u001b[1m151/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m38s\u001b[0m 481ms/step - accuracy: 0.4957 - binary_io_u_5: 0.4953 - loss: 0.9124Batch 152, Loss Value: 0.9341\n",
      "Batch 152, Gradient Norm: 0.0073\n",
      "\u001b[1m152/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m38s\u001b[0m 481ms/step - accuracy: 0.4957 - binary_io_u_5: 0.4954 - loss: 0.9124Batch 153, Loss Value: 0.9343\n",
      "Batch 153, Gradient Norm: 0.0050\n",
      "\u001b[1m153/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m37s\u001b[0m 481ms/step - accuracy: 0.4957 - binary_io_u_5: 0.4954 - loss: 0.9124Batch 154, Loss Value: 0.9342\n",
      "Batch 154, Gradient Norm: 0.0011\n",
      "\u001b[1m154/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m37s\u001b[0m 481ms/step - accuracy: 0.4957 - binary_io_u_5: 0.4954 - loss: 0.9124Batch 155, Loss Value: 0.9343\n",
      "Batch 155, Gradient Norm: 0.0037\n",
      "\u001b[1m155/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m37s\u001b[0m 481ms/step - accuracy: 0.4958 - binary_io_u_5: 0.4955 - loss: 0.9124Batch 156, Loss Value: 0.9342\n",
      "Batch 156, Gradient Norm: 0.0002\n",
      "\u001b[1m156/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m36s\u001b[0m 481ms/step - accuracy: 0.4958 - binary_io_u_5: 0.4955 - loss: 0.9124Batch 157, Loss Value: 0.9321\n",
      "Batch 157, Gradient Norm: 0.1527\n",
      "\u001b[1m157/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m36s\u001b[0m 481ms/step - accuracy: 0.4958 - binary_io_u_5: 0.4955 - loss: 0.9124Batch 158, Loss Value: 0.9343\n",
      "Batch 158, Gradient Norm: 0.0022\n",
      "\u001b[1m158/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m35s\u001b[0m 481ms/step - accuracy: 0.4959 - binary_io_u_5: 0.4956 - loss: 0.9124Batch 159, Loss Value: 0.9316\n",
      "Batch 159, Gradient Norm: 0.0851\n",
      "\u001b[1m159/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m35s\u001b[0m 481ms/step - accuracy: 0.4959 - binary_io_u_5: 0.4956 - loss: 0.9124Batch 160, Loss Value: 0.9342\n",
      "Batch 160, Gradient Norm: 0.0030\n",
      "\u001b[1m160/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m34s\u001b[0m 481ms/step - accuracy: 0.4959 - binary_io_u_5: 0.4956 - loss: 0.9124Batch 161, Loss Value: 0.9342\n",
      "Batch 161, Gradient Norm: 0.0032\n",
      "\u001b[1m161/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m34s\u001b[0m 481ms/step - accuracy: 0.4959 - binary_io_u_5: 0.4956 - loss: 0.9124Batch 162, Loss Value: 0.9342\n",
      "Batch 162, Gradient Norm: 0.0004\n",
      "\u001b[1m162/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m33s\u001b[0m 481ms/step - accuracy: 0.4959 - binary_io_u_5: 0.4956 - loss: 0.9124Batch 163, Loss Value: 0.9342\n",
      "Batch 163, Gradient Norm: 0.0022\n",
      "\u001b[1m163/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m33s\u001b[0m 481ms/step - accuracy: 0.4960 - binary_io_u_5: 0.4957 - loss: 0.9124Batch 164, Loss Value: 0.9342\n",
      "Batch 164, Gradient Norm: 0.0021\n",
      "\u001b[1m164/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m32s\u001b[0m 481ms/step - accuracy: 0.4960 - binary_io_u_5: 0.4957 - loss: 0.9124Batch 165, Loss Value: 0.9342\n",
      "Batch 165, Gradient Norm: 0.0017\n",
      "\u001b[1m165/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m32s\u001b[0m 481ms/step - accuracy: 0.4960 - binary_io_u_5: 0.4957 - loss: 0.9124Batch 166, Loss Value: 0.9342\n",
      "Batch 166, Gradient Norm: 0.0000\n",
      "\u001b[1m166/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m31s\u001b[0m 481ms/step - accuracy: 0.4960 - binary_io_u_5: 0.4957 - loss: 0.9124Batch 167, Loss Value: 0.9342\n",
      "Batch 167, Gradient Norm: 0.0001\n",
      "\u001b[1m167/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m31s\u001b[0m 481ms/step - accuracy: 0.4960 - binary_io_u_5: 0.4957 - loss: 0.9124Batch 168, Loss Value: 0.9342\n",
      "Batch 168, Gradient Norm: 0.0003\n",
      "\u001b[1m168/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m30s\u001b[0m 481ms/step - accuracy: 0.4960 - binary_io_u_5: 0.4957 - loss: 0.9124Batch 169, Loss Value: 0.9342\n",
      "Batch 169, Gradient Norm: 0.0000\n",
      "\u001b[1m169/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m30s\u001b[0m 481ms/step - accuracy: 0.4960 - binary_io_u_5: 0.4957 - loss: 0.9124Batch 170, Loss Value: 0.9342\n",
      "Batch 170, Gradient Norm: 0.0001\n",
      "\u001b[1m170/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m29s\u001b[0m 481ms/step - accuracy: 0.4960 - binary_io_u_5: 0.4957 - loss: 0.9124Batch 171, Loss Value: 0.9342\n",
      "Batch 171, Gradient Norm: 0.0001\n",
      "\u001b[1m171/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m29s\u001b[0m 481ms/step - accuracy: 0.4960 - binary_io_u_5: 0.4957 - loss: 0.9124Batch 172, Loss Value: 0.9342\n",
      "Batch 172, Gradient Norm: 0.0002\n",
      "\u001b[1m172/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m28s\u001b[0m 481ms/step - accuracy: 0.4960 - binary_io_u_5: 0.4958 - loss: 0.9124Batch 173, Loss Value: 0.9341\n",
      "Batch 173, Gradient Norm: 0.0128\n",
      "\u001b[1m173/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m28s\u001b[0m 481ms/step - accuracy: 0.4961 - binary_io_u_5: 0.4958 - loss: 0.9124Batch 174, Loss Value: 0.9342\n",
      "Batch 174, Gradient Norm: 0.0003\n",
      "\u001b[1m174/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m27s\u001b[0m 481ms/step - accuracy: 0.4961 - binary_io_u_5: 0.4958 - loss: 0.9125Batch 175, Loss Value: 0.9342\n",
      "Batch 175, Gradient Norm: 0.0001\n",
      "\u001b[1m175/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m27s\u001b[0m 481ms/step - accuracy: 0.4961 - binary_io_u_5: 0.4958 - loss: 0.9125Batch 176, Loss Value: 0.9342\n",
      "Batch 176, Gradient Norm: 0.0034\n",
      "\u001b[1m176/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m26s\u001b[0m 481ms/step - accuracy: 0.4961 - binary_io_u_5: 0.4958 - loss: 0.9125Batch 177, Loss Value: 0.9342\n",
      "Batch 177, Gradient Norm: 0.0001\n",
      "\u001b[1m177/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m26s\u001b[0m 481ms/step - accuracy: 0.4961 - binary_io_u_5: 0.4958 - loss: 0.9125Batch 178, Loss Value: 0.9344\n",
      "Batch 178, Gradient Norm: 0.0170\n",
      "\u001b[1m178/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m25s\u001b[0m 481ms/step - accuracy: 0.4961 - binary_io_u_5: 0.4958 - loss: 0.9125Batch 179, Loss Value: 0.9342\n",
      "Batch 179, Gradient Norm: 0.0001\n",
      "\u001b[1m179/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m25s\u001b[0m 481ms/step - accuracy: 0.4961 - binary_io_u_5: 0.4958 - loss: 0.9125Batch 180, Loss Value: 0.9342\n",
      "Batch 180, Gradient Norm: 0.0013\n",
      "\u001b[1m180/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m25s\u001b[0m 481ms/step - accuracy: 0.4961 - binary_io_u_5: 0.4958 - loss: 0.9125Batch 181, Loss Value: 0.9342\n",
      "Batch 181, Gradient Norm: 0.0031\n",
      "\u001b[1m181/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m24s\u001b[0m 481ms/step - accuracy: 0.4961 - binary_io_u_5: 0.4958 - loss: 0.9125Batch 182, Loss Value: 0.9341\n",
      "Batch 182, Gradient Norm: 0.0075\n",
      "\u001b[1m182/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m24s\u001b[0m 481ms/step - accuracy: 0.4961 - binary_io_u_5: 0.4958 - loss: 0.9125Batch 183, Loss Value: 0.9343\n",
      "Batch 183, Gradient Norm: 0.0063\n",
      "\u001b[1m183/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m23s\u001b[0m 481ms/step - accuracy: 0.4961 - binary_io_u_5: 0.4958 - loss: 0.9125Batch 184, Loss Value: 0.9343\n",
      "Batch 184, Gradient Norm: 0.0064\n",
      "\u001b[1m184/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m23s\u001b[0m 481ms/step - accuracy: 0.4961 - binary_io_u_5: 0.4958 - loss: 0.9125Batch 185, Loss Value: 0.9343\n",
      "Batch 185, Gradient Norm: 0.0061\n",
      "\u001b[1m185/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m22s\u001b[0m 481ms/step - accuracy: 0.4961 - binary_io_u_5: 0.4958 - loss: 0.9125Batch 186, Loss Value: 0.9338\n",
      "Batch 186, Gradient Norm: 0.0354\n",
      "\u001b[1m186/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m22s\u001b[0m 481ms/step - accuracy: 0.4961 - binary_io_u_5: 0.4959 - loss: 0.9125Batch 187, Loss Value: 0.9336\n",
      "Batch 187, Gradient Norm: 0.0494\n",
      "\u001b[1m187/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m21s\u001b[0m 481ms/step - accuracy: 0.4961 - binary_io_u_5: 0.4959 - loss: 0.9125Batch 188, Loss Value: 0.9343\n",
      "Batch 188, Gradient Norm: 0.0047\n",
      "\u001b[1m188/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m21s\u001b[0m 481ms/step - accuracy: 0.4961 - binary_io_u_5: 0.4959 - loss: 0.9125Batch 189, Loss Value: 0.9341\n",
      "Batch 189, Gradient Norm: 0.0121\n",
      "\u001b[1m189/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m20s\u001b[0m 481ms/step - accuracy: 0.4961 - binary_io_u_5: 0.4959 - loss: 0.9125Batch 190, Loss Value: 0.9342\n",
      "Batch 190, Gradient Norm: 0.0002\n",
      "\u001b[1m190/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m20s\u001b[0m 481ms/step - accuracy: 0.4961 - binary_io_u_5: 0.4959 - loss: 0.9125Batch 191, Loss Value: 0.9342\n",
      "Batch 191, Gradient Norm: 0.0004\n",
      "\u001b[1m191/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m19s\u001b[0m 481ms/step - accuracy: 0.4961 - binary_io_u_5: 0.4959 - loss: 0.9125Batch 192, Loss Value: 0.9343\n",
      "Batch 192, Gradient Norm: 0.0104\n",
      "\u001b[1m192/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m19s\u001b[0m 481ms/step - accuracy: 0.4962 - binary_io_u_5: 0.4959 - loss: 0.9125Batch 193, Loss Value: 0.9342\n",
      "Batch 193, Gradient Norm: 0.0012\n",
      "\u001b[1m193/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m18s\u001b[0m 481ms/step - accuracy: 0.4962 - binary_io_u_5: 0.4959 - loss: 0.9126Batch 194, Loss Value: 0.9342\n",
      "Batch 194, Gradient Norm: 0.0003\n",
      "\u001b[1m194/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m18s\u001b[0m 481ms/step - accuracy: 0.4962 - binary_io_u_5: 0.4959 - loss: 0.9126Batch 195, Loss Value: 0.9375\n",
      "Batch 195, Gradient Norm: 0.2387\n",
      "\u001b[1m195/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m17s\u001b[0m 481ms/step - accuracy: 0.4962 - binary_io_u_5: 0.4959 - loss: 0.9126Batch 196, Loss Value: 0.9300\n",
      "Batch 196, Gradient Norm: 0.2284\n",
      "\u001b[1m196/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m17s\u001b[0m 481ms/step - accuracy: 0.4962 - binary_io_u_5: 0.4959 - loss: 0.9126Batch 197, Loss Value: 0.9345\n",
      "Batch 197, Gradient Norm: 0.0246\n",
      "\u001b[1m197/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m16s\u001b[0m 481ms/step - accuracy: 0.4962 - binary_io_u_5: 0.4959 - loss: 0.9126Batch 198, Loss Value: 0.9341\n",
      "Batch 198, Gradient Norm: 0.0081\n",
      "\u001b[1m198/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m16s\u001b[0m 481ms/step - accuracy: 0.4962 - binary_io_u_5: 0.4959 - loss: 0.9126Batch 199, Loss Value: 0.9253\n",
      "Batch 199, Gradient Norm: 0.2873\n",
      "\u001b[1m199/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m15s\u001b[0m 481ms/step - accuracy: 0.4962 - binary_io_u_5: 0.4959 - loss: 0.9126Batch 200, Loss Value: 0.9342\n",
      "Batch 200, Gradient Norm: 0.0004\n",
      "\u001b[1m200/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m15s\u001b[0m 481ms/step - accuracy: 0.4962 - binary_io_u_5: 0.4959 - loss: 0.9126Batch 201, Loss Value: 0.9342\n",
      "Batch 201, Gradient Norm: 0.0026\n",
      "\u001b[1m201/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m14s\u001b[0m 481ms/step - accuracy: 0.4962 - binary_io_u_5: 0.4959 - loss: 0.9126Batch 202, Loss Value: 0.9339\n",
      "Batch 202, Gradient Norm: 0.0248\n",
      "\u001b[1m202/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m14s\u001b[0m 481ms/step - accuracy: 0.4962 - binary_io_u_5: 0.4959 - loss: 0.9126Batch 203, Loss Value: 0.9342\n",
      "Batch 203, Gradient Norm: 0.0001\n",
      "\u001b[1m203/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m13s\u001b[0m 481ms/step - accuracy: 0.4962 - binary_io_u_5: 0.4959 - loss: 0.9126Batch 204, Loss Value: 0.9348\n",
      "Batch 204, Gradient Norm: 0.0443\n",
      "\u001b[1m204/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m13s\u001b[0m 481ms/step - accuracy: 0.4962 - binary_io_u_5: 0.4959 - loss: 0.9126Batch 205, Loss Value: 0.9342\n",
      "Batch 205, Gradient Norm: 0.0000\n",
      "\u001b[1m205/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m12s\u001b[0m 481ms/step - accuracy: 0.4962 - binary_io_u_5: 0.4959 - loss: 0.9126Batch 206, Loss Value: 0.9342\n",
      "Batch 206, Gradient Norm: 0.0001\n",
      "\u001b[1m206/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m12s\u001b[0m 481ms/step - accuracy: 0.4962 - binary_io_u_5: 0.4959 - loss: 0.9126Batch 207, Loss Value: 0.9340\n",
      "Batch 207, Gradient Norm: 0.0220\n",
      "\u001b[1m207/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m12s\u001b[0m 481ms/step - accuracy: 0.4962 - binary_io_u_5: 0.4959 - loss: 0.9126Batch 208, Loss Value: 0.9342\n",
      "Batch 208, Gradient Norm: 0.0003\n",
      "\u001b[1m208/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m11s\u001b[0m 481ms/step - accuracy: 0.4962 - binary_io_u_5: 0.4959 - loss: 0.9126Batch 209, Loss Value: 0.9342\n",
      "Batch 209, Gradient Norm: 0.0011\n",
      "\u001b[1m209/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m11s\u001b[0m 481ms/step - accuracy: 0.4962 - binary_io_u_5: 0.4959 - loss: 0.9126Batch 210, Loss Value: 0.9339\n",
      "Batch 210, Gradient Norm: 0.0265\n",
      "\u001b[1m210/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m10s\u001b[0m 481ms/step - accuracy: 0.4962 - binary_io_u_5: 0.4960 - loss: 0.9126Batch 211, Loss Value: 0.9342\n",
      "Batch 211, Gradient Norm: 0.0011\n",
      "\u001b[1m211/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m10s\u001b[0m 481ms/step - accuracy: 0.4962 - binary_io_u_5: 0.4960 - loss: 0.9126Batch 212, Loss Value: 0.9342\n",
      "Batch 212, Gradient Norm: 0.0001\n",
      "\u001b[1m212/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m9s\u001b[0m 481ms/step - accuracy: 0.4962 - binary_io_u_5: 0.4960 - loss: 0.9126 Batch 213, Loss Value: 0.9342\n",
      "Batch 213, Gradient Norm: 0.0014\n",
      "\u001b[1m213/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m9s\u001b[0m 481ms/step - accuracy: 0.4962 - binary_io_u_5: 0.4960 - loss: 0.9126Batch 214, Loss Value: 0.9341\n",
      "Batch 214, Gradient Norm: 0.0115\n",
      "\u001b[1m214/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m8s\u001b[0m 481ms/step - accuracy: 0.4962 - binary_io_u_5: 0.4960 - loss: 0.9126Batch 215, Loss Value: 0.9316\n",
      "Batch 215, Gradient Norm: 0.0734\n",
      "\u001b[1m215/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m8s\u001b[0m 481ms/step - accuracy: 0.4962 - binary_io_u_5: 0.4960 - loss: 0.9127Batch 216, Loss Value: 0.9342\n",
      "Batch 216, Gradient Norm: 0.0033\n",
      "\u001b[1m216/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m7s\u001b[0m 481ms/step - accuracy: 0.4962 - binary_io_u_5: 0.4960 - loss: 0.9127Batch 217, Loss Value: 0.9342\n",
      "Batch 217, Gradient Norm: 0.0009\n",
      "\u001b[1m217/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m7s\u001b[0m 481ms/step - accuracy: 0.4962 - binary_io_u_5: 0.4960 - loss: 0.9127Batch 218, Loss Value: 0.9333\n",
      "Batch 218, Gradient Norm: 0.0705\n",
      "\u001b[1m218/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m6s\u001b[0m 481ms/step - accuracy: 0.4962 - binary_io_u_5: 0.4959 - loss: 0.9127Batch 219, Loss Value: 0.9253\n",
      "Batch 219, Gradient Norm: 0.4957\n",
      "\u001b[1m219/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m6s\u001b[0m 481ms/step - accuracy: 0.4962 - binary_io_u_5: 0.4959 - loss: 0.9127Batch 220, Loss Value: 0.9342\n",
      "Batch 220, Gradient Norm: 0.0004\n",
      "\u001b[1m220/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m5s\u001b[0m 481ms/step - accuracy: 0.4962 - binary_io_u_5: 0.4959 - loss: 0.9127Batch 221, Loss Value: 0.9342\n",
      "Batch 221, Gradient Norm: 0.0003\n",
      "\u001b[1m221/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m5s\u001b[0m 481ms/step - accuracy: 0.4962 - binary_io_u_5: 0.4959 - loss: 0.9127Batch 222, Loss Value: 0.9342\n",
      "Batch 222, Gradient Norm: 0.0022\n",
      "\u001b[1m222/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m4s\u001b[0m 481ms/step - accuracy: 0.4962 - binary_io_u_5: 0.4960 - loss: 0.9127Batch 223, Loss Value: 0.9342\n",
      "Batch 223, Gradient Norm: 0.0005\n",
      "\u001b[1m223/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m4s\u001b[0m 481ms/step - accuracy: 0.4962 - binary_io_u_5: 0.4960 - loss: 0.9127Batch 224, Loss Value: 0.9342\n",
      "Batch 224, Gradient Norm: 0.0001\n",
      "\u001b[1m224/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m3s\u001b[0m 481ms/step - accuracy: 0.4962 - binary_io_u_5: 0.4960 - loss: 0.9127Batch 225, Loss Value: 0.9342\n",
      "Batch 225, Gradient Norm: 0.0000\n",
      "\u001b[1m225/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m3s\u001b[0m 481ms/step - accuracy: 0.4962 - binary_io_u_5: 0.4960 - loss: 0.9127Batch 226, Loss Value: 0.9342\n",
      "Batch 226, Gradient Norm: 0.0005\n",
      "\u001b[1m226/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m2s\u001b[0m 481ms/step - accuracy: 0.4962 - binary_io_u_5: 0.4960 - loss: 0.9127Batch 227, Loss Value: 0.9342\n",
      "Batch 227, Gradient Norm: 0.0007\n",
      "\u001b[1m227/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m2s\u001b[0m 481ms/step - accuracy: 0.4962 - binary_io_u_5: 0.4960 - loss: 0.9127Batch 228, Loss Value: 0.9342\n",
      "Batch 228, Gradient Norm: 0.0002\n",
      "\u001b[1m228/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m1s\u001b[0m 481ms/step - accuracy: 0.4962 - binary_io_u_5: 0.4960 - loss: 0.9127Batch 229, Loss Value: 0.9342\n",
      "Batch 229, Gradient Norm: 0.0006\n",
      "\u001b[1m229/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m1s\u001b[0m 481ms/step - accuracy: 0.4962 - binary_io_u_5: 0.4959 - loss: 0.9127Batch 230, Loss Value: 0.9342\n",
      "Batch 230, Gradient Norm: 0.0004\n",
      "\u001b[1m230/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 481ms/step - accuracy: 0.4962 - binary_io_u_5: 0.4959 - loss: 0.9127Batch 231, Loss Value: 0.9309\n",
      "Batch 231, Gradient Norm: 0.1207\n",
      "\u001b[1m231/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 481ms/step - accuracy: 0.4962 - binary_io_u_5: 0.4959 - loss: 0.9127Batch 232, Loss Value: 0.9342\n",
      "Batch 232, Gradient Norm: 0.0012\n",
      "\u001b[1m232/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m113s\u001b[0m 487ms/step - accuracy: 0.4962 - binary_io_u_5: 0.4959 - loss: 0.9127 - val_accuracy: 0.4949 - val_binary_io_u_5: 0.4949 - val_loss: 0.9109 - learning_rate: 5.0000e-04\n",
      "Epoch 20/200\n",
      "Batch 1, Loss Value: 0.8520\n",
      "Batch 1, Gradient Norm: 0.0007\n",
      "\u001b[1m  1/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:58\u001b[0m 512ms/step - accuracy: 0.4500 - binary_io_u_5: 0.4500 - loss: 0.9443Batch 2, Loss Value: 0.8525\n",
      "Batch 2, Gradient Norm: 0.0396\n",
      "\u001b[1m  2/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:49\u001b[0m 474ms/step - accuracy: 0.4450 - binary_io_u_5: 0.4450 - loss: 0.9447Batch 3, Loss Value: 0.8520\n",
      "Batch 3, Gradient Norm: 0.0001\n",
      "\u001b[1m  3/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:48\u001b[0m 475ms/step - accuracy: 0.4522 - binary_io_u_5: 0.4522 - loss: 0.9398Batch 4, Loss Value: 0.8520\n",
      "Batch 4, Gradient Norm: 0.0000\n",
      "\u001b[1m  4/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:47\u001b[0m 474ms/step - accuracy: 0.4548 - binary_io_u_5: 0.4548 - loss: 0.9382Batch 5, Loss Value: 0.8520\n",
      "Batch 5, Gradient Norm: 0.0003\n",
      "\u001b[1m  5/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:47\u001b[0m 476ms/step - accuracy: 0.4566 - binary_io_u_5: 0.4566 - loss: 0.9373Batch 6, Loss Value: 0.8520\n",
      "Batch 6, Gradient Norm: 0.0001\n",
      "\u001b[1m  6/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:47\u001b[0m 476ms/step - accuracy: 0.4591 - binary_io_u_5: 0.4591 - loss: 0.9360Batch 7, Loss Value: 0.8520\n",
      "Batch 7, Gradient Norm: 0.0001\n",
      "\u001b[1m  7/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:47\u001b[0m 477ms/step - accuracy: 0.4615 - binary_io_u_5: 0.4615 - loss: 0.9347Batch 8, Loss Value: 0.8520\n",
      "Batch 8, Gradient Norm: 0.0001\n",
      "\u001b[1m  8/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:46\u001b[0m 477ms/step - accuracy: 0.4633 - binary_io_u_5: 0.4633 - loss: 0.9336Batch 9, Loss Value: 0.8520\n",
      "Batch 9, Gradient Norm: 0.0003\n",
      "\u001b[1m  9/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:46\u001b[0m 478ms/step - accuracy: 0.4647 - binary_io_u_5: 0.4647 - loss: 0.9329Batch 10, Loss Value: 0.8520\n",
      "Batch 10, Gradient Norm: 0.0001\n",
      "\u001b[1m 10/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:46\u001b[0m 478ms/step - accuracy: 0.4649 - binary_io_u_5: 0.4649 - loss: 0.9326Batch 11, Loss Value: 0.8520\n",
      "Batch 11, Gradient Norm: 0.0000\n",
      "\u001b[1m 11/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:45\u001b[0m 479ms/step - accuracy: 0.4652 - binary_io_u_5: 0.4652 - loss: 0.9322Batch 12, Loss Value: 0.8520\n",
      "Batch 12, Gradient Norm: 0.0007\n",
      "\u001b[1m 12/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:45\u001b[0m 479ms/step - accuracy: 0.4652 - binary_io_u_5: 0.4652 - loss: 0.9321Batch 13, Loss Value: 0.8520\n",
      "Batch 13, Gradient Norm: 0.0000\n",
      "\u001b[1m 13/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:44\u001b[0m 479ms/step - accuracy: 0.4652 - binary_io_u_5: 0.4652 - loss: 0.9320Batch 14, Loss Value: 0.8520\n",
      "Batch 14, Gradient Norm: 0.0000\n",
      "\u001b[1m 14/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:44\u001b[0m 479ms/step - accuracy: 0.4656 - binary_io_u_5: 0.4656 - loss: 0.9317Batch 15, Loss Value: 0.8520\n",
      "Batch 15, Gradient Norm: 0.0003\n",
      "\u001b[1m 15/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:44\u001b[0m 479ms/step - accuracy: 0.4661 - binary_io_u_5: 0.4661 - loss: 0.9313Batch 16, Loss Value: 0.8520\n",
      "Batch 16, Gradient Norm: 0.0000\n",
      "\u001b[1m 16/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:43\u001b[0m 480ms/step - accuracy: 0.4667 - binary_io_u_5: 0.4667 - loss: 0.9309Batch 17, Loss Value: 0.8520\n",
      "Batch 17, Gradient Norm: 0.0043\n",
      "\u001b[1m 17/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:43\u001b[0m 479ms/step - accuracy: 0.4672 - binary_io_u_5: 0.4672 - loss: 0.9306Batch 18, Loss Value: 0.8520\n",
      "Batch 18, Gradient Norm: 0.0000\n",
      "\u001b[1m 18/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:42\u001b[0m 480ms/step - accuracy: 0.4678 - binary_io_u_5: 0.4678 - loss: 0.9303Batch 19, Loss Value: 0.8521\n",
      "Batch 19, Gradient Norm: 0.0104\n",
      "\u001b[1m 19/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:42\u001b[0m 480ms/step - accuracy: 0.4685 - binary_io_u_5: 0.4685 - loss: 0.9299Batch 20, Loss Value: 0.8519\n",
      "Batch 20, Gradient Norm: 0.0098\n",
      "\u001b[1m 20/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:41\u001b[0m 480ms/step - accuracy: 0.4689 - binary_io_u_5: 0.4689 - loss: 0.9296Batch 21, Loss Value: 0.8520\n",
      "Batch 21, Gradient Norm: 0.0000\n",
      "\u001b[1m 21/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:41\u001b[0m 479ms/step - accuracy: 0.4694 - binary_io_u_5: 0.4694 - loss: 0.9292Batch 22, Loss Value: 0.8520\n",
      "Batch 22, Gradient Norm: 0.0000\n",
      "\u001b[1m 22/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:40\u001b[0m 479ms/step - accuracy: 0.4697 - binary_io_u_5: 0.4697 - loss: 0.9290Batch 23, Loss Value: 0.8518\n",
      "Batch 23, Gradient Norm: 0.0142\n",
      "\u001b[1m 23/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:40\u001b[0m 479ms/step - accuracy: 0.4701 - binary_io_u_5: 0.4701 - loss: 0.9288Batch 24, Loss Value: 0.8520\n",
      "Batch 24, Gradient Norm: 0.0001\n",
      "\u001b[1m 24/232\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:39\u001b[0m 479ms/step - accuracy: 0.4704 - binary_io_u_5: 0.4704 - loss: 0.9285Batch 25, Loss Value: 0.8519\n",
      "Batch 25, Gradient Norm: 0.0029\n",
      "\u001b[1m 25/232\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:39\u001b[0m 479ms/step - accuracy: 0.4708 - binary_io_u_5: 0.4708 - loss: 0.9283Batch 26, Loss Value: 0.8520\n",
      "Batch 26, Gradient Norm: 0.0002\n",
      "\u001b[1m 26/232\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:38\u001b[0m 479ms/step - accuracy: 0.4711 - binary_io_u_5: 0.4711 - loss: 0.9281Batch 27, Loss Value: 0.8520\n",
      "Batch 27, Gradient Norm: 0.0007\n",
      "\u001b[1m 27/232\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:38\u001b[0m 479ms/step - accuracy: 0.4715 - binary_io_u_5: 0.4715 - loss: 0.9278Batch 28, Loss Value: 0.8520\n",
      "Batch 28, Gradient Norm: 0.0001\n",
      "\u001b[1m 28/232\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:37\u001b[0m 479ms/step - accuracy: 0.4717 - binary_io_u_5: 0.4717 - loss: 0.9276Batch 29, Loss Value: 0.8520\n",
      "Batch 29, Gradient Norm: 0.0000\n",
      "\u001b[1m 29/232\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:37\u001b[0m 480ms/step - accuracy: 0.4720 - binary_io_u_5: 0.4720 - loss: 0.9274Batch 30, Loss Value: 0.8520\n",
      "Batch 30, Gradient Norm: 0.0000\n",
      "\u001b[1m 30/232\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:36\u001b[0m 480ms/step - accuracy: 0.4723 - binary_io_u_5: 0.4723 - loss: 0.9272Batch 31, Loss Value: 0.8520\n",
      "Batch 31, Gradient Norm: 0.0001\n",
      "\u001b[1m 31/232\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:36\u001b[0m 480ms/step - accuracy: 0.4726 - binary_io_u_5: 0.4726 - loss: 0.9271Batch 32, Loss Value: 0.8520\n",
      "Batch 32, Gradient Norm: 0.0000\n",
      "\u001b[1m 32/232\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:35\u001b[0m 479ms/step - accuracy: 0.4727 - binary_io_u_5: 0.4727 - loss: 0.9269Batch 33, Loss Value: 0.8520\n",
      "Batch 33, Gradient Norm: 0.0000\n",
      "\u001b[1m 33/232\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:35\u001b[0m 479ms/step - accuracy: 0.4730 - binary_io_u_5: 0.4730 - loss: 0.9268Batch 34, Loss Value: 0.8520\n",
      "Batch 34, Gradient Norm: 0.0017\n",
      "\u001b[1m 34/232\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:34\u001b[0m 479ms/step - accuracy: 0.4731 - binary_io_u_5: 0.4731 - loss: 0.9266Batch 35, Loss Value: 0.8525\n",
      "Batch 35, Gradient Norm: 0.0439\n",
      "\u001b[1m 35/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:34\u001b[0m 479ms/step - accuracy: 0.4732 - binary_io_u_5: 0.4732 - loss: 0.9265Batch 36, Loss Value: 0.8520\n",
      "Batch 36, Gradient Norm: 0.0010\n",
      "\u001b[1m 36/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:33\u001b[0m 479ms/step - accuracy: 0.4731 - binary_io_u_5: 0.4731 - loss: 0.9265Batch 37, Loss Value: 0.8520\n",
      "Batch 37, Gradient Norm: 0.0001\n",
      "\u001b[1m 37/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:33\u001b[0m 480ms/step - accuracy: 0.4732 - binary_io_u_5: 0.4732 - loss: 0.9264Batch 38, Loss Value: 0.8520\n",
      "Batch 38, Gradient Norm: 0.0002\n",
      "\u001b[1m 38/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:32\u001b[0m 479ms/step - accuracy: 0.4732 - binary_io_u_5: 0.4732 - loss: 0.9263Batch 39, Loss Value: 0.8520\n",
      "Batch 39, Gradient Norm: 0.0003\n",
      "\u001b[1m 39/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:32\u001b[0m 479ms/step - accuracy: 0.4732 - binary_io_u_5: 0.4732 - loss: 0.9262Batch 40, Loss Value: 0.8520\n",
      "Batch 40, Gradient Norm: 0.0000\n",
      "\u001b[1m 40/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:31\u001b[0m 479ms/step - accuracy: 0.4733 - binary_io_u_5: 0.4733 - loss: 0.9261Batch 41, Loss Value: 0.8520\n",
      "Batch 41, Gradient Norm: 0.0000\n",
      "\u001b[1m 41/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:31\u001b[0m 479ms/step - accuracy: 0.4735 - binary_io_u_5: 0.4735 - loss: 0.9259Batch 42, Loss Value: 0.8520\n",
      "Batch 42, Gradient Norm: 0.0000\n",
      "\u001b[1m 42/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:30\u001b[0m 479ms/step - accuracy: 0.4736 - binary_io_u_5: 0.4736 - loss: 0.9258Batch 43, Loss Value: 0.8520\n",
      "Batch 43, Gradient Norm: 0.0000\n",
      "\u001b[1m 43/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:30\u001b[0m 479ms/step - accuracy: 0.4737 - binary_io_u_5: 0.4737 - loss: 0.9257Batch 44, Loss Value: 0.8520\n",
      "Batch 44, Gradient Norm: 0.0000\n",
      "\u001b[1m 44/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:30\u001b[0m 479ms/step - accuracy: 0.4739 - binary_io_u_5: 0.4739 - loss: 0.9255Batch 45, Loss Value: 0.8520\n",
      "Batch 45, Gradient Norm: 0.0001\n",
      "\u001b[1m 45/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:29\u001b[0m 479ms/step - accuracy: 0.4740 - binary_io_u_5: 0.4740 - loss: 0.9254Batch 46, Loss Value: 0.8520\n",
      "Batch 46, Gradient Norm: 0.0000\n",
      "\u001b[1m 46/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:29\u001b[0m 479ms/step - accuracy: 0.4741 - binary_io_u_5: 0.4741 - loss: 0.9253Batch 47, Loss Value: 0.8520\n",
      "Batch 47, Gradient Norm: 0.0000\n",
      "\u001b[1m 47/232\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:28\u001b[0m 479ms/step - accuracy: 0.4743 - binary_io_u_5: 0.4743 - loss: 0.9252Batch 48, Loss Value: 0.8520\n",
      "Batch 48, Gradient Norm: 0.0006\n",
      "\u001b[1m 48/232\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:28\u001b[0m 479ms/step - accuracy: 0.4745 - binary_io_u_5: 0.4745 - loss: 0.9250Batch 49, Loss Value: 0.8520\n",
      "Batch 49, Gradient Norm: 0.0000\n",
      "\u001b[1m 49/232\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:27\u001b[0m 479ms/step - accuracy: 0.4746 - binary_io_u_5: 0.4746 - loss: 0.9249Batch 50, Loss Value: 0.8520\n",
      "Batch 50, Gradient Norm: 0.0000\n",
      "\u001b[1m 50/232\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:27\u001b[0m 479ms/step - accuracy: 0.4748 - binary_io_u_5: 0.4748 - loss: 0.9248Batch 51, Loss Value: 0.8520\n",
      "Batch 51, Gradient Norm: 0.0063\n",
      "\u001b[1m 51/232\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:26\u001b[0m 479ms/step - accuracy: 0.4749 - binary_io_u_5: 0.4749 - loss: 0.9247Batch 52, Loss Value: 0.8520\n",
      "Batch 52, Gradient Norm: 0.0002\n",
      "\u001b[1m 52/232\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:26\u001b[0m 480ms/step - accuracy: 0.4750 - binary_io_u_5: 0.4750 - loss: 0.9246Batch 53, Loss Value: 0.8520\n",
      "Batch 53, Gradient Norm: 0.0001\n",
      "\u001b[1m 53/232\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:25\u001b[0m 480ms/step - accuracy: 0.4751 - binary_io_u_5: 0.4751 - loss: 0.9245Batch 54, Loss Value: 0.8520\n",
      "Batch 54, Gradient Norm: 0.0000\n",
      "\u001b[1m 54/232\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:25\u001b[0m 480ms/step - accuracy: 0.4752 - binary_io_u_5: 0.4752 - loss: 0.9245Batch 55, Loss Value: 0.8520\n",
      "Batch 55, Gradient Norm: 0.0000\n",
      "\u001b[1m 55/232\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:24\u001b[0m 480ms/step - accuracy: 0.4753 - binary_io_u_5: 0.4753 - loss: 0.9244Batch 56, Loss Value: 0.8520\n",
      "Batch 56, Gradient Norm: 0.0000\n",
      "\u001b[1m 56/232\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:24\u001b[0m 480ms/step - accuracy: 0.4754 - binary_io_u_5: 0.4754 - loss: 0.9243Batch 57, Loss Value: 0.8520\n",
      "Batch 57, Gradient Norm: 0.0000\n",
      "\u001b[1m 57/232\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:23\u001b[0m 480ms/step - accuracy: 0.4756 - binary_io_u_5: 0.4756 - loss: 0.9242Batch 58, Loss Value: 0.8520\n",
      "Batch 58, Gradient Norm: 0.0000\n",
      "\u001b[1m 58/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:23\u001b[0m 479ms/step - accuracy: 0.4757 - binary_io_u_5: 0.4757 - loss: 0.9241Batch 59, Loss Value: 0.8520\n",
      "Batch 59, Gradient Norm: 0.0000\n",
      "\u001b[1m 59/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:22\u001b[0m 479ms/step - accuracy: 0.4758 - binary_io_u_5: 0.4758 - loss: 0.9240Batch 60, Loss Value: 0.8520\n",
      "Batch 60, Gradient Norm: 0.0000\n",
      "\u001b[1m 60/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:22\u001b[0m 479ms/step - accuracy: 0.4760 - binary_io_u_5: 0.4760 - loss: 0.9239Batch 61, Loss Value: 0.8520\n",
      "Batch 61, Gradient Norm: 0.0000\n",
      "\u001b[1m 61/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:21\u001b[0m 479ms/step - accuracy: 0.4761 - binary_io_u_5: 0.4761 - loss: 0.9238Batch 62, Loss Value: 0.8520\n",
      "Batch 62, Gradient Norm: 0.0000\n",
      "\u001b[1m 62/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:21\u001b[0m 480ms/step - accuracy: 0.4763 - binary_io_u_5: 0.4763 - loss: 0.9237Batch 63, Loss Value: 0.8520\n",
      "Batch 63, Gradient Norm: 0.0002\n",
      "\u001b[1m 63/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:21\u001b[0m 480ms/step - accuracy: 0.4764 - binary_io_u_5: 0.4764 - loss: 0.9236Batch 64, Loss Value: 0.8520\n",
      "Batch 64, Gradient Norm: 0.0000\n",
      "\u001b[1m 64/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:20\u001b[0m 480ms/step - accuracy: 0.4766 - binary_io_u_5: 0.4766 - loss: 0.9235Batch 65, Loss Value: 0.8520\n",
      "Batch 65, Gradient Norm: 0.0000\n",
      "\u001b[1m 65/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:20\u001b[0m 480ms/step - accuracy: 0.4767 - binary_io_u_5: 0.4767 - loss: 0.9234Batch 66, Loss Value: 0.8520\n",
      "Batch 66, Gradient Norm: 0.0000\n",
      "\u001b[1m 66/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:19\u001b[0m 480ms/step - accuracy: 0.4769 - binary_io_u_5: 0.4769 - loss: 0.9233Batch 67, Loss Value: 0.8520\n",
      "Batch 67, Gradient Norm: 0.0000\n",
      "\u001b[1m 67/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:19\u001b[0m 480ms/step - accuracy: 0.4771 - binary_io_u_5: 0.4771 - loss: 0.9232Batch 68, Loss Value: 0.8520\n",
      "Batch 68, Gradient Norm: 0.0009\n",
      "\u001b[1m 68/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:18\u001b[0m 480ms/step - accuracy: 0.4772 - binary_io_u_5: 0.4772 - loss: 0.9231Batch 69, Loss Value: 0.8520\n",
      "Batch 69, Gradient Norm: 0.0000\n",
      "\u001b[1m 69/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:18\u001b[0m 480ms/step - accuracy: 0.4774 - binary_io_u_5: 0.4774 - loss: 0.9230Batch 70, Loss Value: 0.8520\n",
      "Batch 70, Gradient Norm: 0.0001\n",
      "\u001b[1m 70/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:17\u001b[0m 480ms/step - accuracy: 0.4776 - binary_io_u_5: 0.4776 - loss: 0.9229Batch 71, Loss Value: 0.8520\n",
      "Batch 71, Gradient Norm: 0.0001\n",
      "\u001b[1m 71/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:17\u001b[0m 480ms/step - accuracy: 0.4777 - binary_io_u_5: 0.4777 - loss: 0.9228Batch 72, Loss Value: 0.8520\n",
      "Batch 72, Gradient Norm: 0.0000\n",
      "\u001b[1m 72/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:16\u001b[0m 480ms/step - accuracy: 0.4779 - binary_io_u_5: 0.4779 - loss: 0.9227Batch 73, Loss Value: 0.8520\n",
      "Batch 73, Gradient Norm: 0.0000\n",
      "\u001b[1m 73/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:16\u001b[0m 480ms/step - accuracy: 0.4780 - binary_io_u_5: 0.4780 - loss: 0.9226Batch 74, Loss Value: 0.8520\n",
      "Batch 74, Gradient Norm: 0.0000\n",
      "\u001b[1m 74/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:15\u001b[0m 481ms/step - accuracy: 0.4782 - binary_io_u_5: 0.4782 - loss: 0.9225Batch 75, Loss Value: 0.8520\n",
      "Batch 75, Gradient Norm: 0.0000\n",
      "\u001b[1m 75/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:15\u001b[0m 481ms/step - accuracy: 0.4784 - binary_io_u_5: 0.4784 - loss: 0.9224Batch 76, Loss Value: 0.8520\n",
      "Batch 76, Gradient Norm: 0.0002\n",
      "\u001b[1m 76/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:15\u001b[0m 481ms/step - accuracy: 0.4785 - binary_io_u_5: 0.4785 - loss: 0.9223Batch 77, Loss Value: 0.8520\n",
      "Batch 77, Gradient Norm: 0.0000\n",
      "\u001b[1m 77/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:14\u001b[0m 481ms/step - accuracy: 0.4787 - binary_io_u_5: 0.4787 - loss: 0.9222Batch 78, Loss Value: 0.8520\n",
      "Batch 78, Gradient Norm: 0.0001\n",
      "\u001b[1m 78/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:14\u001b[0m 481ms/step - accuracy: 0.4788 - binary_io_u_5: 0.4788 - loss: 0.9221Batch 79, Loss Value: 0.8520\n",
      "Batch 79, Gradient Norm: 0.0003\n",
      "\u001b[1m 79/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:13\u001b[0m 481ms/step - accuracy: 0.4790 - binary_io_u_5: 0.4790 - loss: 0.9220Batch 80, Loss Value: 0.8520\n",
      "Batch 80, Gradient Norm: 0.0000\n",
      "\u001b[1m 80/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:13\u001b[0m 481ms/step - accuracy: 0.4792 - binary_io_u_5: 0.4792 - loss: 0.9219Batch 81, Loss Value: 0.8520\n",
      "Batch 81, Gradient Norm: 0.0004\n",
      "\u001b[1m 81/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:12\u001b[0m 481ms/step - accuracy: 0.4793 - binary_io_u_5: 0.4793 - loss: 0.9218Batch 82, Loss Value: 0.8520\n",
      "Batch 82, Gradient Norm: 0.0000\n",
      "\u001b[1m 82/232\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m1:12\u001b[0m 481ms/step - accuracy: 0.4794 - binary_io_u_5: 0.4794 - loss: 0.9217Batch 83, Loss Value: 0.8520\n",
      "Batch 83, Gradient Norm: 0.0003\n",
      "\u001b[1m 83/232\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m1:11\u001b[0m 481ms/step - accuracy: 0.4796 - binary_io_u_5: 0.4796 - loss: 0.9216Batch 84, Loss Value: 0.8520\n",
      "Batch 84, Gradient Norm: 0.0004\n",
      "\u001b[1m 84/232\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m1:11\u001b[0m 482ms/step - accuracy: 0.4797 - binary_io_u_5: 0.4797 - loss: 0.9216Batch 85, Loss Value: 0.8520\n",
      "Batch 85, Gradient Norm: 0.0000\n",
      "\u001b[1m 85/232\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m1:10\u001b[0m 482ms/step - accuracy: 0.4798 - binary_io_u_5: 0.4798 - loss: 0.9215Batch 86, Loss Value: 0.8520\n",
      "Batch 86, Gradient Norm: 0.0000\n",
      "\u001b[1m 86/232\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m1:10\u001b[0m 482ms/step - accuracy: 0.4799 - binary_io_u_5: 0.4799 - loss: 0.9214Batch 87, Loss Value: 0.8520\n",
      "Batch 87, Gradient Norm: 0.0000\n",
      "\u001b[1m 87/232\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m1:09\u001b[0m 482ms/step - accuracy: 0.4801 - binary_io_u_5: 0.4801 - loss: 0.9214Batch 88, Loss Value: 0.8520\n",
      "Batch 88, Gradient Norm: 0.0000\n",
      "\u001b[1m 88/232\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m1:09\u001b[0m 482ms/step - accuracy: 0.4802 - binary_io_u_5: 0.4802 - loss: 0.9213Batch 89, Loss Value: 0.8520\n",
      "Batch 89, Gradient Norm: 0.0000\n",
      "\u001b[1m 89/232\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m1:08\u001b[0m 482ms/step - accuracy: 0.4803 - binary_io_u_5: 0.4803 - loss: 0.9212Batch 90, Loss Value: 0.8520\n",
      "Batch 90, Gradient Norm: 0.0003\n",
      "\u001b[1m 90/232\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m1:08\u001b[0m 482ms/step - accuracy: 0.4804 - binary_io_u_5: 0.4804 - loss: 0.9212Batch 91, Loss Value: 0.8520\n",
      "Batch 91, Gradient Norm: 0.0001\n",
      "\u001b[1m 91/232\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m1:07\u001b[0m 482ms/step - accuracy: 0.4805 - binary_io_u_5: 0.4805 - loss: 0.9211Batch 92, Loss Value: 0.8520\n",
      "Batch 92, Gradient Norm: 0.0000\n",
      "\u001b[1m 92/232\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m1:07\u001b[0m 482ms/step - accuracy: 0.4806 - binary_io_u_5: 0.4806 - loss: 0.9210Batch 93, Loss Value: 0.8520\n",
      "Batch 93, Gradient Norm: 0.0001\n",
      "\u001b[1m 93/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m1:06\u001b[0m 482ms/step - accuracy: 0.4807 - binary_io_u_5: 0.4807 - loss: 0.9210Batch 94, Loss Value: 0.8520\n",
      "Batch 94, Gradient Norm: 0.0000\n",
      "\u001b[1m 94/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m1:06\u001b[0m 482ms/step - accuracy: 0.4808 - binary_io_u_5: 0.4808 - loss: 0.9209Batch 95, Loss Value: 0.8520\n",
      "Batch 95, Gradient Norm: 0.0000\n",
      "\u001b[1m 95/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m1:06\u001b[0m 482ms/step - accuracy: 0.4809 - binary_io_u_5: 0.4809 - loss: 0.9209Batch 96, Loss Value: 0.8520\n",
      "Batch 96, Gradient Norm: 0.0000\n",
      "\u001b[1m 96/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m1:05\u001b[0m 482ms/step - accuracy: 0.4811 - binary_io_u_5: 0.4811 - loss: 0.9208Batch 97, Loss Value: 0.8520\n",
      "Batch 97, Gradient Norm: 0.0000\n",
      "\u001b[1m 97/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m1:05\u001b[0m 482ms/step - accuracy: 0.4812 - binary_io_u_5: 0.4812 - loss: 0.9207Batch 98, Loss Value: 0.8520\n",
      "Batch 98, Gradient Norm: 0.0000\n",
      "\u001b[1m 98/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m1:04\u001b[0m 482ms/step - accuracy: 0.4813 - binary_io_u_5: 0.4813 - loss: 0.9206Batch 99, Loss Value: 0.8520\n",
      "Batch 99, Gradient Norm: 0.0000\n",
      "\u001b[1m 99/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m1:04\u001b[0m 482ms/step - accuracy: 0.4814 - binary_io_u_5: 0.4814 - loss: 0.9206Batch 100, Loss Value: 0.8520\n",
      "Batch 100, Gradient Norm: 0.0000\n",
      "\u001b[1m100/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m1:03\u001b[0m 482ms/step - accuracy: 0.4816 - binary_io_u_5: 0.4816 - loss: 0.9205Batch 101, Loss Value: 0.8520\n",
      "Batch 101, Gradient Norm: 0.0000\n",
      "\u001b[1m101/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m1:03\u001b[0m 482ms/step - accuracy: 0.4817 - binary_io_u_5: 0.4817 - loss: 0.9204Batch 102, Loss Value: 0.8520\n",
      "Batch 102, Gradient Norm: 0.0018\n",
      "\u001b[1m102/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m1:02\u001b[0m 482ms/step - accuracy: 0.4818 - binary_io_u_5: 0.4818 - loss: 0.9204Batch 103, Loss Value: 0.8520\n",
      "Batch 103, Gradient Norm: 0.0000\n",
      "\u001b[1m103/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m1:02\u001b[0m 482ms/step - accuracy: 0.4819 - binary_io_u_5: 0.4819 - loss: 0.9203Batch 104, Loss Value: 0.8520\n",
      "Batch 104, Gradient Norm: 0.0000\n",
      "\u001b[1m104/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m1:01\u001b[0m 482ms/step - accuracy: 0.4820 - binary_io_u_5: 0.4820 - loss: 0.9203Batch 105, Loss Value: 0.8520\n",
      "Batch 105, Gradient Norm: 0.0001\n",
      "\u001b[1m105/232\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m1:01\u001b[0m 482ms/step - accuracy: 0.4821 - binary_io_u_5: 0.4821 - loss: 0.9202Batch 106, Loss Value: 0.8520\n",
      "Batch 106, Gradient Norm: 0.0000\n",
      "\u001b[1m106/232\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m1:00\u001b[0m 482ms/step - accuracy: 0.4822 - binary_io_u_5: 0.4822 - loss: 0.9201Batch 107, Loss Value: 0.8520\n",
      "Batch 107, Gradient Norm: 0.0000\n",
      "\u001b[1m107/232\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m1:00\u001b[0m 482ms/step - accuracy: 0.4823 - binary_io_u_5: 0.4823 - loss: 0.9201Batch 108, Loss Value: 0.8520\n",
      "Batch 108, Gradient Norm: 0.0000\n",
      "\u001b[1m108/232\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m59s\u001b[0m 482ms/step - accuracy: 0.4824 - binary_io_u_5: 0.4824 - loss: 0.9200 Batch 109, Loss Value: 0.8520\n",
      "Batch 109, Gradient Norm: 0.0000\n",
      "\u001b[1m109/232\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m59s\u001b[0m 482ms/step - accuracy: 0.4826 - binary_io_u_5: 0.4826 - loss: 0.9200Batch 110, Loss Value: 0.8520\n",
      "Batch 110, Gradient Norm: 0.0000\n",
      "\u001b[1m110/232\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m58s\u001b[0m 482ms/step - accuracy: 0.4827 - binary_io_u_5: 0.4827 - loss: 0.9199Batch 111, Loss Value: 0.8520\n",
      "Batch 111, Gradient Norm: 0.0000\n",
      "\u001b[1m111/232\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m58s\u001b[0m 482ms/step - accuracy: 0.4828 - binary_io_u_5: 0.4828 - loss: 0.9198Batch 112, Loss Value: 0.8520\n",
      "Batch 112, Gradient Norm: 0.0000\n",
      "\u001b[1m112/232\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m57s\u001b[0m 482ms/step - accuracy: 0.4829 - binary_io_u_5: 0.4829 - loss: 0.9198Batch 113, Loss Value: 0.8519\n",
      "Batch 113, Gradient Norm: 0.0028\n",
      "\u001b[1m113/232\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m57s\u001b[0m 482ms/step - accuracy: 0.4830 - binary_io_u_5: 0.4830 - loss: 0.9197Batch 114, Loss Value: 0.8520\n",
      "Batch 114, Gradient Norm: 0.0007\n",
      "\u001b[1m114/232\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m56s\u001b[0m 482ms/step - accuracy: 0.4831 - binary_io_u_5: 0.4831 - loss: 0.9197Batch 115, Loss Value: 0.8520\n",
      "Batch 115, Gradient Norm: 0.0000\n",
      "\u001b[1m115/232\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m56s\u001b[0m 483ms/step - accuracy: 0.4831 - binary_io_u_5: 0.4831 - loss: 0.9196Batch 116, Loss Value: 0.8520\n",
      "Batch 116, Gradient Norm: 0.0001\n",
      "\u001b[1m116/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m55s\u001b[0m 483ms/step - accuracy: 0.4832 - binary_io_u_5: 0.4832 - loss: 0.9196Batch 117, Loss Value: 0.8520\n",
      "Batch 117, Gradient Norm: 0.0000\n",
      "\u001b[1m117/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m55s\u001b[0m 483ms/step - accuracy: 0.4833 - binary_io_u_5: 0.4833 - loss: 0.9195Batch 118, Loss Value: 0.8520\n",
      "Batch 118, Gradient Norm: 0.0001\n",
      "\u001b[1m118/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m55s\u001b[0m 482ms/step - accuracy: 0.4834 - binary_io_u_5: 0.4834 - loss: 0.9195Batch 119, Loss Value: 0.8520\n",
      "Batch 119, Gradient Norm: 0.0000\n",
      "\u001b[1m119/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m54s\u001b[0m 482ms/step - accuracy: 0.4835 - binary_io_u_5: 0.4835 - loss: 0.9195Batch 120, Loss Value: 0.8520\n",
      "Batch 120, Gradient Norm: 0.0000\n",
      "\u001b[1m120/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m54s\u001b[0m 483ms/step - accuracy: 0.4836 - binary_io_u_5: 0.4836 - loss: 0.9194Batch 121, Loss Value: 0.8520\n",
      "Batch 121, Gradient Norm: 0.0000\n",
      "\u001b[1m121/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m53s\u001b[0m 483ms/step - accuracy: 0.4836 - binary_io_u_5: 0.4836 - loss: 0.9194Batch 122, Loss Value: 0.8520\n",
      "Batch 122, Gradient Norm: 0.0000\n",
      "\u001b[1m122/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m53s\u001b[0m 483ms/step - accuracy: 0.4837 - binary_io_u_5: 0.4837 - loss: 0.9193Batch 123, Loss Value: 0.8520\n",
      "Batch 123, Gradient Norm: 0.0002\n",
      "\u001b[1m123/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m52s\u001b[0m 483ms/step - accuracy: 0.4838 - binary_io_u_5: 0.4838 - loss: 0.9193Batch 124, Loss Value: 0.8520\n",
      "Batch 124, Gradient Norm: 0.0000\n",
      "\u001b[1m124/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m52s\u001b[0m 483ms/step - accuracy: 0.4839 - binary_io_u_5: 0.4839 - loss: 0.9193Batch 125, Loss Value: 0.8520\n",
      "Batch 125, Gradient Norm: 0.0000\n",
      "\u001b[1m125/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m51s\u001b[0m 483ms/step - accuracy: 0.4839 - binary_io_u_5: 0.4839 - loss: 0.9192Batch 126, Loss Value: 0.8520\n",
      "Batch 126, Gradient Norm: 0.0003\n",
      "\u001b[1m126/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m51s\u001b[0m 483ms/step - accuracy: 0.4840 - binary_io_u_5: 0.4840 - loss: 0.9192Batch 127, Loss Value: 0.8520\n",
      "Batch 127, Gradient Norm: 0.0000\n",
      "\u001b[1m127/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m50s\u001b[0m 483ms/step - accuracy: 0.4841 - binary_io_u_5: 0.4841 - loss: 0.9192Batch 128, Loss Value: 0.8520\n",
      "Batch 128, Gradient Norm: 0.0000\n",
      "\u001b[1m128/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m50s\u001b[0m 483ms/step - accuracy: 0.4842 - binary_io_u_5: 0.4842 - loss: 0.9191Batch 129, Loss Value: 0.8520\n",
      "Batch 129, Gradient Norm: 0.0000\n",
      "\u001b[1m129/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m49s\u001b[0m 483ms/step - accuracy: 0.4842 - binary_io_u_5: 0.4842 - loss: 0.9191Batch 130, Loss Value: 0.8520\n",
      "Batch 130, Gradient Norm: 0.0000\n",
      "\u001b[1m130/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m49s\u001b[0m 483ms/step - accuracy: 0.4843 - binary_io_u_5: 0.4843 - loss: 0.9191Batch 131, Loss Value: 0.8520\n",
      "Batch 131, Gradient Norm: 0.0000\n",
      "\u001b[1m131/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m48s\u001b[0m 483ms/step - accuracy: 0.4844 - binary_io_u_5: 0.4844 - loss: 0.9190Batch 132, Loss Value: 0.8520\n",
      "Batch 132, Gradient Norm: 0.0002\n",
      "\u001b[1m132/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m48s\u001b[0m 483ms/step - accuracy: 0.4844 - binary_io_u_5: 0.4844 - loss: 0.9190Batch 133, Loss Value: 0.8520\n",
      "Batch 133, Gradient Norm: 0.0000\n",
      "\u001b[1m133/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m47s\u001b[0m 483ms/step - accuracy: 0.4845 - binary_io_u_5: 0.4845 - loss: 0.9190Batch 134, Loss Value: 0.8520\n",
      "Batch 134, Gradient Norm: 0.0000\n",
      "\u001b[1m134/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m47s\u001b[0m 483ms/step - accuracy: 0.4846 - binary_io_u_5: 0.4846 - loss: 0.9189Batch 135, Loss Value: 0.8520\n",
      "Batch 135, Gradient Norm: 0.0001\n",
      "\u001b[1m135/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m46s\u001b[0m 483ms/step - accuracy: 0.4846 - binary_io_u_5: 0.4846 - loss: 0.9189Batch 136, Loss Value: 0.8520\n",
      "Batch 136, Gradient Norm: 0.0016\n",
      "\u001b[1m136/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m46s\u001b[0m 484ms/step - accuracy: 0.4847 - binary_io_u_5: 0.4847 - loss: 0.9189Batch 137, Loss Value: 0.8520\n",
      "Batch 137, Gradient Norm: 0.0000\n",
      "\u001b[1m137/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m45s\u001b[0m 484ms/step - accuracy: 0.4847 - binary_io_u_5: 0.4847 - loss: 0.9188Batch 138, Loss Value: 0.8520\n",
      "Batch 138, Gradient Norm: 0.0000\n",
      "\u001b[1m138/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m45s\u001b[0m 484ms/step - accuracy: 0.4848 - binary_io_u_5: 0.4848 - loss: 0.9188Batch 139, Loss Value: 0.8520\n",
      "Batch 139, Gradient Norm: 0.0000\n",
      "\u001b[1m139/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m44s\u001b[0m 484ms/step - accuracy: 0.4849 - binary_io_u_5: 0.4849 - loss: 0.9188Batch 140, Loss Value: 0.8520\n",
      "Batch 140, Gradient Norm: 0.0002\n",
      "\u001b[1m140/232\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m44s\u001b[0m 484ms/step - accuracy: 0.4849 - binary_io_u_5: 0.4849 - loss: 0.9188Batch 141, Loss Value: 0.8520\n",
      "Batch 141, Gradient Norm: 0.0000\n",
      "\u001b[1m141/232\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m44s\u001b[0m 484ms/step - accuracy: 0.4850 - binary_io_u_5: 0.4850 - loss: 0.9187Batch 142, Loss Value: 0.8520\n",
      "Batch 142, Gradient Norm: 0.0000\n",
      "\u001b[1m142/232\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m43s\u001b[0m 484ms/step - accuracy: 0.4850 - binary_io_u_5: 0.4850 - loss: 0.9187Batch 143, Loss Value: 0.8520\n",
      "Batch 143, Gradient Norm: 0.0000\n",
      "\u001b[1m143/232\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m43s\u001b[0m 484ms/step - accuracy: 0.4851 - binary_io_u_5: 0.4851 - loss: 0.9187Batch 144, Loss Value: 0.8518\n",
      "Batch 144, Gradient Norm: 0.0109\n",
      "\u001b[1m144/232\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m42s\u001b[0m 484ms/step - accuracy: 0.4851 - binary_io_u_5: 0.4851 - loss: 0.9187Batch 145, Loss Value: 0.8520\n",
      "Batch 145, Gradient Norm: 0.0000\n",
      "\u001b[1m145/232\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m42s\u001b[0m 484ms/step - accuracy: 0.4852 - binary_io_u_5: 0.4852 - loss: 0.9186Batch 146, Loss Value: 0.8520\n",
      "Batch 146, Gradient Norm: 0.0000\n",
      "\u001b[1m146/232\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m41s\u001b[0m 484ms/step - accuracy: 0.4852 - binary_io_u_5: 0.4852 - loss: 0.9186Batch 147, Loss Value: 0.8520\n",
      "Batch 147, Gradient Norm: 0.0011\n",
      "\u001b[1m147/232\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m41s\u001b[0m 484ms/step - accuracy: 0.4853 - binary_io_u_5: 0.4853 - loss: 0.9186Batch 148, Loss Value: 0.8520\n",
      "Batch 148, Gradient Norm: 0.0000\n",
      "\u001b[1m148/232\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m40s\u001b[0m 484ms/step - accuracy: 0.4853 - binary_io_u_5: 0.4853 - loss: 0.9186Batch 149, Loss Value: 0.8520\n",
      "Batch 149, Gradient Norm: 0.0000\n",
      "\u001b[1m149/232\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m40s\u001b[0m 484ms/step - accuracy: 0.4854 - binary_io_u_5: 0.4854 - loss: 0.9186Batch 150, Loss Value: 0.8520\n",
      "Batch 150, Gradient Norm: 0.0000\n",
      "\u001b[1m150/232\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m39s\u001b[0m 485ms/step - accuracy: 0.4854 - binary_io_u_5: 0.4854 - loss: 0.9185Batch 151, Loss Value: 0.8520\n",
      "Batch 151, Gradient Norm: 0.0001\n",
      "\u001b[1m151/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m39s\u001b[0m 485ms/step - accuracy: 0.4854 - binary_io_u_5: 0.4854 - loss: 0.9185Batch 152, Loss Value: 0.8520\n",
      "Batch 152, Gradient Norm: 0.0003\n",
      "\u001b[1m152/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m38s\u001b[0m 485ms/step - accuracy: 0.4855 - binary_io_u_5: 0.4855 - loss: 0.9185Batch 153, Loss Value: 0.8520\n",
      "Batch 153, Gradient Norm: 0.0013\n",
      "\u001b[1m153/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m38s\u001b[0m 485ms/step - accuracy: 0.4855 - binary_io_u_5: 0.4855 - loss: 0.9185Batch 154, Loss Value: 0.8520\n",
      "Batch 154, Gradient Norm: 0.0000\n",
      "\u001b[1m154/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m37s\u001b[0m 485ms/step - accuracy: 0.4856 - binary_io_u_5: 0.4856 - loss: 0.9185Batch 155, Loss Value: 0.8520\n",
      "Batch 155, Gradient Norm: 0.0000\n",
      "\u001b[1m155/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m37s\u001b[0m 485ms/step - accuracy: 0.4856 - binary_io_u_5: 0.4856 - loss: 0.9184Batch 156, Loss Value: 0.8520\n",
      "Batch 156, Gradient Norm: 0.0000\n",
      "\u001b[1m156/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m36s\u001b[0m 485ms/step - accuracy: 0.4857 - binary_io_u_5: 0.4857 - loss: 0.9184Batch 157, Loss Value: 0.8520\n",
      "Batch 157, Gradient Norm: 0.0000\n",
      "\u001b[1m157/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m36s\u001b[0m 485ms/step - accuracy: 0.4857 - binary_io_u_5: 0.4857 - loss: 0.9184Batch 158, Loss Value: 0.8520\n",
      "Batch 158, Gradient Norm: 0.0000\n",
      "\u001b[1m158/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m35s\u001b[0m 485ms/step - accuracy: 0.4857 - binary_io_u_5: 0.4857 - loss: 0.9184Batch 159, Loss Value: 0.8520\n",
      "Batch 159, Gradient Norm: 0.0000\n",
      "\u001b[1m159/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m35s\u001b[0m 485ms/step - accuracy: 0.4858 - binary_io_u_5: 0.4858 - loss: 0.9184Batch 160, Loss Value: 0.8520\n",
      "Batch 160, Gradient Norm: 0.0000\n",
      "\u001b[1m160/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m34s\u001b[0m 485ms/step - accuracy: 0.4858 - binary_io_u_5: 0.4858 - loss: 0.9184Batch 161, Loss Value: 0.8520\n",
      "Batch 161, Gradient Norm: 0.0000\n",
      "\u001b[1m161/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m34s\u001b[0m 485ms/step - accuracy: 0.4858 - binary_io_u_5: 0.4858 - loss: 0.9183Batch 162, Loss Value: 0.8520\n",
      "Batch 162, Gradient Norm: 0.0000\n",
      "\u001b[1m162/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m33s\u001b[0m 485ms/step - accuracy: 0.4859 - binary_io_u_5: 0.4859 - loss: 0.9183Batch 163, Loss Value: 0.8520\n",
      "Batch 163, Gradient Norm: 0.0000\n",
      "\u001b[1m163/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m33s\u001b[0m 485ms/step - accuracy: 0.4859 - binary_io_u_5: 0.4859 - loss: 0.9183Batch 164, Loss Value: 0.8520\n",
      "Batch 164, Gradient Norm: 0.0015\n",
      "\u001b[1m164/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m33s\u001b[0m 486ms/step - accuracy: 0.4860 - binary_io_u_5: 0.4860 - loss: 0.9183Batch 165, Loss Value: 0.8521\n",
      "Batch 165, Gradient Norm: 0.0066\n",
      "\u001b[1m165/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m32s\u001b[0m 486ms/step - accuracy: 0.4860 - binary_io_u_5: 0.4860 - loss: 0.9183Batch 166, Loss Value: 0.8520\n",
      "Batch 166, Gradient Norm: 0.0014\n",
      "\u001b[1m166/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m32s\u001b[0m 486ms/step - accuracy: 0.4860 - binary_io_u_5: 0.4860 - loss: 0.9183Batch 167, Loss Value: 0.8520\n",
      "Batch 167, Gradient Norm: 0.0005\n",
      "\u001b[1m167/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m31s\u001b[0m 486ms/step - accuracy: 0.4861 - binary_io_u_5: 0.4861 - loss: 0.9182Batch 168, Loss Value: 0.8520\n",
      "Batch 168, Gradient Norm: 0.0052\n",
      "\u001b[1m168/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m31s\u001b[0m 486ms/step - accuracy: 0.4861 - binary_io_u_5: 0.4861 - loss: 0.9182Batch 169, Loss Value: 0.8520\n",
      "Batch 169, Gradient Norm: 0.0000\n",
      "\u001b[1m169/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m30s\u001b[0m 486ms/step - accuracy: 0.4861 - binary_io_u_5: 0.4861 - loss: 0.9182Batch 170, Loss Value: 0.8520\n",
      "Batch 170, Gradient Norm: 0.0000\n",
      "\u001b[1m170/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m30s\u001b[0m 486ms/step - accuracy: 0.4862 - binary_io_u_5: 0.4862 - loss: 0.9182Batch 171, Loss Value: 0.8520\n",
      "Batch 171, Gradient Norm: 0.0000\n",
      "\u001b[1m171/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m29s\u001b[0m 486ms/step - accuracy: 0.4862 - binary_io_u_5: 0.4862 - loss: 0.9182Batch 172, Loss Value: 0.8520\n",
      "Batch 172, Gradient Norm: 0.0000\n",
      "\u001b[1m172/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m29s\u001b[0m 486ms/step - accuracy: 0.4862 - binary_io_u_5: 0.4862 - loss: 0.9182Batch 173, Loss Value: 0.8520\n",
      "Batch 173, Gradient Norm: 0.0000\n",
      "\u001b[1m173/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m28s\u001b[0m 486ms/step - accuracy: 0.4862 - binary_io_u_5: 0.4862 - loss: 0.9182Batch 174, Loss Value: 0.8520\n",
      "Batch 174, Gradient Norm: 0.0001\n",
      "\u001b[1m174/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m28s\u001b[0m 486ms/step - accuracy: 0.4863 - binary_io_u_5: 0.4863 - loss: 0.9181Batch 175, Loss Value: 0.8520\n",
      "Batch 175, Gradient Norm: 0.0000\n",
      "\u001b[1m175/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m27s\u001b[0m 486ms/step - accuracy: 0.4863 - binary_io_u_5: 0.4863 - loss: 0.9181Batch 176, Loss Value: 0.8520\n",
      "Batch 176, Gradient Norm: 0.0000\n",
      "\u001b[1m176/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m27s\u001b[0m 486ms/step - accuracy: 0.4863 - binary_io_u_5: 0.4863 - loss: 0.9181Batch 177, Loss Value: 0.8520\n",
      "Batch 177, Gradient Norm: 0.0001\n",
      "\u001b[1m177/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m26s\u001b[0m 486ms/step - accuracy: 0.4864 - binary_io_u_5: 0.4864 - loss: 0.9181Batch 178, Loss Value: 0.8520\n",
      "Batch 178, Gradient Norm: 0.0001\n",
      "\u001b[1m178/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m26s\u001b[0m 486ms/step - accuracy: 0.4864 - binary_io_u_5: 0.4864 - loss: 0.9181Batch 179, Loss Value: 0.8520\n",
      "Batch 179, Gradient Norm: 0.0000\n",
      "\u001b[1m179/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m25s\u001b[0m 486ms/step - accuracy: 0.4864 - binary_io_u_5: 0.4864 - loss: 0.9181Batch 180, Loss Value: 0.8520\n",
      "Batch 180, Gradient Norm: 0.0000\n",
      "\u001b[1m180/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m25s\u001b[0m 487ms/step - accuracy: 0.4865 - binary_io_u_5: 0.4865 - loss: 0.9181Batch 181, Loss Value: 0.8520\n",
      "Batch 181, Gradient Norm: 0.0037\n",
      "\u001b[1m181/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m24s\u001b[0m 487ms/step - accuracy: 0.4865 - binary_io_u_5: 0.4865 - loss: 0.9180Batch 182, Loss Value: 0.8520\n",
      "Batch 182, Gradient Norm: 0.0000\n",
      "\u001b[1m182/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m24s\u001b[0m 487ms/step - accuracy: 0.4865 - binary_io_u_5: 0.4865 - loss: 0.9180Batch 183, Loss Value: 0.8520\n",
      "Batch 183, Gradient Norm: 0.0000\n",
      "\u001b[1m183/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m23s\u001b[0m 487ms/step - accuracy: 0.4866 - binary_io_u_5: 0.4866 - loss: 0.9180Batch 184, Loss Value: 0.8520\n",
      "Batch 184, Gradient Norm: 0.0001\n",
      "\u001b[1m184/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m23s\u001b[0m 487ms/step - accuracy: 0.4866 - binary_io_u_5: 0.4866 - loss: 0.9180Batch 185, Loss Value: 0.8520\n",
      "Batch 185, Gradient Norm: 0.0000\n",
      "\u001b[1m185/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m22s\u001b[0m 487ms/step - accuracy: 0.4866 - binary_io_u_5: 0.4866 - loss: 0.9180Batch 186, Loss Value: 0.8520\n",
      "Batch 186, Gradient Norm: 0.0000\n",
      "\u001b[1m186/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m22s\u001b[0m 487ms/step - accuracy: 0.4867 - binary_io_u_5: 0.4867 - loss: 0.9180Batch 187, Loss Value: 0.8520\n",
      "Batch 187, Gradient Norm: 0.0000\n",
      "\u001b[1m187/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m21s\u001b[0m 487ms/step - accuracy: 0.4867 - binary_io_u_5: 0.4867 - loss: 0.9179Batch 188, Loss Value: 0.8520\n",
      "Batch 188, Gradient Norm: 0.0000\n",
      "\u001b[1m188/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m21s\u001b[0m 487ms/step - accuracy: 0.4867 - binary_io_u_5: 0.4867 - loss: 0.9179Batch 189, Loss Value: 0.8520\n",
      "Batch 189, Gradient Norm: 0.0001\n",
      "\u001b[1m189/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m20s\u001b[0m 487ms/step - accuracy: 0.4868 - binary_io_u_5: 0.4868 - loss: 0.9179Batch 190, Loss Value: 0.8520\n",
      "Batch 190, Gradient Norm: 0.0000\n",
      "\u001b[1m190/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m20s\u001b[0m 487ms/step - accuracy: 0.4868 - binary_io_u_5: 0.4868 - loss: 0.9179Batch 191, Loss Value: 0.8520\n",
      "Batch 191, Gradient Norm: 0.0000\n",
      "\u001b[1m191/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m19s\u001b[0m 487ms/step - accuracy: 0.4869 - binary_io_u_5: 0.4869 - loss: 0.9179Batch 192, Loss Value: 0.8520\n",
      "Batch 192, Gradient Norm: 0.0005\n",
      "\u001b[1m192/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m19s\u001b[0m 487ms/step - accuracy: 0.4869 - binary_io_u_5: 0.4869 - loss: 0.9179Batch 193, Loss Value: 0.8520\n",
      "Batch 193, Gradient Norm: 0.0000\n",
      "\u001b[1m193/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m19s\u001b[0m 487ms/step - accuracy: 0.4869 - binary_io_u_5: 0.4869 - loss: 0.9178Batch 194, Loss Value: 0.8520\n",
      "Batch 194, Gradient Norm: 0.0000\n",
      "\u001b[1m194/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m18s\u001b[0m 487ms/step - accuracy: 0.4870 - binary_io_u_5: 0.4870 - loss: 0.9178Batch 195, Loss Value: 0.8520\n",
      "Batch 195, Gradient Norm: 0.0000\n",
      "\u001b[1m195/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m18s\u001b[0m 487ms/step - accuracy: 0.4870 - binary_io_u_5: 0.4870 - loss: 0.9178Batch 196, Loss Value: 0.8519\n",
      "Batch 196, Gradient Norm: 0.0030\n",
      "\u001b[1m196/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m17s\u001b[0m 487ms/step - accuracy: 0.4870 - binary_io_u_5: 0.4870 - loss: 0.9178Batch 197, Loss Value: 0.8520\n",
      "Batch 197, Gradient Norm: 0.0001\n",
      "\u001b[1m197/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m17s\u001b[0m 487ms/step - accuracy: 0.4871 - binary_io_u_5: 0.4871 - loss: 0.9178Batch 198, Loss Value: 0.8520\n",
      "Batch 198, Gradient Norm: 0.0000\n",
      "\u001b[1m198/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m16s\u001b[0m 487ms/step - accuracy: 0.4871 - binary_io_u_5: 0.4871 - loss: 0.9177Batch 199, Loss Value: 0.8520\n",
      "Batch 199, Gradient Norm: 0.0000\n",
      "\u001b[1m199/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m16s\u001b[0m 487ms/step - accuracy: 0.4872 - binary_io_u_5: 0.4872 - loss: 0.9177Batch 200, Loss Value: 0.8520\n",
      "Batch 200, Gradient Norm: 0.0000\n",
      "\u001b[1m200/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m15s\u001b[0m 487ms/step - accuracy: 0.4872 - binary_io_u_5: 0.4872 - loss: 0.9177Batch 201, Loss Value: 0.8520\n",
      "Batch 201, Gradient Norm: 0.0002\n",
      "\u001b[1m201/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m15s\u001b[0m 487ms/step - accuracy: 0.4872 - binary_io_u_5: 0.4872 - loss: 0.9177Batch 202, Loss Value: 0.8520\n",
      "Batch 202, Gradient Norm: 0.0000\n",
      "\u001b[1m202/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m14s\u001b[0m 487ms/step - accuracy: 0.4873 - binary_io_u_5: 0.4873 - loss: 0.9177Batch 203, Loss Value: 0.8520\n",
      "Batch 203, Gradient Norm: 0.0003\n",
      "\u001b[1m203/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m14s\u001b[0m 487ms/step - accuracy: 0.4873 - binary_io_u_5: 0.4873 - loss: 0.9176Batch 204, Loss Value: 0.8520\n",
      "Batch 204, Gradient Norm: 0.0000\n",
      "\u001b[1m204/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m13s\u001b[0m 487ms/step - accuracy: 0.4874 - binary_io_u_5: 0.4874 - loss: 0.9176Batch 205, Loss Value: 0.8520\n",
      "Batch 205, Gradient Norm: 0.0000\n",
      "\u001b[1m205/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m13s\u001b[0m 487ms/step - accuracy: 0.4874 - binary_io_u_5: 0.4874 - loss: 0.9176Batch 206, Loss Value: 0.8520\n",
      "Batch 206, Gradient Norm: 0.0000\n",
      "\u001b[1m206/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m12s\u001b[0m 487ms/step - accuracy: 0.4874 - binary_io_u_5: 0.4874 - loss: 0.9176Batch 207, Loss Value: 0.8518\n",
      "Batch 207, Gradient Norm: 0.0129\n",
      "\u001b[1m207/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m12s\u001b[0m 487ms/step - accuracy: 0.4875 - binary_io_u_5: 0.4875 - loss: 0.9176Batch 208, Loss Value: 0.8520\n",
      "Batch 208, Gradient Norm: 0.0000\n",
      "\u001b[1m208/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m11s\u001b[0m 487ms/step - accuracy: 0.4875 - binary_io_u_5: 0.4875 - loss: 0.9176Batch 209, Loss Value: 0.8520\n",
      "Batch 209, Gradient Norm: 0.0001\n",
      "\u001b[1m209/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m11s\u001b[0m 487ms/step - accuracy: 0.4875 - binary_io_u_5: 0.4875 - loss: 0.9175Batch 210, Loss Value: 0.8520\n",
      "Batch 210, Gradient Norm: 0.0000\n",
      "\u001b[1m210/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m10s\u001b[0m 487ms/step - accuracy: 0.4876 - binary_io_u_5: 0.4876 - loss: 0.9175Batch 211, Loss Value: 0.8520\n",
      "Batch 211, Gradient Norm: 0.0000\n",
      "\u001b[1m211/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m10s\u001b[0m 488ms/step - accuracy: 0.4876 - binary_io_u_5: 0.4876 - loss: 0.9175Batch 212, Loss Value: 0.8520\n",
      "Batch 212, Gradient Norm: 0.0001\n",
      "\u001b[1m212/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m9s\u001b[0m 488ms/step - accuracy: 0.4876 - binary_io_u_5: 0.4876 - loss: 0.9175 Batch 213, Loss Value: 0.8520\n",
      "Batch 213, Gradient Norm: 0.0000\n",
      "\u001b[1m213/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m9s\u001b[0m 488ms/step - accuracy: 0.4877 - binary_io_u_5: 0.4877 - loss: 0.9175Batch 214, Loss Value: 0.8520\n",
      "Batch 214, Gradient Norm: 0.0000\n",
      "\u001b[1m214/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m8s\u001b[0m 488ms/step - accuracy: 0.4877 - binary_io_u_5: 0.4877 - loss: 0.9175Batch 215, Loss Value: 0.8520\n",
      "Batch 215, Gradient Norm: 0.0000\n",
      "\u001b[1m215/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m8s\u001b[0m 488ms/step - accuracy: 0.4877 - binary_io_u_5: 0.4877 - loss: 0.9175Batch 216, Loss Value: 0.8520\n",
      "Batch 216, Gradient Norm: 0.0000\n",
      "\u001b[1m216/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m7s\u001b[0m 488ms/step - accuracy: 0.4877 - binary_io_u_5: 0.4877 - loss: 0.9174Batch 217, Loss Value: 0.8520\n",
      "Batch 217, Gradient Norm: 0.0000\n",
      "\u001b[1m217/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m7s\u001b[0m 488ms/step - accuracy: 0.4878 - binary_io_u_5: 0.4878 - loss: 0.9174Batch 218, Loss Value: 0.8520\n",
      "Batch 218, Gradient Norm: 0.0000\n",
      "\u001b[1m218/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m6s\u001b[0m 488ms/step - accuracy: 0.4878 - binary_io_u_5: 0.4878 - loss: 0.9174Batch 219, Loss Value: 0.8520\n",
      "Batch 219, Gradient Norm: 0.0002\n",
      "\u001b[1m219/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m6s\u001b[0m 488ms/step - accuracy: 0.4878 - binary_io_u_5: 0.4878 - loss: 0.9174Batch 220, Loss Value: 0.8520\n",
      "Batch 220, Gradient Norm: 0.0001\n",
      "\u001b[1m220/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m5s\u001b[0m 488ms/step - accuracy: 0.4879 - binary_io_u_5: 0.4879 - loss: 0.9174Batch 221, Loss Value: 0.8520\n",
      "Batch 221, Gradient Norm: 0.0000\n",
      "\u001b[1m221/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m5s\u001b[0m 488ms/step - accuracy: 0.4879 - binary_io_u_5: 0.4879 - loss: 0.9174Batch 222, Loss Value: 0.8520\n",
      "Batch 222, Gradient Norm: 0.0003\n",
      "\u001b[1m222/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m4s\u001b[0m 488ms/step - accuracy: 0.4879 - binary_io_u_5: 0.4879 - loss: 0.9173Batch 223, Loss Value: 0.8520\n",
      "Batch 223, Gradient Norm: 0.0001\n",
      "\u001b[1m223/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m4s\u001b[0m 488ms/step - accuracy: 0.4880 - binary_io_u_5: 0.4880 - loss: 0.9173Batch 224, Loss Value: 0.8520\n",
      "Batch 224, Gradient Norm: 0.0005\n",
      "\u001b[1m224/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m3s\u001b[0m 488ms/step - accuracy: 0.4880 - binary_io_u_5: 0.4880 - loss: 0.9173Batch 225, Loss Value: 0.8520\n",
      "Batch 225, Gradient Norm: 0.0000\n",
      "\u001b[1m225/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m3s\u001b[0m 488ms/step - accuracy: 0.4880 - binary_io_u_5: 0.4880 - loss: 0.9173Batch 226, Loss Value: 0.8520\n",
      "Batch 226, Gradient Norm: 0.0000\n",
      "\u001b[1m226/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m2s\u001b[0m 488ms/step - accuracy: 0.4881 - binary_io_u_5: 0.4881 - loss: 0.9173Batch 227, Loss Value: 0.8520\n",
      "Batch 227, Gradient Norm: 0.0000\n",
      "\u001b[1m227/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m2s\u001b[0m 488ms/step - accuracy: 0.4881 - binary_io_u_5: 0.4881 - loss: 0.9173Batch 228, Loss Value: 0.8520\n",
      "Batch 228, Gradient Norm: 0.0000\n",
      "\u001b[1m228/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m1s\u001b[0m 488ms/step - accuracy: 0.4881 - binary_io_u_5: 0.4881 - loss: 0.9172Batch 229, Loss Value: 0.8520\n",
      "Batch 229, Gradient Norm: 0.0019\n",
      "\u001b[1m229/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m1s\u001b[0m 488ms/step - accuracy: 0.4882 - binary_io_u_5: 0.4882 - loss: 0.9172Batch 230, Loss Value: 0.8520\n",
      "Batch 230, Gradient Norm: 0.0000\n",
      "\u001b[1m230/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 487ms/step - accuracy: 0.4882 - binary_io_u_5: 0.4882 - loss: 0.9172Batch 231, Loss Value: 0.8520\n",
      "Batch 231, Gradient Norm: 0.0000\n",
      "\u001b[1m231/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 487ms/step - accuracy: 0.4882 - binary_io_u_5: 0.4882 - loss: 0.9172Batch 232, Loss Value: 0.8520\n",
      "Batch 232, Gradient Norm: 0.0000\n",
      "\u001b[1m232/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m115s\u001b[0m 494ms/step - accuracy: 0.4883 - binary_io_u_5: 0.4883 - loss: 0.9172 - val_accuracy: 0.4949 - val_binary_io_u_5: 0.4949 - val_loss: 0.9125 - learning_rate: 5.0000e-04\n",
      "Epoch 21/200\n",
      "Batch 1, Loss Value: 0.9451\n",
      "Batch 1, Gradient Norm: 0.0009\n",
      "\u001b[1m  1/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:59\u001b[0m 515ms/step - accuracy: 0.5400 - binary_io_u_5: 0.5400 - loss: 0.8861Batch 2, Loss Value: 0.9451\n",
      "Batch 2, Gradient Norm: 0.0000\n",
      "\u001b[1m  2/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:49\u001b[0m 474ms/step - accuracy: 0.5250 - binary_io_u_5: 0.5250 - loss: 0.8964Batch 3, Loss Value: 0.9450\n",
      "Batch 3, Gradient Norm: 0.0041\n",
      "\u001b[1m  3/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:49\u001b[0m 480ms/step - accuracy: 0.5211 - binary_io_u_5: 0.5211 - loss: 0.8990Batch 4, Loss Value: 0.9451\n",
      "Batch 4, Gradient Norm: 0.0002\n",
      "\u001b[1m  4/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:50\u001b[0m 484ms/step - accuracy: 0.5171 - binary_io_u_5: 0.5171 - loss: 0.9018Batch 5, Loss Value: 0.9451\n",
      "Batch 5, Gradient Norm: 0.0000\n",
      "\u001b[1m  5/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:50\u001b[0m 488ms/step - accuracy: 0.5145 - binary_io_u_5: 0.5145 - loss: 0.9036Batch 6, Loss Value: 0.9451\n",
      "Batch 6, Gradient Norm: 0.0000\n",
      "\u001b[1m  6/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:51\u001b[0m 493ms/step - accuracy: 0.5129 - binary_io_u_5: 0.5129 - loss: 0.9047Batch 7, Loss Value: 0.9451\n",
      "Batch 7, Gradient Norm: 0.0017\n",
      "\u001b[1m  7/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:50\u001b[0m 493ms/step - accuracy: 0.5125 - binary_io_u_5: 0.5125 - loss: 0.9050Batch 8, Loss Value: 0.9451\n",
      "Batch 8, Gradient Norm: 0.0003\n",
      "\u001b[1m  8/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:50\u001b[0m 493ms/step - accuracy: 0.5128 - binary_io_u_5: 0.5128 - loss: 0.9048Batch 9, Loss Value: 0.9455\n",
      "Batch 9, Gradient Norm: 0.0042\n",
      "\u001b[1m  9/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:49\u001b[0m 492ms/step - accuracy: 0.5121 - binary_io_u_5: 0.5121 - loss: 0.9052Batch 10, Loss Value: 0.9451\n",
      "Batch 10, Gradient Norm: 0.0001\n",
      "\u001b[1m 10/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:49\u001b[0m 491ms/step - accuracy: 0.5116 - binary_io_u_5: 0.5116 - loss: 0.9054Batch 11, Loss Value: 0.9451\n",
      "Batch 11, Gradient Norm: 0.0000\n",
      "\u001b[1m 11/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:48\u001b[0m 490ms/step - accuracy: 0.5104 - binary_io_u_5: 0.5104 - loss: 0.9060Batch 12, Loss Value: 0.9451\n",
      "Batch 12, Gradient Norm: 0.0009\n",
      "\u001b[1m 12/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:47\u001b[0m 490ms/step - accuracy: 0.5097 - binary_io_u_5: 0.5097 - loss: 0.9063Batch 13, Loss Value: 0.9457\n",
      "Batch 13, Gradient Norm: 0.0009\n",
      "\u001b[1m 13/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:47\u001b[0m 493ms/step - accuracy: 0.5089 - binary_io_u_5: 0.5089 - loss: 0.9066Batch 14, Loss Value: 0.9451\n",
      "Batch 14, Gradient Norm: 0.0005\n",
      "\u001b[1m 14/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:47\u001b[0m 493ms/step - accuracy: 0.5084 - binary_io_u_5: 0.5084 - loss: 0.9068Batch 15, Loss Value: 0.9451\n",
      "Batch 15, Gradient Norm: 0.0000\n",
      "\u001b[1m 15/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:47\u001b[0m 495ms/step - accuracy: 0.5079 - binary_io_u_5: 0.5079 - loss: 0.9071Batch 16, Loss Value: 0.9451\n",
      "Batch 16, Gradient Norm: 0.0017\n",
      "\u001b[1m 16/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:47\u001b[0m 496ms/step - accuracy: 0.5072 - binary_io_u_5: 0.5072 - loss: 0.9075Batch 17, Loss Value: 0.9451\n",
      "Batch 17, Gradient Norm: 0.0000\n",
      "\u001b[1m 17/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:46\u001b[0m 497ms/step - accuracy: 0.5067 - binary_io_u_5: 0.5067 - loss: 0.9077Batch 18, Loss Value: 0.9451\n",
      "Batch 18, Gradient Norm: 0.0009\n",
      "\u001b[1m 18/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:46\u001b[0m 497ms/step - accuracy: 0.5064 - binary_io_u_5: 0.5064 - loss: 0.9079Batch 19, Loss Value: 0.9451\n",
      "Batch 19, Gradient Norm: 0.0000\n",
      "\u001b[1m 19/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:45\u001b[0m 497ms/step - accuracy: 0.5060 - binary_io_u_5: 0.5060 - loss: 0.9081Batch 20, Loss Value: 0.9453\n",
      "Batch 20, Gradient Norm: 0.0021\n",
      "\u001b[1m 20/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:45\u001b[0m 496ms/step - accuracy: 0.5057 - binary_io_u_5: 0.5057 - loss: 0.9083Batch 21, Loss Value: 0.9451\n",
      "Batch 21, Gradient Norm: 0.0000\n",
      "\u001b[1m 21/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:44\u001b[0m 495ms/step - accuracy: 0.5056 - binary_io_u_5: 0.5056 - loss: 0.9084Batch 22, Loss Value: 0.9451\n",
      "Batch 22, Gradient Norm: 0.0016\n",
      "\u001b[1m 22/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:43\u001b[0m 494ms/step - accuracy: 0.5054 - binary_io_u_5: 0.5054 - loss: 0.9084Batch 23, Loss Value: 0.9451\n",
      "Batch 23, Gradient Norm: 0.0000\n",
      "\u001b[1m 23/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:43\u001b[0m 493ms/step - accuracy: 0.5052 - binary_io_u_5: 0.5052 - loss: 0.9086Batch 24, Loss Value: 0.9451\n",
      "Batch 24, Gradient Norm: 0.0000\n",
      "\u001b[1m 24/232\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:42\u001b[0m 492ms/step - accuracy: 0.5051 - binary_io_u_5: 0.5051 - loss: 0.9087Batch 25, Loss Value: 0.9451\n",
      "Batch 25, Gradient Norm: 0.0001\n",
      "\u001b[1m 25/232\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:41\u001b[0m 491ms/step - accuracy: 0.5050 - binary_io_u_5: 0.5050 - loss: 0.9087Batch 26, Loss Value: 0.9451\n",
      "Batch 26, Gradient Norm: 0.0000\n",
      "\u001b[1m 26/232\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:41\u001b[0m 490ms/step - accuracy: 0.5049 - binary_io_u_5: 0.5049 - loss: 0.9087Batch 27, Loss Value: 0.9451\n",
      "Batch 27, Gradient Norm: 0.0002\n",
      "\u001b[1m 27/232\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:40\u001b[0m 490ms/step - accuracy: 0.5049 - binary_io_u_5: 0.5049 - loss: 0.9088Batch 28, Loss Value: 0.9450\n",
      "Batch 28, Gradient Norm: 0.0018\n",
      "\u001b[1m 28/232\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:39\u001b[0m 489ms/step - accuracy: 0.5048 - binary_io_u_5: 0.5048 - loss: 0.9088Batch 29, Loss Value: 0.9451\n",
      "Batch 29, Gradient Norm: 0.0000\n",
      "\u001b[1m 29/232\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:39\u001b[0m 489ms/step - accuracy: 0.5048 - binary_io_u_5: 0.5048 - loss: 0.9088Batch 30, Loss Value: 0.9451\n",
      "Batch 30, Gradient Norm: 0.0005\n",
      "\u001b[1m 30/232\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:38\u001b[0m 488ms/step - accuracy: 0.5047 - binary_io_u_5: 0.5047 - loss: 0.9089Batch 31, Loss Value: 0.9451\n",
      "Batch 31, Gradient Norm: 0.0000\n",
      "\u001b[1m 31/232\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:38\u001b[0m 488ms/step - accuracy: 0.5046 - binary_io_u_5: 0.5046 - loss: 0.9089Batch 32, Loss Value: 0.9455\n",
      "Batch 32, Gradient Norm: 0.0025\n",
      "\u001b[1m 32/232\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:37\u001b[0m 488ms/step - accuracy: 0.5046 - binary_io_u_5: 0.5046 - loss: 0.9090Batch 33, Loss Value: 0.9455\n",
      "Batch 33, Gradient Norm: 0.0175\n",
      "\u001b[1m 33/232\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:36\u001b[0m 487ms/step - accuracy: 0.5045 - binary_io_u_5: 0.5045 - loss: 0.9090Batch 34, Loss Value: 0.9451\n",
      "Batch 34, Gradient Norm: 0.0000\n",
      "\u001b[1m 34/232\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:36\u001b[0m 487ms/step - accuracy: 0.5044 - binary_io_u_5: 0.5044 - loss: 0.9091Batch 35, Loss Value: 0.9455\n",
      "Batch 35, Gradient Norm: 0.0100\n",
      "\u001b[1m 35/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:35\u001b[0m 487ms/step - accuracy: 0.5044 - binary_io_u_5: 0.5044 - loss: 0.9091Batch 36, Loss Value: 0.9451\n",
      "Batch 36, Gradient Norm: 0.0022\n",
      "\u001b[1m 36/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:35\u001b[0m 486ms/step - accuracy: 0.5043 - binary_io_u_5: 0.5043 - loss: 0.9092Batch 37, Loss Value: 0.9451\n",
      "Batch 37, Gradient Norm: 0.0008\n",
      "\u001b[1m 37/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:34\u001b[0m 486ms/step - accuracy: 0.5042 - binary_io_u_5: 0.5042 - loss: 0.9092Batch 38, Loss Value: 0.9451\n",
      "Batch 38, Gradient Norm: 0.0003\n",
      "\u001b[1m 38/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:34\u001b[0m 486ms/step - accuracy: 0.5041 - binary_io_u_5: 0.5041 - loss: 0.9093Batch 39, Loss Value: 0.9451\n",
      "Batch 39, Gradient Norm: 0.0000\n",
      "\u001b[1m 39/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:33\u001b[0m 485ms/step - accuracy: 0.5040 - binary_io_u_5: 0.5040 - loss: 0.9093Batch 40, Loss Value: 0.9450\n",
      "Batch 40, Gradient Norm: 0.0015\n",
      "\u001b[1m 40/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:33\u001b[0m 485ms/step - accuracy: 0.5039 - binary_io_u_5: 0.5039 - loss: 0.9094Batch 41, Loss Value: 0.9451\n",
      "Batch 41, Gradient Norm: 0.0000\n",
      "\u001b[1m 41/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:32\u001b[0m 485ms/step - accuracy: 0.5038 - binary_io_u_5: 0.5038 - loss: 0.9094Batch 42, Loss Value: 0.9454\n",
      "Batch 42, Gradient Norm: 0.0012\n",
      "\u001b[1m 42/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:32\u001b[0m 485ms/step - accuracy: 0.5037 - binary_io_u_5: 0.5037 - loss: 0.9095Batch 43, Loss Value: 0.9451\n",
      "Batch 43, Gradient Norm: 0.0000\n",
      "\u001b[1m 43/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:31\u001b[0m 484ms/step - accuracy: 0.5036 - binary_io_u_5: 0.5036 - loss: 0.9096Batch 44, Loss Value: 0.9451\n",
      "Batch 44, Gradient Norm: 0.0000\n",
      "\u001b[1m 44/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:31\u001b[0m 484ms/step - accuracy: 0.5034 - binary_io_u_5: 0.5034 - loss: 0.9097Batch 45, Loss Value: 0.9451\n",
      "Batch 45, Gradient Norm: 0.0005\n",
      "\u001b[1m 45/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:30\u001b[0m 484ms/step - accuracy: 0.5033 - binary_io_u_5: 0.5033 - loss: 0.9097Batch 46, Loss Value: 0.9451\n",
      "Batch 46, Gradient Norm: 0.0000\n",
      "\u001b[1m 46/232\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:29\u001b[0m 484ms/step - accuracy: 0.5032 - binary_io_u_5: 0.5032 - loss: 0.9098Batch 47, Loss Value: 0.9451\n",
      "Batch 47, Gradient Norm: 0.0000\n",
      "\u001b[1m 47/232\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:29\u001b[0m 484ms/step - accuracy: 0.5030 - binary_io_u_5: 0.5030 - loss: 0.9099Batch 48, Loss Value: 0.9451\n",
      "Batch 48, Gradient Norm: 0.0000\n",
      "\u001b[1m 48/232\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:29\u001b[0m 484ms/step - accuracy: 0.5028 - binary_io_u_5: 0.5028 - loss: 0.9100Batch 49, Loss Value: 0.9451\n",
      "Batch 49, Gradient Norm: 0.0000\n",
      "\u001b[1m 49/232\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:28\u001b[0m 484ms/step - accuracy: 0.5027 - binary_io_u_5: 0.5027 - loss: 0.9101Batch 50, Loss Value: 0.9451\n",
      "Batch 50, Gradient Norm: 0.0085\n",
      "\u001b[1m 50/232\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:28\u001b[0m 484ms/step - accuracy: 0.5026 - binary_io_u_5: 0.5026 - loss: 0.9102Batch 51, Loss Value: 0.9451\n",
      "Batch 51, Gradient Norm: 0.0000\n",
      "\u001b[1m 51/232\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:27\u001b[0m 485ms/step - accuracy: 0.5024 - binary_io_u_5: 0.5024 - loss: 0.9103Batch 52, Loss Value: 0.9451\n",
      "Batch 52, Gradient Norm: 0.0000\n",
      "\u001b[1m 52/232\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:27\u001b[0m 485ms/step - accuracy: 0.5023 - binary_io_u_5: 0.5023 - loss: 0.9104Batch 53, Loss Value: 0.9451\n",
      "Batch 53, Gradient Norm: 0.0000\n",
      "\u001b[1m 53/232\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:26\u001b[0m 484ms/step - accuracy: 0.5022 - binary_io_u_5: 0.5022 - loss: 0.9104Batch 54, Loss Value: 0.9451\n",
      "Batch 54, Gradient Norm: 0.0000\n",
      "\u001b[1m 54/232\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:26\u001b[0m 484ms/step - accuracy: 0.5021 - binary_io_u_5: 0.5021 - loss: 0.9105Batch 55, Loss Value: 0.9451\n",
      "Batch 55, Gradient Norm: 0.0000\n",
      "\u001b[1m 55/232\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:25\u001b[0m 484ms/step - accuracy: 0.5019 - binary_io_u_5: 0.5019 - loss: 0.9106Batch 56, Loss Value: 0.9451\n",
      "Batch 56, Gradient Norm: 0.0000\n",
      "\u001b[1m 56/232\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:25\u001b[0m 484ms/step - accuracy: 0.5018 - binary_io_u_5: 0.5018 - loss: 0.9107Batch 57, Loss Value: 0.9451\n",
      "Batch 57, Gradient Norm: 0.0012\n",
      "\u001b[1m 57/232\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:24\u001b[0m 484ms/step - accuracy: 0.5017 - binary_io_u_5: 0.5017 - loss: 0.9108Batch 58, Loss Value: 0.9451\n",
      "Batch 58, Gradient Norm: 0.0000\n",
      "\u001b[1m 58/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:24\u001b[0m 484ms/step - accuracy: 0.5015 - binary_io_u_5: 0.5015 - loss: 0.9108Batch 59, Loss Value: 0.9451\n",
      "Batch 59, Gradient Norm: 0.0000\n",
      "\u001b[1m 59/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:23\u001b[0m 484ms/step - accuracy: 0.5014 - binary_io_u_5: 0.5014 - loss: 0.9109Batch 60, Loss Value: 0.9451\n",
      "Batch 60, Gradient Norm: 0.0004\n",
      "\u001b[1m 60/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:23\u001b[0m 484ms/step - accuracy: 0.5012 - binary_io_u_5: 0.5012 - loss: 0.9110Batch 61, Loss Value: 0.9451\n",
      "Batch 61, Gradient Norm: 0.0000\n",
      "\u001b[1m 61/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:22\u001b[0m 484ms/step - accuracy: 0.5011 - binary_io_u_5: 0.5011 - loss: 0.9111Batch 62, Loss Value: 0.9451\n",
      "Batch 62, Gradient Norm: 0.0028\n",
      "\u001b[1m 62/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:22\u001b[0m 484ms/step - accuracy: 0.5010 - binary_io_u_5: 0.5010 - loss: 0.9111Batch 63, Loss Value: 0.9451\n",
      "Batch 63, Gradient Norm: 0.0000\n",
      "\u001b[1m 63/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:21\u001b[0m 483ms/step - accuracy: 0.5008 - binary_io_u_5: 0.5008 - loss: 0.9112Batch 64, Loss Value: 0.9452\n",
      "Batch 64, Gradient Norm: 0.0021\n",
      "\u001b[1m 64/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:21\u001b[0m 483ms/step - accuracy: 0.5007 - binary_io_u_5: 0.5007 - loss: 0.9113Batch 65, Loss Value: 0.9451\n",
      "Batch 65, Gradient Norm: 0.0009\n",
      "\u001b[1m 65/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:20\u001b[0m 483ms/step - accuracy: 0.5005 - binary_io_u_5: 0.5005 - loss: 0.9113Batch 66, Loss Value: 0.9451\n",
      "Batch 66, Gradient Norm: 0.0003\n",
      "\u001b[1m 66/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:20\u001b[0m 483ms/step - accuracy: 0.5004 - binary_io_u_5: 0.5004 - loss: 0.9114Batch 67, Loss Value: 0.9451\n",
      "Batch 67, Gradient Norm: 0.0003\n",
      "\u001b[1m 67/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:19\u001b[0m 483ms/step - accuracy: 0.5003 - binary_io_u_5: 0.5003 - loss: 0.9115Batch 68, Loss Value: 0.9451\n",
      "Batch 68, Gradient Norm: 0.0000\n",
      "\u001b[1m 68/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:19\u001b[0m 483ms/step - accuracy: 0.5001 - binary_io_u_5: 0.5001 - loss: 0.9115Batch 69, Loss Value: 0.9451\n",
      "Batch 69, Gradient Norm: 0.0000\n",
      "\u001b[1m 69/232\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:18\u001b[0m 483ms/step - accuracy: 0.5000 - binary_io_u_5: 0.5000 - loss: 0.9116Batch 70, Loss Value: 0.9451\n",
      "Batch 70, Gradient Norm: 0.0028\n",
      "\u001b[1m 70/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:18\u001b[0m 483ms/step - accuracy: 0.4998 - binary_io_u_5: 0.4998 - loss: 0.9117Batch 71, Loss Value: 0.9451\n",
      "Batch 71, Gradient Norm: 0.0000\n",
      "\u001b[1m 71/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:17\u001b[0m 483ms/step - accuracy: 0.4997 - binary_io_u_5: 0.4997 - loss: 0.9117Batch 72, Loss Value: 0.9452\n",
      "Batch 72, Gradient Norm: 0.0033\n",
      "\u001b[1m 72/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:17\u001b[0m 483ms/step - accuracy: 0.4996 - binary_io_u_5: 0.4996 - loss: 0.9118Batch 73, Loss Value: 0.9450\n",
      "Batch 73, Gradient Norm: 0.0027\n",
      "\u001b[1m 73/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:16\u001b[0m 484ms/step - accuracy: 0.4994 - binary_io_u_5: 0.4994 - loss: 0.9119Batch 74, Loss Value: 0.9451\n",
      "Batch 74, Gradient Norm: 0.0000\n",
      "\u001b[1m 74/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:16\u001b[0m 484ms/step - accuracy: 0.4993 - binary_io_u_5: 0.4993 - loss: 0.9119Batch 75, Loss Value: 0.9451\n",
      "Batch 75, Gradient Norm: 0.0002\n",
      "\u001b[1m 75/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:15\u001b[0m 484ms/step - accuracy: 0.4992 - binary_io_u_5: 0.4992 - loss: 0.9120Batch 76, Loss Value: 0.9451\n",
      "Batch 76, Gradient Norm: 0.0048\n",
      "\u001b[1m 76/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:15\u001b[0m 484ms/step - accuracy: 0.4991 - binary_io_u_5: 0.4991 - loss: 0.9120Batch 77, Loss Value: 0.9451\n",
      "Batch 77, Gradient Norm: 0.0010\n",
      "\u001b[1m 77/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:15\u001b[0m 484ms/step - accuracy: 0.4990 - binary_io_u_5: 0.4990 - loss: 0.9121Batch 78, Loss Value: 0.9451\n",
      "Batch 78, Gradient Norm: 0.0005\n",
      "\u001b[1m 78/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:14\u001b[0m 484ms/step - accuracy: 0.4989 - binary_io_u_5: 0.4989 - loss: 0.9122Batch 79, Loss Value: 0.9451\n",
      "Batch 79, Gradient Norm: 0.0004\n",
      "\u001b[1m 79/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:14\u001b[0m 484ms/step - accuracy: 0.4988 - binary_io_u_5: 0.4988 - loss: 0.9122Batch 80, Loss Value: 0.9451\n",
      "Batch 80, Gradient Norm: 0.0004\n",
      "\u001b[1m 80/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:13\u001b[0m 484ms/step - accuracy: 0.4987 - binary_io_u_5: 0.4987 - loss: 0.9122Batch 81, Loss Value: 0.9451\n",
      "Batch 81, Gradient Norm: 0.0001\n",
      "\u001b[1m 81/232\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:13\u001b[0m 484ms/step - accuracy: 0.4986 - binary_io_u_5: 0.4986 - loss: 0.9123Batch 82, Loss Value: 0.9451\n",
      "Batch 82, Gradient Norm: 0.0009\n",
      "\u001b[1m 82/232\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m1:12\u001b[0m 484ms/step - accuracy: 0.4986 - binary_io_u_5: 0.4986 - loss: 0.9123Batch 83, Loss Value: 0.9451\n",
      "Batch 83, Gradient Norm: 0.0001\n",
      "\u001b[1m 83/232\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m1:12\u001b[0m 484ms/step - accuracy: 0.4985 - binary_io_u_5: 0.4985 - loss: 0.9123Batch 84, Loss Value: 0.9452\n",
      "Batch 84, Gradient Norm: 0.0057\n",
      "\u001b[1m 84/232\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m1:11\u001b[0m 484ms/step - accuracy: 0.4985 - binary_io_u_5: 0.4985 - loss: 0.9124Batch 85, Loss Value: 0.9451\n",
      "Batch 85, Gradient Norm: 0.0000\n",
      "\u001b[1m 85/232\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m1:11\u001b[0m 483ms/step - accuracy: 0.4984 - binary_io_u_5: 0.4984 - loss: 0.9124Batch 86, Loss Value: 0.9456\n",
      "Batch 86, Gradient Norm: 0.0059\n",
      "\u001b[1m 86/232\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m1:10\u001b[0m 483ms/step - accuracy: 0.4984 - binary_io_u_5: 0.4984 - loss: 0.9124Batch 87, Loss Value: 0.9450\n",
      "Batch 87, Gradient Norm: 0.0046\n",
      "\u001b[1m 87/232\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m1:10\u001b[0m 483ms/step - accuracy: 0.4983 - binary_io_u_5: 0.4983 - loss: 0.9124Batch 88, Loss Value: 0.9452\n",
      "Batch 88, Gradient Norm: 0.0033\n",
      "\u001b[1m 88/232\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m1:09\u001b[0m 483ms/step - accuracy: 0.4982 - binary_io_u_5: 0.4982 - loss: 0.9125Batch 89, Loss Value: 0.9451\n",
      "Batch 89, Gradient Norm: 0.0000\n",
      "\u001b[1m 89/232\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m1:09\u001b[0m 483ms/step - accuracy: 0.4981 - binary_io_u_5: 0.4981 - loss: 0.9125Batch 90, Loss Value: 0.9451\n",
      "Batch 90, Gradient Norm: 0.0001\n",
      "\u001b[1m 90/232\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m1:08\u001b[0m 483ms/step - accuracy: 0.4981 - binary_io_u_5: 0.4981 - loss: 0.9125Batch 91, Loss Value: 0.9450\n",
      "Batch 91, Gradient Norm: 0.0031\n",
      "\u001b[1m 91/232\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m1:08\u001b[0m 483ms/step - accuracy: 0.4980 - binary_io_u_5: 0.4980 - loss: 0.9125Batch 92, Loss Value: 0.9451\n",
      "Batch 92, Gradient Norm: 0.0008\n",
      "\u001b[1m 92/232\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m1:07\u001b[0m 483ms/step - accuracy: 0.4980 - binary_io_u_5: 0.4980 - loss: 0.9125Batch 93, Loss Value: 0.9451\n",
      "Batch 93, Gradient Norm: 0.0004\n",
      "\u001b[1m 93/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m1:07\u001b[0m 483ms/step - accuracy: 0.4979 - binary_io_u_5: 0.4979 - loss: 0.9126Batch 94, Loss Value: 0.9451\n",
      "Batch 94, Gradient Norm: 0.0000\n",
      "\u001b[1m 94/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m1:06\u001b[0m 483ms/step - accuracy: 0.4979 - binary_io_u_5: 0.4979 - loss: 0.9126Batch 95, Loss Value: 0.9451\n",
      "Batch 95, Gradient Norm: 0.0000\n",
      "\u001b[1m 95/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m1:06\u001b[0m 483ms/step - accuracy: 0.4978 - binary_io_u_5: 0.4978 - loss: 0.9126Batch 96, Loss Value: 0.9451\n",
      "Batch 96, Gradient Norm: 0.0006\n",
      "\u001b[1m 96/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m1:05\u001b[0m 483ms/step - accuracy: 0.4977 - binary_io_u_5: 0.4977 - loss: 0.9126Batch 97, Loss Value: 0.9451\n",
      "Batch 97, Gradient Norm: 0.0000\n",
      "\u001b[1m 97/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m1:05\u001b[0m 483ms/step - accuracy: 0.4977 - binary_io_u_5: 0.4977 - loss: 0.9127Batch 98, Loss Value: 0.9451\n",
      "Batch 98, Gradient Norm: 0.0000\n",
      "\u001b[1m 98/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m1:04\u001b[0m 483ms/step - accuracy: 0.4976 - binary_io_u_5: 0.4976 - loss: 0.9127Batch 99, Loss Value: 0.9451\n",
      "Batch 99, Gradient Norm: 0.0006\n",
      "\u001b[1m 99/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m1:04\u001b[0m 483ms/step - accuracy: 0.4976 - binary_io_u_5: 0.4976 - loss: 0.9127Batch 100, Loss Value: 0.9451\n",
      "Batch 100, Gradient Norm: 0.0007\n",
      "\u001b[1m100/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m1:03\u001b[0m 483ms/step - accuracy: 0.4976 - binary_io_u_5: 0.4976 - loss: 0.9127Batch 101, Loss Value: 0.9451\n",
      "Batch 101, Gradient Norm: 0.0000\n",
      "\u001b[1m101/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m1:03\u001b[0m 483ms/step - accuracy: 0.4975 - binary_io_u_5: 0.4975 - loss: 0.9127Batch 102, Loss Value: 0.9451\n",
      "Batch 102, Gradient Norm: 0.0000\n",
      "\u001b[1m102/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m1:02\u001b[0m 483ms/step - accuracy: 0.4975 - binary_io_u_5: 0.4975 - loss: 0.9127Batch 103, Loss Value: 0.9451\n",
      "Batch 103, Gradient Norm: 0.0017\n",
      "\u001b[1m103/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m1:02\u001b[0m 483ms/step - accuracy: 0.4975 - binary_io_u_5: 0.4975 - loss: 0.9127Batch 104, Loss Value: 0.9451\n",
      "Batch 104, Gradient Norm: 0.0000\n",
      "\u001b[1m104/232\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m1:01\u001b[0m 482ms/step - accuracy: 0.4974 - binary_io_u_5: 0.4974 - loss: 0.9127Batch 105, Loss Value: 0.9451\n",
      "Batch 105, Gradient Norm: 0.0003\n",
      "\u001b[1m105/232\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m1:01\u001b[0m 482ms/step - accuracy: 0.4974 - binary_io_u_5: 0.4974 - loss: 0.9127Batch 106, Loss Value: 0.9451\n",
      "Batch 106, Gradient Norm: 0.0000\n",
      "\u001b[1m106/232\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m1:00\u001b[0m 482ms/step - accuracy: 0.4974 - binary_io_u_5: 0.4974 - loss: 0.9127Batch 107, Loss Value: 0.9384\n",
      "Batch 107, Gradient Norm: 0.1080\n",
      "\u001b[1m107/232\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m1:00\u001b[0m 482ms/step - accuracy: 0.4974 - binary_io_u_5: 0.4974 - loss: 0.9127Batch 108, Loss Value: 0.9451\n",
      "Batch 108, Gradient Norm: 0.0038\n",
      "\u001b[1m108/232\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m59s\u001b[0m 482ms/step - accuracy: 0.4974 - binary_io_u_5: 0.4974 - loss: 0.9127 Batch 109, Loss Value: 0.9451\n",
      "Batch 109, Gradient Norm: 0.0033\n",
      "\u001b[1m109/232\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m59s\u001b[0m 482ms/step - accuracy: 0.4973 - binary_io_u_5: 0.4973 - loss: 0.9127Batch 110, Loss Value: 0.9454\n",
      "Batch 110, Gradient Norm: 0.0023\n",
      "\u001b[1m110/232\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m58s\u001b[0m 482ms/step - accuracy: 0.4973 - binary_io_u_5: 0.4973 - loss: 0.9127Batch 111, Loss Value: 0.9419\n",
      "Batch 111, Gradient Norm: 0.1139\n",
      "\u001b[1m111/232\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m58s\u001b[0m 482ms/step - accuracy: 0.4973 - binary_io_u_5: 0.4973 - loss: 0.9127Batch 112, Loss Value: 0.9451\n",
      "Batch 112, Gradient Norm: 0.0004\n",
      "\u001b[1m112/232\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m57s\u001b[0m 482ms/step - accuracy: 0.4973 - binary_io_u_5: 0.4973 - loss: 0.9127Batch 113, Loss Value: 0.9451\n",
      "Batch 113, Gradient Norm: 0.0000\n",
      "\u001b[1m113/232\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m57s\u001b[0m 482ms/step - accuracy: 0.4973 - binary_io_u_5: 0.4973 - loss: 0.9128Batch 114, Loss Value: 0.9451\n",
      "Batch 114, Gradient Norm: 0.0000\n",
      "\u001b[1m114/232\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m56s\u001b[0m 482ms/step - accuracy: 0.4973 - binary_io_u_5: 0.4973 - loss: 0.9128Batch 115, Loss Value: 0.9451\n",
      "Batch 115, Gradient Norm: 0.0001\n",
      "\u001b[1m115/232\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m56s\u001b[0m 483ms/step - accuracy: 0.4972 - binary_io_u_5: 0.4972 - loss: 0.9128Batch 116, Loss Value: 0.9451\n",
      "Batch 116, Gradient Norm: 0.0002\n",
      "\u001b[1m116/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m56s\u001b[0m 483ms/step - accuracy: 0.4972 - binary_io_u_5: 0.4972 - loss: 0.9128Batch 117, Loss Value: 0.9456\n",
      "Batch 117, Gradient Norm: 0.0213\n",
      "\u001b[1m117/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m55s\u001b[0m 483ms/step - accuracy: 0.4972 - binary_io_u_5: 0.4972 - loss: 0.9127Batch 118, Loss Value: 0.9451\n",
      "Batch 118, Gradient Norm: 0.0000\n",
      "\u001b[1m118/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m55s\u001b[0m 483ms/step - accuracy: 0.4972 - binary_io_u_5: 0.4972 - loss: 0.9127Batch 119, Loss Value: 0.9451\n",
      "Batch 119, Gradient Norm: 0.0000\n",
      "\u001b[1m119/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m54s\u001b[0m 484ms/step - accuracy: 0.4972 - binary_io_u_5: 0.4972 - loss: 0.9127Batch 120, Loss Value: 0.9451\n",
      "Batch 120, Gradient Norm: 0.0066\n",
      "\u001b[1m120/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m54s\u001b[0m 484ms/step - accuracy: 0.4972 - binary_io_u_5: 0.4972 - loss: 0.9127Batch 121, Loss Value: 0.9454\n",
      "Batch 121, Gradient Norm: 0.0171\n",
      "\u001b[1m121/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m53s\u001b[0m 484ms/step - accuracy: 0.4972 - binary_io_u_5: 0.4972 - loss: 0.9127Batch 122, Loss Value: 0.9451\n",
      "Batch 122, Gradient Norm: 0.0000\n",
      "\u001b[1m122/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m53s\u001b[0m 484ms/step - accuracy: 0.4972 - binary_io_u_5: 0.4972 - loss: 0.9128Batch 123, Loss Value: 0.9455\n",
      "Batch 123, Gradient Norm: 0.0011\n",
      "\u001b[1m123/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m52s\u001b[0m 484ms/step - accuracy: 0.4971 - binary_io_u_5: 0.4971 - loss: 0.9128Batch 124, Loss Value: 0.9453\n",
      "Batch 124, Gradient Norm: 0.0018\n",
      "\u001b[1m124/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m52s\u001b[0m 484ms/step - accuracy: 0.4971 - binary_io_u_5: 0.4971 - loss: 0.9128Batch 125, Loss Value: 0.9451\n",
      "Batch 125, Gradient Norm: 0.0000\n",
      "\u001b[1m125/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m51s\u001b[0m 484ms/step - accuracy: 0.4971 - binary_io_u_5: 0.4971 - loss: 0.9128Batch 126, Loss Value: 0.9451\n",
      "Batch 126, Gradient Norm: 0.0000\n",
      "\u001b[1m126/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m51s\u001b[0m 484ms/step - accuracy: 0.4971 - binary_io_u_5: 0.4971 - loss: 0.9128Batch 127, Loss Value: 0.9451\n",
      "Batch 127, Gradient Norm: 0.0004\n",
      "\u001b[1m127/232\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m50s\u001b[0m 484ms/step - accuracy: 0.4971 - binary_io_u_5: 0.4971 - loss: 0.9128Batch 128, Loss Value: 0.9450\n",
      "Batch 128, Gradient Norm: 0.0024\n",
      "\u001b[1m128/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m50s\u001b[0m 484ms/step - accuracy: 0.4970 - binary_io_u_5: 0.4970 - loss: 0.9128Batch 129, Loss Value: 0.9454\n",
      "Batch 129, Gradient Norm: 0.0198\n",
      "\u001b[1m129/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m49s\u001b[0m 484ms/step - accuracy: 0.4970 - binary_io_u_5: 0.4970 - loss: 0.9128Batch 130, Loss Value: 0.9451\n",
      "Batch 130, Gradient Norm: 0.0001\n",
      "\u001b[1m130/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m49s\u001b[0m 484ms/step - accuracy: 0.4970 - binary_io_u_5: 0.4970 - loss: 0.9128Batch 131, Loss Value: 0.9451\n",
      "Batch 131, Gradient Norm: 0.0002\n",
      "\u001b[1m131/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m48s\u001b[0m 484ms/step - accuracy: 0.4970 - binary_io_u_5: 0.4970 - loss: 0.9128Batch 132, Loss Value: 0.9451\n",
      "Batch 132, Gradient Norm: 0.0000\n",
      "\u001b[1m132/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m48s\u001b[0m 484ms/step - accuracy: 0.4970 - binary_io_u_5: 0.4970 - loss: 0.9128Batch 133, Loss Value: 0.9453\n",
      "Batch 133, Gradient Norm: 0.0023\n",
      "\u001b[1m133/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m47s\u001b[0m 484ms/step - accuracy: 0.4969 - binary_io_u_5: 0.4969 - loss: 0.9128Batch 134, Loss Value: 0.9453\n",
      "Batch 134, Gradient Norm: 0.0164\n",
      "\u001b[1m134/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m47s\u001b[0m 484ms/step - accuracy: 0.4969 - binary_io_u_5: 0.4969 - loss: 0.9128Batch 135, Loss Value: 0.9451\n",
      "Batch 135, Gradient Norm: 0.0000\n",
      "\u001b[1m135/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m46s\u001b[0m 484ms/step - accuracy: 0.4969 - binary_io_u_5: 0.4969 - loss: 0.9128Batch 136, Loss Value: 0.9451\n",
      "Batch 136, Gradient Norm: 0.0037\n",
      "\u001b[1m136/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m46s\u001b[0m 484ms/step - accuracy: 0.4969 - binary_io_u_5: 0.4969 - loss: 0.9128Batch 137, Loss Value: 0.9450\n",
      "Batch 137, Gradient Norm: 0.0051\n",
      "\u001b[1m137/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m46s\u001b[0m 484ms/step - accuracy: 0.4969 - binary_io_u_5: 0.4969 - loss: 0.9128Batch 138, Loss Value: 0.9388\n",
      "Batch 138, Gradient Norm: 0.3780\n",
      "\u001b[1m138/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m45s\u001b[0m 485ms/step - accuracy: 0.4968 - binary_io_u_5: 0.4968 - loss: 0.9129Batch 139, Loss Value: 0.9451\n",
      "Batch 139, Gradient Norm: 0.0007\n",
      "\u001b[1m139/232\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m45s\u001b[0m 485ms/step - accuracy: 0.4968 - binary_io_u_5: 0.4968 - loss: 0.9129Batch 140, Loss Value: 0.9451\n",
      "Batch 140, Gradient Norm: 0.0001\n",
      "\u001b[1m140/232\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m44s\u001b[0m 485ms/step - accuracy: 0.4968 - binary_io_u_5: 0.4968 - loss: 0.9129Batch 141, Loss Value: 0.9451\n",
      "Batch 141, Gradient Norm: 0.0000\n",
      "\u001b[1m141/232\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m44s\u001b[0m 485ms/step - accuracy: 0.4968 - binary_io_u_5: 0.4968 - loss: 0.9129Batch 142, Loss Value: 0.9452\n",
      "Batch 142, Gradient Norm: 0.0060\n",
      "\u001b[1m142/232\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m43s\u001b[0m 485ms/step - accuracy: 0.4967 - binary_io_u_5: 0.4967 - loss: 0.9129Batch 143, Loss Value: 0.9451\n",
      "Batch 143, Gradient Norm: 0.0017\n",
      "\u001b[1m143/232\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m43s\u001b[0m 485ms/step - accuracy: 0.4967 - binary_io_u_5: 0.4967 - loss: 0.9129Batch 144, Loss Value: 0.9451\n",
      "Batch 144, Gradient Norm: 0.0000\n",
      "\u001b[1m144/232\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m42s\u001b[0m 485ms/step - accuracy: 0.4967 - binary_io_u_5: 0.4967 - loss: 0.9129Batch 145, Loss Value: 0.9451\n",
      "Batch 145, Gradient Norm: 0.0002\n",
      "\u001b[1m145/232\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m42s\u001b[0m 485ms/step - accuracy: 0.4967 - binary_io_u_5: 0.4967 - loss: 0.9129Batch 146, Loss Value: 0.9451\n",
      "Batch 146, Gradient Norm: 0.0006\n",
      "\u001b[1m146/232\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m41s\u001b[0m 485ms/step - accuracy: 0.4967 - binary_io_u_5: 0.4967 - loss: 0.9129Batch 147, Loss Value: 0.9451\n",
      "Batch 147, Gradient Norm: 0.0000\n",
      "\u001b[1m147/232\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m41s\u001b[0m 485ms/step - accuracy: 0.4967 - binary_io_u_5: 0.4967 - loss: 0.9129Batch 148, Loss Value: 0.9451\n",
      "Batch 148, Gradient Norm: 0.0005\n",
      "\u001b[1m148/232\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m40s\u001b[0m 485ms/step - accuracy: 0.4966 - binary_io_u_5: 0.4966 - loss: 0.9129Batch 149, Loss Value: 0.9451\n",
      "Batch 149, Gradient Norm: 0.0000\n",
      "\u001b[1m149/232\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m40s\u001b[0m 485ms/step - accuracy: 0.4966 - binary_io_u_5: 0.4966 - loss: 0.9129Batch 150, Loss Value: 0.9451\n",
      "Batch 150, Gradient Norm: 0.0014\n",
      "\u001b[1m150/232\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m39s\u001b[0m 485ms/step - accuracy: 0.4966 - binary_io_u_5: 0.4966 - loss: 0.9129Batch 151, Loss Value: 0.9451\n",
      "Batch 151, Gradient Norm: 0.0000\n",
      "\u001b[1m151/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m39s\u001b[0m 485ms/step - accuracy: 0.4966 - binary_io_u_5: 0.4966 - loss: 0.9130Batch 152, Loss Value: 0.9451\n",
      "Batch 152, Gradient Norm: 0.0000\n",
      "\u001b[1m152/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m38s\u001b[0m 485ms/step - accuracy: 0.4966 - binary_io_u_5: 0.4966 - loss: 0.9130Batch 153, Loss Value: 0.9451\n",
      "Batch 153, Gradient Norm: 0.0001\n",
      "\u001b[1m153/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m38s\u001b[0m 485ms/step - accuracy: 0.4966 - binary_io_u_5: 0.4966 - loss: 0.9130Batch 154, Loss Value: 0.9451\n",
      "Batch 154, Gradient Norm: 0.0000\n",
      "\u001b[1m154/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m37s\u001b[0m 485ms/step - accuracy: 0.4966 - binary_io_u_5: 0.4966 - loss: 0.9130Batch 155, Loss Value: 0.9451\n",
      "Batch 155, Gradient Norm: 0.0000\n",
      "\u001b[1m155/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m37s\u001b[0m 485ms/step - accuracy: 0.4965 - binary_io_u_5: 0.4965 - loss: 0.9130Batch 156, Loss Value: 0.9451\n",
      "Batch 156, Gradient Norm: 0.0000\n",
      "\u001b[1m156/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m36s\u001b[0m 485ms/step - accuracy: 0.4965 - binary_io_u_5: 0.4965 - loss: 0.9130Batch 157, Loss Value: 0.9451\n",
      "Batch 157, Gradient Norm: 0.0001\n",
      "\u001b[1m157/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m36s\u001b[0m 485ms/step - accuracy: 0.4965 - binary_io_u_5: 0.4965 - loss: 0.9130Batch 158, Loss Value: 0.9451\n",
      "Batch 158, Gradient Norm: 0.0002\n",
      "\u001b[1m158/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m35s\u001b[0m 484ms/step - accuracy: 0.4965 - binary_io_u_5: 0.4965 - loss: 0.9130Batch 159, Loss Value: 0.9451\n",
      "Batch 159, Gradient Norm: 0.0000\n",
      "\u001b[1m159/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m35s\u001b[0m 484ms/step - accuracy: 0.4965 - binary_io_u_5: 0.4965 - loss: 0.9130Batch 160, Loss Value: 0.9451\n",
      "Batch 160, Gradient Norm: 0.0000\n",
      "\u001b[1m160/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m34s\u001b[0m 484ms/step - accuracy: 0.4965 - binary_io_u_5: 0.4965 - loss: 0.9130Batch 161, Loss Value: 0.9455\n",
      "Batch 161, Gradient Norm: 0.0087\n",
      "\u001b[1m161/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m34s\u001b[0m 484ms/step - accuracy: 0.4965 - binary_io_u_5: 0.4965 - loss: 0.9130Batch 162, Loss Value: 0.9450\n",
      "Batch 162, Gradient Norm: 0.0016\n",
      "\u001b[1m162/232\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m33s\u001b[0m 484ms/step - accuracy: 0.4964 - binary_io_u_5: 0.4964 - loss: 0.9130Batch 163, Loss Value: 0.9451\n",
      "Batch 163, Gradient Norm: 0.0010\n",
      "\u001b[1m163/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m33s\u001b[0m 484ms/step - accuracy: 0.4964 - binary_io_u_5: 0.4964 - loss: 0.9130Batch 164, Loss Value: 0.9451\n",
      "Batch 164, Gradient Norm: 0.0000\n",
      "\u001b[1m164/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m32s\u001b[0m 484ms/step - accuracy: 0.4964 - binary_io_u_5: 0.4964 - loss: 0.9130Batch 165, Loss Value: 0.9451\n",
      "Batch 165, Gradient Norm: 0.0000\n",
      "\u001b[1m165/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m32s\u001b[0m 484ms/step - accuracy: 0.4964 - binary_io_u_5: 0.4964 - loss: 0.9131Batch 166, Loss Value: 0.9450\n",
      "Batch 166, Gradient Norm: 0.0017\n",
      "\u001b[1m166/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m31s\u001b[0m 484ms/step - accuracy: 0.4964 - binary_io_u_5: 0.4964 - loss: 0.9131Batch 167, Loss Value: 0.9451\n",
      "Batch 167, Gradient Norm: 0.0000\n",
      "\u001b[1m167/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m31s\u001b[0m 484ms/step - accuracy: 0.4964 - binary_io_u_5: 0.4964 - loss: 0.9131Batch 168, Loss Value: 0.9451\n",
      "Batch 168, Gradient Norm: 0.0002\n",
      "\u001b[1m168/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m30s\u001b[0m 484ms/step - accuracy: 0.4963 - binary_io_u_5: 0.4963 - loss: 0.9131Batch 169, Loss Value: 0.9451\n",
      "Batch 169, Gradient Norm: 0.0000\n",
      "\u001b[1m169/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m30s\u001b[0m 484ms/step - accuracy: 0.4963 - binary_io_u_5: 0.4963 - loss: 0.9131Batch 170, Loss Value: 0.9451\n",
      "Batch 170, Gradient Norm: 0.0000\n",
      "\u001b[1m170/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m29s\u001b[0m 484ms/step - accuracy: 0.4963 - binary_io_u_5: 0.4963 - loss: 0.9131Batch 171, Loss Value: 0.9451\n",
      "Batch 171, Gradient Norm: 0.0000\n",
      "\u001b[1m171/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m29s\u001b[0m 484ms/step - accuracy: 0.4963 - binary_io_u_5: 0.4963 - loss: 0.9131Batch 172, Loss Value: 0.9451\n",
      "Batch 172, Gradient Norm: 0.0043\n",
      "\u001b[1m172/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m29s\u001b[0m 484ms/step - accuracy: 0.4963 - binary_io_u_5: 0.4963 - loss: 0.9131Batch 173, Loss Value: 0.9451\n",
      "Batch 173, Gradient Norm: 0.0002\n",
      "\u001b[1m173/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m28s\u001b[0m 484ms/step - accuracy: 0.4962 - binary_io_u_5: 0.4962 - loss: 0.9131Batch 174, Loss Value: 0.9451\n",
      "Batch 174, Gradient Norm: 0.0000\n",
      "\u001b[1m174/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m28s\u001b[0m 484ms/step - accuracy: 0.4962 - binary_io_u_5: 0.4962 - loss: 0.9131Batch 175, Loss Value: 0.9451\n",
      "Batch 175, Gradient Norm: 0.0000\n",
      "\u001b[1m175/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m27s\u001b[0m 484ms/step - accuracy: 0.4962 - binary_io_u_5: 0.4962 - loss: 0.9132Batch 176, Loss Value: 0.9451\n",
      "Batch 176, Gradient Norm: 0.0002\n",
      "\u001b[1m176/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m27s\u001b[0m 484ms/step - accuracy: 0.4962 - binary_io_u_5: 0.4962 - loss: 0.9132Batch 177, Loss Value: 0.9451\n",
      "Batch 177, Gradient Norm: 0.0000\n",
      "\u001b[1m177/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m26s\u001b[0m 484ms/step - accuracy: 0.4962 - binary_io_u_5: 0.4962 - loss: 0.9132Batch 178, Loss Value: 0.9451\n",
      "Batch 178, Gradient Norm: 0.0000\n",
      "\u001b[1m178/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m26s\u001b[0m 484ms/step - accuracy: 0.4961 - binary_io_u_5: 0.4961 - loss: 0.9132Batch 179, Loss Value: 0.9451\n",
      "Batch 179, Gradient Norm: 0.0001\n",
      "\u001b[1m179/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m25s\u001b[0m 484ms/step - accuracy: 0.4961 - binary_io_u_5: 0.4961 - loss: 0.9132Batch 180, Loss Value: 0.9451\n",
      "Batch 180, Gradient Norm: 0.0004\n",
      "\u001b[1m180/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m25s\u001b[0m 484ms/step - accuracy: 0.4961 - binary_io_u_5: 0.4961 - loss: 0.9132Batch 181, Loss Value: 0.9455\n",
      "Batch 181, Gradient Norm: 0.0084\n",
      "\u001b[1m181/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m24s\u001b[0m 484ms/step - accuracy: 0.4961 - binary_io_u_5: 0.4961 - loss: 0.9132Batch 182, Loss Value: 0.9451\n",
      "Batch 182, Gradient Norm: 0.0006\n",
      "\u001b[1m182/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m24s\u001b[0m 484ms/step - accuracy: 0.4961 - binary_io_u_5: 0.4961 - loss: 0.9132Batch 183, Loss Value: 0.9453\n",
      "Batch 183, Gradient Norm: 0.0034\n",
      "\u001b[1m183/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m23s\u001b[0m 484ms/step - accuracy: 0.4960 - binary_io_u_5: 0.4960 - loss: 0.9132Batch 184, Loss Value: 0.9451\n",
      "Batch 184, Gradient Norm: 0.0000\n",
      "\u001b[1m184/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m23s\u001b[0m 484ms/step - accuracy: 0.4960 - binary_io_u_5: 0.4960 - loss: 0.9132Batch 185, Loss Value: 0.9451\n",
      "Batch 185, Gradient Norm: 0.0016\n",
      "\u001b[1m185/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m22s\u001b[0m 484ms/step - accuracy: 0.4960 - binary_io_u_5: 0.4960 - loss: 0.9133Batch 186, Loss Value: 0.9451\n",
      "Batch 186, Gradient Norm: 0.0001\n",
      "\u001b[1m186/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m22s\u001b[0m 484ms/step - accuracy: 0.4960 - binary_io_u_5: 0.4960 - loss: 0.9133Batch 187, Loss Value: 0.9451\n",
      "Batch 187, Gradient Norm: 0.0001\n",
      "\u001b[1m187/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m21s\u001b[0m 484ms/step - accuracy: 0.4960 - binary_io_u_5: 0.4960 - loss: 0.9133Batch 188, Loss Value: 0.9451\n",
      "Batch 188, Gradient Norm: 0.0000\n",
      "\u001b[1m188/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m21s\u001b[0m 484ms/step - accuracy: 0.4959 - binary_io_u_5: 0.4959 - loss: 0.9133Batch 189, Loss Value: 0.9451\n",
      "Batch 189, Gradient Norm: 0.0000\n",
      "\u001b[1m189/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m20s\u001b[0m 484ms/step - accuracy: 0.4959 - binary_io_u_5: 0.4959 - loss: 0.9133Batch 190, Loss Value: 0.9451\n",
      "Batch 190, Gradient Norm: 0.0000\n",
      "\u001b[1m190/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m20s\u001b[0m 484ms/step - accuracy: 0.4959 - binary_io_u_5: 0.4959 - loss: 0.9133Batch 191, Loss Value: 0.9451\n",
      "Batch 191, Gradient Norm: 0.0000\n",
      "\u001b[1m191/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m19s\u001b[0m 484ms/step - accuracy: 0.4959 - binary_io_u_5: 0.4959 - loss: 0.9133Batch 192, Loss Value: 0.9451\n",
      "Batch 192, Gradient Norm: 0.0005\n",
      "\u001b[1m192/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m19s\u001b[0m 484ms/step - accuracy: 0.4959 - binary_io_u_5: 0.4959 - loss: 0.9133Batch 193, Loss Value: 0.9451\n",
      "Batch 193, Gradient Norm: 0.0000\n",
      "\u001b[1m193/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m18s\u001b[0m 484ms/step - accuracy: 0.4959 - binary_io_u_5: 0.4959 - loss: 0.9133Batch 194, Loss Value: 0.9451\n",
      "Batch 194, Gradient Norm: 0.0000\n",
      "\u001b[1m194/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m18s\u001b[0m 484ms/step - accuracy: 0.4958 - binary_io_u_5: 0.4958 - loss: 0.9133Batch 195, Loss Value: 0.9451\n",
      "Batch 195, Gradient Norm: 0.0000\n",
      "\u001b[1m195/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m17s\u001b[0m 484ms/step - accuracy: 0.4958 - binary_io_u_5: 0.4958 - loss: 0.9134Batch 196, Loss Value: 0.9451\n",
      "Batch 196, Gradient Norm: 0.0097\n",
      "\u001b[1m196/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m17s\u001b[0m 484ms/step - accuracy: 0.4958 - binary_io_u_5: 0.4958 - loss: 0.9134Batch 197, Loss Value: 0.9451\n",
      "Batch 197, Gradient Norm: 0.0004\n",
      "\u001b[1m197/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m16s\u001b[0m 484ms/step - accuracy: 0.4958 - binary_io_u_5: 0.4958 - loss: 0.9134Batch 198, Loss Value: 0.9451\n",
      "Batch 198, Gradient Norm: 0.0000\n",
      "\u001b[1m198/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m16s\u001b[0m 484ms/step - accuracy: 0.4958 - binary_io_u_5: 0.4958 - loss: 0.9134Batch 199, Loss Value: 0.9449\n",
      "Batch 199, Gradient Norm: 0.0018\n",
      "\u001b[1m199/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m15s\u001b[0m 484ms/step - accuracy: 0.4958 - binary_io_u_5: 0.4958 - loss: 0.9134Batch 200, Loss Value: 0.9451\n",
      "Batch 200, Gradient Norm: 0.0000\n",
      "\u001b[1m200/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m15s\u001b[0m 484ms/step - accuracy: 0.4958 - binary_io_u_5: 0.4958 - loss: 0.9134Batch 201, Loss Value: 0.9451\n",
      "Batch 201, Gradient Norm: 0.0006\n",
      "\u001b[1m201/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m14s\u001b[0m 484ms/step - accuracy: 0.4958 - binary_io_u_5: 0.4958 - loss: 0.9134Batch 202, Loss Value: 0.9451\n",
      "Batch 202, Gradient Norm: 0.0008\n",
      "\u001b[1m202/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m14s\u001b[0m 484ms/step - accuracy: 0.4957 - binary_io_u_5: 0.4957 - loss: 0.9134Batch 203, Loss Value: 0.9451\n",
      "Batch 203, Gradient Norm: 0.0000\n",
      "\u001b[1m203/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m14s\u001b[0m 484ms/step - accuracy: 0.4957 - binary_io_u_5: 0.4957 - loss: 0.9134Batch 204, Loss Value: 0.9451\n",
      "Batch 204, Gradient Norm: 0.0000\n",
      "\u001b[1m204/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m13s\u001b[0m 485ms/step - accuracy: 0.4957 - binary_io_u_5: 0.4957 - loss: 0.9134Batch 205, Loss Value: 0.9451\n",
      "Batch 205, Gradient Norm: 0.0001\n",
      "\u001b[1m205/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m13s\u001b[0m 485ms/step - accuracy: 0.4957 - binary_io_u_5: 0.4957 - loss: 0.9134Batch 206, Loss Value: 0.9451\n",
      "Batch 206, Gradient Norm: 0.0002\n",
      "\u001b[1m206/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m12s\u001b[0m 485ms/step - accuracy: 0.4957 - binary_io_u_5: 0.4957 - loss: 0.9134Batch 207, Loss Value: 0.9451\n",
      "Batch 207, Gradient Norm: 0.0002\n",
      "\u001b[1m207/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m12s\u001b[0m 485ms/step - accuracy: 0.4957 - binary_io_u_5: 0.4957 - loss: 0.9134Batch 208, Loss Value: 0.9451\n",
      "Batch 208, Gradient Norm: 0.0000\n",
      "\u001b[1m208/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m11s\u001b[0m 485ms/step - accuracy: 0.4957 - binary_io_u_5: 0.4957 - loss: 0.9134Batch 209, Loss Value: 0.9451\n",
      "Batch 209, Gradient Norm: 0.0000\n",
      "\u001b[1m209/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m11s\u001b[0m 486ms/step - accuracy: 0.4957 - binary_io_u_5: 0.4957 - loss: 0.9134Batch 210, Loss Value: 0.9452\n",
      "Batch 210, Gradient Norm: 0.0092\n",
      "\u001b[1m210/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m10s\u001b[0m 486ms/step - accuracy: 0.4957 - binary_io_u_5: 0.4957 - loss: 0.9134Batch 211, Loss Value: 0.9451\n",
      "Batch 211, Gradient Norm: 0.0000\n",
      "\u001b[1m211/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m10s\u001b[0m 485ms/step - accuracy: 0.4957 - binary_io_u_5: 0.4957 - loss: 0.9134Batch 212, Loss Value: 0.9451\n",
      "Batch 212, Gradient Norm: 0.0000\n",
      "\u001b[1m212/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m9s\u001b[0m 485ms/step - accuracy: 0.4957 - binary_io_u_5: 0.4957 - loss: 0.9134 Batch 213, Loss Value: 0.9451\n",
      "Batch 213, Gradient Norm: 0.0000\n",
      "\u001b[1m213/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m9s\u001b[0m 485ms/step - accuracy: 0.4956 - binary_io_u_5: 0.4956 - loss: 0.9134Batch 214, Loss Value: 0.9451\n",
      "Batch 214, Gradient Norm: 0.0042\n",
      "\u001b[1m214/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m8s\u001b[0m 485ms/step - accuracy: 0.4956 - binary_io_u_5: 0.4956 - loss: 0.9135Batch 215, Loss Value: 0.9451\n",
      "Batch 215, Gradient Norm: 0.0004\n",
      "\u001b[1m215/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m8s\u001b[0m 485ms/step - accuracy: 0.4956 - binary_io_u_5: 0.4956 - loss: 0.9135Batch 216, Loss Value: 0.9451\n",
      "Batch 216, Gradient Norm: 0.0005\n",
      "\u001b[1m216/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m7s\u001b[0m 485ms/step - accuracy: 0.4956 - binary_io_u_5: 0.4956 - loss: 0.9135Batch 217, Loss Value: 0.9451\n",
      "Batch 217, Gradient Norm: 0.0005\n",
      "\u001b[1m217/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m7s\u001b[0m 485ms/step - accuracy: 0.4956 - binary_io_u_5: 0.4956 - loss: 0.9135Batch 218, Loss Value: 0.9450\n",
      "Batch 218, Gradient Norm: 0.0028\n",
      "\u001b[1m218/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m6s\u001b[0m 485ms/step - accuracy: 0.4956 - binary_io_u_5: 0.4956 - loss: 0.9135Batch 219, Loss Value: 0.9451\n",
      "Batch 219, Gradient Norm: 0.0000\n",
      "\u001b[1m219/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m6s\u001b[0m 485ms/step - accuracy: 0.4956 - binary_io_u_5: 0.4956 - loss: 0.9135Batch 220, Loss Value: 0.9452\n",
      "Batch 220, Gradient Norm: 0.0103\n",
      "\u001b[1m220/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m5s\u001b[0m 485ms/step - accuracy: 0.4956 - binary_io_u_5: 0.4956 - loss: 0.9135Batch 221, Loss Value: 0.9451\n",
      "Batch 221, Gradient Norm: 0.0001\n",
      "\u001b[1m221/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m5s\u001b[0m 485ms/step - accuracy: 0.4956 - binary_io_u_5: 0.4956 - loss: 0.9135Batch 222, Loss Value: 0.9451\n",
      "Batch 222, Gradient Norm: 0.0001\n",
      "\u001b[1m222/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m4s\u001b[0m 485ms/step - accuracy: 0.4956 - binary_io_u_5: 0.4956 - loss: 0.9135Batch 223, Loss Value: 0.9311\n",
      "Batch 223, Gradient Norm: 0.3059\n",
      "\u001b[1m223/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m4s\u001b[0m 485ms/step - accuracy: 0.4956 - binary_io_u_5: 0.4956 - loss: 0.9135Batch 224, Loss Value: 0.9451\n",
      "Batch 224, Gradient Norm: 0.0000\n",
      "\u001b[1m224/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m3s\u001b[0m 485ms/step - accuracy: 0.4956 - binary_io_u_5: 0.4956 - loss: 0.9135Batch 225, Loss Value: 0.9451\n",
      "Batch 225, Gradient Norm: 0.0000\n",
      "\u001b[1m225/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m3s\u001b[0m 484ms/step - accuracy: 0.4956 - binary_io_u_5: 0.4956 - loss: 0.9135Batch 226, Loss Value: 0.9451\n",
      "Batch 226, Gradient Norm: 0.0000\n",
      "\u001b[1m226/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m2s\u001b[0m 484ms/step - accuracy: 0.4956 - binary_io_u_5: 0.4956 - loss: 0.9135Batch 227, Loss Value: 0.9451\n",
      "Batch 227, Gradient Norm: 0.0001\n",
      "\u001b[1m227/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m2s\u001b[0m 484ms/step - accuracy: 0.4956 - binary_io_u_5: 0.4956 - loss: 0.9135Batch 228, Loss Value: 0.9451\n",
      "Batch 228, Gradient Norm: 0.0000\n",
      "\u001b[1m228/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m1s\u001b[0m 484ms/step - accuracy: 0.4956 - binary_io_u_5: 0.4956 - loss: 0.9135Batch 229, Loss Value: 0.9451\n",
      "Batch 229, Gradient Norm: 0.0000\n",
      "\u001b[1m229/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m1s\u001b[0m 484ms/step - accuracy: 0.4956 - binary_io_u_5: 0.4956 - loss: 0.9135Batch 230, Loss Value: 0.9451\n",
      "Batch 230, Gradient Norm: 0.0000\n",
      "\u001b[1m230/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 484ms/step - accuracy: 0.4956 - binary_io_u_5: 0.4956 - loss: 0.9135Batch 231, Loss Value: 0.9451\n",
      "Batch 231, Gradient Norm: 0.0010\n",
      "\u001b[1m231/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 484ms/step - accuracy: 0.4956 - binary_io_u_5: 0.4956 - loss: 0.9135Batch 232, Loss Value: 0.9451\n",
      "Batch 232, Gradient Norm: 0.0000\n",
      "\u001b[1m232/232\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m114s\u001b[0m 490ms/step - accuracy: 0.4956 - binary_io_u_5: 0.4956 - loss: 0.9135 - val_accuracy: 0.4949 - val_binary_io_u_5: 0.4949 - val_loss: 0.9130 - learning_rate: 5.0000e-04\n",
      "Epoch 22/200\n",
      "Batch 1, Loss Value: 0.9344\n",
      "Batch 1, Gradient Norm: 0.0184\n",
      "\u001b[1m  1/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:55\u001b[0m 501ms/step - accuracy: 0.5400 - binary_io_u_5: 0.5400 - loss: 0.8861Batch 2, Loss Value: 0.9342\n",
      "Batch 2, Gradient Norm: 0.0000\n",
      "\u001b[1m  2/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:48\u001b[0m 471ms/step - accuracy: 0.5375 - binary_io_u_5: 0.5375 - loss: 0.8878Batch 3, Loss Value: 0.9342\n",
      "Batch 3, Gradient Norm: 0.0002\n",
      "\u001b[1m  3/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:48\u001b[0m 472ms/step - accuracy: 0.5361 - binary_io_u_5: 0.5361 - loss: 0.8887Batch 4, Loss Value: 0.9334\n",
      "Batch 4, Gradient Norm: 0.0736\n",
      "\u001b[1m  4/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:47\u001b[0m 472ms/step - accuracy: 0.5277 - binary_io_u_5: 0.5277 - loss: 0.8928Batch 5, Loss Value: 0.9342\n",
      "Batch 5, Gradient Norm: 0.0002\n",
      "\u001b[1m  5/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:46\u001b[0m 471ms/step - accuracy: 0.5186 - binary_io_u_5: 0.5186 - loss: 0.8969Batch 6, Loss Value: 0.9342\n",
      "Batch 6, Gradient Norm: 0.0001\n",
      "\u001b[1m  6/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:46\u001b[0m 472ms/step - accuracy: 0.5130 - binary_io_u_5: 0.5130 - loss: 0.8997Batch 7, Loss Value: 0.9342\n",
      "Batch 7, Gradient Norm: 0.0005\n",
      "\u001b[1m  7/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:46\u001b[0m 472ms/step - accuracy: 0.5089 - binary_io_u_5: 0.5089 - loss: 0.9019Batch 8, Loss Value: 0.9342\n",
      "Batch 8, Gradient Norm: 0.0000\n",
      "\u001b[1m  8/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:45\u001b[0m 472ms/step - accuracy: 0.5051 - binary_io_u_5: 0.5051 - loss: 0.9041Batch 9, Loss Value: 0.9342\n",
      "Batch 9, Gradient Norm: 0.0000\n",
      "\u001b[1m  9/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:45\u001b[0m 472ms/step - accuracy: 0.5031 - binary_io_u_5: 0.5031 - loss: 0.9053Batch 10, Loss Value: 0.9342\n",
      "Batch 10, Gradient Norm: 0.0000\n",
      "\u001b[1m 10/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:44\u001b[0m 472ms/step - accuracy: 0.5025 - binary_io_u_5: 0.5025 - loss: 0.9056Batch 11, Loss Value: 0.9342\n",
      "Batch 11, Gradient Norm: 0.0002\n",
      "\u001b[1m 11/232\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:44\u001b[0m 472ms/step - accuracy: 0.5021 - binary_io_u_5: 0.5021 - loss: 0.9058Batch 12, Loss Value: 0.9342\n",
      "Batch 12, Gradient Norm: 0.0000\n",
      "\u001b[1m 12/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:43\u001b[0m 471ms/step - accuracy: 0.5021 - binary_io_u_5: 0.5021 - loss: 0.9058Batch 13, Loss Value: 0.9342\n",
      "Batch 13, Gradient Norm: 0.0000\n",
      "\u001b[1m 13/232\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:43\u001b[0m 472ms/step - accuracy: 0.5021 - binary_io_u_5: 0.5021 - loss: 0.9059"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = ''  # Force CPU\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, optimizers\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator as KerasImageDataGenerator\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "\n",
    "# Configuration\n",
    "base_dir = \"F:/Education/NSU/CSE/CSE499/Implementation/Image Data\"\n",
    "input_even_dir = os.path.join(base_dir, \"gray_image\", \"even_images\")\n",
    "input_odd_dir = os.path.join(base_dir, \"gray_image\", \"odd_images\")\n",
    "output_dir = os.path.join(base_dir, \"matrix\")\n",
    "input_shape = (32, 32, 1)\n",
    "output_shape = (50, 50, 1)\n",
    "batch_size = 4\n",
    "learning_rate = 1e-3\n",
    "epochs = 200\n",
    "block_size = 10\n",
    "\n",
    "# Attention Gate\n",
    "def attention_gate(gating, skip, num_filters):\n",
    "    g = layers.Conv2D(num_filters, 1, padding='same')(gating)\n",
    "    x = layers.Conv2D(num_filters, 1, padding='same')(skip)\n",
    "    x = layers.Add()([g, x])\n",
    "    x = layers.Activation('relu')(x)\n",
    "    x = layers.Conv2D(1, 1, padding='same')(x)\n",
    "    x = layers.Activation('sigmoid')(x)\n",
    "    return layers.Multiply()([skip, x])\n",
    "\n",
    "# Custom Layer\n",
    "class BlockUniformityLayer(layers.Layer):\n",
    "    def __init__(self, block_size=10, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.block_size = block_size\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = tf.reshape(inputs, [-1, 5, self.block_size, 5, self.block_size, 1])\n",
    "        x = tf.reduce_mean(x, axis=[2, 4])  # -> (batch_size, 5, 5, 1)\n",
    "        x = tf.keras.activations.sigmoid(10.0 * (x - 0.5))  # Stable sigmoid\n",
    "        x = tf.repeat(x, self.block_size, axis=1)  # -> (batch_size, 50, 5, 1)\n",
    "        x = tf.repeat(x, self.block_size, axis=2)  # -> (batch_size, 50, 50, 1)\n",
    "        return x\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], 50, 50, 1)\n",
    "\n",
    "# Loss Functions\n",
    "def dice_loss(y_true, y_pred, smooth=1e-6):\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    y_pred = tf.cast(y_pred, tf.float32)\n",
    "    intersection = tf.reduce_sum(y_true * y_pred)\n",
    "    union = tf.reduce_sum(y_true) + tf.reduce_sum(y_pred)\n",
    "    dice = (2. * intersection + smooth) / (union + smooth)\n",
    "    return tf.clip_by_value(1 - dice, 0.0, 1.0)\n",
    "\n",
    "def weighted_binary_crossentropy(y_true, y_pred):\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    y_pred = tf.cast(y_pred, tf.float32)\n",
    "    bce = tf.keras.losses.binary_crossentropy(y_true, y_pred)\n",
    "    bce = tf.expand_dims(bce, axis=-1)\n",
    "    block_weight = tf.reduce_mean(tf.reshape(y_true, [-1, 5, block_size, 5, block_size, 1]), axis=[2, 4])\n",
    "    block_weight = tf.where(block_weight > 0.5, 2.0, 1.0)\n",
    "    block_weight = tf.repeat(block_weight, block_size, axis=1)\n",
    "    block_weight = tf.repeat(block_weight, block_size, axis=2)\n",
    "    weighted_bce = tf.reduce_mean(bce * block_weight)\n",
    "    return tf.clip_by_value(weighted_bce, 0.0, 1.0)\n",
    "\n",
    "def combined_loss(y_true, y_pred):\n",
    "    y_true = tf.cast(y_true, tf.float32)\n",
    "    y_pred = tf.cast(y_pred, tf.float32)\n",
    "    return 0.9 * weighted_binary_crossentropy(y_true, y_pred) + 0.1 * dice_loss(y_true, y_pred)\n",
    "\n",
    "# Fixed Gradient Debugging Callback\n",
    "class GradientNormCallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, train_gen):\n",
    "        super().__init__()\n",
    "        self.train_gen = train_gen\n",
    "        \n",
    "    def on_batch_end(self, batch, logs=None):\n",
    "        model = self.model\n",
    "        # Get a batch from the generator\n",
    "        (even, odd), y_true = next(iter(self.train_gen))\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = model([even, odd], training=True)\n",
    "            loss = model.loss(y_true, y_pred)\n",
    "        \n",
    "        loss_value = float(loss.numpy()) if tf.is_tensor(loss) else loss\n",
    "        print(f\"Batch {batch+1}, Loss Value: {loss_value:.4f}\")\n",
    "        \n",
    "        grads = tape.gradient(loss, model.trainable_variables)\n",
    "        grad_norms = [tf.norm(g).numpy() if g is not None else 0 for g in grads]\n",
    "        total_grad_norm = np.sqrt(sum([g**2 for g in grad_norms]))\n",
    "        print(f\"Batch {batch+1}, Gradient Norm: {total_grad_norm:.4f}\")\n",
    "\n",
    "# Complex Model\n",
    "def build_dual_input_model(input_shape, output_shape):\n",
    "    input_even = layers.Input(shape=input_shape, name=\"even_input\")\n",
    "    input_odd = layers.Input(shape=input_shape, name=\"odd_input\")\n",
    "\n",
    "    # Feature Engineering: Compute statistics\n",
    "    even_stats = layers.Lambda(lambda x: tf.reduce_mean(x, axis=[1, 2, 3], keepdims=True))(input_even)\n",
    "    odd_stats = layers.Lambda(lambda x: tf.reduce_mean(x, axis=[1, 2, 3], keepdims=True))(input_odd)\n",
    "\n",
    "    # Encoder\n",
    "    def encoder_block(x, filters):\n",
    "        x = layers.Conv2D(filters, 3, padding=\"same\", activation=\"relu\")(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.Conv2D(filters, 3, padding=\"same\", activation=\"relu\")(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        skip = x\n",
    "        x = layers.MaxPooling2D()(x)\n",
    "        return x, skip\n",
    "\n",
    "    # Even branch\n",
    "    e1, skip_e1 = encoder_block(input_even, 64)\n",
    "    e2, skip_e2 = encoder_block(e1, 128)\n",
    "    e3, skip_e3 = encoder_block(e2, 256)\n",
    "    e4 = layers.Conv2D(512, 3, padding=\"same\", activation=\"relu\")(e3)\n",
    "    e4 = layers.BatchNormalization()(e4)\n",
    "    e4 = layers.Dropout(0.3)(e4)\n",
    "\n",
    "    # Odd branch\n",
    "    o1, skip_o1 = encoder_block(input_odd, 64)\n",
    "    o2, skip_o2 = encoder_block(o1, 128)\n",
    "    o3, skip_o3 = encoder_block(o2, 256)\n",
    "    o4 = layers.Conv2D(512, 3, padding=\"same\", activation=\"relu\")(o3)\n",
    "    o4 = layers.BatchNormalization()(o4)\n",
    "    o4 = layers.Dropout(0.3)(o4)\n",
    "\n",
    "    # Transformer module\n",
    "    x = layers.Concatenate()([e4, o4])\n",
    "    x = layers.Conv2D(512, 1, padding=\"same\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Reshape((-1, 512))(x)\n",
    "    x = layers.MultiHeadAttention(num_heads=8, key_dim=64)(x, x)\n",
    "    x = layers.LayerNormalization()(x)\n",
    "    x = layers.Reshape((4, 4, 512))(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "\n",
    "    # Decoder with attention\n",
    "    x = layers.Conv2DTranspose(256, 3, strides=2, padding=\"same\", activation=\"relu\")(x)\n",
    "    x = attention_gate(x, layers.Concatenate()([skip_e3, skip_o3]), 256)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Conv2D(256, 3, padding=\"same\", activation=\"relu\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "\n",
    "    x = layers.Conv2DTranspose(128, 3, strides=2, padding=\"same\", activation=\"relu\")(x)\n",
    "    x = attention_gate(x, layers.Concatenate()([skip_e2, skip_o2]), 128)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Conv2D(128, 3, padding=\"same\", activation=\"relu\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "\n",
    "    x = layers.Conv2DTranspose(64, 3, strides=2, padding=\"same\", activation=\"relu\")(x)\n",
    "    x = attention_gate(x, layers.Concatenate()([skip_e1, skip_o1]), 64)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Conv2D(64, 3, padding=\"same\", activation=\"relu\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "\n",
    "    # Incorporate stats\n",
    "    stats = layers.Concatenate()([even_stats, odd_stats])\n",
    "    stats = layers.Dense(64, activation=\"relu\")(stats)\n",
    "    stats = layers.Reshape((1, 1, 64))(stats)\n",
    "    stats = layers.UpSampling2D(size=(32, 32))(stats)\n",
    "    x = layers.Concatenate()([x, stats])\n",
    "\n",
    "    x = layers.Conv2D(32, 3, padding=\"same\", activation=\"relu\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Conv2D(1, 3, padding=\"same\")(x)\n",
    "    x = layers.Resizing(output_shape[0], output_shape[1])(x)\n",
    "    x = BlockUniformityLayer(block_size=10)(x)\n",
    "    output = layers.Activation(\"sigmoid\")(x)\n",
    "\n",
    "    return models.Model(inputs=[input_even, input_odd], outputs=output)\n",
    "\n",
    "# Data Generator\n",
    "class ImagePairGenerator(Sequence):\n",
    "    def __init__(self, even_dir, odd_dir, out_dir, batch_size, input_shape, output_shape, pairs, augment=False):\n",
    "        self.even_dir = even_dir\n",
    "        self.odd_dir = odd_dir\n",
    "        self.out_dir = out_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.input_shape = input_shape\n",
    "        self.output_shape = output_shape\n",
    "        self.pairs = pairs\n",
    "        self.augment = augment\n",
    "        self.augmentation = KerasImageDataGenerator(\n",
    "            rotation_range=30,\n",
    "            width_shift_range=0.3,\n",
    "            height_shift_range=0.3,\n",
    "            zoom_range=0.3,\n",
    "            shear_range=0.2,\n",
    "            horizontal_flip=True,\n",
    "            vertical_flip=True,\n",
    "            fill_mode='constant',\n",
    "            cval=0.0\n",
    "        ) if augment else None\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.pairs) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch = self.pairs[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        even_batch = np.zeros((len(batch), *self.input_shape), dtype=np.float32)\n",
    "        odd_batch = np.zeros((len(batch), *self.input_shape), dtype=np.float32)\n",
    "        out_batch = np.zeros((len(batch), *self.output_shape), dtype=np.float32)\n",
    "\n",
    "        for i, (e_path, o_path, m_path) in enumerate(batch):\n",
    "            try:\n",
    "                # Load and preprocess even image\n",
    "                even = np.array(Image.open(e_path).convert('L').resize(self.input_shape[:2]))\n",
    "                even = np.expand_dims(even, axis=-1).astype(np.float32) / 255.0\n",
    "                \n",
    "                # Load and preprocess odd image\n",
    "                odd = np.array(Image.open(o_path).convert('L').resize(self.input_shape[:2]))\n",
    "                odd = np.expand_dims(odd, axis=-1).astype(np.float32) / 255.0\n",
    "                \n",
    "                # Load and preprocess output matrix\n",
    "                matrix_img = np.array(Image.open(m_path).convert('L').resize(self.output_shape[:2]))\n",
    "                matrix_img = matrix_img.astype(np.float32)\n",
    "                \n",
    "                # Binarize and block process the output\n",
    "                matrix_bin = (matrix_img > 50).astype(np.float32)  # Threshold at 50\n",
    "                matrix_bin = matrix_bin.reshape(5, block_size, 5, block_size).mean(axis=(1, 3))\n",
    "                matrix_bin = np.repeat(\n",
    "                    np.repeat(\n",
    "                        (matrix_bin > 0.5).astype(np.float32),  # Threshold blocks at 0.5\n",
    "                        block_size, \n",
    "                        axis=0\n",
    "                    ),\n",
    "                    block_size, \n",
    "                    axis=1\n",
    "                )\n",
    "                matrix_bin = np.expand_dims(matrix_bin, axis=-1)\n",
    "\n",
    "                # Data augmentation if enabled\n",
    "                if self.augment:\n",
    "                    seed = np.random.randint(1e6)\n",
    "                    even = self.augmentation.random_transform(even, seed=seed)\n",
    "                    odd = self.augmentation.random_transform(odd, seed=seed)\n",
    "                    \n",
    "                    # Add slight noise\n",
    "                    even += np.random.normal(0, 0.01, even.shape).astype(np.float32)\n",
    "                    odd += np.random.normal(0, 0.01, odd.shape).astype(np.float32)\n",
    "                    \n",
    "                    # Clip to valid range\n",
    "                    even = np.clip(even, 0, 1)\n",
    "                    odd = np.clip(odd, 0, 1)\n",
    "\n",
    "                # Add edge detection features\n",
    "                even_edges = cv2.Canny((even.squeeze() * 255).astype(np.uint8), 100, 200)\n",
    "                odd_edges = cv2.Canny((odd.squeeze() * 255).astype(np.uint8), 100, 200)\n",
    "                \n",
    "                # Stack original and edge channels\n",
    "                even_batch[i] = np.concatenate([\n",
    "                    even,\n",
    "                    np.expand_dims(even_edges, axis=-1).astype(np.float32) / 255.0\n",
    "                ], axis=-1)[..., :1]  # Keep only first channel for compatibility\n",
    "                \n",
    "                odd_batch[i] = np.concatenate([\n",
    "                    odd,\n",
    "                    np.expand_dims(odd_edges, axis=-1).astype(np.float32) / 255.0\n",
    "                ], axis=-1)[..., :1]  # Keep only first channel for compatibility\n",
    "                \n",
    "                out_batch[i] = matrix_bin\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing pair {e_path}, {o_path}, {m_path}: {str(e)}\")\n",
    "                # Fill with zeros if error occurs\n",
    "                even_batch[i] = np.zeros(self.input_shape, dtype=np.float32)\n",
    "                odd_batch[i] = np.zeros(self.input_shape, dtype=np.float32)\n",
    "                out_batch[i] = np.zeros(self.output_shape, dtype=np.float32)\n",
    "                continue\n",
    "\n",
    "        return (even_batch, odd_batch), out_batch\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        \"\"\"Shuffle pairs after each epoch\"\"\"\n",
    "        np.random.shuffle(self.pairs)\n",
    "\n",
    "# Prepare dataset\n",
    "def get_valid_pairs():\n",
    "    pairs = []\n",
    "    missing_ids = []\n",
    "    for f in os.listdir(output_dir):\n",
    "        if not f.endswith('.png'):\n",
    "            continue\n",
    "        base = f[:-4]\n",
    "        even_path = os.path.join(input_even_dir, f\"{base}_even.png\")\n",
    "        odd_path = os.path.join(input_odd_dir, f\"{base}_odd.png\")\n",
    "        out_path = os.path.join(output_dir, f)\n",
    "        if os.path.exists(even_path) and os.path.exists(odd_path):\n",
    "            pairs.append((even_path, odd_path, out_path))\n",
    "        else:\n",
    "            missing_ids.append(base)\n",
    "    if missing_ids:\n",
    "        print(f\"Warning: Missing input files for {len(missing_ids)} IDs: {', '.join(missing_ids[:10])}{', ...' if len(missing_ids) > 10 else ''}\")\n",
    "    return pairs\n",
    "\n",
    "# Inspect data\n",
    "def inspect_data(generator, num_samples=3):\n",
    "    (even_inputs, odd_inputs), outputs = generator[0]\n",
    "    print(\"Even input shape:\", even_inputs.shape)\n",
    "    print(\"Odd input shape:\", odd_inputs.shape)\n",
    "    print(\"Output shape:\", outputs.shape)\n",
    "    print(\"Unique values in ground truth:\", np.unique(outputs))\n",
    "    plt.figure(figsize=(15, 5 * num_samples))\n",
    "    for i in range(min(num_samples, len(even_inputs))):\n",
    "        plt.subplot(num_samples, 3, i * 3 + 1)\n",
    "        plt.imshow(even_inputs[i].squeeze(), cmap='gray')\n",
    "        plt.title(f'Even Input {i+1}')\n",
    "        plt.axis('off')\n",
    "        plt.subplot(num_samples, 3, i * 3 + 2)\n",
    "        plt.imshow(odd_inputs[i].squeeze(), cmap='gray')\n",
    "        plt.title(f'Odd Input {i+1}')\n",
    "        plt.axis('off')\n",
    "        plt.subplot(num_samples, 3, i * 3 + 3)\n",
    "        plt.imshow(outputs[i].squeeze(), cmap='gray', vmin=0, vmax=1)\n",
    "        plt.title(f'Ground Truth {i+1}')\n",
    "        plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize predictions\n",
    "def visualize_predictions(model, generator, num_samples=3):\n",
    "    (even_inputs, odd_inputs), true_outputs = generator[0]\n",
    "    predictions = model.predict([even_inputs, odd_inputs])\n",
    "    predictions = (predictions > 0.4).astype(np.float32)  # Adjusted threshold\n",
    "    print(f\"Prediction shape: {predictions.shape}, Ground truth shape: {true_outputs.shape}\")\n",
    "\n",
    "    plt.figure(figsize=(15, 5 * num_samples))\n",
    "    for i in range(min(num_samples, len(even_inputs))):\n",
    "        plt.subplot(num_samples, 4, i * 4 + 1)\n",
    "        plt.imshow(even_inputs[i].squeeze(), cmap='gray')\n",
    "        plt.title(f'Even Input {i+1}')\n",
    "        plt.axis('off')\n",
    "        plt.subplot(num_samples, 4, i * 4 + 2)\n",
    "        plt.imshow(odd_inputs[i].squeeze(), cmap='gray')\n",
    "        plt.title(f'Odd Input {i+1}')\n",
    "        plt.axis('off')\n",
    "        plt.subplot(num_samples, 4, i * 4 + 3)\n",
    "        plt.imshow(true_outputs[i].squeeze(), cmap='gray', vmin=0, vmax=1)\n",
    "        plt.title(f'Ground Truth {i+1}')\n",
    "        plt.axis('off')\n",
    "        plt.subplot(num_samples, 4, i * 4 + 4)\n",
    "        plt.imshow(predictions[i].squeeze(), cmap='gray', vmin=0, vmax=1)\n",
    "        plt.title(f'Predicted Output {i+1}')\n",
    "        plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    print(\"Unique values in predictions:\", np.unique(predictions))\n",
    "\n",
    "# Evaluate\n",
    "def evaluate_model(model, gen):\n",
    "    (e, o), y_true = gen[0]\n",
    "    y_pred = model.predict([e, o])\n",
    "    loss = model.evaluate(gen, verbose=0)\n",
    "    if isinstance(loss, list):\n",
    "        print(f\"Loss: {loss[0]:.4f}, Accuracy: {loss[1]:.4f}, IoU: {loss[2]:.4f}\")\n",
    "    else:\n",
    "        print(f\"Loss: {loss:.4f}\")\n",
    "\n",
    "    try:\n",
    "        y_true_tf = tf.convert_to_tensor(y_true, dtype=tf.float32)\n",
    "        y_pred_tf = tf.convert_to_tensor(y_pred, dtype=tf.float32)\n",
    "        ssim = tf.image.ssim(y_true_tf, y_pred_tf, max_val=1.0).numpy().mean()\n",
    "        psnr = tf.image.psnr(y_true_tf, y_pred_tf, max_val=1.0).numpy().mean()\n",
    "        print(f\"SSIM: {ssim:.4f}, PSNR: {psnr:.2f} dB\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error calculating metrics: {str(e)}\")\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    pairs = get_valid_pairs()\n",
    "    print(f\"Found {len(pairs)} valid input-output pairs.\")\n",
    "    np.random.shuffle(pairs)\n",
    "    split = int(0.8 * len(pairs))\n",
    "    train_pairs = pairs[:split]\n",
    "    val_pairs = pairs[split:]\n",
    "\n",
    "    train_gen = ImagePairGenerator(input_even_dir, input_odd_dir, output_dir, batch_size, input_shape, output_shape, train_pairs, augment=True)\n",
    "    val_gen = ImagePairGenerator(input_even_dir, input_odd_dir, output_dir, batch_size, input_shape, output_shape, val_pairs)\n",
    "\n",
    "    # Inspect data before training\n",
    "    print(\"Inspecting training data samples...\")\n",
    "    inspect_data(train_gen)\n",
    "\n",
    "    # Build and compile model\n",
    "    model = build_dual_input_model(input_shape, output_shape)\n",
    "    model.compile(optimizer=optimizers.Adam(learning_rate, clipnorm=1.0),\n",
    "                 loss=combined_loss,\n",
    "                 metrics=['accuracy', tf.keras.metrics.BinaryIoU(target_class_ids=[1])])\n",
    "\n",
    "    callbacks = [\n",
    "        tf.keras.callbacks.ModelCheckpoint('best_model.keras', monitor='val_loss', save_best_only=True),\n",
    "        tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=50, restore_best_weights=True),\n",
    "        tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10),\n",
    "        GradientNormCallback(train_gen)\n",
    "    ]\n",
    "\n",
    "    # Train the model\n",
    "    print(\"Starting training...\")\n",
    "    history = model.fit(\n",
    "        train_gen,\n",
    "        validation_data=val_gen,\n",
    "        epochs=epochs,\n",
    "        callbacks=callbacks\n",
    "    )\n",
    "    model.save(\"image_translation_model_final.keras\")\n",
    "\n",
    "    # Plot history\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.plot(history.history['loss'], label='Train Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Val Loss')\n",
    "    plt.title('Loss')\n",
    "    plt.legend()\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.plot(history.history['accuracy'], label='Train Acc')\n",
    "    plt.plot(history.history['val_accuracy'], label='Val Acc')\n",
    "    plt.title('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.plot(history.history['binary_io_u'], label='Train IoU')\n",
    "    plt.plot(history.history['val_binary_io_u'], label='Val IoU')\n",
    "    plt.title('IoU')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Evaluate and visualize\n",
    "    print(\"Evaluating model...\")\n",
    "    evaluate_model(model, val_gen)\n",
    "    print(\"Visualizing predictions...\")\n",
    "    visualize_predictions(model, val_gen)\n",
    "\n",
    "    # Print TensorFlow version and device info\n",
    "    print(\"TensorFlow Version:\", tf.__version__)\n",
    "    print(\"Devices:\", tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1887396-d221-4365-b37b-578fa6fd7032",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
