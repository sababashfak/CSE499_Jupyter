{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a73b1e39-232b-4ce3-913d-855fcc58c197",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import ViTModel, ViTConfig\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28461ee3-53f6-46fe-bcd2-843b39186c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "data_dir = r\"F:/Education/NSU/CSE/CSE499/Implementation/Image Data\"\n",
    "gray_image_dir = os.path.join(data_dir, \"gray_image\")\n",
    "even_dir = os.path.join(gray_image_dir, \"even_images\")\n",
    "odd_dir = os.path.join(gray_image_dir, \"odd_images\")\n",
    "matrix_dir = os.path.join(data_dir, \"matrix\")\n",
    "image_size = (50, 50)\n",
    "batch_size = 8\n",
    "num_epochs = 10\n",
    "learning_rate = 2e-5\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2bf4191-6d0c-439a-b0b6-ca144d80b5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Dataset\n",
    "class ImagePairDataset(Dataset):\n",
    "    def __init__(self, even_dir, odd_dir, matrix_dir, transform=None):\n",
    "        self.even_dir = even_dir\n",
    "        self.odd_dir = odd_dir\n",
    "        self.matrix_dir = matrix_dir\n",
    "        self.transform = transform\n",
    "        \n",
    "        # Get list of image IDs from even_images (assuming same IDs in odd_images)\n",
    "        even_files = [f for f in os.listdir(even_dir) if f.endswith(\"_even.png\")]\n",
    "        self.ids = []\n",
    "        for f in even_files:\n",
    "            match = re.match(r\"(\\d+)_even\\.png\", f)\n",
    "            if match:\n",
    "                id_str = match.group(1)\n",
    "                odd_file = f\"{id_str}_odd.png\"\n",
    "                matrix_file = f\"{id_str}.png\"\n",
    "                if (os.path.exists(os.path.join(odd_dir, odd_file)) and \n",
    "                    os.path.exists(os.path.join(matrix_dir, matrix_file))):\n",
    "                    self.ids.append(id_str)\n",
    "        \n",
    "        print(f\"Found {len(self.ids)} matching image pairs\")\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        id_str = self.ids[idx]\n",
    "        \n",
    "        # Load even and odd images\n",
    "        even_img = Image.open(os.path.join(self.even_dir, f\"{id_str}_even.png\")).convert(\"L\")\n",
    "        odd_img = Image.open(os.path.join(self.odd_dir, f\"{id_str}_odd.png\")).convert(\"L\")\n",
    "        matrix_img = Image.open(os.path.join(self.matrix_dir, f\"{id_str}.png\")).convert(\"L\")\n",
    "        \n",
    "        # Convert to numpy and normalize\n",
    "        even_img = np.array(even_img, dtype=np.float32) / 255.0\n",
    "        odd_img = np.array(odd_img, dtype=np.float32) / 255.0\n",
    "        matrix_img = np.array(matrix_img, dtype=np.float32) / 255.0\n",
    "        \n",
    "        # Combine even and odd into 2-channel input\n",
    "        input_img = np.stack([even_img, odd_img], axis=2)  # Shape: (50, 50, 2)\n",
    "        \n",
    "        # Convert to torch tensors\n",
    "        input_img = torch.tensor(input_img).permute(2, 0, 1)  # Shape: (2, 50, 50)\n",
    "        matrix_img = torch.tensor(matrix_img).flatten()  # Shape: (2500,)\n",
    "        \n",
    "        return input_img, matrix_img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b0e6716-b465-43a4-bb26-0557c1cf12bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom ViT Model for Regression\n",
    "class ViTForImageRegression(nn.Module):\n",
    "    def __init__(self, model_name=\"google/vit-base-patch16-224\"):\n",
    "        super(ViTForImageRegression, self).__init__()\n",
    "        self.vit = ViTModel.from_pretrained(model_name)\n",
    "        self.vit.config.patch_size = 10  # Adjust for 50x50 input\n",
    "        self.regressor = nn.Linear(self.vit.config.hidden_size, 50 * 50)  # Output 50x50 pixels\n",
    "    \n",
    "    def forward(self, pixel_values):\n",
    "        outputs = self.vit(pixel_values=pixel_values)\n",
    "        pooled_output = outputs.pooler_output  # [CLS] token\n",
    "        return self.regressor(pooled_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5fa472bc-8bdb-4ac5-8374-d8e86a0f081c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute SSIM for a batch\n",
    "def compute_batch_ssim(preds, targets, size=(50, 50)):\n",
    "    ssim_scores = []\n",
    "    preds = preds.cpu().numpy().reshape(-1, *size)\n",
    "    targets = targets.cpu().numpy().reshape(-1, *size)\n",
    "    for i in range(len(preds)):\n",
    "        score = ssim(preds[i], targets[i], data_range=1.0)\n",
    "        ssim_scores.append(score)\n",
    "    return np.mean(ssim_scores)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9fa10a00-b20e-49c3-be61-55584d7e14d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and Evaluation\n",
    "def train_model():\n",
    "    # Initialize dataset and dataloader\n",
    "    dataset = ImagePairDataset(even_dir, odd_dir, matrix_dir)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    # Initialize model\n",
    "    model = ViTForImageRegression().to(device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_mse = 0.0\n",
    "        train_mae = 0.0\n",
    "        train_ssim = 0.0\n",
    "        num_batches = 0\n",
    "        \n",
    "        for inputs, targets in tqdm(dataloader, desc=f\"Epoch {epoch + 1}/{num_epochs}\"):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            \n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Compute metrics\n",
    "            train_loss += loss.item()\n",
    "            preds = outputs.detach()\n",
    "            train_mse += mean_squared_error(targets.cpu().numpy(), preds.cpu().numpy())\n",
    "            train_mae += mean_absolute_error(targets.cpu().numpy(), preds.cpu().numpy())\n",
    "            train_ssim += compute_batch_ssim(preds, targets)\n",
    "            num_batches += 1\n",
    "        \n",
    "        # Average metrics\n",
    "        train_loss /= num_batches\n",
    "        train_mse /= num_batches\n",
    "        train_mae /= num_batches\n",
    "        train_ssim /= num_batches\n",
    "        \n",
    "        print(f\"Epoch {epoch + 1}:\")\n",
    "        print(f\"  Train Loss: {train_loss:.6f}\")\n",
    "        print(f\"  Train MSE: {train_mse:.6f}\")\n",
    "        print(f\"  Train MAE: {train_mae:.6f}\")\n",
    "        print(f\"  Train SSIM: {train_ssim:.4f}\")\n",
    "        \n",
    "        # Visualize a sample prediction\n",
    "        if epoch == num_epochs - 1:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                sample_inputs, sample_targets = next(iter(dataloader))\n",
    "                sample_inputs = sample_inputs[:2].to(device)\n",
    "                sample_preds = model(sample_inputs)\n",
    "                \n",
    "                fig, axes = plt.subplots(2, 2, figsize=(10, 10))\n",
    "                for i in range(2):\n",
    "                    pred_img = sample_preds[i].cpu().numpy().reshape(image_size)\n",
    "                    target_img = sample_targets[i].cpu().numpy().reshape(image_size)\n",
    "                    \n",
    "                    axes[i, 0].imshow(pred_img, cmap='gray')\n",
    "                    axes[i, 0].set_title(f\"Predicted Image {i + 1}\")\n",
    "                    axes[i, 0].axis('off')\n",
    "                    \n",
    "                    axes[i, 1].imshow(target_img, cmap='gray')\n",
    "                    axes[i, 1].set_title(f\"Target Image {i + 1}\")\n",
    "                    axes[i, 1].axis('off')\n",
    "                \n",
    "                plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0eddba03-ca40-4d0a-b52a-32f391ff42a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1161 matching image pairs\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff37bdfe41e7461a89e9b84c0137a014",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/69.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\User\\.cache\\huggingface\\hub\\models--google--vit-base-patch16-224. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4d6cd88e8004ee1b3da18a52d602a36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)dda6f3b8b58e7256e8f44b4ea6aa9696162ccb5d:   0%|          | 0.00/346M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch 1/10:   0%|                                                                              | 0/146 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Make sure that the channel dimension of the pixel values match with the one set in the configuration. Expected 3 but got 2.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m----> 2\u001b[0m     train_model()\n",
      "Cell \u001b[1;32mIn[6], line 25\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m()\u001b[0m\n\u001b[0;32m     22\u001b[0m inputs, targets \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(device), targets\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(inputs)\n\u001b[0;32m     26\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, targets)\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# Backward pass\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[4], line 10\u001b[0m, in \u001b[0;36mViTForImageRegression.forward\u001b[1;34m(self, pixel_values)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, pixel_values):\n\u001b[1;32m---> 10\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvit(pixel_values\u001b[38;5;241m=\u001b[39mpixel_values)\n\u001b[0;32m     11\u001b[0m     pooled_output \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mpooler_output  \u001b[38;5;66;03m# [CLS] token\u001b[39;00m\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mregressor(pooled_output)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\models\\vit\\modeling_vit.py:550\u001b[0m, in \u001b[0;36mViTModel.forward\u001b[1;34m(self, pixel_values, bool_masked_pos, head_mask, output_attentions, output_hidden_states, interpolate_pos_encoding, return_dict)\u001b[0m\n\u001b[0;32m    547\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pixel_values\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m expected_dtype:\n\u001b[0;32m    548\u001b[0m     pixel_values \u001b[38;5;241m=\u001b[39m pixel_values\u001b[38;5;241m.\u001b[39mto(expected_dtype)\n\u001b[1;32m--> 550\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(\n\u001b[0;32m    551\u001b[0m     pixel_values, bool_masked_pos\u001b[38;5;241m=\u001b[39mbool_masked_pos, interpolate_pos_encoding\u001b[38;5;241m=\u001b[39minterpolate_pos_encoding\n\u001b[0;32m    552\u001b[0m )\n\u001b[0;32m    554\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(\n\u001b[0;32m    555\u001b[0m     embedding_output,\n\u001b[0;32m    556\u001b[0m     head_mask\u001b[38;5;241m=\u001b[39mhead_mask,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    559\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[0;32m    560\u001b[0m )\n\u001b[0;32m    561\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\models\\vit\\modeling_vit.py:106\u001b[0m, in \u001b[0;36mViTEmbeddings.forward\u001b[1;34m(self, pixel_values, bool_masked_pos, interpolate_pos_encoding)\u001b[0m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m    100\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    101\u001b[0m     pixel_values: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m    102\u001b[0m     bool_masked_pos: Optional[torch\u001b[38;5;241m.\u001b[39mBoolTensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    103\u001b[0m     interpolate_pos_encoding: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    104\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m    105\u001b[0m     batch_size, num_channels, height, width \u001b[38;5;241m=\u001b[39m pixel_values\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m--> 106\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpatch_embeddings(pixel_values, interpolate_pos_encoding\u001b[38;5;241m=\u001b[39minterpolate_pos_encoding)\n\u001b[0;32m    108\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m bool_masked_pos \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    109\u001b[0m         seq_length \u001b[38;5;241m=\u001b[39m embeddings\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\models\\vit\\modeling_vit.py:155\u001b[0m, in \u001b[0;36mViTPatchEmbeddings.forward\u001b[1;34m(self, pixel_values, interpolate_pos_encoding)\u001b[0m\n\u001b[0;32m    153\u001b[0m batch_size, num_channels, height, width \u001b[38;5;241m=\u001b[39m pixel_values\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m num_channels \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_channels:\n\u001b[1;32m--> 155\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    156\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMake sure that the channel dimension of the pixel values match with the one set in the configuration.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    157\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m Expected \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_channels\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_channels\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    158\u001b[0m     )\n\u001b[0;32m    159\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m interpolate_pos_encoding:\n\u001b[0;32m    160\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m height \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_size[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;129;01mor\u001b[39;00m width \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_size[\u001b[38;5;241m1\u001b[39m]:\n",
      "\u001b[1;31mValueError\u001b[0m: Make sure that the channel dimension of the pixel values match with the one set in the configuration. Expected 3 but got 2."
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8732066e-8d63-4b5d-8854-c185f37e5f82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\user\\anaconda3\\lib\\site-packages (2.7.1)\n",
      "Requirement already satisfied: transformers in c:\\users\\user\\anaconda3\\lib\\site-packages (4.52.4)\n",
      "Requirement already satisfied: pillow in c:\\users\\user\\anaconda3\\lib\\site-packages (10.4.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\user\\anaconda3\\lib\\site-packages (1.26.4)\n",
      "Requirement already satisfied: scikit-image in c:\\users\\user\\anaconda3\\lib\\site-packages (0.24.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\user\\anaconda3\\lib\\site-packages (1.5.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\user\\anaconda3\\lib\\site-packages (4.66.5)\n",
      "Requirement already satisfied: filelock in c:\\users\\user\\anaconda3\\lib\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\user\\anaconda3\\lib\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\user\\anaconda3\\lib\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\user\\anaconda3\\lib\\site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\user\\anaconda3\\lib\\site-packages (from torch) (75.1.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from transformers) (0.33.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\user\\anaconda3\\lib\\site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in c:\\users\\user\\anaconda3\\lib\\site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\user\\anaconda3\\lib\\site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\user\\anaconda3\\lib\\site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: scipy>=1.9 in c:\\users\\user\\anaconda3\\lib\\site-packages (from scikit-image) (1.13.1)\n",
      "Requirement already satisfied: imageio>=2.33 in c:\\users\\user\\anaconda3\\lib\\site-packages (from scikit-image) (2.33.1)\n",
      "Requirement already satisfied: tifffile>=2022.8.12 in c:\\users\\user\\anaconda3\\lib\\site-packages (from scikit-image) (2023.4.12)\n",
      "Requirement already satisfied: lazy-loader>=0.4 in c:\\users\\user\\anaconda3\\lib\\site-packages (from scikit-image) (0.4)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\anaconda3\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\anaconda3\\lib\\site-packages (from requests->transformers) (2024.8.30)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch transformers pillow numpy scikit-image scikit-learn tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40ffa381-b9de-4ccf-9305-f5a7fe00cd05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting hf_xet\n",
      "  Downloading hf_xet-1.1.5-cp37-abi3-win_amd64.whl.metadata (883 bytes)\n",
      "Downloading hf_xet-1.1.5-cp37-abi3-win_amd64.whl (2.7 MB)\n",
      "   ---------------------------------------- 0.0/2.7 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 0.3/2.7 MB ? eta -:--:--\n",
      "   -------------------------- ------------- 1.8/2.7 MB 6.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.7/2.7 MB 7.9 MB/s eta 0:00:00\n",
      "Installing collected packages: hf_xet\n",
      "Successfully installed hf_xet-1.1.5\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install hf_xet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c12ea18-1471-4e69-98c5-98b21092e757",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1161 matching image pairs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   0%|                                                                              | 0/116 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input image size (32*32) doesn't match model (50*50).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 209\u001b[0m\n\u001b[0;32m    206\u001b[0m                 plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[0;32m    208\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 209\u001b[0m     train_model()\n",
      "Cell \u001b[1;32mIn[1], line 135\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m()\u001b[0m\n\u001b[0;32m    132\u001b[0m inputs, targets \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(device), targets\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m    134\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[1;32m--> 135\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(inputs)\n\u001b[0;32m    136\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, targets)\n\u001b[0;32m    138\u001b[0m \u001b[38;5;66;03m# Backward pass\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[1], line 92\u001b[0m, in \u001b[0;36mViTForImageRegression.forward\u001b[1;34m(self, pixel_values)\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, pixel_values):\n\u001b[1;32m---> 92\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvit(pixel_values\u001b[38;5;241m=\u001b[39mpixel_values)\n\u001b[0;32m     93\u001b[0m     pooled_output \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mpooler_output\n\u001b[0;32m     94\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mregressor(pooled_output)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\models\\vit\\modeling_vit.py:550\u001b[0m, in \u001b[0;36mViTModel.forward\u001b[1;34m(self, pixel_values, bool_masked_pos, head_mask, output_attentions, output_hidden_states, interpolate_pos_encoding, return_dict)\u001b[0m\n\u001b[0;32m    547\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pixel_values\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m expected_dtype:\n\u001b[0;32m    548\u001b[0m     pixel_values \u001b[38;5;241m=\u001b[39m pixel_values\u001b[38;5;241m.\u001b[39mto(expected_dtype)\n\u001b[1;32m--> 550\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(\n\u001b[0;32m    551\u001b[0m     pixel_values, bool_masked_pos\u001b[38;5;241m=\u001b[39mbool_masked_pos, interpolate_pos_encoding\u001b[38;5;241m=\u001b[39minterpolate_pos_encoding\n\u001b[0;32m    552\u001b[0m )\n\u001b[0;32m    554\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(\n\u001b[0;32m    555\u001b[0m     embedding_output,\n\u001b[0;32m    556\u001b[0m     head_mask\u001b[38;5;241m=\u001b[39mhead_mask,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    559\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[0;32m    560\u001b[0m )\n\u001b[0;32m    561\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\models\\vit\\modeling_vit.py:106\u001b[0m, in \u001b[0;36mViTEmbeddings.forward\u001b[1;34m(self, pixel_values, bool_masked_pos, interpolate_pos_encoding)\u001b[0m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m    100\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    101\u001b[0m     pixel_values: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m    102\u001b[0m     bool_masked_pos: Optional[torch\u001b[38;5;241m.\u001b[39mBoolTensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    103\u001b[0m     interpolate_pos_encoding: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    104\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m    105\u001b[0m     batch_size, num_channels, height, width \u001b[38;5;241m=\u001b[39m pixel_values\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m--> 106\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpatch_embeddings(pixel_values, interpolate_pos_encoding\u001b[38;5;241m=\u001b[39minterpolate_pos_encoding)\n\u001b[0;32m    108\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m bool_masked_pos \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    109\u001b[0m         seq_length \u001b[38;5;241m=\u001b[39m embeddings\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\models\\vit\\modeling_vit.py:161\u001b[0m, in \u001b[0;36mViTPatchEmbeddings.forward\u001b[1;34m(self, pixel_values, interpolate_pos_encoding)\u001b[0m\n\u001b[0;32m    159\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m interpolate_pos_encoding:\n\u001b[0;32m    160\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m height \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_size[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;129;01mor\u001b[39;00m width \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_size[\u001b[38;5;241m1\u001b[39m]:\n\u001b[1;32m--> 161\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    162\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInput image size (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mheight\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m*\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mwidth\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt match model\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    163\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_size[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m*\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_size[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    164\u001b[0m         )\n\u001b[0;32m    165\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprojection(pixel_values)\u001b[38;5;241m.\u001b[39mflatten(\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m    166\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m embeddings\n",
      "\u001b[1;31mValueError\u001b[0m: Input image size (32*32) doesn't match model (50*50)."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from transformers import ViTModel, ViTConfig\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Configuration\n",
    "data_dir = r\"F:/Education/NSU/CSE/CSE499/Implementation/Image Data\"\n",
    "gray_image_dir = os.path.join(data_dir, \"gray_image\")\n",
    "even_dir = os.path.join(gray_image_dir, \"even_images\")\n",
    "odd_dir = os.path.join(gray_image_dir, \"odd_images\")\n",
    "matrix_dir = os.path.join(data_dir, \"matrix\")\n",
    "image_size = (50, 50)\n",
    "batch_size = 8\n",
    "num_epochs = 10\n",
    "learning_rate = 2e-5\n",
    "train_split = 0.8  # 80% train, 20% test\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Custom Dataset\n",
    "class ImagePairDataset(Dataset):\n",
    "    def __init__(self, even_dir, odd_dir, matrix_dir):\n",
    "        self.even_dir = even_dir\n",
    "        self.odd_dir = odd_dir\n",
    "        self.matrix_dir = matrix_dir\n",
    "        \n",
    "        # Get list of image IDs\n",
    "        even_files = [f for f in os.listdir(even_dir) if f.endswith(\"_even.png\")]\n",
    "        self.ids = []\n",
    "        for f in even_files:\n",
    "            match = re.match(r\"(\\d+)_even\\.png\", f)\n",
    "            if match:\n",
    "                id_str = match.group(1)\n",
    "                odd_file = f\"{id_str}_odd.png\"\n",
    "                matrix_file = f\"{id_str}.png\"\n",
    "                if (os.path.exists(os.path.join(odd_dir, odd_file)) and \n",
    "                    os.path.exists(os.path.join(matrix_dir, matrix_file))):\n",
    "                    self.ids.append(id_str)\n",
    "        \n",
    "        print(f\"Found {len(self.ids)} matching image pairs\")\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        id_str = self.ids[idx]\n",
    "        \n",
    "        # Load images\n",
    "        even_img = Image.open(os.path.join(self.even_dir, f\"{id_str}_even.png\")).convert(\"L\")\n",
    "        odd_img = Image.open(os.path.join(self.odd_dir, f\"{id_str}_odd.png\")).convert(\"L\")\n",
    "        matrix_img = Image.open(os.path.join(self.matrix_dir, f\"{id_str}.png\")).convert(\"L\")\n",
    "        \n",
    "        # Convert to numpy and normalize\n",
    "        even_img = np.array(even_img, dtype=np.float32) / 255.0\n",
    "        odd_img = np.array(odd_img, dtype=np.float32) / 255.0\n",
    "        matrix_img = np.array(matrix_img, dtype=np.float32) / 255.0\n",
    "        \n",
    "        # Combine into 2-channel input\n",
    "        input_img = np.stack([even_img, odd_img], axis=2)  # Shape: (50, 50, 2)\n",
    "        \n",
    "        # Convert to torch tensors\n",
    "        input_img = torch.tensor(input_img).permute(2, 0, 1)  # Shape: (2, 50, 50)\n",
    "        matrix_img = torch.tensor(matrix_img).flatten()  # Shape: (2500,)\n",
    "        \n",
    "        return input_img, matrix_img\n",
    "\n",
    "# Custom ViT Model for Regression\n",
    "class ViTForImageRegression(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ViTForImageRegression, self).__init__()\n",
    "        # Custom ViT configuration for 2-channel 50x50 images\n",
    "        config = ViTConfig(\n",
    "            image_size=50,\n",
    "            patch_size=10,  # 50x50 divides evenly into 5x5 patches\n",
    "            num_channels=2,  # 2-channel input\n",
    "            hidden_size=768,\n",
    "            num_hidden_layers=12,\n",
    "            num_attention_heads=12,\n",
    "            intermediate_size=3072\n",
    "        )\n",
    "        self.vit = ViTModel(config)\n",
    "        self.regressor = nn.Linear(config.hidden_size, 50 * 50)  # Output 50x50 pixels\n",
    "    \n",
    "    def forward(self, pixel_values):\n",
    "        outputs = self.vit(pixel_values=pixel_values)\n",
    "        pooled_output = outputs.pooler_output\n",
    "        return self.regressor(pooled_output)\n",
    "\n",
    "# Compute SSIM for a batch\n",
    "def compute_batch_ssim(preds, targets, size=(50, 50)):\n",
    "    ssim_scores = []\n",
    "    preds = preds.cpu().numpy().reshape(-1, *size)\n",
    "    targets = targets.cpu().numpy().reshape(-1, *size)\n",
    "    for i in range(len(preds)):\n",
    "        score = ssim(preds[i], targets[i], data_range=1.0)\n",
    "        ssim_scores.append(score)\n",
    "    return np.mean(ssim_scores)\n",
    "\n",
    "# Training and Evaluation\n",
    "def train_model():\n",
    "    # Initialize dataset and split\n",
    "    dataset = ImagePairDataset(even_dir, odd_dir, matrix_dir)\n",
    "    train_size = int(train_split * len(dataset))\n",
    "    test_size = len(dataset) - train_size\n",
    "    train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    # Initialize model\n",
    "    model = ViTForImageRegression().to(device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_mse = 0.0\n",
    "        train_mae = 0.0\n",
    "        train_ssim = 0.0\n",
    "        num_batches = 0\n",
    "        \n",
    "        for inputs, targets in tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{num_epochs}\"):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            \n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Compute metrics\n",
    "            train_loss += loss.item()\n",
    "            preds = outputs.detach()\n",
    "            train_mse += mean_squared_error(targets.cpu().numpy(), preds.cpu().numpy())\n",
    "            train_mae += mean_absolute_error(targets.cpu().numpy(), preds.cpu().numpy())\n",
    "            train_ssim += compute_batch_ssim(preds, targets)\n",
    "            num_batches += 1\n",
    "        \n",
    "        # Average metrics\n",
    "        train_loss /= num_batches\n",
    "        train_mse /= num_batches\n",
    "        train_mae /= num_batches\n",
    "        train_ssim /= num_batches\n",
    "        \n",
    "        # Evaluate on test set\n",
    "        model.eval()\n",
    "        test_loss = 0.0\n",
    "        test_mse = 0.0\n",
    "        test_mae = 0.0\n",
    "        test_ssim = 0.0\n",
    "        num_test_batches = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in test_loader:\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                \n",
    "                test_loss += loss.item()\n",
    "                test_mse += mean_squared_error(targets.cpu().numpy(), outputs.cpu().numpy())\n",
    "                test_mae += mean_absolute_error(targets.cpu().numpy(), outputs.cpu().numpy())\n",
    "                test_ssim += compute_batch_ssim(outputs, targets)\n",
    "                num_test_batches += 1\n",
    "        \n",
    "        test_loss /= num_test_batches\n",
    "        test_mse /= num_test_batches\n",
    "        test_mae /= num_test_batches\n",
    "        test_ssim /= num_test_batches\n",
    "        \n",
    "        print(f\"Epoch {epoch + 1}:\")\n",
    "        print(f\"  Train Loss: {train_loss:.6f}, MSE: {train_mse:.6f}, MAE: {train_mae:.6f}, SSIM: {train_ssim:.4f}\")\n",
    "        print(f\"  Test Loss: {test_loss:.6f}, MSE: {test_mse:.6f}, MAE: {test_mae:.6f}, SSIM: {test_ssim:.4f}\")\n",
    "        \n",
    "        # Visualize sample predictions\n",
    "        if epoch == num_epochs - 1:\n",
    "            with torch.no_grad():\n",
    "                sample_inputs, sample_targets = next(iter(test_loader))\n",
    "                sample_inputs = sample_inputs[:2].to(device)\n",
    "                sample_preds = model(sample_inputs)\n",
    "                \n",
    "                fig, axes = plt.subplots(2, 2, figsize=(10, 10))\n",
    "                for i in range(2):\n",
    "                    pred_img = sample_preds[i].cpu().numpy().reshape(image_size)\n",
    "                    target_img = sample_targets[i].cpu().numpy().reshape(image_size)\n",
    "                    \n",
    "                    axes[i, 0].imshow(pred_img, cmap='gray')\n",
    "                    axes[i, 0].set_title(f\"Predicted Image {i + 1}\")\n",
    "                    axes[i, 0].axis('off')\n",
    "                    \n",
    "                    axes[i, 1].imshow(target_img, cmap='gray')\n",
    "                    axes[i, 1].set_title(f\"Target Image {i + 1}\")\n",
    "                    axes[i, 1].axis('off')\n",
    "                \n",
    "                plt.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "414541be-7bcd-456c-91a5-89ea99906a29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1161 matching image pairs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|████████████████████████████████████████████████████████████████████| 116/116 [00:54<00:00,  2.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:\n",
      "  Train Loss: 0.312746, MSE: 0.312746, MAE: 0.508686, SSIM: 0.0189\n",
      "  Test Loss: 0.252115, MSE: 0.252115, MAE: 0.499534, SSIM: 0.0711\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|████████████████████████████████████████████████████████████████████| 116/116 [00:47<00:00,  2.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2:\n",
      "  Train Loss: 0.250286, MSE: 0.250286, MAE: 0.497794, SSIM: 0.0835\n",
      "  Test Loss: 0.249805, MSE: 0.249805, MAE: 0.496010, SSIM: 0.0748\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|████████████████████████████████████████████████████████████████████| 116/116 [00:47<00:00,  2.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3:\n",
      "  Train Loss: 0.248170, MSE: 0.248170, MAE: 0.493898, SSIM: 0.0861\n",
      "  Test Loss: 0.248273, MSE: 0.248273, MAE: 0.493984, SSIM: 0.0933\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|████████████████████████████████████████████████████████████████████| 116/116 [00:46<00:00,  2.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4:\n",
      "  Train Loss: 0.247287, MSE: 0.247287, MAE: 0.492253, SSIM: 0.0951\n",
      "  Test Loss: 0.248706, MSE: 0.248706, MAE: 0.493606, SSIM: 0.0895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 100%|████████████████████████████████████████████████████████████████████| 116/116 [00:46<00:00,  2.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5:\n",
      "  Train Loss: 0.247486, MSE: 0.247486, MAE: 0.492066, SSIM: 0.0964\n",
      "  Test Loss: 0.247491, MSE: 0.247491, MAE: 0.493641, SSIM: 0.1028\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: 100%|████████████████████████████████████████████████████████████████████| 116/116 [00:47<00:00,  2.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6:\n",
      "  Train Loss: 0.246874, MSE: 0.246874, MAE: 0.491326, SSIM: 0.1019\n",
      "  Test Loss: 0.247176, MSE: 0.247176, MAE: 0.491491, SSIM: 0.1067\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: 100%|████████████████████████████████████████████████████████████████████| 116/116 [00:46<00:00,  2.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7:\n",
      "  Train Loss: 0.246818, MSE: 0.246818, MAE: 0.491049, SSIM: 0.1055\n",
      "  Test Loss: 0.247679, MSE: 0.247679, MAE: 0.491856, SSIM: 0.1177\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: 100%|████████████████████████████████████████████████████████████████████| 116/116 [00:46<00:00,  2.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8:\n",
      "  Train Loss: 0.246391, MSE: 0.246391, MAE: 0.490368, SSIM: 0.1072\n",
      "  Test Loss: 0.248773, MSE: 0.248773, MAE: 0.492344, SSIM: 0.0918\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10: 100%|████████████████████████████████████████████████████████████████████| 116/116 [00:46<00:00,  2.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9:\n",
      "  Train Loss: 0.246246, MSE: 0.246246, MAE: 0.490015, SSIM: 0.1094\n",
      "  Test Loss: 0.247747, MSE: 0.247747, MAE: 0.492391, SSIM: 0.1201\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|███████████████████████████████████████████████████████████████████| 116/116 [00:46<00:00,  2.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10:\n",
      "  Train Loss: 0.245807, MSE: 0.245807, MAE: 0.489832, SSIM: 0.1140\n",
      "  Test Loss: 0.246949, MSE: 0.246949, MAE: 0.492189, SSIM: 0.1146\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x1000 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from transformers import ViTModel, ViTConfig\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Configuration\n",
    "data_dir = r\"F:/Education/NSU/CSE/CSE499/Implementation/Image Data\"\n",
    "gray_image_dir = os.path.join(data_dir, \"gray_image\")\n",
    "even_dir = os.path.join(gray_image_dir, \"even_images\")\n",
    "odd_dir = os.path.join(gray_image_dir, \"odd_images\")\n",
    "matrix_dir = os.path.join(data_dir, \"matrix\")\n",
    "image_size = (50, 50)  # Target size for all images\n",
    "batch_size = 8\n",
    "num_epochs = 10\n",
    "learning_rate = 2e-5\n",
    "train_split = 0.8  # 80% train, 20% test\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Custom Dataset\n",
    "class ImagePairDataset(Dataset):\n",
    "    def __init__(self, even_dir, odd_dir, matrix_dir, target_size=(50, 50)):\n",
    "        self.even_dir = even_dir\n",
    "        self.odd_dir = odd_dir\n",
    "        self.matrix_dir = matrix_dir\n",
    "        self.target_size = target_size\n",
    "        \n",
    "        # Get list of image IDs\n",
    "        even_files = [f for f in os.listdir(even_dir) if f.endswith(\"_even.png\")]\n",
    "        self.ids = []\n",
    "        for f in even_files:\n",
    "            match = re.match(r\"(\\d+)_even\\.png\", f)\n",
    "            if match:\n",
    "                id_str = match.group(1)\n",
    "                odd_file = f\"{id_str}_odd.png\"\n",
    "                matrix_file = f\"{id_str}.png\"\n",
    "                if (os.path.exists(os.path.join(odd_dir, odd_file)) and \n",
    "                    os.path.exists(os.path.join(matrix_dir, matrix_file))):\n",
    "                    self.ids.append(id_str)\n",
    "        \n",
    "        print(f\"Found {len(self.ids)} matching image pairs\")\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        id_str = self.ids[idx]\n",
    "        \n",
    "        # Load images\n",
    "        even_img = Image.open(os.path.join(self.even_dir, f\"{id_str}_even.png\")).convert(\"L\")\n",
    "        odd_img = Image.open(os.path.join(self.odd_dir, f\"{id_str}_odd.png\")).convert(\"L\")\n",
    "        matrix_img = Image.open(os.path.join(self.matrix_dir, f\"{id_str}.png\")).convert(\"L\")\n",
    "        \n",
    "        # Resize images to target size\n",
    "        even_img = even_img.resize(self.target_size, Image.Resampling.LANCZOS)\n",
    "        odd_img = odd_img.resize(self.target_size, Image.Resampling.LANCZOS)\n",
    "        matrix_img = matrix_img.resize(self.target_size, Image.Resampling.LANCZOS)\n",
    "        \n",
    "        # Convert to numpy and normalize\n",
    "        even_img = np.array(even_img, dtype=np.float32) / 255.0\n",
    "        odd_img = np.array(odd_img, dtype=np.float32) / 255.0\n",
    "        matrix_img = np.array(matrix_img, dtype=np.float32) / 255.0\n",
    "        \n",
    "        # Combine into 2-channel input\n",
    "        input_img = np.stack([even_img, odd_img], axis=2)  # Shape: (50, 50, 2)\n",
    "        \n",
    "        # Convert to torch tensors\n",
    "        input_img = torch.tensor(input_img).permute(2, 0, 1)  # Shape: (2, 50, 50)\n",
    "        matrix_img = torch.tensor(matrix_img).flatten()  # Shape: (2500,)\n",
    "        \n",
    "        return input_img, matrix_img\n",
    "\n",
    "# Custom ViT Model for Regression\n",
    "class ViTForImageRegression(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ViTForImageRegression, self).__init__()\n",
    "        # Custom ViT configuration\n",
    "        config = ViTConfig(\n",
    "            image_size=50,\n",
    "            patch_size=10,  # 50x50 divides into 5x5 patches\n",
    "            num_channels=2,  # 2-channel input\n",
    "            hidden_size=768,\n",
    "            num_hidden_layers=12,\n",
    "            num_attention_heads=12,\n",
    "            intermediate_size=3072\n",
    "        )\n",
    "        self.vit = ViTModel(config)\n",
    "        self.regressor = nn.Linear(config.hidden_size, 50 * 50)  # Output 50x50 pixels\n",
    "    \n",
    "    def forward(self, pixel_values):\n",
    "        outputs = self.vit(pixel_values=pixel_values)\n",
    "        pooled_output = outputs.pooler_output\n",
    "        return self.regressor(pooled_output)\n",
    "\n",
    "# Compute SSIM for a batch\n",
    "def compute_batch_ssim(preds, targets, size=(50, 50)):\n",
    "    ssim_scores = []\n",
    "    preds = preds.cpu().numpy().reshape(-1, *size)\n",
    "    targets = targets.cpu().numpy().reshape(-1, *size)\n",
    "    for i in range(len(preds)):\n",
    "        score = ssim(preds[i], targets[i], data_range=1.0)\n",
    "        ssim_scores.append(score)\n",
    "    return np.mean(ssim_scores)\n",
    "\n",
    "# Training and Evaluation\n",
    "def train_model():\n",
    "    # Initialize dataset and split\n",
    "    dataset = ImagePairDataset(even_dir, odd_dir, matrix_dir, target_size=image_size)\n",
    "    train_size = int(train_split * len(dataset))\n",
    "    test_size = len(dataset) - train_size\n",
    "    train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    # Initialize model\n",
    "    model = ViTForImageRegression().to(device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_mse = 0.0\n",
    "        train_mae = 0.0\n",
    "        train_ssim = 0.0\n",
    "        num_batches = 0\n",
    "        \n",
    "        for inputs, targets in tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{num_epochs}\"):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            \n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Compute metrics\n",
    "            train_loss += loss.item()\n",
    "            preds = outputs.detach()\n",
    "            train_mse += mean_squared_error(targets.cpu().numpy(), preds.cpu().numpy())\n",
    "            train_mae += mean_absolute_error(targets.cpu().numpy(), preds.cpu().numpy())\n",
    "            train_ssim += compute_batch_ssim(preds, targets)\n",
    "            num_batches += 1\n",
    "        \n",
    "        # Average metrics\n",
    "        train_loss /= num_batches\n",
    "        train_mse /= num_batches\n",
    "        train_mae /= num_batches\n",
    "        train_ssim /= num_batches\n",
    "        \n",
    "        # Evaluate on test set\n",
    "        model.eval()\n",
    "        test_loss = 0.0\n",
    "        test_mse = 0.0\n",
    "        test_mae = 0.0\n",
    "        test_ssim = 0.0\n",
    "        num_test_batches = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in test_loader:\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                \n",
    "                test_loss += loss.item()\n",
    "                test_mse += mean_squared_error(targets.cpu().numpy(), outputs.cpu().numpy())\n",
    "                test_mae += mean_absolute_error(targets.cpu().numpy(), outputs.cpu().numpy())\n",
    "                test_ssim += compute_batch_ssim(outputs, targets)\n",
    "                num_test_batches += 1\n",
    "        \n",
    "        test_loss /= num_test_batches\n",
    "        test_mse /= num_test_batches\n",
    "        test_mae /= num_test_batches\n",
    "        test_ssim /= num_test_batches\n",
    "        \n",
    "        print(f\"Epoch {epoch + 1}:\")\n",
    "        print(f\"  Train Loss: {train_loss:.6f}, MSE: {train_mse:.6f}, MAE: {train_mae:.6f}, SSIM: {train_ssim:.4f}\")\n",
    "        print(f\"  Test Loss: {test_loss:.6f}, MSE: {test_mse:.6f}, MAE: {test_mae:.6f}, SSIM: {test_ssim:.4f}\")\n",
    "        \n",
    "        # Visualize sample predictions\n",
    "        if epoch == num_epochs - 1:\n",
    "            with torch.no_grad():\n",
    "                sample_inputs, sample_targets = next(iter(test_loader))\n",
    "                sample_inputs = sample_inputs[:2].to(device)\n",
    "                sample_preds = model(sample_inputs)\n",
    "                \n",
    "                fig, axes = plt.subplots(2, 2, figsize=(10, 10))\n",
    "                for i in range(2):\n",
    "                    pred_img = sample_preds[i].cpu().numpy().reshape(image_size)\n",
    "                    target_img = sample_targets[i].cpu().numpy().reshape(image_size)\n",
    "                    \n",
    "                    axes[i, 0].imshow(pred_img, cmap='gray')\n",
    "                    axes[i, 0].set_title(f\"Predicted Image {i + 1}\")\n",
    "                    axes[i, 0].axis('off')\n",
    "                    \n",
    "                    axes[i, 1].imshow(target_img, cmap='gray')\n",
    "                    axes[i, 1].set_title(f\"Target Image {i + 1}\")\n",
    "                    axes[i, 1].axis('off')\n",
    "                \n",
    "                plt.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "255f2be7-2374-4999-a6ac-f26aa154233f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1161 matching image pairs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20: 100%|████████████████████████████████████████████████████████████████████| 116/116 [00:48<00:00,  2.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:\n",
      "  Train Loss: 0.276374, MSE: 0.276374, MAE: 0.496570, SSIM: 0.0417\n",
      "  Test Loss: 0.247401, MSE: 0.247401, MAE: 0.490353, SSIM: 0.0581\n",
      "Saved best model at epoch 1 with Test SSIM: 0.0581\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/20: 100%|████████████████████████████████████████████████████████████████████| 116/116 [00:48<00:00,  2.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2:\n",
      "  Train Loss: 0.246635, MSE: 0.246635, MAE: 0.487813, SSIM: 0.0567\n",
      "  Test Loss: 0.247000, MSE: 0.247000, MAE: 0.488523, SSIM: 0.0616\n",
      "Saved best model at epoch 2 with Test SSIM: 0.0616\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/20: 100%|████████████████████████████████████████████████████████████████████| 116/116 [00:47<00:00,  2.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3:\n",
      "  Train Loss: 0.245547, MSE: 0.245547, MAE: 0.486300, SSIM: 0.0676\n",
      "  Test Loss: 0.245303, MSE: 0.245303, MAE: 0.486921, SSIM: 0.0676\n",
      "Saved best model at epoch 3 with Test SSIM: 0.0676\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/20: 100%|████████████████████████████████████████████████████████████████████| 116/116 [00:47<00:00,  2.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4:\n",
      "  Train Loss: 0.245788, MSE: 0.245788, MAE: 0.486384, SSIM: 0.0672\n",
      "  Test Loss: 0.245966, MSE: 0.245966, MAE: 0.485915, SSIM: 0.0658\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/20: 100%|████████████████████████████████████████████████████████████████████| 116/116 [00:47<00:00,  2.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5:\n",
      "  Train Loss: 0.244861, MSE: 0.244861, MAE: 0.485104, SSIM: 0.0727\n",
      "  Test Loss: 0.245082, MSE: 0.245082, MAE: 0.483192, SSIM: 0.0680\n",
      "Saved best model at epoch 5 with Test SSIM: 0.0680\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/20: 100%|████████████████████████████████████████████████████████████████████| 116/116 [00:46<00:00,  2.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6:\n",
      "  Train Loss: 0.245025, MSE: 0.245025, MAE: 0.485262, SSIM: 0.0763\n",
      "  Test Loss: 0.245446, MSE: 0.245446, MAE: 0.486079, SSIM: 0.0723\n",
      "Saved best model at epoch 6 with Test SSIM: 0.0723\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/20: 100%|████████████████████████████████████████████████████████████████████| 116/116 [00:46<00:00,  2.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7:\n",
      "  Train Loss: 0.244898, MSE: 0.244898, MAE: 0.485481, SSIM: 0.0798\n",
      "  Test Loss: 0.244273, MSE: 0.244273, MAE: 0.485641, SSIM: 0.0824\n",
      "Saved best model at epoch 7 with Test SSIM: 0.0824\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/20: 100%|████████████████████████████████████████████████████████████████████| 116/116 [00:46<00:00,  2.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8:\n",
      "  Train Loss: 0.244247, MSE: 0.244247, MAE: 0.484920, SSIM: 0.0826\n",
      "  Test Loss: 0.243751, MSE: 0.243751, MAE: 0.486308, SSIM: 0.0791\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/20: 100%|████████████████████████████████████████████████████████████████████| 116/116 [00:46<00:00,  2.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9:\n",
      "  Train Loss: 0.244575, MSE: 0.244575, MAE: 0.484873, SSIM: 0.0811\n",
      "  Test Loss: 0.244519, MSE: 0.244519, MAE: 0.484764, SSIM: 0.0783\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/20: 100%|███████████████████████████████████████████████████████████████████| 116/116 [00:47<00:00,  2.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10:\n",
      "  Train Loss: 0.243940, MSE: 0.243940, MAE: 0.484032, SSIM: 0.0847\n",
      "  Test Loss: 0.244135, MSE: 0.244135, MAE: 0.486347, SSIM: 0.0798\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/20: 100%|███████████████████████████████████████████████████████████████████| 116/116 [00:47<00:00,  2.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11:\n",
      "  Train Loss: 0.244027, MSE: 0.244027, MAE: 0.484111, SSIM: 0.0844\n",
      "  Test Loss: 0.242518, MSE: 0.242518, MAE: 0.483718, SSIM: 0.0821\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/20: 100%|███████████████████████████████████████████████████████████████████| 116/116 [00:47<00:00,  2.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12:\n",
      "  Train Loss: 0.243697, MSE: 0.243697, MAE: 0.484138, SSIM: 0.0865\n",
      "  Test Loss: 0.242945, MSE: 0.242945, MAE: 0.483558, SSIM: 0.0872\n",
      "Saved best model at epoch 12 with Test SSIM: 0.0872\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/20: 100%|███████████████████████████████████████████████████████████████████| 116/116 [00:47<00:00,  2.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13:\n",
      "  Train Loss: 0.243652, MSE: 0.243652, MAE: 0.483541, SSIM: 0.0850\n",
      "  Test Loss: 0.242194, MSE: 0.242194, MAE: 0.483249, SSIM: 0.0820\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/20: 100%|███████████████████████████████████████████████████████████████████| 116/116 [00:47<00:00,  2.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14:\n",
      "  Train Loss: 0.242526, MSE: 0.242526, MAE: 0.482070, SSIM: 0.0770\n",
      "  Test Loss: 0.242068, MSE: 0.242068, MAE: 0.479556, SSIM: 0.0662\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/20: 100%|███████████████████████████████████████████████████████████████████| 116/116 [00:46<00:00,  2.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15:\n",
      "  Train Loss: 0.241570, MSE: 0.241570, MAE: 0.479549, SSIM: 0.0770\n",
      "  Test Loss: 0.241369, MSE: 0.241369, MAE: 0.479227, SSIM: 0.0874\n",
      "Saved best model at epoch 15 with Test SSIM: 0.0874\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/20: 100%|███████████████████████████████████████████████████████████████████| 116/116 [00:46<00:00,  2.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16:\n",
      "  Train Loss: 0.240691, MSE: 0.240691, MAE: 0.478413, SSIM: 0.0869\n",
      "  Test Loss: 0.241029, MSE: 0.241030, MAE: 0.479130, SSIM: 0.0843\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/20: 100%|███████████████████████████████████████████████████████████████████| 116/116 [00:46<00:00,  2.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17:\n",
      "  Train Loss: 0.239836, MSE: 0.239836, MAE: 0.477109, SSIM: 0.0924\n",
      "  Test Loss: 0.240447, MSE: 0.240447, MAE: 0.478126, SSIM: 0.0953\n",
      "Saved best model at epoch 17 with Test SSIM: 0.0953\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/20: 100%|███████████████████████████████████████████████████████████████████| 116/116 [00:47<00:00,  2.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18:\n",
      "  Train Loss: 0.240061, MSE: 0.240061, MAE: 0.477590, SSIM: 0.0952\n",
      "  Test Loss: 0.240527, MSE: 0.240527, MAE: 0.478856, SSIM: 0.0890\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/20: 100%|███████████████████████████████████████████████████████████████████| 116/116 [00:46<00:00,  2.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19:\n",
      "  Train Loss: 0.239867, MSE: 0.239867, MAE: 0.477592, SSIM: 0.0989\n",
      "  Test Loss: 0.239245, MSE: 0.239245, MAE: 0.475519, SSIM: 0.0967\n",
      "Saved best model at epoch 19 with Test SSIM: 0.0967\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/20: 100%|███████████████████████████████████████████████████████████████████| 116/116 [00:47<00:00,  2.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20:\n",
      "  Train Loss: 0.239396, MSE: 0.239396, MAE: 0.476666, SSIM: 0.1014\n",
      "  Test Loss: 0.239048, MSE: 0.239048, MAE: 0.476174, SSIM: 0.0980\n",
      "Saved best model at epoch 20 with Test SSIM: 0.0980\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA8gAAAPdCAYAAABWSUEqAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAACRWklEQVR4nOzdeZjddXn//9eZs87ZZt+SyZ4QgkQWEZEimyC7KCpW5SJQtdjaakVEbfFLAKVYcKGKou0FKd9SccdLLoFawaUXICJ7wxIC2Wc9s8+Zs39+f/jLfB0SuG/sCSHwfFwXfzB55pz3Zz1zz5nMhIIgCAQAAAAAwGtcw95eAAAAAAAArwQMyAAAAAAAiAEZAAAAAABJDMgAAAAAAEhiQAYAAAAAQBIDMgAAAAAAkhiQAQAAAACQxIAMAAAAAIAkBmQAAAAAACQxIOMVZt26dQqFQrP/RSIR9fb26vzzz9f27dtfljUsXrxY55133uz///KXv1QoFNIvf/nLl/Q499xzj9auXauxsbG6rk+SzjvvPC1evNjsjj32WB144IF1f/5XksnJSV188cV629vepo6ODoVCIa1du3ZvLwsA8P/749f1F/vvpb7O7mnr16/X2rVrtWnTJle/83OYBx54YM8ubC+76aab9Od//udauXKlGhoaXJ+PAPuSyN5eALA7N954o/bff3/NzMzo17/+tf7xH/9Rv/rVr/TYY48plUq9rGs59NBDde+99+qAAw54SX/vnnvu0WWXXabzzjtPzc3Ne2ZxUC6X07e//W0ddNBBesc73qF//dd/3dtLAgD8kXvvvXfO/19xxRW6++67ddddd835+Et9nd3T1q9fr8suu0zHHnssQ+Af+b//9/+qv79fhx9+uGq1msrl8t5eElBXDMh4RTrwwAN12GGHSZKOO+44VatVXXHFFbr11lv1gQ98YLd/J5/PK5lM1n0t2WxWRxxxRN0fF/WxaNEijY6OKhQKaXh4mAEZAF5hnv8a2tHRoYaGhrq9tu6p13/s3p133qmGhj98E+rpp5+uxx9/fC+vCKgvvsUa+4SdL6KbN2+W9IdvMU6n03rsscf0tre9TZlMRm9961slSaVSSZ///Oe1//77Kx6Pq6OjQ+eff76GhobmPGa5XNbFF1+s7u5uJZNJHXXUUbr//vt3ee4X+hbr3/72tzrjjDPU1tamRCKhZcuW6e/+7u8kSWvXrtWnPvUpSdKSJUt2++1j3/3ud/XmN79ZqVRK6XRaJ510kh566KFdnn/dunVauXKl4vG4Vq1apZtuuulP2oc7hUIh/c3f/I1uvPFGrVy5Uo2NjTrssMN03333KQgCXX311VqyZInS6bSOP/54PfPMM3P+/s9//nOdeeaZ6u3tVSKR0PLly3XBBRdoeHh4l+f6yU9+ote//vWKx+NaunSprr32Wq1du1ahUGhOFwSBvvGNb+jggw9WY2OjWlpa9O53v1vPPvusa3ue/3gAgH3Lddddp6OPPlqdnZ1KpVJavXq1/umf/mmXdyd3/tOhX//61zryyCOVTCb1F3/xF5Kkbdu26d3vfrcymYyam5v1gQ98QL/73e8UCoW0bt26OY/zwAMP6O1vf7taW1uVSCR0yCGH6Hvf+97sn69bt07vec97JP3hC/U7X2ue/ziWnZ+vPPnkkzrppJOUSqXU09Ojq666SpJ033336aijjlIqldJ+++2nf/u3f5vz94eGhvTXf/3XOuCAA5ROp9XZ2anjjz9ev/nNb3Z5rnpu/4vZORwDr1a8g4x9ws4hraOjY/ZjpVJJb3/723XBBRfoM5/5jCqVimq1ms4880z95je/0cUXX6wjjzxSmzdv1qWXXqpjjz1WDzzwgBobGyVJH/7wh3XTTTfpoosu0oknnqjHH39cZ511liYnJ8313HnnnTrjjDO0atUqffnLX9bChQu1adMm/ed//qck6UMf+pBGRkb0ta99TT/60Y/U09Mj6f99+9iVV16pSy65ROeff74uueQSlUolXX311XrLW96i+++/f7Zbt26dzj//fJ155pn60pe+pPHxca1du1bFYvF/9QJ122236aGHHtJVV12lUCikT3/60zrttNO0Zs0aPfvss/r617+u8fFxXXjhhXrXu96lhx9+eHYI3bhxo9785jfrQx/6kJqamrRp0yZ9+ctf1lFHHaXHHntM0WhUknTHHXforLPO0tFHH63vfve7qlQquuaaazQwMLDLei644AKtW7dOH/vYx/TFL35RIyMjuvzyy3XkkUfqkUceUVdX15+8rQCAV76NGzfq/e9/v5YsWaJYLKZHHnlEX/jCF/Tkk0/qhhtumNP29fXpnHPO0cUXX6wrr7xSDQ0Nmp6e1nHHHaeRkRF98Ytf1PLly3XHHXfove997y7Pdffdd+vkk0/Wm970Jl1//fVqamrSLbfcove+973K5/M677zzdNppp+nKK6/U3//93+u6667ToYceKklatmzZS962crmss846Sx/5yEf0qU99Sv/xH/+hz372s5qYmNAPf/hDffrTn1Zvb6++9rWv6bzzztOBBx6oN7zhDZKkkZERSdKll16q7u5uTU1N6cc//rGOPfZY/eIXv9Cxxx4rSXXffuA1LQBeQW688cZAUnDfffcF5XI5mJycDG677bago6MjyGQyQX9/fxAEQbBmzZpAUnDDDTfM+fvf+c53AknBD3/4wzkf/93vfhdICr7xjW8EQRAETzzxRCAp+MQnPjGnu/nmmwNJwZo1a2Y/dvfddweSgrvvvnv2Y8uWLQuWLVsWzMzMvOC2XH311YGk4Lnnnpvz8S1btgSRSCT427/92zkfn5ycDLq7u4Ozzz47CIIgqFarwbx584JDDz00qNVqs92mTZuCaDQaLFq06AWfe6djjjkmeN3rXjfnY5KC7u7uYGpqavZjt956ayApOPjgg+c811e/+tVAUvDoo4/u9vFrtVpQLpeDzZs3B5KCn/zkJ7N/9sY3vjFYsGBBUCwW52xjW1tb8Me3nnvvvTeQFHzpS1+a89hbt24NGhsbg4svvtjczp2GhoYCScGll17q/jsAgJfXmjVrglQq9YJ/Xq1Wg3K5HNx0001BOBwORkZGZv/smGOOCSQFv/jFL+b8neuuuy6QFNx+++1zPn7BBRcEkoIbb7xx9mP7779/cMghhwTlcnlOe/rppwc9PT1BtVoNgiAIvv/97+/y+v9idn4O87vf/W7Otj7/85JyuRx0dHQEkoIHH3xw9uO5XC4Ih8PBhRde+ILPUalUgnK5HLz1rW8N3vnOd+7R7fc47bTTXJ+PAPsSvkcCr0hHHHGEotGoMpmMTj/9dHV3d+v222/f5Z3Ed73rXXP+/7bbblNzc7POOOMMVSqV2f8OPvhgdXd3z36L89133y1Ju/x75rPPPluRyIt/Y8XTTz+tjRs36oMf/KASicRL3rY777xTlUpF55577pw1JhIJHXPMMbNrfOqpp7Rjxw69//3vn/MtxIsWLdKRRx75kp/3jx133HFzftjZqlWrJEmnnHLKnOfa+fGd39ouSYODg/rIRz6iBQsWKBKJKBqNatGiRZKkJ554QtIfvpL9wAMP6B3veIdisdjs302n0zrjjDPmrOW2225TKBTSOeecM2d/dHd366CDDnrF/VRTAED9PfTQQ3r729+utrY2hcNhRaNRnXvuuapWq3r66afntC0tLTr++OPnfOxXv/qVMpmMTj755Dkff9/73jfn/5955hk9+eSTs6//f/y6c+qpp6qvr09PPfVUXbctFArp1FNPnf3/SCSi5cuXq6enR4cccsjsx1tbW9XZ2TnnNVeSrr/+eh166KFKJBKzr7u/+MUvZl9zpVf29gP7Gr7FGq9IN910k1atWqVIJKKurq7Zb1H+Y8lkUtlsds7HBgYGNDY2Nmco+2M7/51sLpeTJHV3d8/580gkora2thdd285/y9zb2+vbmOfZ+S3Gb3zjG3f75zu/dfqF1rjzY95fO7E7ra2tc/5/5/56oY8XCgVJUq1W09ve9jbt2LFDn/vc57R69WqlUinVajUdccQRmpmZkSSNjo4qCILdfmv08z82MDDwgq0kLV269E/YQgDAvmLLli16y1veopUrV+raa6/V4sWLlUgkdP/99+ujH/3o7GvLTrv7nCCXy7lfcyTpoosu0kUXXbTb9ezuZ2r8bySTyV2+oB6LxXZ5zd358Z2vuZL05S9/WZ/85Cf1kY98RFdccYXa29sVDof1uc99bs6A/ErefmBfw4CMV6RVq1bN/hTrF7K7H8zU3t6utrY23XHHHbv9O5lMRpJmh+D+/n7Nnz9/9s8rlcrsYPpCdv476G3btr1o90La29slST/4wQ9m33ndnT9e4/Pt7mMvh8cff1yPPPKI1q1bpzVr1sx+/Pk/yKulpUWhUGi3/974+Wtvb29XKBTSb37zG8Xj8V363X0MAPDqceutt2p6elo/+tGP5rwuPvzww7vtd/f639bWttsftLm71xxJ+uxnP6uzzjprt4+/cuVK79L3uH//93/Xscceq29+85tzPv78n5fyat1+YG9gQMaryumnn65bbrlF1WpVb3rTm16w2/lDLW6++ebZH4QhSd/73vdUqVRe9Dn2228/LVu2TDfccIMuvPDCFxzgdn78+V/5PumkkxSJRLRx48ZdvkX8j61cuVI9PT36zne+owsvvHD2E4LNmzfrnnvu0bx58150nXvCzjU8f5u/9a1vzfn/VCqlww47TLfeequuueaa2Xeip6amdNttt81pTz/9dF111VXavn27zj777D24egDAK9HuXluCINC//Mu/uB/jmGOO0fe+9z3dfvvtOuWUU2Y/fsstt8zpVq5cqRUrVuiRRx7RlVde+aKP+UKv4y+nUCi0y2vuo48+qnvvvVcLFiyY/die2H7gtYoBGa8qf/7nf66bb75Zp556qj7+8Y/r8MMPVzQa1bZt23T33XfrzDPP1Dvf+U6tWrVK55xzjr761a8qGo3qhBNO0OOPP65rrrlml2/b3p3rrrtOZ5xxho444gh94hOf0MKFC7VlyxbdeeeduvnmmyVJq1evliRde+21WrNmjaLRqFauXKnFixfr8ssv1z/8wz/o2Wef1cknn6yWlhYNDAzo/vvvVyqV0mWXXaaGhgZdccUV+tCHPqR3vvOd+vCHP6yxsTGtXbt2t992/XLYf//9tWzZMn3mM59REARqbW3VT3/6U/385z/fpb388st12mmn6aSTTtLHP/5xVatVXX311Uqn07M/lVOS/uzP/kx/+Zd/qfPPP18PPPCAjj76aKVSKfX19em///u/tXr1av3VX/3Vi67r9ttv1/T09OxX1NevX68f/OAHkqRTTz2V348JAK9gJ554omKxmN73vvfp4osvVqFQ0De/+U2Njo66H2PNmjX6yle+onPOOUef//zntXz5ct1+++268847Jc391UTf+ta3dMopp+ikk07Seeedp/nz52tkZERPPPGEHnzwQX3/+9+XJB144IGSpG9/+9vKZDJKJBJasmSJ+U+x6un000/XFVdcoUsvvVTHHHOMnnrqKV1++eVasmTJnC/o74ntfyHr16/X+vXrJf3hHep8Pj/7mnvAAQfM/iYOYJ+1d39GGDDX7n4C5O682E/ALJfLwTXXXBMcdNBBQSKRCNLpdLD//vsHF1xwQbBhw4bZrlgsBp/85CeDzs7OIJFIBEcccURw7733BosWLTJ/inUQ/OGnL59yyilBU1NTEI/Hg2XLlu3yU7E/+9nPBvPmzQsaGhp2eYxbb701OO6444JsNhvE4/Fg0aJFwbvf/e7gv/7rv+Y8xr/+678GK1asCGKxWLDffvsFN9xwQ7BmzZr/1U+x/uhHPzrnY88991wgKbj66qvnfHzntn//+9+f/dj69euDE088MchkMkFLS0vwnve8J9iyZctuf3r0j3/842D16tVBLBYLFi5cGFx11VXBxz72saClpWWXtd5www3Bm970piCVSgWNjY3BsmXLgnPPPTd44IEHzO1ctGhRIGm3/z3/p4gDAPau3b2G//SnP5193Z4/f37wqU99Krj99tt3ee3c3evaTlu2bAnOOuusIJ1OB5lMJnjXu94V/OxnP9vltywEQRA88sgjwdlnnx10dnYG0Wg06O7uDo4//vjg+uuvn9N99atfDZYsWRKEw+Fdfhr0873QT7He3ecrL7QdixYtCk477bTZ/y8Wi8FFF10UzJ8/P0gkEsGhhx4a3Hrrrbv9PGBPbP/uXHrppS/4mstvkcCrQSgIguDlG8cBvJaVy2UdfPDBmj9//uzvjAYAYE+58sordckll2jLli1/8g/X3Je91rcf+FPwLdYA9pgPfvCDOvHEE9XT06P+/n5df/31euKJJ3Tttdfu7aUBAF5lvv71r0v6wz8HKpfLuuuuu/TP//zPOuecc14Tw+FrffuBemFABrDHTE5O6qKLLtLQ0JCi0agOPfRQ/exnP9MJJ5ywt5cGAHiVSSaT+spXvqJNmzapWCxq4cKF+vSnP61LLrlkby/tZfFa336gXvgWawAAAAAAJDXYCQAAAAAAr34MyAAAAAAAiAEZAAAAAABJL+GHdK1YscJsCoWC2bS3t5vN2NiYZ0kqlUpmk0wm69Lk83mzSSQSZhMOh81GkoaHh82msbHRbIrFYl0eJxqN1qWRfPspm82aTSqVMhvPP7E/6aSTzKarq6su65H+8IOrLOl02mw8xzYSsS9xz/nvWY8kZTIZs/EcW8+1PT09bTaxWMxsRkdHzUaSQqGQ2VSrVbMZGRkxG8/+9hy35uZms5GkrVu3mo3n3u05buVy2Ww899sNGzaYzeOPP242kjQ1NWU2d9xxh9ns2LHD9Xx7kuc8BV5p+HE4eLXinvzK47nf8A4yAAAAAABiQAYAAAAAQBIDMgAAAAAAkhiQAQAAAACQxIAMAAAAAIAkBmQAAAAAACQxIAMAAAAAIIkBGQAAAAAASVLEG5566qlmUywWzaa5udlsduzY4VmSZmZmzKalpcVsUqmU2XjWHY/Hzcb7C8OTyaTZZLNZsymXy2bjWfdBBx1kNiMjI2YjSel02mz6+/vNZnBw0Gw8+zsajZpNrVYzG8+aJWnp0qVm49k2z/H3bFtjY6PZeDU02F9zy+fzZrNp0yazGRsbM5v29naz8dy3JN8vlvfsS88+8ojFYmbT0dHheizPvdSzvz3n9saNG83Gc4848MADzSaXy5mNJD322GNmU6lUXI+1t3nOU9i8r9UAgFcf3kEGAAAAAEAMyAAAAAAASGJABgAAAABAEgMyAAAAAACSGJABAAAAAJDEgAwAAAAAgCQGZAAAAAAAJDEgAwAAAAAgSYp4w8nJSbM54YQTzKZUKplNb2+va02xWMxsWltbzSYIArPp6Ogwm4MOOshs+vv7zUaSQqGQ2RSLxbo00WjUbPL5vNl49qMkVSoVs2lqajKbcDhsNps3bzabRCJhNp7zKBLxXU4jIyNmk0qlzCYej5vN+Pi42XiOv+eYSVKtVjMbz3HzrGnevHlmMzY2ZjadnZ1mI0l9fX1m47lPLliwwGw89wnPsd2+fbvZSFI6nTabHTt2uB7Lks1mzcbzOuE5j7znbVtbm9l4zxMAALBv4x1kAAAAAADEgAwAAAAAgCQGZAAAAAAAJDEgAwAAAAAgiQEZAAAAAABJDMgAAAAAAEhiQAYAAAAAQBIDMgAAAAAAkqSINzzuuOPM5i1veYvZbNu2zWzS6bRrTbFYzGyGhobMJpFImE08Hjeb6elpswmCwGwkqVqtmk04HHY9lmVgYMBsisWi2fT29rqez3NMZmZmzGZ8fNxsSqWS2eRyObNpaLC/lpRMJs1Gkpqamszm2WefNZtVq1aZjec8qtVqZpPNZs3G+1jlctlsPMdtZGTEbNrb283Gc21LUkdHh9l49lN/f7/ZeO5JnntJW1ub2UhSPp83G8/57blPeI5/NBo1m0jEfvnynEeSNDY2ZjZPPPGE67EAAMC+jXeQAQAAAAAQAzIAAAAAAJIYkAEAAAAAkMSADAAAAACAJAZkAAAAAAAkMSADAAAAACCJARkAAAAAAEkMyAAAAAAASGJABgAAAABAkhTxhsuXLzebkZERs+nq6jKb8fFx15qmpqbMJhwOm01Dg/11gmq1ajblctls+vr6zEaSOjs7zaZQKJhNpVJxPZ8lnU6bTbFYdD3WvHnzzOaJJ54wG88x8ezHeu2j+fPnu7oNGzaYjed62759u9k0NzebTTKZNJvJyUmzkaTGxkazSSQSZuPZNs9x8zyOZz2S716yadMms2lrazObmZkZs+no6DCbfD5vNpLU09NjNp5jWyqVXM9n8VzbTU1NZtPa2up6vuHhYbPxXJMAAGDfxzvIAAAAAACIARkAAAAAAEkMyAAAAAAASGJABgAAAABAEgMyAAAAAACSGJABAAAAAJDEgAwAAAAAgCQGZAAAAAAAJEkRbxiPx80mCAKz2bhxo9msXr3ataZnnnnGbGq1mtmkUimzaWxsNJuZmRmzWblypdlI0tTUlNkMDw+bjWfbPMctk8mYzcTEhNlI0tatW82mo6PDbBKJhNls2LDBbJqbm82ms7PTbHbs2GE2ku+Y5PN5s4nFYq7ns3iuo0WLFrkeq1AomM2mTZvMZuHChWaTy+XMZnp6ui6NJIXDYbMplUpm4zlPIhH71uw5jyqVitlIvmsynU6bTbVaNRvPeVsul83Gc79taPB9DbilpcVsPPcbAACw7+MdZAAAAAAAxIAMAAAAAIAkBmQAAAAAACQxIAMAAAAAIIkBGQAAAAAASQzIAAAAAABIYkAGAAAAAEASAzIAAAAAAJKkiDd88sknzWbZsmVmk8/nzWbTpk2eJWnBggVmMzw8bDZBEJhNNBo1m0KhYDbT09NmI0nNzc1m41l3X1+f2TQ2NppNf3+/2UQivtPJs22e51u4cKHZeLa/qanJbKampswmFouZjZfnmHier1armY3nOhoZGTEbyXcOHHLIIWbz7LPPmo3n/G9vbzebSqViNpI0MzNjNuFwuC6NZ9sGBwfNJpPJmI3ku096tv/1r3+92QwNDdVlPaFQyGy82//b3/7WbKrVquuxAADAvo13kAEAAAAAEAMyAAAAAACSGJABAAAAAJDEgAwAAAAAgCQGZAAAAAAAJDEgAwAAAAAgiQEZAAAAAABJDMgAAAAAAEiSIt7wwAMPNJsgCMxm8eLFZjM+Pu5Zku69916z6e3tNZtarWY2k5OTZhONRs1m27ZtZiNJbW1trs7S0GB/DaRYLJrN/PnzzcZz/CWpVCqZTUdHh9nkcjmzKRQKZlMul+vSZDIZs5Gkqakps8nn82bT2tpqNv39/WYTCoXq8lySlM1mzcZzfXuupcbGRrOZmZkxG8+xlaTm5mazqVarZlOvc3JkZMRsvFKplNkMDAyYTV9fn9l4ts1zbD3nWnt7u9lIvvuNZ9sAAMC+j3eQAQAAAAAQAzIAAAAAAJIYkAEAAAAAkMSADAAAAACAJAZkAAAAAAAkMSADAAAAACCJARkAAAAAAEkMyAAAAAAASJIi3jCXy5lNV1eX2YyMjJhNIpFwrWn//fc3m1KpZDaTk5N1aebNm2c23d3dZiNJ09PTZtPR0WE24XDYbDo7O81mZmamLo8j+c6lTZs2mc3o6KjZeM5Jzz5qaLC/lhQKhcxGklKplNl49renCYLAbDzntmcfSVIkYt9S6nW9NTU1mU2lUjEbz3UrSYODg3V5vr6+PrNZvHix2bS3t5uN95zM5/Nms3r1arPxXNuebfPsI8856VmzJN1xxx1mk8lkXI8FAAD2bbyDDAAAAACAGJABAAAAAJDEgAwAAAAAgCQGZAAAAAAAJDEgAwAAAAAgiQEZAAAAAABJDMgAAAAAAEhiQAYAAAAAQJIU8YbpdNpsotFoXZpqtepa08zMjNmMj4+bTTabNZuWlhaz6ejoMJsnn3zSbLw829/Y2Gg25XLZbGKxWF0eR/Idk97eXrPxHLft27ebzaJFi8ymWCyaTalUMhtJisfjdXksz5o811soFKrLeiQpl8uZTaFQMJtIxL41bdmyxWwaGuyvAYbDYbORpKmpKbMJgsBsPPcSz3nrud949qMkdXV1mY3nvuy5T3j20fDwsNlkMhmz8dxrJCmVSpmN57wFAOwZns9VgHrhHWQAAAAAAMSADAAAAACAJAZkAAAAAAAkMSADAAAAACCJARkAAAAAAEkMyAAAAAAASGJABgAAAABAEgMyAAAAAACSGJABAAAAAJAkRbxhIpEwm8nJSbPZsGGD2XR3d7vWVCgUzKahwf4awMjIiNmUy2WziUajZlOtVs1G8q27UqmYTTweN5tisViXJggCs5GklpYWsxkbGzObSMQ+fefPn+9ZkimZTJqNd/snJibMZmpqymx6e3vNJhwOm00+nzeb4eFhs5Gkjo4Os5menjYbz/nW1dVlNp5r27MeyXfteq43z/Ol02mzWbBggdl47reSbz95ts1zva1fv95sPNeb5/6fSqXMRqrfMQFeTbyvafUQCoVetucCAAvvIAMAAAAAIAZkAAAAAAAkMSADAAAAACCJARkAAAAAAEkMyAAAAAAASGJABgAAAABAEgMyAAAAAACSGJABAAAAAJAkRbzhtm3bzKZSqZhNe3u72QwNDbnW1NTUZDaxWMxsgiAwG8+6N2zYYDbz5883G++apqenXY9lmZycNJuRkRGzefLJJ13P19bWZjYTExNm4zkmnn2USqXMJpFImI3n/JekeDxuNqOjo2bjWff27dvNpqury2yi0ajZSNLMzIzZFAoFs9lvv/3MZmxszGzmzZtnNs8884zZSNLChQvNJpPJmE2pVDKbWq1mNjt27DAbzzki+e5LU1NTZuM5Jtls1mwaGuyv3XrOI899VJLS6bTZ1Ot+u6eFQqG9vYRXBe+5AwB49eEdZAAAAAAAxIAMAAAAAIAkBmQAAAAAACQxIAMAAAAAIIkBGQAAAAAASQzIAAAAAABIYkAGAAAAAEASAzIAAAAAAJKkiDeMx+NmU6lUzCYIArNpbGx0rSkajZpNtVo1m+npabOZmJioy3MNDg6ajSTlcjmzaW1tNZvh4WGzSaVSZuM5/j09PWYjSaVSyWy6urrMxrPuqakps0mn02bjOR6e81GSksmk2axYscJsPOeSZ019fX1mE4vFzEbyXd/d3d1m47kmPfvRc0329vaajeS7BjzrLhaLZuPZj577bVtbm9lI0tjYmNl49mUmkzGb0dHRujxOKBQyG885IkmdnZ1m4zn+AABg38c7yAAAAAAAiAEZAAAAAABJDMgAAAAAAEhiQAYAAAAAQBIDMgAAAAAAkhiQAQAAAACQxIAMAAAAAIAkBmQAAAAAACRJEW8YBIHZJBIJs5mcnDSbSMS3rE2bNplNV1eX2bS0tJjN0NCQ2cRisbo8lyQ1NjaazZYtW+qypkwmYzahUMhsvHp7e80ml8uZzczMjNmk02mz8Zxv0WjUbMrlstlI0vT0tNl49nc8Hnc9n8VzbadSKddjZbNZsykWi2bj2batW7eajWfdzc3NZiP5jkmlUjGbcDhsNtVq1Ww6OjrMZmBgwGwkKZlM1mVNnvv7+Pi42XiOm+ce6blvS77r23MvAQAA+z7eQQYAAAAAQAzIAAAAAABIYkAGAAAAAEASAzIAAAAAAJIYkAEAAAAAkMSADAAAAACAJAZkAAAAAAAkMSADAAAAACBJinjDhgZ7li6VSmbT1NRkNs3NzZ4laWJiwmwaGxvNJpFImE1HR4fZjI+Pm83g4KDZSFI+nzebSMQ+fMlksi7PFQSB2WSzWbORpP7+frOJRqNm4zm2qVTKbEZHR82mpaXFbEKhkNl4TU1N1eVx4vG42VQqFbOJxWKu5ysWi2bjOd886/Yck3K5bDa5XM5sJN/1vXr1arMZGBgwm6GhIbPxXJM9PT1mI/mOSaFQMJtMJmM2nnPbc9w8z+U5tyXf/d1zbgMAgH0f7yADAAAAACAGZAAAAAAAJDEgAwAAAAAgiQEZAAAAAABJDMgAAAAAAEhiQAYAAAAAQBIDMgAAAAAAkhiQAQAAAACQJEW8Ya1WM5vW1lazGRwcNJu2tjbXmrLZrNkUi0XXY1kqlYrZlMtls0mlUq7na2pqMpuJiQmz8eyjarXqWpNlaGjI1S1evNhsPNsWCoXMxnP8o9Go2YyMjJhNY2Oj2UhSLBYzm56eHrPJ5XJm47lu0+m02SQSCbPxrsmzLzs6OsxmcnLSbDz70XNtS1ImkzGbjRs3mo1nf3d3d5tNc3Oz2XjuSVL9rpNSqWQ2nuvEe5+0eF6TJN81Wa81AQCAVzbeQQYAAAAAQAzIAAAAAABIYkAGAAAAAEASAzIAAAAAAJIYkAEAAAAAkMSADAAAAACAJAZkAAAAAAAkMSADAAAAACCJARkAAAAAAElSxBvOnz/fbJ5++mmz6erqMpstW7a41jQ2NmY2S5cuNZuGBvvrBLVazWxKpZLZxONxs5GkUChkNoVCwWyi0ajZNDU1mc3GjRvNpr293Wwk336anp42G8/2e45tOp02m8HBQbNpa2szG0kaGRkxm0qlYjaedQdBYDaefT00NGQ2ku/8XrZsmdnMzMyYjec86u/vN5uWlhazkaSJiYm6PJbnnBwdHTWbbDZrNl6xWMxscrmc2SQSCbPxbFskYr80eRrPa5Lku3dVq1XXY+HVwfMaLPnusbB59qP3mADA/xbvIAMAAAAAIAZkAAAAAAAkMSADAAAAACCJARkAAAAAAEkMyAAAAAAASGJABgAAAABAEgMyAAAAAACSGJABAAAAAJAkRbzh2NiY2bS2tppNPp83m0wm41mSisWi2RQKBbOpVqtm09jYaDbz5883m1wuZzaSbz9FIvbhK5VKdXmueDxuNg0Nvq+3jI+P1+2xLNPT02bT0dFhNvPmzTMbz7kmSe3t7WYzOTlpNtls1mxGRkbMxrNuz5olaWhoyGzK5bLZTE1NmU1XV1ddnstz/UvSihUrzGZgYMBsKpWK2XjW3dzcbDae4y+9vPf3aDRqNp59lEwmzWbJkiVmI/mO24IFC1yPBQAA9m28gwwAAAAAgBiQAQAAAACQxIAMAAAAAIAkBmQAAAAAACQxIAMAAAAAIIkBGQAAAAAASQzIAAAAAABIYkAGAAAAAECSFPGGlUrFbOLxuNnUajWzyWQyrjVNTk6aTSqVMpuJiQmzmZqaMpt0Ol2X9UjS6Oio2cybN89sPMdtenrabLq6uswmEvGdTn19fWZTLpfNJp/Pm41n3UEQmI3n3PasWfKdt55j4nmcxsbGujzO5s2bzUaS2tvbzSYWi5lNKBQyG8/x9zyXZx9Jvv3kOZc89wBPs2XLFrPxbpuH597tuZdGo1Gz8VxLAwMDZpPNZs1G8t27ZmZmXI8FAAD2bbyDDAAAAACAGJABAAAAAJDEgAwAAAAAgCQGZAAAAAAAJDEgAwAAAAAgiQEZAAAAAABJDMgAAAAAAEhiQAYAAAAAQJIU8YaZTMZs0um02QwODppNoVBwramnp8dsgiAwm3w+bzbJZNJsQqGQ2UxMTJiNJM2fP99shoaGzKZSqZhNc3Oz2ZTLZbMplUpmI/n2ZSKRMJt4PG42nnNpbGzMbLLZrNl4tkuSJicnzcazvz3Ntm3bzMazbs/56F1TOBw2m3nz5pmN59r2nJOe61+SlixZYjZbt241G8/+TqVSZuO533q3zfNYU1NTZrNgwQKzqVarZuM5brlczmxisZjZeJ/vkEMOcT0WAKD+PK/5+yrP/ICXF+8gAwAAAAAgBmQAAAAAACQxIAMAAAAAIIkBGQAAAAAASQzIAAAAAABIYkAGAAAAAEASAzIAAAAAAJIYkAEAAAAAkCRFvOHg4KDZTExMmE0ikTCbaDTqWlOtVjMbzy8Wb2lpMZupqSmzKZfLZhMOh83G+1jNzc1mk8/nzcazpsbGRrMZGBgwG8l3DnjOpVgsZjaFQsFs0um02Wzbts1s5s2bZzaSND4+bjbVatVsisWi2bS1tZmN59qu57YtXbrUbDzb1tBgf30vlUqZjec8kqSRkRGzWbx4sdk0NTWZjee69dz/crmc2Ui+e6DH5s2bzcZz38pms2bjuY947luSNDY2ZjannHKK67GAV4tQKLS3lzCH5/M5AKgH3kEGAAAAAEAMyAAAAAAASGJABgAAAABAEgMyAAAAAACSGJABAAAAAJDEgAwAAAAAgCQGZAAAAAAAJDEgAwAAAAAgSYp4w0qlYjalUslsGhsbzaa9vd21pg0bNpiN5xfdx+Nxs2losL+W4Nn+crlsNpJUKBTMJhaLmU1zc7PZDA8P16VZuHCh2UjSc889ZzaZTMZswuGw2bS1tZlNLpczm66uLrPxymazZlOtVs1mYmLCbDzH37OeZ555xmwk37XkuU4815tnH3nuW57z0Wvp0qVm09fXZzaec9tz3DzniOQ7vp7ru6mpqS7N+vXrzcZzrg0MDJiN5HtdamlpcT0WXls8n2MAAPYtvIMMAAAAAIAYkAEAAAAAkMSADAAAAACAJAZkAAAAAAAkMSADAAAAACCJARkAAAAAAEkMyAAAAAAASGJABgAAAABAEgMyAAAAAACSpIg3bGlpMZt8Pm82/f39ZlMoFFxrikajZlOr1cxmcnLS9XyWUChkNul02vVY5XLZbDz7aePGjWbT29trNpGIfaqMj4+bjSQtXLjQbEqlktk0NzebjWdN7e3tZtPQYH8tyXNuS1JnZ6ers8ybN89spqamzKZSqZiNZ19LUjweN5uxsTGz6ejoMJsFCxaYzYMPPmg2K1euNBtJKhaLZuM5bz33JM/5FgSB2UxPT5uN5NsHnnPJc0/ynG+tra1mEw6HzSaXy5mNJCWTSbMZHh52PRYAANi38Q4yAAAAAABiQAYAAAAAQBIDMgAAAAAAkhiQAQAAAACQxIAMAAAAAIAkBmQAAAAAACQxIAMAAAAAIIkBGQAAAAAASVLEG4ZCIbMJh8Nm09TUZDblctm1Js/zZbNZs0kkEmazZcuWujyOZ82SVK1WzWZ8fNxsUqmU2eRyObOp57YFQVCXplKpmM3Y2JjZeLbNczxaWlrMRpKKxaLZeI7bhg0bzGbBggVm49m2RYsWmY0kPf3002bT2dlZlzVt3rzZbF73uteZzejoqNlIUq1Wc3UWz/nW3t5uNgMDA2YzPT3tWlOhUDCb3t5es/Fcb/F43GyGhobMZvHixWYzODhoNpI0MjJiNpGI++USAADsw3gHGQAAAAAAMSADAAAAACCJARkAAAAAAEkMyAAAAAAASGJABgAAAABAEgMyAAAAAACSGJABAAAAAJDEgAwAAAAAgCQpFARB4Al//vOfm00ikTCbYrFoNuFw2LMkRaNRs4lEImbj2QXJZNJscrmc2RQKBbPxdkuXLjWbTZs2mY1nH3kex7MeybdtqVTKbGq1mtkMDQ3V5bm6urrMZmpqymwkqaHB/rpUT09PXZ4vFouZzbPPPms2nmtN8t0Duru7zcazjzzb1tzcbDae81+Stm7dajYdHR11eb6ZmRmzmZ6erksjSf39/WYTCoXMxnMPKJVKZtPY2Gg2//M//2M2K1euNBtJ6uvrMxvPuXT44Ye7nm9P8hwnYF/k/HQV2Odw3355ee4lvIMMAAAAAIAYkAEAAAAAkMSADAAAAACAJAZkAAAAAAAkMSADAAAAACCJARkAAAAAAEkMyAAAAAAASGJABgAAAABAkhTxhtPT02YzNjZmNq2trWaTSCQ8S9Lk5KTZTExMmE00GjUbzy+Vbm5uNptarWY2ktTY2Gg2W7ZsqcvjFItFs+nq6jKbUqlkNpKUSqXMplqtmo3nPJk3b57ZeI6/5zzKZDJmI0mFQsFsPOd2uVw2m4GBAbPxXJOe4yH59uXixYvNxnPerl+/3mw6OzvNxnP+S1JDg/31xKmpKbPx7KOZmRmz8dxvPOeaJC1ZssRsKpWK2YyMjJjN4OCg2XjOyWQyaTae/Sj5rrdsNut6LAAAsG/jHWQAAAAAAMSADAAAAACAJAZkAAAAAAAkMSADAAAAACCJARkAAAAAAEkMyAAAAAAASGJABgAAAABAEgMyAAAAAACSpIg3HBoaMpu2tjazyWazZtPf3+9aUyqVMpve3l6zmZ6eNptYLGY2hULBbCYnJ81G8u3LcrlsNqFQyGxKpVJdHqdarZqNJIXDYbMZGRkxG8/2e843T5NMJs3Gc41IUmdnp9nMzMyYTWNjo9lUKhWzKRaLZuM5RyTfNZnP581mdHTUbKLRqNl4tn9sbMxsvM/nOW6ebWtpaTEbz/nm2X5vl06nzSYIArPxXEuec9KznqmpKbORpEWLFpnN4OCg67EAAMC+jXeQAQAAAAAQAzIAAAAAAJIYkAEAAAAAkMSADAAAAACAJAZkAAAAAAAkMSADAAAAACCJARkAAAAAAEkMyAAAAAAASJIi3vB1r3ud2Tz77LNmMzQ0ZDbbt293rampqclsGhsb69KEQiGzicfjZpPJZMzGu6YgCMxmZGTEbJqbm81mamrKbLwGBwfNxrP9ra2tZlMoFMxmcnLSbCYmJswmEvFdTn19fWaTzWbNZmBgwGza29vNxrNuz3okqVwum01/f7/Z1OvaLpVKZrN8+XKzkaRt27aZTTgcNpuFCxeazfj4uNmkUimz2bBhg9lI/vuSpVgsms3Y2JjZLFu2zGw89z/vNZnP582mVqu5HgsAAOzbeAcZAAAAAAAxIAMAAAAAIIkBGQAAAAAASQzIAAAAAABIYkAGAAAAAEASAzIAAAAAAJIYkAEAAAAAkMSADAAAAACAJCniDQcHB82mvb3dbMLhsNksXbrUtabJyUmz6e/vN5t0Om02nm3bvn17XR5Hkp566imzaWiwv77R1tZmNhMTE2bT1NRkNqOjo2YjST09PWbjOd/K5bLZVCoVs/Fsf1dXl9l4zkfJdw5MT0+bTWNjo9lUq1WzqdVqZlMsFs1GkiIR+5aSyWTMJh6Pm01fX5/ZLFiwoC6PI0lTU1Nm09LSYjYjIyNmMz4+bjaefT0wMGA2ktTa2mo2zzzzjNl4jtvixYvNJhqNmk0+nzcbz/1PknK5nNl49jcAANj38Q4yAAAAAABiQAYAAAAAQBIDMgAAAAAAkhiQAQAAAACQxIAMAAAAAIAkBmQAAAAAACQxIAMAAAAAIIkBGQAAAAAASQzIAAAAAABIkkJBEASe8K677jKbWCxmNuVy2WxGRkY8S1IoFDKbZDJpNtls1mymp6fNJh6Pm8327dvNRpImJibMJpVKmY1n2zzHZHJy0mwqlYrZSJLnlMtkMmaTTqfrsqbm5mazKRaLZuM5/yVpZmbGbMLhsNl4zu1arWY21WrVbDz7SJLWr19vNm94wxvMJpfLmU1bW5vZjI6Omk1ra6vZSNKGDRvMxnNNTk1NmY3nuo1EImbjuW95H8uzn8bGxszGs6ampiaz8VwjhULBbCTf+ea5Bk488UTX8+1JntdFYF/k/HQVr1Lc21AvnnsJ7yADAAAAACAGZAAAAAAAJDEgAwAAAAAgiQEZAAAAAABJDMgAAAAAAEhiQAYAAAAAQBIDMgAAAAAAkhiQAQAAAACQJEW84d133202f/Znf2Y2w8PDZrNs2TLXmlKplNls3rzZbEZGRsymUqmYTSKRMJvVq1ebjSTdcsstZnPYYYeZjeeXYY+Pj5tNOBw2m0wmYzaSVCqVXJ0lErFP33Q6bTbFYtFs8vm82USjUbPxGhoaMptyuWw2nu3v6uoym7GxMbORfNdkLpczG8/+npycrMvjeK5tSWppaTEbz37yXJOefeRZTzKZNBtJymazZuO5TzQ1NZlNR0eHa00Wz33bs48k371r6dKlrscCgFeDUCi0t5cA7DW8gwwAAAAAgBiQAQAAAACQxIAMAAAAAIAkBmQAAAAAACQxIAMAAAAAIIkBGQAAAAAASQzIAAAAAABIYkAGAAAAAECSFPGGjz/+uNkce+yxZrN8+XKzGRwc9CxJo6OjZtPb22s2fX19ZhMOh83Gs+7nnnvObCRp5cqVZjM2NmY2pVLJbHp6esymsbHRbPL5vNlI0sTEhNk0NTWZTblcNptoNGo2nn00OTlpNs3NzWYjSblczmwiEfvS7O7uNhvPvp6amjKbgYEBs5F81/fMzIzZtLW1mU0oFDIbz3708lzfnmNSKBTMxrOPYrGY2QRBYDaSVK1Wzaa9vd1sarWa2XjuE+l02mw82+Y5tyVpfHzcbFpbW83Gc97uad5jjvrw3IcAAPsW3kEGAAAAAEAMyAAAAAAASGJABgAAAABAEgMyAAAAAACSGJABAAAAAJDEgAwAAAAAgCQGZAAAAAAAJDEgAwAAAAAgSYp4wxUrVphNNBo1m0KhYDaxWMy1plAoVJfn6+zsfNkep7+/32y8RkZGzGbr1q1ms23bNrMplUpms3DhQrORpEQiYTaZTMZscrmc2XiOWyRiXwaVSsVsxsfHzUby7ct0Om02U1NTZuO5loaGhszGc8wk6ZlnnjGbgw46yGx27NhhNp77TWNjo9nE43GzkaR8Pl+Xx/LctzzHbXBw0GwaGnxfA/Vcb2NjY2bjOSdbW1vNZnh42GxSqZTZeK9Jz7kEAABeG3gHGQAAAAAAMSADAAAAACCJARkAAAAAAEkMyAAAAAAASGJABgAAAABAEgMyAAAAAACSGJABAAAAAJDEgAwAAAAAgCQp4g3Xr19vNu94xzvMJh6Pm01PT49nSRocHDSbUChkNolEwmy6u7vNZmhoyGympqbMxqtarZrNkiVLzGZkZMRsli5dajb5fN5sJKlQKJjNzMyM2UxPT5vNAQccYDbDw8Nm09raajblctlsJKm9vd1s0um02VQqFbPxnCOpVMpsOjs7zUaSnn322bqsadWqVWYzMTFhNqVSyWy8xy2bzZqN5/7W0tJiNhs2bDAbz33Lsx5JikTslwLPdevZtoYG++uynus/FouZjWe7JN/15t2XAABg38Y7yAAAAAAAiAEZAAAAAABJDMgAAAAAAEhiQAYAAAAAQBIDMgAAAAAAkhiQAQAAAACQxIAMAAAAAIAkBmQAAAAAACRJEW84NjZmNq2trWYzMzNjNoODg54lKZFImE00GjWbZDJpNuVy2WxKpZLZeFUqFbMJhUJmMzw8bDZBEJjN0NCQ2RQKBbORpLa2NrPxnG+edefzebPxnCOeYzs9PW02ku/YhsNhs/Eck46ODrPxHLenn37abCRpxYoVZuM5Jjt27DCbTCZjNp7jls1mzUaSpqamzMZzTkYi7tvui2posL++GYvFXI/lOSbj4+Nm4zmXli1bZjaea8Rz//fua8+x9TwfXns817zntRo2z370HA/4cG7jtYx3kAEAAAAAEAMyAAAAAACSGJABAAAAAJDEgAwAAAAAgCQGZAAAAAAAJDEgAwAAAAAgiQEZAAAAAABJDMgAAAAAAEhiQAYAAAAAQJIUqeeDhUIhs2lsbDSbqakp1/NlMhmzKRaLZlMoFMwmkUiYTVdXl9l4efbB6Oio2bS3t9fluSYnJ81m+fLlZiNJfX19ZhONRs2mtbXVbDz7aGRkxGySyaTZpFIps5GkIAjMpr+/3/VYllKpZDaec6RSqbie77HHHjObgw8+2Gw813ZLS4vZ5HI5s/Fum+ec9JwD5XLZbDz3Es+xrVarZiP57pMdHR1m47nehoaG6rIez/Z7rltJampqMpunn37abDzX0p7meR323IMAAHit4h1kAAAAAADEgAwAAAAAgCQGZAAAAAAAJDEgAwAAAAAgiQEZAAAAAABJDMgAAAAAAEhiQAYAAAAAQBIDMgAAAAAAkqSIN8xms2YzODhoNu3t7WbT1tbmWtP4+LjZlMtls2lubjabUChkNkEQmM3ExITZSFKhUDCbycnJujxOMpk0G8/x9/I8n6fZsWOH2SxcuNBsPOdINBo1m0qlYjaS75gsWrTIbDZv3mw24XC4LuuJx+NmI0lHHnmk2QwPD5tNQ4P9tTvP9e85jzzH1isWi5nN0NBQXZ7Ls49qtVrdHqu1tdX1WJaRkZG6PFcmkzGbYrHoWlM+nzcb7zUAAAD2bbyDDAAAAACAGJABAAAAAJDEgAwAAAAAgCQGZAAAAAAAJDEgAwAAAAAgiQEZAAAAAABJDMgAAAAAAEhiQAYAAAAAQJIU8YYjIyNms3nzZrNZtGiR2UxMTLjWlEwmzaZUKplNpVKpy+OUy2WzSSQSZiNJDQ321y6i0ajZpFIpsxkaGjKb3t5es5mcnDQbServ7zeb9vZ2s4nFYmbj2Y+edc+fP99sBgYGzEaSli5dajYbNmwwm66uLrOpVqt1aXK5nNlIvmtp4cKFZuPZl93d3WbjObc9160kzZs3z2yKxaLZeM5tz7XtuY6895uWlhaz8dxvPWtqbm42G8+1HQ6HzWZmZsZsJKmpqclsPPcSAACw7+MVHwAAAAAAMSADAAAAACCJARkAAAAAAEkMyAAAAAAASGJABgAAAABAEgMyAAAAAACSGJABAAAAAJDEgAwAAAAAgCQp4g0TiYTZTE1Nmc3Q0JDZlEol15pisZjZVCoVs5k/f77Z7Nixw2zS6bTZhEIhs5F821ar1czGs7/b2trMxrPufD5vNpJvP3kaD8/xb2xsNJtcLmc2ixcv9izJtZ8815vn2EYi9iXe2tpqNkEQmI0klctls1m/fr3ZZDIZsxkeHjabaDRqNtPT02YjSSMjI3V5Ps8+8lzbnuM2MDBgNpLv+vbsp87OTrOZmZkxG89rQEOD/fXdlpYWs5F85xsAAHht4B1kAAAAAADEgAwAAAAAgCQGZAAAAAAAJDEgAwAAAAAgiQEZAAAAAABJDMgAAAAAAEhiQAYAAAAAQBIDMgAAAAAAkqSIN9y6davZDA4Omk02mzWbqakp15q2b99uNqtXrzabsbExsymVSmZTqVTMZtWqVWYjScPDw2YTi8XMZnR01GzC4bDZdHR01GU9khSPx82mWq2azdDQkNk0NzebzX777Wc2mzdvNpstW7aYjeRb09KlS81mw4YNZtPa2mo2+XzebMbHx81G8h1bz7Z5zknP+ZbJZMwmnU6bjSQ1Njaajede4tlHqVTKbDz3iJaWFrORfPe3QqFgNp57oGf7o9Go2Xj2tef1RpKCIDCbXC5nNkuWLHE9HwDgpfHcp1/tQqHQ3l7CawbvIAMAAAAAIAZkAAAAAAAkMSADAAAAACCJARkAAAAAAEkMyAAAAAAASGJABgAAAABAEgMyAAAAAACSGJABAAAAAJAkRbzhzMyM2SQSCbNJp9PepzS97nWvM5tCoWA2ra2tZlMul83Gs/1jY2NmI0mRiH1oGhrsr2+Ew2GzqVQqZrNjxw6z8ewjSUqlUmYzPT1tNl1dXWbjOSbz5883m+3bt5vN4YcfbjaS71ryHNujjjrKbDy/VD4IArPp7+83G+9jec7tBQsWmM3w8LDZeM7/eDxuNl49PT11eb5qtWo2nmPrOW+9z1er1cxmZGTEbDzbPzExYTa9vb1m09fXZzaS77XE+1gA9h7PfRH143nNB/ZFvIMMAAAAAIAYkAEAAAAAkMSADAAAAACAJAZkAAAAAAAkMSADAAAAACCJARkAAAAAAEkMyAAAAAAASGJABgAAAABAEgMyAAAAAACSpIg3TKVSZrNp0yaz2bJli9nE43HPkhQEgdkMDQ2ZzbPPPms2LS0tZjM+Pm424XDYbCQpl8uZzetf/3qzmZmZcT2fpbW11WyGh4ddj+XZB/vtt5/ZlEols9m2bZvZjI6Oms3y5cvNZnBw0GwkqVKpmM3SpUvNZmxszGyamprMxrMfQ6GQ2Xi7zs5Os8nn82bjOSc9+6i7u9tsJKm/v99sPPckzzm5aNEisymXy2bT3NxsNpK0efPmuj2WJZ1Om43nGmlosL++G4vFXGvyPJ/n+AMAgH0f7yADAAAAACAGZAAAAAAAJDEgAwAAAAAgiQEZAAAAAABJDMgAAAAAAEhiQAYAAAAAQBIDMgAAAAAAkhiQAQAAAACQJEW8YRAEZjM0NFSXplwuu9YUDofNpqenx2zi8bjZPProo2aTzWbNZmpqymwkqaHB/trFwMCA2RQKBbNJJpNmk8/nzcZzjkhSsVg0G8+2edbtWZPn+I+Pj5tNd3e32Ui+YzI4OGg2iUTCbHbs2GE2qVTKbDo7O81GkrZt22Y2nnM7nU6bzZYtW8zGs21ejY2NZuNZd29vr9l4zv8FCxaYzejoqNlIvnuu5/z2XJOe621mZsZsmpqazMbzGiFJ69evN5uuri7XYwEAgH0b7yADAAAAACAGZAAAAAAAJDEgAwAAAAAgiQEZAAAAAABJDMgAAAAAAEhiQAYAAAAAQBIDMgAAAAAAkhiQAQAAAACQJEW8YTgcNptCoWA2Dz30kNmsXr3ataauri6z2bFjh9lMTk6aTUdHh9ls3brVbDo7O81G8u3LoaEhs/Ect/7+frN53eteZzbT09NmI0mxWMxsPNvf1NRkNul02mwGBgbMplgsmo1nX0tSNBo1m+7ubrPxHLfGxkazSaVSZuPZfsm3bZs2bTKblpYWs/Fck5GIfYvz7Efvmvr6+symWq2ajeee5LmO6nnc8vm82Wzbts1sent7zcZzn/Rcb559LUkrVqwwG+95AgAA9m28gwwAAAAAgBiQAQAAAACQxIAMAAAAAIAkBmQAAAAAACQxIAMAAAAAIIkBGQAAAAAASQzIAAAAAABIYkAGAAAAAECSFPGGMzMzZtPU1GQ26XTabCIR37KefvppswmHw2bjWffY2JjZtLW1mU1LS4vZSNL//M//mE02mzWbnp4eswmCwGxGR0fNZuHChWYjSePj42bT2tpqNvl83mympqbMJpFImI1nHzU0+L7eFAqFzMZzTuZyOdfzWbZu3Wo27e3trseqVCpm47kGpqenzSaZTJpNoVAwG889SZImJibMxnMuRaNRs8lkMmbjOSc955rku795Hstz765Wq2bj2Y9PPPGE2axevdpsJGnbtm1m43kNBAAA+z7eQQYAAAAAQAzIAAAAAABIYkAGAAAAAEASAzIAAAAAAJIYkAEAAAAAkMSADAAAAACAJAZkAAAAAAAkMSADAAAAACBJinjDYrFoNgMDA2azfft2s1mxYoVrTdFo1GxaW1vNJh6Pm83IyIjZdHd3m83Q0JDZSFJbW5vZpFIps3nmmWfMprm52WxyuZzZhEIhs5Gk/v5+s2lvbzcbz/GfnJw0m1gsZja1Ws1s8vm82UhST0+P2Tz33HNmEw6Hzaahwf4aWDKZNJsgCMxGkhobG81mamrKbDzHxPM4nvNoZmbGbCSpWq2ajeca8NxLCoWC2SxatMhsvOdkOp02my1btpiN577luW496+7o6DCbTZs2mY3kO28990AAALDv4x1kAAAAAADEgAwAAAAAgCQGZAAAAAAAJDEgAwAAAAAgiQEZAAAAAABJDMgAAAAAAEhiQAYAAAAAQBIDMgAAAAAAkqSIN2xubjabsbExs8nlct6nNKXTabMJgsBstm/fbjYtLS1mU61WzWbp0qVmI0mDg4NmE41Gzaanp8dsPOtuaLC/luI9tqtXr67Lmqanp82mXC6bTSgUMptKpWI2nnNEksbHx83Gs789+yiVSplNW1ub2fT19ZmN5LsmJycnzaazs9NsCoWC2cRiMbPxHH9vVywWzSYcDptNU1OT2cTjcbMplUpmI/nOb89rgGf7PffkSMR+aWpvbzcbz7Um+Y7Jk08+6XosAACwb+MdZAAAAAAAxIAMAAAAAIAkBmQAAAAAACQxIAMAAAAAIIkBGQAAAAAASQzIAAAAAABIYkAGAAAAAEASAzIAAAAAAJIYkAEAAAAAkCRFvGE2mzWb1atXm80b3/hGs2lvb3etaXh42GwmJibMJh6Pm02tVjObSqViNlNTU2YjSdVq1Wyee+45s1m0aJHZZDIZsymVSmbT2tpqNpKUy+XMplwu1+X5YrGY2QRBUJfHmZ6eNhvvY3nOt8nJSbNpaLC/BhaJ2LeBZDJpNpK0fft2s2lpaTGbsbExs/Gck4VCwWw8x1+S0um02XjuAY2NjWbjObc9x99zb5Pqd00ODAyYTT6fN5vBwUGz8Rxbz/1P8r2WPPzww67HAgAA+zbeQQYAAAAAQAzIAAAAAABIYkAGAAAAAEASAzIAAAAAAJIYkAEAAAAAkMSADAAAAACAJAZkAAAAAAAkMSADAAAAACBJinjDbdu2mc2CBQvM5rnnnjObpUuXutbU2tpqNlNTU2ZTKBTMZuHChWYzNjZmNh0dHWYjSTMzM2Zz4IEHms3o6KjZhEIhsykWi2YzPT1tNpIUj8fNprOzsy7PV6lUzCaZTJpNEARm49kuybe/s9ms2XjWnc/n69JUq1WzkXz3gP7+frPx3AM814jnHPHcR7z6+vrM5sknnzSbiYkJsymXy3V5HEkaHBw0G8+9NJfLmU00GjUbz3Gr1Wpms3XrVrORfK8BjzzyiOuxAADAvo13kAEAAAAAEAMyAAAAAACSGJABAAAAAJDEgAwAAAAAgCQGZAAAAAAAJDEgAwAAAAAgiQEZAAAAAABJDMgAAAAAAEiSQkEQBJ5w0aJFZjNv3jyzWb58udkceeSRniVp1apVZjM0NGQ25XLZbKLRqGtNlng87upGR0fNJhwOm82yZcvMplqtms3Y2JjZeM4RScpms2ZTKBTMpqmpyWw8+3FiYsJsuru7zebRRx81G0nq7e01mwULFpiNZ9tqtZrZLFmyxGx+//vfm43kO25PP/202fz2t791PZ/Fs4+KxaLrsRKJhNnMzMyYjedeEolEzGbr1q1m4zUyMlKXxwmFQmbjud8kk0mz8Wy/955UqVTM5sknnzQb58vpHuU5Bq+Edb6WeI4JsC96Jd5LuN5g8Zy3vIMMAAAAAIAYkAEAAAAAkMSADAAAAACAJAZkAAAAAAAkMSADAAAAACCJARkAAAAAAEkMyAAAAAAASGJABgAAAABAkhQKXom/5RsAAAAAgJcZ7yADAAAAACAGZAAAAAAAJDEgAwAAAAAgiQEZAAAAAABJDMgAAAAAAEhiQAYAAAAAQBIDMgAAAAAAkhiQAQAAAACQxIAMAAAAAIAkBmQAAAAAACQxIAMAAAAAIIkBGQAAAAAASQzIAAAAAABIYkAGAAAAAEASAzJeYdatW6dQKDT7XyQSUW9vr84//3xt3779ZVnD4sWLdd55583+/y9/+UuFQiH98pe/fEmPc88992jt2rUaGxur6/ok6bzzztPixYvN7thjj9WBBx5Y9+d/Jbnrrrv0F3/xF9p///2VSqU0f/58nXnmmfr973+/t5cGAJDmvK6/2H8v9XV2T1u/fr3Wrl2rTZs2ufqdn8M88MADe3Zhe1FfX58uueQSvfnNb1Z7e7uy2aze8IY36Nvf/raq1ereXh5QF5G9vQBgd2688Ubtv//+mpmZ0a9//Wv94z/+o371q1/pscceUyqVelnXcuihh+ree+/VAQcc8JL+3j333KPLLrtM5513npqbm/fM4qBvfvObyuVy+vjHP64DDjhAQ0ND+tKXvqQjjjhCd955p44//vi9vUQAeE2799575/z/FVdcobvvvlt33XXXnI+/1NfZPW39+vW67LLLdOyxx7q+KP1a8Pvf/1433XSTzj33XH3uc59TNBrV7bffrr/6q7/SfffdpxtuuGFvLxH4X2NAxivSgQceqMMOO0ySdNxxx6lareqKK67Qrbfeqg984AO7/Tv5fF7JZLLua8lmszriiCPq/rioj+uuu06dnZ1zPnbyySdr+fLluvLKKxmQAWAve/5raEdHhxoaGur22rqnXv+xqz/7sz/Txo0bFY1GZz924oknqlQq6brrrtNll12mBQsW7MUVAv97fIs19gk7X0Q3b94s6Q/fYpxOp/XYY4/pbW97mzKZjN761rdKkkqlkj7/+c9r//33VzweV0dHh84//3wNDQ3NecxyuayLL75Y3d3dSiaTOuqoo3T//ffv8twv9C3Wv/3tb3XGGWeora1NiURCy5Yt09/93d9JktauXatPfepTkqQlS5bs9tvHvvvd7+rNb36zUqmU0um0TjrpJD300EO7PP+6deu0cuVKxeNxrVq1SjfddNOftA93CoVC+pu/+RvdeOONWrlypRobG3XYYYfpvvvuUxAEuvrqq7VkyRKl02kdf/zxeuaZZ+b8/Z///Oc688wz1dvbq0QioeXLl+uCCy7Q8PDwLs/1k5/8RK9//esVj8e1dOlSXXvttVq7dq1CodCcLggCfeMb39DBBx+sxsZGtbS06N3vfreeffZZc3uePxxLUjqd1gEHHKCtW7e+xL0DANgbrrvuOh199NHq7OxUKpXS6tWr9U//9E8ql8tzup3/dOjXv/61jjzySCWTSf3FX/yFJGnbtm1697vfrUwmo+bmZn3gAx/Q7373O4VCIa1bt27O4zzwwAN6+9vfrtbWViUSCR1yyCH63ve+N/vn69at03ve8x5Jf/hC/c7X8ec/jmXn5ytPPvmkTjrpJKVSKfX09Oiqq66SJN1333066qijlEqltN9+++nf/u3f5vz9oaEh/fVf/7UOOOAApdNpdXZ26vjjj9dvfvObXZ6rntv/QlpaWuYMxzsdfvjhs2sA9nW8g4x9ws4hraOjY/ZjpVJJb3/723XBBRfoM5/5jCqVimq1ms4880z95je/0cUXX6wjjzxSmzdv1qWXXqpjjz1WDzzwgBobGyVJH/7wh3XTTTfpoosu0oknnqjHH39cZ511liYnJ8313HnnnTrjjDO0atUqffnLX9bChQu1adMm/ed//qck6UMf+pBGRkb0ta99TT/60Y/U09Mj6f99+9iVV16pSy65ROeff74uueQSlUolXX311XrLW96i+++/f7Zbt26dzj//fJ155pn60pe+pPHxca1du1bFYlENDX/617duu+02PfTQQ7rqqqsUCoX06U9/WqeddprWrFmjZ599Vl//+tc1Pj6uCy+8UO9617v08MMPzw61Gzdu1Jvf/GZ96EMfUlNTkzZt2qQvf/nLOuqoo/TYY4/NvnDecccdOuuss3T00Ufru9/9riqViq655hoNDAzssp4LLrhA69at08c+9jF98Ytf1MjIiC6//HIdeeSReuSRR9TV1fWStm98fFwPPvgg7x4DwD5i48aNev/7368lS5YoFovpkUce0Re+8AU9+eSTu3zbbl9fn8455xxdfPHFuvLKK9XQ0KDp6Wkdd9xxGhkZ0Re/+EUtX75cd9xxh9773vfu8lx33323Tj75ZL3pTW/S9ddfr6amJt1yyy1673vfq3w+r/POO0+nnXaarrzySv393/+9rrvuOh166KGSpGXLlr3kbSuXyzrrrLP0kY98RJ/61Kf0H//xH/rsZz+riYkJ/fCHP9SnP/1p9fb26mtf+5rOO+88HXjggXrDG94gSRoZGZEkXXrpperu7tbU1JR+/OMf69hjj9UvfvELHXvssZJU9+1/qe666y5FIhHtt99+L/nvAq84AfAKcuONNwaSgvvuuy8ol8vB5ORkcNtttwUdHR1BJpMJ+vv7gyAIgjVr1gSSghtuuGHO3//Od74TSAp++MMfzvn47373u0BS8I1vfCMIgiB44oknAknBJz7xiTndzTffHEgK1qxZM/uxu+++O5AU3H333bMfW7ZsWbBs2bJgZmbmBbfl6quvDiQFzz333JyPb9myJYhEIsHf/u3fzvn45ORk0N3dHZx99tlBEARBtVoN5s2bFxx66KFBrVab7TZt2hREo9Fg0aJFL/jcOx1zzDHB6173ujkfkxR0d3cHU1NTsx+79dZbA0nBwQcfPOe5vvrVrwaSgkcffXS3j1+r1YJyuRxs3rw5kBT85Cc/mf2zN77xjcGCBQuCYrE4Zxvb2tqCP7713HvvvYGk4Etf+tKcx966dWvQ2NgYXHzxxeZ2Pt8HPvCBIBKJBA888MBL/rsAgD1rzZo1QSqVesE/r1arQblcDm666aYgHA4HIyMjs392zDHHBJKCX/ziF3P+znXXXRdICm6//fY5H7/gggsCScGNN944+7H9998/OOSQQ4JyuTynPf3004Oenp6gWq0GQRAE3//+93d5/X8xOz+H+d3vfjdnW5//eUm5XA46OjoCScGDDz44+/FcLheEw+HgwgsvfMHnqFQqQblcDt761rcG73znO/fo9nvdeeedQUNDwy6fUwH7Kr7FGq9IRxxxhKLRqDKZjE4//XR1d3fr9ttv3+WdxHe9611z/v+2225Tc3OzzjjjDFUqldn/Dj74YHV3d89+i/Pdd98tSbv8e+azzz5bkciLf2PF008/rY0bN+qDH/ygEonES962O++8U5VKReeee+6cNSYSCR1zzDGza3zqqae0Y8cOvf/975/zLcmLFi3SkUce+ZKf948dd9xxc37Y2apVqyRJp5xyypzn2vnxnd/aLkmDg4P6yEc+ogULFigSiSgajWrRokWSpCeeeELSH76S/cADD+gd73iHYrHY7N9Np9M644wz5qzltttuUygU0jnnnDNnf3R3d+uggw56yT/V9HOf+5xuvvlmfeUrX5n9CjwA4JXtoYce0tvf/na1tbUpHA4rGo3q3HPPVbVa1dNPPz2nbWlp2eU7hH71q18pk8no5JNPnvPx973vfXP+/5lnntGTTz45+/r/x687p556qvr6+vTUU0/VddtCoZBOPfXU2f+PRCJavny5enp6dMghh8x+vLW1VZ2dnXNecyXp+uuv16GHHqpEIjH7uvuLX/xi9jVX2nvb/+CDD+rss8/WEUccoX/8x390/z3glYxvscYr0k033aRVq1YpEomoq6tr9luU/1gymVQ2m53zsYGBAY2Njc0Zyv7Yzn8nm8vlJEnd3d1z/jwSiaitre1F17bz3zL39vb6NuZ5dn6L8Rvf+Mbd/vnOb51+oTXu/Jj3107sTmtr65z/37m/XujjhUJBklSr1fS2t71NO3bs0Oc+9zmtXr1aqVRKtVpNRxxxhGZmZiRJo6OjCoJgt98a/fyPDQwMvGArSUuXLnVv12WXXabPf/7z+sIXvqC/+Zu/cf89AMDes2XLFr3lLW/RypUrde2112rx4sVKJBK6//779dGPfnT2tWWn3X1OkMvl3K85knTRRRfpoosu2u16dvczNf43ksnkLl9Qj8Viu7zm7vz4ztdcSfryl7+sT37yk/rIRz6iK664Qu3t7QqHw/rc5z43Z0DeG9v/0EMP6cQTT9SKFSv0s5/9TPF43PX3gFc6BmS8Iq1atWr2p1i/kOf/oCdJam9vV1tbm+64447d/p1MJiNJs0Nwf3+/5s+fP/vnlUpldjB9ITv/HfSf+oMo2tvbJUk/+MEPZt953Z0/XuPz7e5jL4fHH39cjzzyiNatW6c1a9bMfvz5P8irpaVFoVBot//e+Plrb29vVygU0m9+85vdvrh6X3Avu+wyrV27VmvXrtXf//3fu/4OAGDvu/XWWzU9Pa0f/ehHc14XH3744d32u3v9b2tr2+0P2tzda44kffazn9VZZ52128dfuXKld+l73L//+7/r2GOP1Te/+c05H3/+z0t5ubf/oYce0gknnKBFixbpP//zP9XU1GT+HWBfwYCMV5XTTz9dt9xyi6rVqt70pje9YLfzh1rcfPPNc74N93vf+54qlcqLPsd+++2nZcuW6YYbbtCFF174ggPczo8//yvfJ510kiKRiDZu3LjLt4j/sZUrV6qnp0ff+c53dOGFF85+QrB582bdc889mjdv3ouuc0/YuYbnb/O3vvWtOf+fSqV02GGH6dZbb9U111wz+0701NSUbrvttjnt6aefrquuukrbt2/X2Wef/Set64orrtDatWt1ySWX6NJLL/2THgMAsHfs7rUlCAL9y7/8i/sxjjnmGH3ve9/T7bffrlNOOWX247fccsucbuXKlVqxYoUeeeQRXXnllS/6mC/0Ov5yCoVCu7zmPvroo7r33nvn/DqlPbH9L+Thhx/WCSecoN7eXv385z9XS0vLn/Q4wCsVAzJeVf78z/9cN998s0499VR9/OMf1+GHH65oNKpt27bp7rvv1plnnql3vvOdWrVqlc455xx99atfVTQa1QknnKDHH39c11xzzS7ftr071113nc444wwdccQR+sQnPqGFCxdqy5YtuvPOO3XzzTdLklavXi1Juvbaa7VmzRpFo1GtXLlSixcv1uWXX65/+Id/0LPPPquTTz5ZLS0tGhgY0P33369UKqXLLrtMDQ0NuuKKK/ShD31I73znO/XhD39YY2NjWrt27W6/7frlsP/++2vZsmX6zGc+oyAI1Nraqp/+9Kf6+c9/vkt7+eWX67TTTtNJJ52kj3/846pWq7r66quVTqdnfyqn9IffqfiXf/mXOv/88/XAAw/o6KOPViqVUl9fn/77v/9bq1ev1l/91V+94Jq+9KUv6f/8n/+jk08+Waeddpruu+++OX/O77AGgFe2E088UbFYTO973/t08cUXq1Ao6Jvf/KZGR0fdj7FmzRp95Stf0TnnnKPPf/7zWr58uW6//XbdeeedkjTnNz9861vf0imnnKKTTjpJ5513nubPn6+RkRE98cQTevDBB/X9739fknTggQdKkr797W8rk8kokUhoyZIl5j/FqqfTTz9dV1xxhS699FIdc8wxeuqpp3T55ZdryZIlc76gvye2f3eeeuopnXDCCZKkL3zhC9qwYYM2bNgw++fLli2b8xtHgH3S3v0ZYcBcu/sJkLvzYj8Bs1wuB9dcc01w0EEHBYlEIkin08H+++8fXHDBBcGGDRtmu2KxGHzyk58MOjs7g0QiERxxxBHBvffeGyxatMj8KdZB8IefvnzKKacETU1NQTweD5YtW7bLT3D87Gc/G8ybNy9oaGjY5TFuvfXW4Ljjjguy2WwQj8eDRYsWBe9+97uD//qv/5rzGP/6r/8arFixIojFYsF+++0X3HDDDcGaNWv+Vz/F+qMf/eicjz333HOBpODqq6+e8/Gd2/79739/9mPr168PTjzxxCCTyQQtLS3Be97znmDLli2BpODSSy+d8/d//OMfB6tXrw5isViwcOHC4Kqrrgo+9rGPBS0tLbus9YYbbgje9KY3BalUKmhsbAyWLVsWnHvuueZPot75E01f6D8AwCvL7l7Df/rTn86+bs+fPz/41Kc+Fdx+++27vHbu7nVtpy1btgRnnXVWkE6ng0wmE7zrXe8Kfvazn+3yWxaCIAgeeeSR4Oyzzw46OzuDaDQadHd3B8cff3xw/fXXz+m++tWvBkuWLAnC4fAuPw36+V7op1jv7vOVF9qORYsWBaeddtrs/xeLxeCiiy4K5s+fHyQSieDQQw8Nbr311t1+HrAntv+FtvGF/nux/QPsK0JBEAQv1zAO4LWtXC7r4IMP1vz582d/ZzQAAHvKlVdeqUsuuURbtmz5k3+45r7stb79wJ+Cb7EGsMd88IMf1Iknnqienh719/fr+uuv1xNPPKFrr712by8NAPAq8/Wvf13SH/45ULlc1l133aV//ud/1jnnnPOaGA5f69sP1AsDMoA9ZnJyUhdddJGGhoYUjUZ16KGH6mc/+9nsv18CAKBeksmkvvKVr2jTpk0qFotauHChPv3pT+uSSy7Z20t7WbzWtx+oF77FGgAAAAAASQ12AgAAAADAqx8DMgAAAAAAYkAGAAAAAEASAzIAAAAAAJJewk+xPuSQQ8ymr6/PbIrFotnE43HXmlpaWswmkUi4HstSKBTq8jjpdNrVDQ4Omo3n56tVKhWzicViZhMOh82mXC6bjSSFQiGz8Rxbz5paW1vN5sMf/rDZLF++3Gw8+1qSJiYmzMaz7vHxcbPxnP+efd3U1GQ2ku+c3Lx5s9mUSiWzyefzZpPJZMxmx44dZiP57ksdHR11eb5sNms2yWTSbJqbm81Gkqanp83Gc5+IRqNmU61WzcZzbO+//36z+cEPfmA2kjQ1NVWXZsOGDa7n25M891dgX/Rq/pmyXLfAy8dzL+EdZAAAAAAAxIAMAAAAAIAkBmQAAAAAACQxIAMAAAAAIIkBGQAAAAAASQzIAAAAAABIYkAGAAAAAEDSS/g9yJ7fg+r5/a6pVMpsvL8r2PN7Z9va2szG87s7PWvy/J7UWq1mNpJ02GGHmY1n2zy/K3jx4sVmc+KJJ5rN8PCw2Ui+3/fn+V2xnt/d6jlvPb+bO5fLmY339yB79rdnTV1dXWbjOd88v9/Wu22RiH1L8ay7v7/fbDzXreea7OzsNBtJamiwv564bds2s/HcJz2/K9hzjniuf8n3O+w9vzfQc2w3bdpkNp5j6/ld0d7tf/rpp83Gew0Arxav5t87DAAvhneQAQAAAAAQAzIAAAAAAJIYkAEAAAAAkMSADAAAAACAJAZkAAAAAAAkMSADAAAAACCJARkAAAAAAEkMyAAAAAAASJIi3nB8fNxsDj30ULNJJBJm09bW5lpTa2ur2bS3t5tNS0uL2XR2dprNgQceaDa5XM5sJKlWq5lNQ4P99Y1qtWo2yWTSbCYnJ82mUCiYjSQ1NjaaTU9PT13WNDQ0ZDbNzc1mk81mzWZmZsZsJGliYsLVWTz70fNcnsfxHtupqSmzCYfDZhOLxcwmlUqZjef4p9Nps5F8+8CzL1esWGE2w8PDZuO5/j37WpKWLFliNmNjY2ZTLpfNpru722xKpZLZeO7/xWLRbLxr8rwGAgCAfR/vIAMAAAAAIAZkAAAAAAAkMSADAAAAACCJARkAAAAAAEkMyAAAAAAASGJABgAAAABAEgMyAAAAAACSGJABAAAAAJAkRbzhxz/+cbOZN2+e2UxPT5vNihUrXGtqbW01m7GxMbOpVqtmEwSB2ZRKJbNJpVJmI0kjIyNmk81mzWZgYMBsJicnzaZWq5lNV1eX2UhSLpczm5mZGbPxnEue4zYxMWE2+XzebNra2sxGksLhsNmMj4+bjfdcspTLZbOpVCqux0okEmbjuSaj0ajZjI6Omo1nH3mOhyS1t7ebjeee5NmXjY2NrjVZPPcISSoUCmYTj8fNxrNuz30yk8mYTbFYNJvVq1ebjSTdc889ZtPX1+d6rFcLz73z5RYKhfb2EgAArwG8gwwAAAAAgBiQAQAAAACQxIAMAAAAAIAkBmQAAAAAACQxIAMAAAAAIIkBGQAAAAAASQzIAAAAAABIYkAGAAAAAEASAzIAAAAAAJKkiDd8wxveYDbDw8Nm093dbTaNjY2uNXmer1armU0kYu+GarVqNrFYzGyee+45s5Gk1tZWsxkfHzebhgb7ayChUMhsPPtxbGzMbCRp/vz5ZvPwww+bzczMjNl0dXXV5XESiURdnkuSNm/ebDYLFy40m4GBAbPxrDuVSplNsVg0G8l3LjU3N5vN9u3bzaZQKJiN59j29PSYjeS7T0xOTpqN516Sz+fNprOz02y812Q2mzUbz/ZHo9G6PNfExITZdHR0mE1TU5PZSL59mclkXI+1twVBsLeXAOAl8ly3ntdXAPXBO8gAAAAAAIgBGQAAAAAASQzIAAAAAABIYkAGAAAAAEASAzIAAAAAAJIYkAEAAAAAkMSADAAAAACAJAZkAAAAAAAkSRF3GLHTiYkJsykUCmazbNky15rGx8fNJhqNmk08HjebZDJpNmNjY2bT3d1tNpJULpfr0qTTabMZHBw0m+bmZrMpFotmI0kPP/yw2bS3t5uN51waGBgwG8+2ZbNZs/Gc/5KUSCTMxrPuej3Xtm3bzMZ73k5PT5uNZ9taWlrMZmZmxmzy+bzZ5HI5s5F815LnHuC5TjKZTF3WEwSB2Ui+a8lzDXiez3Pf8py3nuPf29trNpL0xBNP1OX5sGd5zq9QKPQyrOS1wbMvvfcYANiX8A4yAAAAAABiQAYAAAAAQBIDMgAAAAAAkhiQAQAAAACQxIAMAAAAAIAkBmQAAAAAACQxIAMAAAAAIIkBGQAAAAAASVLEG27fvt1sPL8wfmpqymy2bdvmWlNbW5vZTE9Pux7L0tBgfy2hXC7X5bkkKZlM1qXx7O/Ozk6zGRsbMxuvlpYWs5mcnDSbpqYms4nH42bj2Y/FYtFsotGo2Ui+8ySbzdblcTz7MZVKmc3ExITZSL57wLJly8zmqaeeMhvPuj370XNsJd+1FInYt9RMJmM2tVrNbIaHh83Gc41Ivm175plnzObwww83m8HBQbMZHx83m0QiYTY9PT1mI/n2pfc8AQAA+zbeQQYAAAAAQAzIAAAAAABIYkAGAAAAAEASAzIAAAAAAJIYkAEAAAAAkMSADAAAAACAJAZkAAAAAAAkMSADAAAAACBJinjDrq4us8nlcmaz3377mU2pVHKtaePGjWbT3d1tNjMzM2YzPT1tNul02myGh4fNRpKKxeLLtibP47S1tZnN5OSk2UhSLBYzm6VLl5rN5s2bzaZarZqNZ1/H43GzaWxsNBvJt/2ea6C5udlsxsbGzKahwf46WSKRMBtJSiaTZlOpVMyms7PTbMrlstnk83mziUR8t0HPcQuFQnV5Ps+2ea63IAjMRpLC4bDZFAoFs9m2bZvZeM43z3703Ns856MktbS0mM0TTzzheiwAALBv4x1kAAAAAADEgAwAAAAAgCQGZAAAAAAAJDEgAwAAAAAgiQEZAAAAAABJDMgAAAAAAEhiQAYAAAAAQBIDMgAAAAAAkqSINxwdHTWbcDhcl6a1tdW1pq6uLrMJhUJmMz09bTbj4+Nmk06nzaa3t9dsJGl4eNhsFixYYDZDQ0Nm097ebjZBENRlPd41bdq0yWz6+/vNxnNMWlpazMZzPJLJpNlIUiwWM5tKpWI2tVrN9XyWXC5nNvPmzXM9lmfdnn05MjJiNqlUqi7raWpqMhtJKhQKZjM1NWU2W7duNZulS5eajWfdnnNNkkqlUl3WVK1Wzaanp8dstm/fXpfnWrFihdlIvvMkkUi4HgsAAOzbeAcZAAAAAAAxIAMAAAAAIIkBGQAAAAAASQzIAAAAAABIYkAGAAAAAEASAzIAAAAAAJIYkAEAAAAAkMSADAAAAACAJCniDYvFotnEYjGzCYVCZlOpVFxrqtVqZlMoFMwmHA6bTSaTMZumpiaz6e/vNxtJqlarZjM1NWU2iUTCbPL5vNk0NjaaTT2PWzqdNpve3l6zyeVyZuM5b+PxuNl4jpkkRSL2ZefZR55mdHTUbNra2sxmaGjIbCQpm82ajeeYeO43w8PDZuM5J0ulktlIvuPruZd0dXWZzcTEhNl47qWec0SSFi5caDaTk5Nmk0ql6vI4MzMzZuO5jqLRqNlIvvu7534LAAD2fbyDDAAAAACAGJABAAAAAJDEgAwAAAAAgCQGZAAAAAAAJDEgAwAAAAAgiQEZAAAAAABJDMgAAAAAAEhiQAYAAAAAQBIDMgAAAAAAkqSIN5w/f77ZbNiwwWxyuZzZdHd3u9Y0MjJiNtVq1Wyi0ajZTExMmE0qlTIbz5olKZlMmk2xWDSbxsZGsymXy2ZTqVTMJhwOm40kZTIZs5mcnDSbSMQ+fTs6OsymUCiYTUtLi9l4ec4lT5NOp83Gs+6ZmRmz8RwPSarVambjOU8812Rvb6/ZjI2NmY3n/Jd8a0okEmYzPDxsNp2dnWbjObaDg4NmI0mPPvqo2XjON8816dl+z370XLeeYyb5zoH29nbXYwHYe0Kh0N5eAoBXAd5BBgAAAABADMgAAAAAAEhiQAYAAAAAQBIDMgAAAAAAkhiQAQAAAACQxIAMAAAAAIAkBmQAAAAAACQxIAMAAAAAIEmKeMOtW7eaTblcNptUKmU2O3bscK3J81iJRMJsPL9YfvHixWbT19dnNgsWLDAbSSoWi2YzPT1tNtFo1GwmJibMJp/Pm02hUDAbSWpubjYbz7nk2bZqtWo2TU1NZlOr1czGc8wkqbGx0WxmZmbMxrP9DQ3218A6OzvNxrNmSRobGzObIAjMpqury2ympqbMpre312w89zbJd96m02mz6ejoMBvPPclzvrW2tpqNJCWTSbPxXJOlUslsPNs/OTlpNp7jH4vFzEaS2tvbzeb3v/+967Gwd3nuL57rCz7sSwCvRryDDAAAAACAGJABAAAAAJDEgAwAAAAAgCQGZAAAAAAAJDEgAwAAAAAgiQEZAAAAAABJDMgAAAAAAEhiQAYAAAAAQJIU8YbRaNRsmpub6/I46XTasyRls1mzyefzZjMzM2M2U1NTZuPZth07dpiNJJXLZbPx7KeRkRGzicVidXkuz772Pl+tVjObTCZjNrlczmwiEfsyGBsbMxvPmiUpmUyazYIFC8xmYmLC9Xz1eJxCoeB6LM+xbWxsNJvR0VGzqdc56blv1fP5POdbQ4P9tUvP+RaPx81G8t3fKpWK2Xiuyf7+frPx3NtTqZTZePa1JC1cuNBsPOc2AADY9/EOMgAAAAAAYkAGAAAAAEASAzIAAAAAAJIYkAEAAAAAkMSADAAAAACAJAZkAAAAAAAkMSADAAAAACCJARkAAAAAAElSxBuGw2H7wSL2w1Wr1bo0kvTkk0+aTWtrq9kkEgmzKRaLrjVZPOuRfPtg06ZNZpNOp83Gc2w92x+LxcxGkpqamsxmaGjIbKLRqNnUajWzicfjdXkcz/kvSZVKxWxKpZLZeLbfs6ZCoWA2nmtE8p0Dnn3Z0dFhNrlczmzK5bLZdHZ2mo33sfL5vNl47gEzMzNm41n3wMCA2UhSNps1G885OT09bTaee5vncVKplNl4zhFJWr58udk0Nja6Huu1JBQK7e0lAABQd7yDDAAAAACAGJABAAAAAJDEgAwAAAAAgCQGZAAAAAAAJDEgAwAAAAAgiQEZAAAAAABJDMgAAAAAAEhiQAYAAAAAQJIU8YbT09Nm09PTYzYNDfZM3tzc7FmSisWi2bS0tJhNJpMxm4mJCbMZGxszG69SqWQ2yWTSbKLRqNl49mNjY6PZZLNZs5GkoaEhsymXy2ZTKBTMxrNuz7FtbW01m0jEdzl59nc+nzcbz/Gv13PF43GzkaSZmRmz8Rxbz770rCkIArPx3Nsk33m7dOlSsxkZGTGb0dFRs/Ecf+816REOh82mqanJbDzniOd1wnOP9Nz/Jd/5Vs/7+54UCoX29hIAANin8Q4yAAAAAABiQAYAAAAAQBIDMgAAAAAAkhiQAQAAAACQxIAMAAAAAIAkBmQAAAAAACQxIAMAAAAAIIkBGQAAAAAASVLEG05PT5tNNps1m9HRUbMJhUKuNXmeL5/Pm025XDabarVqNg0N9tcbksmk2UhSU1OT2Xj2pedxPMe2UqmYzXPPPWc2krR48WKzyeVyZlMoFMwmFouZjef4T0xMmE0QBGYj+c6B5uZms6nXteRZT1dXl9lI0qZNm8zGc751dnaajWf729razMZ7v2ltbTWbLVu2mE0mkzGblpYWswmHw2bjuW9JUqlUMptIxH658OzLRCJhNh0dHWbjuSd5tz+dTpuN5/gDAIB9H+8gAwAAAAAgBmQAAAAAACQxIAMAAAAAIIkBGQAAAAAASQzIAAAAAABIYkAGAAAAAEASAzIAAAAAAJIYkAEAAAAAkMSADAAAAACAJCniDVtbW81mZGTEbFpaWsxmaGjItaZ8Pm82S5cuNZtKpWI2xWKxLuuJx+NmI0nJZNJsRkdHzWZqasps0um02WzevNlsmpubzUaSSqWS2Xj2t2dfeo5JQ4P9daLp6Wmz6ezsNBtJKpfLZjM8PGw2nv1dr33kvSZDoZDZeO4BhULBbDzHZGZmxmx6e3vNxvtYPT09ZhMOh80ml8uZTWNjo9l4riNJqlarZuM5T2q1mtlMTk6ajWfbPOfatm3bzEbynZMTExOuxwIA4LUgCIK9vYRdeD438OAdZAAAAAAAxIAMAAAAAIAkBmQAAAAAACQxIAMAAAAAIIkBGQAAAAAASQzIAAAAAABIYkAGAAAAAEASAzIAAAAAAJKkiDccHh42m3nz5tXlcbLZrGtN5XLZbMbGxsymUqmYTTweN5v99tvPbLZs2WI2ktTf3282nl/Q3dBgfw2kVCqZTXt7u9l4FYtFs0kkEmYzMTFRj+Wot7fXbDy/eLxarbqer7W11WxGR0fNJplMmo1nX09PT5uN53h4eZ4vHA6bTU9Pj9l4ru1CoWA2krRgwQKzGRkZMRvPvvRck7FYzGw8x1+S8vm82TQ3N7seqx7P5WkaGxvNZvHixZ4laWhoyGzqtf0AgNcOz+fqeOXhHWQAAAAAAMSADAAAAACAJAZkAAAAAAAkMSADAAAAACCJARkAAAAAAEkMyAAAAAAASGJABgAAAABAEgMyAAAAAACSpIg3DIVCZlOpVMwmmUyaTSqVcq1peHjYbGq1mtk0NNhfJxgZGTGbSMTena2trWYjSZs3bzab+fPnm83MzIzZ5PN5s4nH42bjNTExYTblctlsRkdHzSabzZpNsVg0Gw/PeSRJk5OTZuO5lgqFgtmEw+G6NFNTU2Yj+fZBU1OT2XjOW881mclkzMZzT5J813dbW5vZTE9Pm82CBQvM5rnnnjObnp4es5GkIAjMxnNOeq5tz3nr2dee5/Letzzb5rknAQBeOs9rEPBy4h1kAAAAAADEgAwAAAAAgCQGZAAAAAAAJDEgAwAAAAAgiQEZAAAAAABJDMgAAAAAAEhiQAYAAAAAQBIDMgAAAAAAkqSIN+zs7DSb5uZmsxkYGDCbkZERz5Jca/L88vFt27aZTWNjo9nEYjGz2bhxo9lIUkdHh9mMjo6aTaFQMJu2tjazKZVKZpPP581GklpbW82mWCyajeeYRCL2Ke45Rzz70bNdkjQzM2M2DQ321648+2h8fNxsUqmU2XjObUmKx+NmE41GzcZzbOfNm2c24XDYbMbGxsxG8t3fNm/ebDae88Rz/D37emJiwmwkKZvNmo1nP3nuW7VazWw810hfX5/ZLFiwwGwk3zl51FFHuR4LAPYWz+czAGy8gwwAAAAAgBiQAQAAAACQxIAMAAAAAIAkBmQAAAAAACQxIAMAAAAAIIkBGQAAAAAASQzIAAAAAABIYkAGAAAAAECSFAqcv1X8t7/9rdmUy2WzSafTnqdzyefzZpPNZs1mYmLCbCqVitm0tLSYTbFYNBtJSiaTZuPZfs+6U6mU2cRiMbMZHBw0G0mqVqtm4zmXQqFQXZpMJmM227ZtM5umpiazkaRarWY2U1NTZtPe3m42nvNtdHTUbBYsWGA2khSNRs2mq6vLbDzH33Pr8txvGhp8Xyf03Cc8PNe2Z9s86/EcW8l33Eqlktl4zttwOGw2bW1tZjM2NmY2iUTCbCQpEomYzfT0tNkcffTRrufbkzz3POC1zPlpL4DXMN5BBgAAAABADMgAAAAAAEhiQAYAAAAAQBIDMgAAAAAAkhiQAQAAAACQxIAMAAAAAIAkBmQAAAAAACQxIAMAAAAAIEmKeMN8Pm820WjUbKrVqtl0dHS41lSpVMxmYGDAbDKZjNmEQiGzKZVKZjM5OWk2kjQ+Pm42yWTSbMrlstl41t3QYH8tJZ1Om40kDQ0N1eWxPMc/ErFP8R07dphNa2ur2RQKBbORfPvS00xPT5tNOBw2m/b2drMZGxszG0lqbGw0m+bmZrPx3EvqdY14jr/ku072228/s/Gc/57rtp7n5PDwsNksXrzYbDzH1mPr1q1m4zm2nmMm+V6XPPsbwJ8mCIK9vQQAmMU7yAAAAAAAiAEZAAAAAABJDMgAAAAAAEhiQAYAAAAAQBIDMgAAAAAAkhiQAQAAAACQxIAMAAAAAIAkBmQAAAAAACQxIAMAAAAAIEmKeMOGhvrM0mNjY2YTCoVcj1WpVMwmHo+bzeTkpNlUq1WzicViZuMVidiHJp/Pm01fX5/ZLFu2zGwKhYLZ5HI5s5Gk5uZms/Ec23Q6bTbT09N1eZxwOGw2nnNbkhYtWlSX56vVambjuZZmZmbMxrOPJKlcLpvN9u3bzWbVqlVm09HRYTb/8z//YzaLFy82G8m3n0ZGRszGc257rn/PsfWek57j6zluHk1NTWaTzWbNxvOaND4+7lqT5969Y8cOsznwwANdzwdgLs/9LAiCl2ElAMA7yAAAAAAASGJABgAAAABAEgMyAAAAAACSGJABAAAAAJDEgAwAAAAAgCQGZAAAAAAAJDEgAwAAAAAgiQEZAAAAAABJUsQbJpNJs5mZmTGbaDRqNsVi0bWmcDhsNu3t7WYzMTFhNoOD/19797Ic6ZnVe/ifUuqUOktVJdfBptrgJmhgwIQZE4ZcC3fBTTBiwOUQdAQDHB0dLnfbLrsOKh1KZymVytyjHTuC2BFrAdnuNjzP+Bdfvt/pVa3MQR2WzerqatkMh71L/vDwUDad6/3y5cuyuby8LJvpdFo2nWckScbjcdksLNTf3dze3pbN9fV12XSuY+cZ2d7eLpskOT4+LptPP/20bL7++uuy2d3dLZvO/ei8R0ny6tWrsvnLv/zLsunsAZ3m888/L5vO858kk8mkbAaDQdl0rvfTp0/LpvNMdt6j7po6z0DnOJ3m/v6+bFZWVubSJL19ovO3CwD46fMLMgAAAMSADAAAAEkMyAAAAJDEgAwAAABJDMgAAACQxIAMAAAASQzIAAAAkMSADAAAAEmSYTe8u7srm62trbI5Pj4um6Wlpdaa1tbWyubm5qZsRqNR2Tx//rxsJpNJ2Xz48KFsumt6+fJl2RwdHbU+r3J+fl423fvW6abTadlcXl6WzdnZWdk8PDyUzWeffVY2p6enZZMkg8GgbGazWdk8ffp0LsfpnP9XX31VNkmysbFRNp170tlLrq6uyqbzHC0vL5dN0ru/6+vrZbO7u1s2nX1rdXW1bFZWVsom6e3Lt7e3ZXNwcDCXNXWeo++++65snjx5UjZJsrBQf1fc2ZMBgJ8+vyADAABADMgAAACQxIAMAAAASQzIAAAAkMSADAAAAEkMyAAAAJDEgAwAAABJDMgAAACQJBl2w4uLi7IZj8dls7m5WTbDYW9Z33//fdnc3d2VzcrKStnc3t6Wzf7+ftlMp9OySXrXoHP+GxsbZTOv+7a0tFQ2STIYDMrm+vq6bGazWdk8efKkbG5ububSrK+vl03Su94nJydzOc75+XnZ7O7uls3q6mrZJL337enTp2XTef47z8jOzk7ZnJ6elk2STCaTsrm8vCybtbW1uRzn4OCgbDrvf9J7v+f1vH38+LFslpeXy6bzjCws9L4D7rxvnTUBAD99fkEGAACAGJABAAAgiQEZAAAAkhiQAQAAIIkBGQAAAJIYkAEAACCJARkAAACSGJABAAAgSTLshuPxeC4f+Pjx47J58+ZN61iffvpp2VxcXLSOVRkO60t1f39fNt3reHNzUzaj0ahsJpNJ2cxms7JZWVkpm9PT07JJkp2dnbK5vb0tm7W1tbk0+/v7ZXN0dFQ2d3d3ZZP07ltn3dPptGwWFxfLprPu7rltbm7O5Vid+985t6urq7k0Se8deHh4KJvOe7K6ulo2Hz9+LJvOPpL0zq2zprOzs7Lp7KXz2ttev35dNknys5/9rGx++9vfto4FAPy0+QUZAAAAYkAGAACAJAZkAAAASGJABgAAgCQGZAAAAEhiQAYAAIAkBmQAAABIYkAGAACAJMmwG+7v75fNyclJ2bx+/bpsJpNJa02dYy0tLZXN8vJy2QyH9aXqNHt7e2WTJGtra2VzfX1dNp1r2blGt7e3ZbOw0Pu+5fT0tGxGo1HZbG5ulk3n/Dvr6Rync42S3nU6Pj4um87zdnBwUDb39/dl07230+m0bN6/f182nee/c//H43HZfPrpp2WTJB8+fCibzjv5i1/8omw6z2Tnnrx586Zskt61vLy8LJvOO3B+fl4229vbZdN5/jt7e5IcHh7O5fMAgJ8+vyADAABADMgAAACQxIAMAAAASQzIAAAAkMSADAAAAEkMyAAAAJDEgAwAAABJDMgAAACQJBl2w+l0Wja7u7v/rcX8X1dXV61uYaGe7z9+/DiX42xtbZXNZDIpm8FgUDZJb903Nzdls7e3Vzanp6dl8/jx47K5vb0tmyRZW1srm7Ozs7LZ3t4um+vr67k0T548KZvxeFw2SbK0tFQ2+/v7ZXN/fz+XNS0vL5fN+fl52SS9c1tZWSmb0WhUNp19YnV1tWxevXpVNknv3X327FnZfPvtt2Xz8PDQWlOlu5c+evSobDrr7ty3Tz75pGwWFxfLpvNsd/btJDk5OSmbzrME/O509uDZbPYjrAT4n84vyAAAABADMgAAACQxIAMAAEASAzIAAAAkMSADAABAEgMyAAAAJDEgAwAAQBIDMgAAACQxIAMAAECSZNgNl5aWymZtba1sbm5uuh9Zuru7K5tnz56VzeLiYtmMx+Oy2d3dLZujo6OySZKPHz+WzcHBQetYlcFgUDbHx8dlc3t72/q82WxWNsvLy2VzcnJSNhsbG2Wzt7dXNm/fvi2b0WhUNklyfX1dNgsL9XdXKysrZXN/f99aU+XRo0et7v3793M51ldffVU2z58/n8t6VldXyyZJDg8Py6Zzb8/Ozspmc3OzbLa2tspme3u7bJLk9PS0bJ4+fVo2nX2rc406f0s6zfn5edkkycPDQ9l09hKA36fOv+fmqfPvOfgp8gsyAAAAxIAMAAAASQzIAAAAkMSADAAAAEkMyAAAAJDEgAwAAABJDMgAAACQxIAMAAAASZJhN/zHf/zHsvn7v//7sjk+Pi6bp0+ftta0sbFRNt98803ZjEajsrm9vS2b5eXlstnf3y+bJPnuu+9aXWVlZaVsOtfx5uambPb29lprenh4KJvJZFI2a2trZdM5/85/dN85zubmZtkkveft9PS0bC4uLsrm+fPnZdO5/5eXl2WT9M6t8yw9evSobM7Pz+fyWQsLve8Jnzx5Ujad/a2zT3TObTwel033nezorKmzv3Xe2/v7+7LpnP90Oi2bpPc3p7NvAX/4BoPB73sJwB84vyADAABADMgAAACQxIAMAAAASQzIAAAAkMSADAAAAEkMyAAAAJDEgAwAAABJDMgAAACQJBnMZrNZJ/ziiy/K5p//+Z/LZm1trWy+//77zpJycHAwl887Pj4um85/LH9zc1M2XdPptGwuLy/LZjKZlM2TJ0/KpnMdO2tOktPT07JZXFwsm4WF+vud/f39uRzn4uKibFZXV8smSY6OjspmOByWzc7OTtnc3t7O5bNOTk7KJkn29vbm8nmda/ljHifpXYPOPtHR2Us653Z+ft76vM4e8PDwUDad8+88k0tLS2VzfX09l89Kkqurq7Lp7BN/93d/1/q836V5PYPQ0fwn5I/G8//j+kO7/zAvfkEGAACAGJABAAAgiQEZAAAAkhiQAQAAIIkBGQAAAJIYkAEAACCJARkAAACSGJABAAAgSTLshr/4xS/KZmVlpWweHh7K5vHjx6013d/fl03nP41/8eLFXD5rY2OjbA4PD8sm6f3n651ze/PmTdmcnp7OpVldXS2bbtd5lhYW6u93zs/Py6ZzrafTadl01pwka2trZdM5t87nLS0tlc379+/ncpwk+fDhQ9l8/vnnZXN9fV02nb1kOKy3uM796K6pc51Go1HZdO7t3d1d2XSe7e6xbm5uyqZz/pPJpGw613p5eblsFhcXyyZJnjx5Ujad9wT+f7rvIQB/GPyCDAAAADEgAwAAQBIDMgAAACQxIAMAAEASAzIAAAAkMSADAABAEgMyAAAAJDEgAwAAQJJk2A2//vrrstnZ2Smb8XhcNpPJpLOk3N3dlc1sNiub5eXlstnc3Cyb8/Pzsrm6uiqbpH8NKj//+c/L5uPHj2UzGo3KpnM/kt65LS4uls319XXZfPbZZ2XTuW+d5/bi4qJskt657e3tlU3n/Duftbq6WjZbW1tlkyQfPnwom4eHh7LpvG8dnfs2nU5bx+p0nXPrvEtnZ2dls76+XjZra2tlk/Sud2ef6DxLCwv197Kd69h5JjtrTpLBYFA2nesN8L9JZ+/s/Dsc/tD4BRkAAABiQAYAAIAkBmQAAABIYkAGAACAJAZkAAAASGJABgAAgCQGZAAAAEhiQAYAAIAkybAbPjw8lM3CQj1vLy8vl83KykprTR1LS0tls7GxUTaTyaRsLi4uWmvq6Kz77u6ubN68eVM2a2trZXN7e1s28zSdTstmPB6XTee+jUajsuncj3fv3pVN91jX19dlc3Z2Vjarq6tl07nWV1dXZZMkT58+LZvLy8uyOTk5KZvOuXV0j9O5J+vr62VzdHRUNp39tnMdt7e3yyZJ3r9/XzbDYf3n4vXr12XzxRdflE1nT765uSmbzt+k7rHm9bwBvzuz2axsBoPBj7AS4KfML8gAAAAQAzIAAAAkMSADAABAEgMyAAAAJDEgAwAAQBIDMgAAACQxIAMAAEASAzIAAAAkMSADAABAkmTYDRcXF8tmbW2tbG5vb8vm5OSktabRaFQ2g8GgbGazWdnc3d2Vzd7eXtksLS2VTffzOs1nn31WNufn52Xz8ePHsnn58mXZJMnh4WHZTKfTsnnx4kXZdJ6lzrnt7u6WzebmZtkkyWQyKZt3796VzcPDQ9lsb2+XTec9ury8LJsk+eUvf1k2f/7nfz6XNe3v75dNZ93Ly8tl013TyspK2ayurs7lsy4uLsrm6uqqbJLePtlZ98JC/Z3r9fV12XTWvbW1VTbDYe9PXOcZeP/+fetY8B/N698hAPw4/IIMAAAAMSADAABAEgMyAAAAJDEgAwAAQBIDMgAAACQxIAMAAEASAzIAAAAkMSADAABAkmTYDXd2dsrm17/+ddl8+umnZbO3t9dZUq6ursrm7u6ubMbjcdk8ffq0bO7v78vm7du3ZZMks9msbI6Ojsrm5OSkbA4ODsqmc/+XlpbKJkm2t7fLZnFxsWw61/KLL74om4eHh7LpnNtkMimbrp/97Gdl85vf/KZsOs/2dDotm93d3bLpdpeXl2XTuf8fPnwom7W1tbIZDAZlkyTLy8tls7q6WjadPalzTzrPbadJkq2trbJZWKi/T/3kk0/KpnP/O9d6ZWWlbDrXOun9LensyQDAT59fkAEAACAGZAAAAEhiQAYAAIAkBmQAAABIYkAGAACAJAZkAAAASGJABgAAgCQGZAAAAEiSDLvh1dVV2fzbv/1b2fzpn/5p2dze3naWlMFgUDaLi4tlMxzWl+Hs7Ky1pnl8VpKMRqOy2d7eLpvO+R8dHZXN48ePy6bzjCTJ6elp2WxtbZXN0tJS2YzH47JZX18vm9lsVjb39/dlk/Su5ddff102BwcHZdM5/463b9+2utXV1bJ5+fJl2fzmN78pm88//7xsOs/a5eVl2STJo0ePymZhof7OsbNvda7ju3fv5nKcpPcObG5ulk3nvu3u7pZN533rvP/T6bRskmQymZRN530DAH76/IIMAAAAMSADAABAEgMyAAAAJDEgAwAAQBIDMgAAACQxIAMAAEASAzIAAAAkMSADAABAkmTYDSeTSdl89dVXZXN8fFw2d3d3rTWtrKyUzXg8Lpvt7e2yOTk5KZvHjx+Xzc7OTtkkyYcPH8pmaWmpbC4uLspmcXGxbGazWdmcnZ2VTfdYw2H9aD579mwua7q+vi6bjqdPn7a6zudtbm6WzdHRUdl0nu3RaFQ2XQ8PD2Xz6tWrsums6fz8vGwGg0HZdHWud+ed7Dz/6+vrZdN5/jtrTnrP5M3NTdlsbW2VzXQ6LZvOntRpOp+VzPc5AQB+2vyCDAAAADEgAwAAQBIDMgAAACQxIAMAAEASAzIAAAAkMSADAABAEgMyAAAAJDEgAwAAQJJk2A3fv39fNh8+fCibnZ2dsjk8POwsKe/evSubZ8+elc319XXZ3NzclM3l5WXZbGxslE2SfPLJJ3P5vMXFxbJZWKi/J1ldXS2brq2trbKZzWZlc3R0VDadc3vx4kXZ3N7els3x8XHZJL178vjx47LpvJOda31+fl42V1dXZdP18uXLshkMBmXTeUaePHlSNp13u2s6nZZNZ93Ly8tl07lv6+vrZdNd02QyaR2r0rm39/f3ZfPmzZuy6fy9Seb7DADw/8zr7zn8mPyCDAAAADEgAwAAQBIDMgAAACQxIAMAAEASAzIAAAAkMSADAABAEgMyAAAAJDEgAwAAQJJk2A07/9H3ZDIpm4eHh7JZWVlpremP//iP5/J5y8vLZXN/f182a2trZdP9z9BHo1HZ3NzclE3n3K6uruaynvF4XDZJ756cn5+Xzfb29lyavb29svnVr35VNi9evCibJBkO69duaWmpbB4/ftz6vMrPf/7zsvnyyy9bx+q8A519Ymdnp2w6z8jFxUXZ3N7elk2SrK6uls3CQv2dY2d/6+w3neeou990rlPn3I6Pj8umcx0769nf3y+b9+/fl02STKfTsunsW8Afvu6+2Pl3L/A/k1+QAQAAIAZkAAAASGJABgAAgCQGZAAAAEhiQAYAAIAkBmQAAABIYkAGAACAJAZkAAAASGJABgAAgCTJsBuORqOyub6+Lpuzs7OyWVpaaq3p6uqqbMbjcdlcXl6WzZMnT8rm/Py8bFZXV8smST58+FA2L1++LJu7u7uyubm5KZvO/X94eCibbvfFF1+UzcnJSdl07u3CQv090fPnz8um8zwmyenpadn80R/9UdkcHx+XzcHBwVyOs7OzUzZJMp1Oy6Zz/zvH2dzcLJvOO7m/v182SXJ4eFg2s9msbJaXl8tma2urbAaDQdkMh70tvrPuzr788ePHsunsW53jdM6/s28lvfN///5961jwX9F5njvP6Y+ts26Anxq/IAMAAEAMyAAAAJDEgAwAAABJDMgAAACQxIAMAAAASQzIAAAAkMSADAAAAEkMyAAAAJAkGXbDlZWVsvnhhx/K5ptvvimbra2tzpKysFDP95ubm2Xz7Nmzsnn16lXZLC8vz6VJksPDw7LpnP/t7W3ZPH78uGzOzs7KZjablU3Su7+np6dlMxqN5nKczvm/fv26bDrPUZIcHx+XzcXFRdl03snO+Q+H9TZwcHBQNknvOel83mQyKZuTk5Oy2djYKJvBYFA2SbK7u9vqKjs7O2XTed6eP39eNtPptLOkjMfjstnf3y+bzjO5trZWNp37v7S0VDZXV1dlkyRff/112fzZn/1Z61jwH3X/Nla6exUA/z1+QQYAAIAYkAEAACCJARkAAACSGJABAAAgiQEZAAAAkhiQAQAAIIkBGQAAAJIYkAEAACBJMuyGDw8Pc2n+5V/+pWz+5m/+prWmra2tsjk+Pi6bb7/9di6fdXh4WDYHBwdlkyTb29tlc3FxUTbr6+tl8+7du7J5+fJl2XTOP0kmk0nZLC8vl83KykrZdM7/m2++KZv7+/uyOTo6KpskGY1GZTMYDMrm8vKybGazWdns7e3N5ThJMh6Py+b7778vm52dnbLpvCPDYb3Fdd6jJNnc3Cybk5OTsllcXJxLc3p6Wjad809679Ld3V3Z/PDDD2Wzu7tbNp1rvbS0VDbv378vmyR59OhR2XT+vvFf191jAOB3zS/IAAAAEAMyAAAAJDEgAwAAQBIDMgAAACQxIAMAAEASAzIAAAAkMSADAABAEgMyAAAAJEmG3XA2m5XN2traXI6zsNCb209PT8tmMpmUzcbGRtkcHx+XzZMnT8pmc3OzbJLk22+/LZuDg4Oy6ZzbeDwum8513N/fL5skub6+LpvpdFo2l5eXZXNyclI2o9GobDoeHh7mcpwkefbsWdl89913ZdN5l968eVM2L168KJskub29LZtPPvlkLscZDAZl07knS0tLZZMkHz9+LJvhsN5SO+9bZ5/s7CXdvfTi4qJsOu/k6urqXJrOvvXq1auyWV9fL5skef369VzW9L9N5zkFgJ8avyADAABADMgAAACQxIAMAAAASQzIAAAAkMSADAAAAEkMyAAAAJDEgAwAAABJDMgAAACQJBl2w8lkUjanp6dl88MPP3Q/snRzc1M2L1++LJvOuV1cXJTNYDAom7Ozs7JJks3NzbJZWVkpm8PDw7JZWloqm/Pz87IZjUZlkyRv3rwpm7/+678um9vb27I5Ojoqm8617tz/u7u7skmSra2tsvn+++/LZjisX9/pdFo2e3t7ZdO5/0nvGTg5OSmbnZ2dsvnw4UPZPH/+vGy6961zvZeXl8vm/v6+bDrvZGc9nWud9J6Tzv7eWXfnes9ms7LpPLdffvll2STJ7u5u2XTODYD/vM6/nzt/F2Be/IIMAAAAMSADAABAEgMyAAAAJDEgAwAAQBIDMgAAACQxIAMAAEASAzIAAAAkMSADAABAkmTYDS8vL8um8594v3v3rmxWVlZaa9rf3y+b+/v7sjk5OSmb7e3tstnY2Cib4bB3yS8uLsqmc27dz6ssLS2VzdXVVetYf/VXf1U2nXvSOf/OuqfTadlMJpOy6T63nffk9va2dazKaDQqm62trbI5Ojqa2+d9+PChbDrXcnd3dy7HeXh4KJskWViov0/s3LfO/e/sJd11d3Tegc67NB6Py6ZzHTvH6ayn+052nu9f/vKXZfO3f/u3rc8DAP5zBoPBXI7T+XeYX5ABAAAgBmQAAABIYkAGAACAJAZkAAAASGJABgAAgCQGZAAAAEhiQAYAAIAkBmQAAABIYkAGAACAJMmwG66trZXNeDwum9lsVjanp6etNV1dXZXN7e1t2SwvL5fN5uZm2Sws1N83dJokGY1GZfPdd9+VzfPnz8tmf3+/bCaTSdl07m2SvH37di7HevLkSdlcX1+31lTpPCOLi4utY93f35dN5/6fn5+Xzd3d3Y+2niQ5Ojoqm729vbLp7CWHh4etNVUGg0Gr29jYKJuTk5Oy6TxL0+m0bDr3rfNZSW/dNzc3ZfPv//7vZdN5J7/99tu5HKfzjiTJ5eVl2XT2rX/4h39ofR78V3T+Lnb3M+B/JnvAfPgFGQAAAGJABgAAgCQGZAAAAEhiQAYAAIAkBmQAAABIYkAGAACAJAZkAAAASGJABgAAgCTJsBt+//33ZbO8vFw2R0dHZfPrX/+6tabnz5+Xzd3d3VyalZWVsnn79m3ZbG1tlU2SfPPNN2Xz9OnTsvnhhx/KZmGh/p5kY2OjbM7Pz8sm6V3L3d3dsjk+Pi6b2WzWWlNlbW1tbp81Go3KZn19vWyWlpbKZjwez+U4Nzc3ZZMkn332Wdm8evWqbDrP2+PHj8vm6uqqbA4ODsomSR4eHsrmyy+/LJvOXtrZJz9+/Fg2nTUnycnJSdmcnp6WTed6d/abzno6zs7OWl3n3e2c2/82g8Gg1c1rHwaYh+7exf9efkEGAACAGJABAAAgiQEZAAAAkhiQAQAAIIkBGQAAAJIYkAEAACCJARkAAACSGJABAAAgSTLshn/xF39RNsfHx2Wzv79fNisrK601/cmf/EnZnJyclM3t7W3ZrK6uls35+XnZXF1dlU2SrK+vl80PP/xQNi9evCibzrpfv35dNp17myTX19dl86tf/Woun9d5ljr3v/NZv/3tb8smSWazWdmMRqOy6ZzbxcVF2WxtbZXNu3fvyiZJLi8vy+af/umfyuZf//Vf5/JZw2G9xZ2enpZNkjx79qxsOu9SZy+ZTqdlMx6Py2YwGJRN0tuXOuvu6NyTzvvW2ZM61yhJdnd3Wx0Avx/dv2cwD35BBgAAgBiQAQAAIIkBGQAAAJIYkAEAACCJARkAAACSGJABAAAgiQEZAAAAkhiQAQAAIEkymM1ms9/3IgAAAOD3zS/IAAAAEAMyAAAAJDEgAwAAQBIDMgAAACQxIAMAAEASAzIAAAAkMSADAABAEgMyAAAAJDEgAwAAQJLk/wDP2xT7tkyPggAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 1000x1000 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import transforms\n",
    "from transformers import ViTModel, ViTConfig\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Ensure plots display in Jupyter\n",
    "%matplotlib inline\n",
    "\n",
    "# Configuration\n",
    "data_dir = r\"F:/Education/NSU/CSE/CSE499/Implementation/Image Data\"\n",
    "gray_image_dir = os.path.join(data_dir, \"gray_image\")\n",
    "even_dir = os.path.join(gray_image_dir, \"even_images\")\n",
    "odd_dir = os.path.join(gray_image_dir, \"odd_images\")\n",
    "matrix_dir = os.path.join(data_dir, \"matrix\")\n",
    "image_size = (50, 50)\n",
    "batch_size = 8\n",
    "num_epochs = 20  # Increased for better convergence\n",
    "learning_rate = 5e-5  # Slightly higher for faster learning\n",
    "train_split = 0.8\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model_save_path = \"best_vit_model.pth\"  # Path to save best model\n",
    "\n",
    "# Data augmentation\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "])\n",
    "\n",
    "# Custom Dataset\n",
    "class ImagePairDataset(Dataset):\n",
    "    def __init__(self, even_dir, odd_dir, matrix_dir, target_size=(50, 50), transform=None):\n",
    "        self.even_dir = even_dir\n",
    "        self.odd_dir = odd_dir\n",
    "        self.matrix_dir = matrix_dir\n",
    "        self.target_size = target_size\n",
    "        self.transform = transform\n",
    "        \n",
    "        # Get list of image IDs\n",
    "        even_files = [f for f in os.listdir(even_dir) if f.endswith(\"_even.png\")]\n",
    "        self.ids = []\n",
    "        for f in even_files:\n",
    "            match = re.match(r\"(\\d+)_even\\.png\", f)\n",
    "            if match:\n",
    "                id_str = match.group(1)\n",
    "                odd_file = f\"{id_str}_odd.png\"\n",
    "                matrix_file = f\"{id_str}.png\"\n",
    "                if (os.path.exists(os.path.join(odd_dir, odd_file)) and \n",
    "                    os.path.exists(os.path.join(matrix_dir, matrix_file))):\n",
    "                    self.ids.append(id_str)\n",
    "        \n",
    "        print(f\"Found {len(self.ids)} matching image pairs\")\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        id_str = self.ids[idx]\n",
    "        \n",
    "        # Load images\n",
    "        even_img = Image.open(os.path.join(self.even_dir, f\"{id_str}_even.png\")).convert(\"L\")\n",
    "        odd_img = Image.open(os.path.join(self.odd_dir, f\"{id_str}_odd.png\")).convert(\"L\")\n",
    "        matrix_img = Image.open(os.path.join(self.matrix_dir, f\"{id_str}.png\")).convert(\"L\")\n",
    "        \n",
    "        # Resize images\n",
    "        even_img = even_img.resize(self.target_size, Image.Resampling.LANCZOS)\n",
    "        odd_img = odd_img.resize(self.target_size, Image.Resampling.LANCZOS)\n",
    "        matrix_img = matrix_img.resize(self.target_size, Image.Resampling.LANCZOS)\n",
    "        \n",
    "        # Verify image sizes\n",
    "        if even_img.size != self.target_size or odd_img.size != self.target_size or matrix_img.size != self.target_size:\n",
    "            raise ValueError(f\"Image size mismatch for ID {id_str}: expected {self.target_size}\")\n",
    "        \n",
    "        # Convert to numpy and normalize\n",
    "        even_img = np.array(even_img, dtype=np.float32) / 255.0\n",
    "        odd_img = np.array(odd_img, dtype=np.float32) / 255.0\n",
    "        matrix_img = np.array(matrix_img, dtype=np.float32) / 255.0\n",
    "        \n",
    "        # Apply augmentation\n",
    "        if self.transform:\n",
    "            # Stack images for consistent augmentation\n",
    "            stacked_img = np.stack([even_img, odd_img, matrix_img], axis=2)\n",
    "            stacked_img = Image.fromarray((stacked_img * 255).astype(np.uint8))\n",
    "            stacked_img = self.transform(stacked_img)\n",
    "            stacked_img = np.array(stacked_img, dtype=np.float32) / 255.0\n",
    "            even_img, odd_img, matrix_img = stacked_img[:, :, 0], stacked_img[:, :, 1], stacked_img[:, :, 2]\n",
    "        \n",
    "        # Combine into 2-channel input\n",
    "        input_img = np.stack([even_img, odd_img], axis=2)  # Shape: (50, 50, 2)\n",
    "        \n",
    "        # Convert to torch tensors\n",
    "        input_img = torch.tensor(input_img).permute(2, 0, 1)  # Shape: (2, 50, 50)\n",
    "        matrix_img = torch.tensor(matrix_img).flatten()  # Shape: (2500,)\n",
    "        \n",
    "        return input_img, matrix_img\n",
    "\n",
    "# Custom ViT Model for Regression\n",
    "class ViTForImageRegression(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ViTForImageRegression, self).__init__()\n",
    "        config = ViTConfig(\n",
    "            image_size=50,\n",
    "            patch_size=10,\n",
    "            num_channels=2,\n",
    "            hidden_size=768,\n",
    "            num_hidden_layers=12,\n",
    "            num_attention_heads=12,\n",
    "            intermediate_size=3072\n",
    "        )\n",
    "        self.vit = ViTModel(config)\n",
    "        self.regressor = nn.Linear(config.hidden_size, 50 * 50)\n",
    "        \n",
    "        # Optional: Convolutional decoder (uncomment to use)\n",
    "        # self.decoder = nn.Sequential(\n",
    "        #     nn.Linear(config.hidden_size, 256 * 5 * 5),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.Unflatten(1, (256, 5, 5)),\n",
    "        #     nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),\n",
    "        #     nn.ReLU(),\n",
    "        #     nn.ConvTranspose2d(64, 1, kernel_size=4, stride=2, padding=1),\n",
    "        #     nn.Sigmoid()  # Output 50x50x1\n",
    "        # )\n",
    "    \n",
    "    def forward(self, pixel_values):\n",
    "        outputs = self.vit(pixel_values=pixel_values)\n",
    "        pooled_output = outputs.pooler_output\n",
    "        return self.regressor(pooled_output)\n",
    "        # For convolutional decoder:\n",
    "        # return self.decoder(pooled_output).squeeze(1).flatten(1)\n",
    "\n",
    "# Compute SSIM for a batch\n",
    "def compute_batch_ssim(preds, targets, size=(50, 50)):\n",
    "    ssim_scores = []\n",
    "    preds = preds.cpu().numpy().reshape(-1, *size)\n",
    "    targets = targets.cpu().numpy().reshape(-1, *size)\n",
    "    for i in range(len(preds)):\n",
    "        score = ssim(preds[i], targets[i], data_range=1.0)\n",
    "        ssim_scores.append(score)\n",
    "    return np.mean(ssim_scores)\n",
    "\n",
    "# Training and Evaluation\n",
    "def train_model():\n",
    "    # Initialize dataset and split\n",
    "    dataset = ImagePairDataset(even_dir, odd_dir, matrix_dir, target_size=image_size, transform=transform)\n",
    "    train_size = int(train_split * len(dataset))\n",
    "    test_size = len(dataset) - train_size\n",
    "    train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    # Initialize model\n",
    "    model = ViTForImageRegression().to(device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    best_test_ssim = 0.0\n",
    "    best_epoch = 0\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_mse = 0.0\n",
    "        train_mae = 0.0\n",
    "        train_ssim = 0.0\n",
    "        num_batches = 0\n",
    "        \n",
    "        for inputs, targets in tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{num_epochs}\"):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            \n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Compute metrics\n",
    "            train_loss += loss.item()\n",
    "            preds = outputs.detach()\n",
    "            train_mse += mean_squared_error(targets.cpu().numpy(), preds.cpu().numpy())\n",
    "            train_mae += mean_absolute_error(targets.cpu().numpy(), preds.cpu().numpy())\n",
    "            train_ssim += compute_batch_ssim(preds, targets)\n",
    "            num_batches += 1\n",
    "        \n",
    "        # Average train metrics\n",
    "        train_loss /= num_batches\n",
    "        train_mse /= num_batches\n",
    "        train_mae /= num_batches\n",
    "        train_ssim /= num_batches\n",
    "        \n",
    "        # Evaluate on test set\n",
    "        model.eval()\n",
    "        test_loss = 0.0\n",
    "        test_mse = 0.0\n",
    "        test_mae = 0.0\n",
    "        test_ssim = 0.0\n",
    "        num_test_batches = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in test_loader:\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                \n",
    "                test_loss += loss.item()\n",
    "                test_mse += mean_squared_error(targets.cpu().numpy(), outputs.cpu().numpy())\n",
    "                test_mae += mean_absolute_error(targets.cpu().numpy(), outputs.cpu().numpy())\n",
    "                test_ssim += compute_batch_ssim(outputs, targets)\n",
    "                num_test_batches += 1\n",
    "        \n",
    "        test_loss /= num_test_batches\n",
    "        test_mse /= num_test_batches\n",
    "        test_mae /= num_test_batches\n",
    "        test_ssim /= num_test_batches\n",
    "        \n",
    "        print(f\"Epoch {epoch + 1}:\")\n",
    "        print(f\"  Train Loss: {train_loss:.6f}, MSE: {train_mse:.6f}, MAE: {train_mae:.6f}, SSIM: {train_ssim:.4f}\")\n",
    "        print(f\"  Test Loss: {test_loss:.6f}, MSE: {test_mse:.6f}, MAE: {test_mae:.6f}, SSIM: {test_ssim:.4f}\")\n",
    "        \n",
    "        # Save best model based on test SSIM\n",
    "        if test_ssim > best_test_ssim:\n",
    "            best_test_ssim = test_ssim\n",
    "            best_epoch = epoch + 1\n",
    "            torch.save(model.state_dict(), model_save_path)\n",
    "            print(f\"Saved best model at epoch {best_epoch} with Test SSIM: {best_test_ssim:.4f}\")\n",
    "        \n",
    "        # Visualize sample predictions\n",
    "        if epoch == num_epochs - 1:\n",
    "            with torch.no_grad():\n",
    "                sample_inputs, sample_targets = next(iter(test_loader))\n",
    "                sample_inputs = sample_inputs[:2].to(device)\n",
    "                sample_preds = model(sample_inputs)\n",
    "                \n",
    "                fig, axes = plt.subplots(2, 2, figsize=(10, 10))\n",
    "                for i in range(2):\n",
    "                    pred_img = sample_preds[i].cpu().numpy().reshape(image_size)\n",
    "                    target_img = sample_targets[i].cpu().numpy().reshape(image_size)\n",
    "                    \n",
    "                    axes[i, 0].imshow(pred_img, cmap='gray')\n",
    "                    axes[i, 0].set_title(f\"Predicted Image {i + 1}\")\n",
    "                    axes[i, 0].axis('off')\n",
    "                    \n",
    "                    axes[i, 1].imshow(target_img, cmap='gray')\n",
    "                    axes[i, 1].set_title(f\"Target Image {i + 1}\")\n",
    "                    axes[i, 1].axis('off')\n",
    "                \n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e2614f4-0feb-4a80-afc5-e7094be000d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchvision\n",
      "  Downloading torchvision-0.22.1-cp312-cp312-win_amd64.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\user\\anaconda3\\lib\\site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: torch==2.7.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from torchvision) (2.7.1)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from torchvision) (10.4.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\user\\anaconda3\\lib\\site-packages (from torch==2.7.1->torchvision) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from torch==2.7.1->torchvision) (4.11.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\user\\anaconda3\\lib\\site-packages (from torch==2.7.1->torchvision) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\user\\anaconda3\\lib\\site-packages (from torch==2.7.1->torchvision) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from torch==2.7.1->torchvision) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\user\\anaconda3\\lib\\site-packages (from torch==2.7.1->torchvision) (2024.6.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\user\\anaconda3\\lib\\site-packages (from torch==2.7.1->torchvision) (75.1.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from sympy>=1.13.3->torch==2.7.1->torchvision) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from jinja2->torch==2.7.1->torchvision) (2.1.3)\n",
      "Downloading torchvision-0.22.1-cp312-cp312-win_amd64.whl (1.7 MB)\n",
      "   ---------------------------------------- 0.0/1.7 MB ? eta -:--:--\n",
      "   ------ --------------------------------- 0.3/1.7 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.7/1.7 MB 7.1 MB/s eta 0:00:00\n",
      "Installing collected packages: torchvision\n",
      "Successfully installed torchvision-0.22.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b01faf82-c2ce-4f37-80db-02a760362dc0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
